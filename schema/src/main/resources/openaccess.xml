<?xml version="1.0"?>

<?xml-stylesheet type="text/xsl" href="/resources/spdi-app.xsl"?>
<response>
	<query>mapreduce </query>
	<apiKey>jqs3v62pb9q84z87b98mem59</apiKey>
	<result>
		<total>6</total>
		<start>1</start>
		<pageLength>10</pageLength>
	</result>
	<records>
		<result>
			<Publisher xml:lang="en">
				<PublisherInfo>
					<PublisherName>BMC</PublisherName>
					<PublisherLocation/>
				</PublisherInfo>
				<Journal>
					<JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="Unnumbered">
						<JournalID>1471-2105</JournalID>
						<JournalPrintISSN>1471-2105</JournalPrintISSN>
						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>
						<JournalTitle>BMC Bioinformatics</JournalTitle>
						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>
						<JournalSubjectGroup>
							<JournalSubject Type="Primary">Life Sciences</JournalSubject>
							<JournalSubject Priority="1" Type="Secondary">Bioinformatics</JournalSubject>
							<JournalSubject Priority="2" Type="Secondary">Microarrays</JournalSubject>
							<JournalSubject Priority="3" Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>
							<JournalSubject Priority="4" Type="Secondary">Computer Appl. in Life Sciences</JournalSubject>
							<JournalSubject Priority="5" Type="Secondary">Combinatorial Libraries</JournalSubject>
							<JournalSubject Priority="6" Type="Secondary">Algorithms</JournalSubject>
						</JournalSubjectGroup>
					</JournalInfo>
					<Volume>
						<VolumeInfo TocLevels="0" VolumeType="Regular">
							<VolumeIDStart>11</VolumeIDStart>
							<VolumeIDEnd>11</VolumeIDEnd>
							<VolumeIssueCount/>
						</VolumeInfo>
						<Issue IssueType="Regular">
							<IssueInfo TocLevels="0">
								<IssueIDStart>Suppl 1</IssueIDStart>
								<IssueIDEnd>Suppl 1</IssueIDEnd>
								<IssueArticleCount/>
								<IssueHistory>
									<PrintDate>
										<Year>2010</Year>
										<Month>1</Month>
										<Day>18</Day>
									</PrintDate>
								</IssueHistory>
								<IssueCopyright>
									<CopyrightHolderName>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolderName>
									<CopyrightYear>2010</CopyrightYear>
								</IssueCopyright>
							</IssueInfo>
							<Article ID="Art1">
								<ArticleInfo ArticleType="Abstract" ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">
									<ArticleID>1471-2105-11-S1-S15</ArticleID>
									<ArticleDOI>10.1186/1471-2105-11-S1-S15</ArticleDOI>
									<ArticleSequenceNumber/>
									<ArticleTitle Language="En">MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
									<ArticleSubTitle Language="En"/>
									<ArticleCategory>Research</ArticleCategory>
									<ArticleFirstPage>S15</ArticleFirstPage>
									<ArticleLastPage>S15</ArticleLastPage>
									<ArticleHistory>
										<Received>
											<Year/>
											<Month/>
											<Day/>
										</Received>
										<Revised>
											<Year/>
											<Month/>
											<Day/>
										</Revised>
										<Accepted>
											<Year/>
											<Month/>
											<Day/>
										</Accepted>
									</ArticleHistory>
									<ArticleEditorialResponsibility/>
									<ArticleCopyright>
										<CopyrightHolderName>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolderName>
										<CopyrightYear>2010</CopyrightYear>
									</ArticleCopyright>
									<ArticleGrants Type="OpenChoice">
										<MetadataGrant Grant="OpenAccess"/>
										<AbstractGrant Grant="OpenAccess"/>
										<BodyPDFGrant Grant="Restricted"/>
										<BodyHTMLGrant Grant="Restricted"/>
										<BibliographyGrant Grant="Restricted"/>
										<ESMGrant Grant="Restricted"/>
									</ArticleGrants>
									<ArticleContext>
										<JournalID/>
										<VolumeIDStart>11</VolumeIDStart>
										<VolumeIDEnd>11</VolumeIDEnd>
										<IssueIDStart>Suppl 1</IssueIDStart>
										<IssueIDEnd>Suppl 1</IssueIDEnd>
									</ArticleContext>
								</ArticleInfo>
								<ArticleHeader>
									<AuthorGroup>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Suzanne</GivenName>
												<FamilyName>Matthews</FamilyName>
											</AuthorName>
											<Contact>
												<Email>sjm@cse.tamu.edu</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Tiffani</GivenName>
												<FamilyName>Williams</FamilyName>
											</AuthorName>
											<Contact>
												<Email>tlw@cse.tamu.edu</Email>
											</Contact>
										</Author>
										<Affiliation ID="I1">
											<OrgName>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</OrgName>
											<OrgAddress>
												<Postcode/>
												<City/>
												<State/>
											</OrgAddress>
										</Affiliation>
									</AuthorGroup>
									<Abstract ID="Abs1" Language="En">
										<Heading>Abstract</Heading>
										<AbstractSection>
											<Heading>Background</Heading>
											<Para>MapReduce is a parallel framework that has been used effectively to design large-scale parallel applications for large computing clusters. In this paper, we evaluate the viability of the MapReduce framework for designing phylogenetic applications. The problem of interest is generating the all-to-all Robinson-Foulds distance matrix, which has many applications for visualizing and clustering large collections of evolutionary trees. We introduce MrsRF ( <Emphasis Type="Italic">MapReduce Speeds up RF</Emphasis> ), a multi-core algorithm to generate a <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> Robinson-Foulds distance matrix between <Emphasis Type="Italic">t</Emphasis> trees using the MapReduce paradigm.</Para>
										</AbstractSection>
										<AbstractSection>
											<Heading>Results</Heading>
											<Para>We studied the performance of our MrsRF algorithm on two large biological trees sets consisting of 20,000 trees of 150 taxa each and 33,306 trees of 567 taxa each. Our experiments show that MrsRF is a scalable approach reaching a speedup of over 18 on 32 total cores. Our results also show that achieving top speedup on a multi-core cluster requires different cluster configurations. Finally, we show how to use an RF matrix to summarize collections of phylogenetic trees visually.</Para>
										</AbstractSection>
										<AbstractSection>
											<Heading>Conclusion</Heading>
											<Para>Our results show that MapReduce is a promising paradigm for developing multi-core phylogenetic applications. The results also demonstrate that different multi-core configurations must be tested in order to obtain optimum performance. We conclude that RF matrices play a critical role in developing techniques to summarize large collections of trees.</Para>
										</AbstractSection>
									</Abstract>
									<KeywordGroup Language="En">
										<Heading>Keywords</Heading>
										<Keyword>MapReduce</Keyword>
										<Keyword>analyzing</Keyword>
										<Keyword>MrsRF</Keyword>
										<Keyword>evolutionary</Keyword>
										<Keyword>trees</Keyword>
										<Keyword>collections</Keyword>
										<Keyword>large</Keyword>
										<Keyword>efficient</Keyword>
										<Keyword>algorithm</Keyword>
									</KeywordGroup>
								</ArticleHeader>
								<Body>
									<Section1 ID="Sec_61803">
										<Heading>Background</Heading>
										<Para>MapReduce <CitationRef CitationID="B1">1</CitationRef> is an exciting new paradigm for designing parallel applications. It was popularized by Google to support the parallel and distributed execution of data intensive applications. To process petabytes of data, Google executes thousands of MapReduce applications per day. There is interest within the bioinformatics community to harness the power of MapReduce to develop parallel applications to process large datasets of genomic data. For example, CloudBurst <CitationRef CitationID="B2">2</CitationRef> , a MapReduce application for sequence analysis, has recently been released. In this paper, we study whether MapReduce can be used to develop efficient parallel phylogenetic applications for multi-core platforms.</Para>
										<Para>We develop a new algorithm called <Emphasis Type="Italic">MrsRF (MapReduce Speeds up RF)</Emphasis> for computing the all-pairs Robinson-Foulds distance between <Emphasis Type="Italic">t</Emphasis> evolutionary trees on multi-core computing platforms. The RF distance is a popular measure for computing the differences in evolutionary relationships between <Emphasis Type="Italic">t</Emphasis> phylogenetic trees of interest. There are several applications for using RF matrices such as visualizing collections of trees <CitationRef CitationID="B3">3</CitationRef>
											<CitationRef CitationID="B4">4</CitationRef> and clustering tree collections <CitationRef CitationID="B5">5</CitationRef> .</Para>
										<Para>The <Emphasis Type="Italic">novelty</Emphasis> of our work centers around using MapReduce in a non-standard way. Typical uses of the MapReduce framework reduce the final output into a smaller representation than the initial input. One of the interesting aspects of the all-pairs RF distance problem is that the output size (a <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix) is much larger than the input size ( <Emphasis Type="Italic">t</Emphasis> phylogenetic trees). Under the all-pairs RF problem, we are significantly expanding the data. For <Emphasis Type="Italic">k</Emphasis> total cores, how they are partitioned among the <Emphasis Type="Italic">N</Emphasis> nodes (or physical machines), where each node consists of <Emphasis Type="Italic">c</Emphasis> computing cores, has a significant impact on performance since cores on the same node share resources such as memory bandwidth. For example, with 32 total cores, a 16 nodes by 2 cores (16 × 2) cluster configuration outperforms 8 × 4, 4 × 8, and 32 × 1 configurations in our experiments. Hence, multiple configurations should be tested in order to attain optimum performance on a multi-core platform.</Para>
										<Para>We ran our experiments on 20,000 and 33,306 biological tree collections consisting of 150 and 567 taxa, respectively. MrsRF was implemented using Phoenix <CitationRef CitationID="B6">6</CitationRef> , a MapReduce implementation for shared memory multi-core platforms, and OpenMPI <CitationRef CitationID="B7">7</CitationRef> . Our results show that MrsRF is a promising methodology for parallelizing the all-pairs RF distance problem. In our experiments, MrsRF shows good overall speedup. On 8 cores, MrsRF is over 6 times faster than the best-performing sequential algorithm, which is also MrsRF run on a single core. For 32 cores, it is 18 times faster than the serial version of MrsRF. Speedup resulted from allowing the underlying MapReduce runtime system to schedule communication on the multi-core system, which greatly simplifies MrsRF's implementation.</Para>
										<Para>A common trend in phylogenetics is encapsulating the result into a single consensus tree, where the assumption is the information discarded is less important than the information retained. However, many of the trees may contain elements of the "true" evolutionary tree and their relationships should not be ignored. Hence, we show how to use RF matrices to improve the summarization of a phylogenetic analysis. Overall, our results provide evidence that large computations involving phylogenetic trees can take advantage of the MapReduce framework to design high-performance phylogenetic applications.</Para>
									</Section1>
									<Section1 ID="Sec_09504">
										<Heading>Methods</Heading>
										<Section2 ID="Sec_15769">
											<Heading>MapReduce</Heading>
											<Para>MapReduce <CitationRef CitationID="B1">1</CitationRef> is a popular parallel model that automates parallel computation largely in the background, making it easier to develop a parallel program. Popularized by Google in 2004, it has since been used for a variety of diverse applications such as distributed sort and grep, Google web indexing, and data processing by large companies such as Amazon, Yahoo! and Facebook. The central features of the MapReduce framework are two functions:map()andreduce(). Themap()function produces a set of intermediate key/value pairs. Thereduce()function accepts an intermediate key and a set of values and merges them together. Both themap()andreduce()functions are written serially by the programmer. The underlying MapReduce framework takes care of scheduling these functions on the multi-core system.</Para>
											<Para>Figure <InternalRef RefID="F1">1</InternalRef> gives an overview of how the MapReduce paradigm operates in order to count the number of words in a file. Each instance of the map function (or <Emphasis Type="Italic">mapper</Emphasis> ) receives one line of input. Each mapper takes its line of input and splits it into words. The mapper then outputs a (key, value) pair of the word and the value 1. Since all the lines are independent from each other, all mappers run in parallel.</Para>
											<Figure Category="Standard" Float="No" ID="F1">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Word count example using the MapReduce paradigm</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-1" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Word count example using the MapReduce paradigm</Emphasis> .</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>As each mapper outputs (key,value) pairs, these pairs are merged to form keys with associated lists of values. In the reduce phase, each instance of the reduce function (or <Emphasis Type="Italic">reducer</Emphasis> ) takes as input a key and associated list of values. In Figure <InternalRef RefID="F1">1</InternalRef> , the fifth reducer takes in as input the keyfishand the associated list1-1-1-1, as "fish" occurred four times in the input file. Each reducer takes its list of values, sums all of its member's values, and outputs this sum of values with the key.</Para>
											<Section3 ID="Sec_23686">
												<Heading>Phoenix: a MapReduce library</Heading>
												<Para>The underlying MapReduce framework of MrsRF is based on Phoenix <CitationRef CitationID="B6">6</CitationRef> , a multi-core MapReduce approach. Phoenix is a threads-based implementation of MapReduce designed specifically for multi-core systems where all computing cores have access to shared memory. It dynamically schedules map and reduce tasks to available compute cores. Hadoop <CitationRef CitationID="B8">8</CitationRef> is the most popular framework for developing MapReduce applications. We developed MrsRF implementations in both Phoenix and Hadoop. For our RF matrix application, we were able to achieve significantly better performance using Phoenix. Consider a <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> multi-core cluster configuration, where <Emphasis Type="Italic">N</Emphasis> represents the number of physical nodes (or machines) of the cluster and <Emphasis Type="Italic">c</Emphasis> is the number of computing cores per node. Each of the cores on a node share access to memory. Phoenix works on a 1 × <Emphasis Type="Italic">c</Emphasis> configuration. We augment Phoenix with OpenMPI in order to use distributed-memory clusters, where <Emphasis Type="Italic">N</Emphasis> ≥ 1. Our MrsRF implementation is available publicly from the web <CitationRef CitationID="B9">9</CitationRef> .</Para>
											</Section3>
										</Section2>
										<Section2 ID="Sec_65827">
											<Heading>Robinson-Foulds distance</Heading>
											<Para>The Robinson-Foulds (RF) distance <CitationRef CitationID="B10">10</CitationRef> is one of the most common methods used to compare the topological differences between two trees. For tree <Emphasis Type="Italic">T</Emphasis> on <Emphasis Type="Italic">n</Emphasis> taxa, removing an edge (or bipartition) splits tree <Emphasis Type="Italic">T</Emphasis> into two independent sets, <Emphasis Type="Italic">S</Emphasis>
												<Subscript>1</Subscript> and <Emphasis Type="Italic">S</Emphasis>
												<Subscript>2</Subscript> . Each of the <Emphasis Type="Italic">n</Emphasis> taxa belong to either <Emphasis Type="Italic">S</Emphasis>
												<Subscript>1</Subscript> or <Emphasis Type="Italic">S</Emphasis>
												<Subscript>2</Subscript> . Consider bipartition <Emphasis Type="Italic">B</Emphasis> in tree <Emphasis Type="Italic">T</Emphasis> . We can represent this bipartition as <Emphasis Type="Italic">S</Emphasis>
												<Subscript>1</Subscript> | <Emphasis Type="Italic">S</Emphasis>
												<Subscript>2</Subscript> , where <Emphasis Type="Italic">S</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">i</Emphasis>
												</Subscript> contains the names of the taxa in that set. The RF distance computes the topological distance between two trees by comparing their set of bipartitions. Letdefine the set of bipartitions found in tree <Emphasis Type="Italic">T</Emphasis> . The RF distance between trees <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">i</Emphasis>
												</Subscript> and <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">j</Emphasis>
												</Subscript> is:</Para>
											<Para>In this paper, we develop a multi-core algorithm to compute the <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix for a collection of <Emphasis Type="Italic">t</Emphasis> trees. Entry ( <Emphasis Type="Italic">i</Emphasis> , <Emphasis Type="Italic">j</Emphasis> ) in the RF matrix represents the RF distance between trees <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">i</Emphasis>
												</Subscript> and <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">j</Emphasis>
												</Subscript> . Finally, our results shows the RF rates instead of RF distances. The <Emphasis Type="Italic">RF rate</Emphasis> is obtained by normalizing the RF distance by the number of internal edges and multiplying by 100. For <Emphasis Type="Italic">n</Emphasis> taxa, there are <Emphasis Type="Italic">n</Emphasis> - 3 internal edges in a binary tree. Hence the maximum RF distance between two trees is <Emphasis Type="Italic">n</Emphasis> - 3, which results in an RF rate of 100%. Thus, the RF rate varies between 0% and 100% signifying that the two trees <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">i</Emphasis>
												</Subscript> and <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">j</Emphasis>
												</Subscript> are identical and maximally different, respectively.</Para>
											<Section3 ID="Sec_99442">
												<Heading>HashRF</Heading>
												<Para>Our MrsRF algorithm for multi-core platforms is based on the HashRF algorithm <CitationRef CitationID="B11">11</CitationRef>
													<CitationRef CitationID="B12">12</CitationRef> , a fast, sequential algorithm for computing an all-to-all RF matrix to compare <Emphasis Type="Italic">t</Emphasis> trees on <Emphasis Type="Italic">n</Emphasis> taxa. For a bipartition <Emphasis Type="Italic">B</Emphasis> , HashRF uses a global hash table <Emphasis Type="Italic">H</Emphasis> to store that bipartition along with the identities of the trees (TIDs) that contain that bipartition. HashRF uses two uniform hash functions <Emphasis Type="Italic">h</Emphasis>
													<Subscript>1</Subscript> and <Emphasis Type="Italic">h</Emphasis>
													<Subscript>2</Subscript> , where the <Emphasis Type="Italic">h</Emphasis>
													<Subscript>1</Subscript> value represents the hash table location for storing the bipartition <Emphasis Type="Italic">B</Emphasis> and <Emphasis Type="Italic">h</Emphasis>
													<Subscript>2</Subscript> provides a shortened bipartition identity (BID) for this bipartition. Moreover, for each ( <Emphasis Type="Italic">h</Emphasis>
													<Subscript>1</Subscript> ( <Emphasis Type="Italic">B</Emphasis> ), <Emphasis Type="Italic">h</Emphasis>
													<Subscript>2</Subscript> ( <Emphasis Type="Italic">B</Emphasis> )) pair, a list of trees (TIDs) containing bipartition <Emphasis Type="Italic">B</Emphasis> is also stored in the hash table <Emphasis Type="Italic">H</Emphasis> . To compute the RF matrix <Emphasis Type="Italic">M</Emphasis> , each index ( <Emphasis Type="Italic">h</Emphasis>
													<Subscript>1</Subscript> value) of the hash table <Emphasis Type="Italic">H</Emphasis> is visited. At <Emphasis Type="Italic">H</Emphasis> [ <Emphasis Type="Italic">h</Emphasis>
													<Subscript>1</Subscript> ], each <Emphasis Type="Italic">h</Emphasis>
													<Subscript>2</Subscript> value (representing a unique bipartition <Emphasis Type="Italic">B</Emphasis> ) is visited and its list of tree identities (TIDs) are extracted. For each pair of trees <Emphasis Type="Italic">T</Emphasis>
													<Subscript>
														<Emphasis Type="Italic">i</Emphasis>
													</Subscript> and <Emphasis Type="Italic">T</Emphasis>
													<Subscript>
														<Emphasis Type="Italic">j</Emphasis>
													</Subscript> in the list of TIDs, entry <Emphasis Type="Italic">M</Emphasis> [ <Emphasis Type="Italic">T</Emphasis>
													<Subscript>
														<Emphasis Type="Italic">i</Emphasis>
													</Subscript> , <Emphasis Type="Italic">T</Emphasis>
													<Subscript>
														<Emphasis Type="Italic">j</Emphasis>
													</Subscript> ] is incremented by one to compute a similarity matrix. Once the hash table has been traversed, entry <Emphasis Type="Italic">M</Emphasis> [ <Emphasis Type="Italic">i</Emphasis> , <Emphasis Type="Italic">j</Emphasis> ] is subtracted from <Emphasis Type="Italic">n</Emphasis> - 3, the maximum RF distance, to produce the RF matrix. The worst-case running time of HashRF is <Emphasis Type="Italic">O</Emphasis> ( <Emphasis Type="Italic">nt</Emphasis>
													<Superscript>2</Superscript> ).</Para>
											</Section3>
										</Section2>
										<Section2 ID="Sec_92013">
											<Heading>MrsRF: Computing a t × t RF matrix</Heading>
											<Para>We introduce <Emphasis Type="Italic">MrsRF (MapReduce Speeds up RF)</Emphasis> , a multi-core all-to-all RF distance matrix algorithm using the MapReduce framework. The design of MrsRF is motivated by the HashRF algorithm. Moreover, in MrsRF, bipartitions are analogous to words in the MapReduce word count example. MrsRF takes as input a tree file containing <Emphasis Type="Italic">t</Emphasis> trees and a <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> cluster configuration. The number of cluster nodes specifies the number of physical machines that executes the code. The number of cores is the number of CPUs within each node. For serial execution, <Emphasis Type="Italic">N</Emphasis> = 1 and <Emphasis Type="Italic">c</Emphasis> = 1. If instead one wanted to run MrsRF on 2 machines each containing 4 CPUs, the respective <Emphasis Type="Italic">N</Emphasis> and <Emphasis Type="Italic">c</Emphasis> values would be 2 and 4.</Para>
											<Para>There are two main steps to our MrsRF algorithm. First, we organize the <Emphasis Type="Italic">N</Emphasis> nodes into a grid in order to partition the <Emphasis Type="Italic">t</Emphasis> input trees among the nodes. Phoenix, the underlying MapReduce library, automatically distributes the input for a single node amongst its <Emphasis Type="Italic">c</Emphasis> cores. That is, it works for 1 × <Emphasis Type="Italic">c</Emphasis> cluster configurations. As a result, we manually partition the input among the <Emphasis Type="Italic">N</Emphasis> nodes. If <Emphasis Type="Italic">N</Emphasis> is a perfect square, then we assume the nodes are organized into agrid. If <Emphasis Type="Italic">N</Emphasis> is not a perfect square, let <Emphasis Type="Italic">i</Emphasis> = ⌊⌋. If <Emphasis Type="Italic">N</Emphasis> mod <Emphasis Type="Italic">i</Emphasis> = 0, then we assume a <Emphasis Type="Italic">N</Emphasis> / <Emphasis Type="Italic">i</Emphasis> × <Emphasis Type="Italic">i</Emphasis> grid of nodes. Otherwise, we decrement <Emphasis Type="Italic">i</Emphasis> until it divides <Emphasis Type="Italic">N</Emphasis> evenly. For <Emphasis Type="Italic">N</Emphasis> = 4, the <Emphasis Type="Italic">N</Emphasis> nodes are partitioned into a 2 × 2 grid (see Figure <InternalRef RefID="F2">2</InternalRef> ). If <Emphasis Type="Italic">N</Emphasis> = 18, we obtain a 6 × 3 grid. The size of the input tree file has no bearing on how the <Emphasis Type="Italic">N</Emphasis> nodes are organized into a grid.</Para>
											<Figure Category="Standard" Float="No" ID="F2">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Global partitioning scheme of the MrsRF algorithm</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-2" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Global partitioning scheme of the MrsRF algorithm</Emphasis> . Here, <Emphasis Type="Italic">N</Emphasis> = 4. Each of these 4 submatrices will be calculated by a separate instance of MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ).</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>Secondly, once the nodes are organized in a grid using OpenMPI, the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm is executed on each node to compute a <Emphasis Type="Italic">p</Emphasis> × <Emphasis Type="Italic">q</Emphasis> submatrix. For example, consider node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>2</Subscript> in Figure <InternalRef RefID="F2">2</InternalRef> . Letandrepresent the row and column trees, respectively, in the submatrix. For node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>2</Subscript> ,= { <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> /2</Subscript> , ..., <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> -1</Subscript> } and= { <Emphasis Type="Italic">T</Emphasis>
												<Subscript>0</Subscript> , ..., <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> /2 - 1</Subscript> }. Hence, the size of node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>2</Subscript> 's submatrix is <Emphasis Type="Italic">p</Emphasis> × <Emphasis Type="Italic">q</Emphasis> , where <Emphasis Type="Italic">p</Emphasis> = || and <Emphasis Type="Italic">q</Emphasis> = ||.</Para>
											<Para>Once each node is finished computing its <Emphasis Type="Italic">p</Emphasis> × <Emphasis Type="Italic">q</Emphasis> submatrix, the final <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix is the concatenation of the <Emphasis Type="Italic">N</Emphasis> submatrices.</Para>
										</Section2>
										<Section2 ID="Sec_47327">
											<Heading>MrsRF(p, q): Computing a p × q RF submatrix</Heading>
											<Para>The heart of our MrsRF algorithm lies in the subprogram MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ), which runs independently on each of the <Emphasis Type="Italic">N</Emphasis> nodes. Each node has access to the input file containing the <Emphasis Type="Italic">t</Emphasis> trees and is responsible for retrieving the appropriate trees for itsandsets. A node knows which trees belongs to itsandsets based on its identifier within the node grid. In Figure <InternalRef RefID="F2">2</InternalRef> ,= { <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> /2</Subscript> , ..., <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> -1</Subscript> } and= { <Emphasis Type="Italic">T</Emphasis>
												<Subscript>0</Subscript> , ..., <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> /2 - 1</Subscript> } on node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>2</Subscript> . If the number of computing cores on node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>2</Subscript> is eight, then the trees associated with setsandwill each be split into four files, yielding a total of eight input files for the eight cores. Under MapReduce, these eight files will be automatically assigned to the cores on node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>2</Subscript> .</Para>
											<Para>Under MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) the trees inare compared to the trees in. If, all trees are compared to each other. Node <Emphasis Type="Italic">N</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">i</Emphasis>
												</Subscript> 's submatrix is created in parallel using two MapReduce phases as described below.</Para>
											<Section3 ID="Sec_32261">
												<Heading>Phase 1 of the MrsRF(p, q) algorithm</Heading>
												<Section4 ID="Sec_34841">
													<Heading>The first map stage</Heading>
													<Para>Similarly to HashRF, the first MapReduce phase is responsible for generating the global hash table. That is, every bipartition is given a unique identifier (key). Its values are the tree identities (TIDs) that contain that bipartition. The number of mappers correspond to the number of cores utilized on a particular node. Each mapper sends its trees to HashBase to create a local hash table. HashBase is our name for a modification to HashRF that outputs a hash table from its input trees. Each line from the hash table that is provided to MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) from HashBase consists of a bipartition <Emphasis Type="Italic">B</Emphasis>
														<Subscript>i</Subscript>
														<Superscript/>
														<Emphasis Type="Italic"/> and the associated list of tree ids that were found to share it. In addition, all the bipartitions that are found are given a marker to denote which input file created it. This is to ensure that bipartitions shared within a tree file are not compared to each other. The bipartition and its list of tree ids form a (key, value) pair, which is emitted as an intermediate for the reduce stage.</Para>
													<Para>In Figure <InternalRef RefID="F3">3</InternalRef> , there are two input files, each containing two trees each. Trees in the first file are only compared to trees contained in the second file. In the figure, we assume there are only two mappers, where each mapper is responsible for handling one of the two input files. Each mapper creates a local hash table based on the trees that it receives by using a marker of "1" (for file 1) or "2" (for file 2) to keep track of trees and their bipartitions from theandsets, respectively. Each mapper then emits its marked hash table to the reduce stage. For example, in Figure <InternalRef RefID="F3">3</InternalRef> , the first mapper emits the following (key, value) pairs: ( <Emphasis Type="Italic">AB</Emphasis> | <Emphasis Type="Italic">CDE</Emphasis> , (1, <Emphasis Type="Italic">T</Emphasis> 0, <Emphasis Type="Italic">T</Emphasis> 1)), ( <Emphasis Type="Italic">ABC</Emphasis> | <Emphasis Type="Italic">DE</Emphasis> , (1, <Emphasis Type="Italic">T</Emphasis> 0)), and ( <Emphasis Type="Italic">ABD</Emphasis> | <Emphasis Type="Italic">CE</Emphasis> , (1, <Emphasis Type="Italic">T</Emphasis> 1)). These (key, value) pairs are processed in an intermediate stage, where each reducer processes all of the values associated with a particular key.</Para>
													<Figure Category="Standard" Float="No" ID="F3">
														<Caption Language="En">
															<CaptionContent>
																<SimplePara>Phase 1 of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm</SimplePara>
															</CaptionContent>
														</Caption>
														<MediaObject>
															<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-3" Format="GIF" Rendition="Preview" Type="Linedraw"/>
															<TextObject>
																<Para>
																	<Emphasis Type="Bold">Phase 1 of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm</Emphasis> . Two mappers and two reducers are used to process the input files, where= { <Emphasis Type="Italic">T</Emphasis>
																	<Subscript>0</Subscript> , <Emphasis Type="Italic">T</Emphasis>
																	<Subscript>1</Subscript> } and= { <Emphasis Type="Italic">T</Emphasis>
																	<Subscript>2</Subscript> , <Emphasis Type="Italic">T</Emphasis>
																	<Subscript>3</Subscript> }.</Para>
															</TextObject>
														</MediaObject>
													</Figure>
												</Section4>
												<Section4 ID="Sec_56070">
													<Heading>The first reduce stage</Heading>
													<Para>Once the map stage completes, each of the <Emphasis Type="Italic">r</Emphasis> reducers takes as input a (key, list(value)) pair with bipartition <Emphasis Type="Italic">B</Emphasis>
														<Subscript>
															<Emphasis Type="Italic">i</Emphasis>
														</Subscript> as the key and a list of tree id lists as the value. There will be at most <Emphasis Type="Italic">m</Emphasis> lists of tree ids for each bipartition. Each reducer then combines these <Emphasis Type="Italic">O</Emphasis> ( <Emphasis Type="Italic">m</Emphasis> ) lists in a manner such that the trees from file 1 are separated from the trees from file 2 to form a single line in the global hash table. Each row of the global hash table represents a unique bipartition among the <Emphasis Type="Italic">t</Emphasis> trees. Continuing with our example from Figure <InternalRef RefID="F3">3</InternalRef> , the first reducer processes the lists associated with keys <Emphasis Type="Italic">ABD</Emphasis> | <Emphasis Type="Italic">CE</Emphasis> , and <Emphasis Type="Italic">ABC</Emphasis> | <Emphasis Type="Italic">DE</Emphasis> . Thus, the first reducer receives the list of lists ( <Emphasis Type="Italic">ABD</Emphasis> | <Emphasis Type="Italic">CE</Emphasis> ,(1, <Emphasis Type="Italic">T</Emphasis> 1)) and ( <Emphasis Type="Italic">ABC</Emphasis> | <Emphasis Type="Italic">DE</Emphasis> , (1, <Emphasis Type="Italic">T</Emphasis> 0), (2, <Emphasis Type="Italic">T</Emphasis> 2, <Emphasis Type="Italic">T</Emphasis> 3)) and outputs the final (key, value) pairs of ( <Emphasis Type="Italic">ABD</Emphasis> | <Emphasis Type="Italic">CE</Emphasis> , ( <Emphasis Type="Italic">T</Emphasis> 1||)), and ( <Emphasis Type="Italic">ABC</Emphasis> | <Emphasis Type="Italic">DE</Emphasis> , ( <Emphasis Type="Italic">T</Emphasis> 0 || <Emphasis Type="Italic">T</Emphasis> 2, <Emphasis Type="Italic">T</Emphasis> 3)), respectively. The symbol || denotes a partition that separates trees from the first input file with trees from the second input file.</Para>
												</Section4>
											</Section3>
											<Section3 ID="Sec_09890">
												<Heading>Phase 2 of the MrsRF(p, q) algorithm</Heading>
												<Section4 ID="Sec_71378">
													<Heading>The second map stage</Heading>
													<Para>Each of the <Emphasis Type="Italic">m</Emphasis> mappers receives an equal portion of the global hash table based on the total number of comparisons required to process the (key,value) pairs. In Figure <InternalRef RefID="F4">4</InternalRef> , key ( <Emphasis Type="Italic">ABC</Emphasis> | <Emphasis Type="Italic">DE</Emphasis> ) has as its list of values ( <Emphasis Type="Italic">T</Emphasis> 0|| <Emphasis Type="Italic">T</Emphasis> 2, <Emphasis Type="Italic">T</Emphasis> 3). Two total comparisons will be done since <Emphasis Type="Italic">T</Emphasis> 0 is compared to <Emphasis Type="Italic">T</Emphasis> 2 and <Emphasis Type="Italic">T</Emphasis> 3. In general, if there are <Emphasis Type="Italic">u</Emphasis> tree ids on the left side of || and <Emphasis Type="Italic">v</Emphasis> trees on the right side, then <Emphasis Type="Italic">uv</Emphasis> total comparisons are required. Each mapper then computes a local similarity matrix from its portion of the hash table. In the second reduce stage, this similarity matrix will be converted into a RF (or dissimilarity) matrix.</Para>
													<Figure Category="Standard" Float="No" ID="F4">
														<Caption Language="En">
															<CaptionContent>
																<SimplePara>Phase 2 of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm</SimplePara>
															</CaptionContent>
														</Caption>
														<MediaObject>
															<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-4" Format="GIF" Rendition="Preview" Type="Linedraw"/>
															<TextObject>
																<Para>
																	<Emphasis Type="Bold">Phase 2 of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm</Emphasis> . Once again, there are two mappers and two reducers. The horizontal bars between elements represent a partition that separates trees from one file from trees in the other. Bipartitions containing elements on only one side of the partition are discarded.</Para>
															</TextObject>
														</MediaObject>
													</Figure>
													<Para>Consider Figure <InternalRef RefID="F4">4</InternalRef> . The first mapper has two rows of the global hash table assigned to it. Next, it computes a <Emphasis Type="Italic">p</Emphasis> × <Emphasis Type="Italic">q</Emphasis> similarity matrix from those hash table elements. In our example, the resulting similarity matrix is of size 2 × 2. To do this, it compares the tree ids in the first partition of a row to the tree ids located in the second partition of a row, and increments the local similarity matrix accordingly. For example, for the hash table row ( <Emphasis Type="Italic">ABC</Emphasis> | <Emphasis Type="Italic">DE</Emphasis> , ( <Emphasis Type="Italic">T</Emphasis> 0 || <Emphasis Type="Italic">T</Emphasis> 2, <Emphasis Type="Italic">T</Emphasis> 3)), the first mapper increments by one the locations ( <Emphasis Type="Italic">T</Emphasis> 0, <Emphasis Type="Italic">T</Emphasis> 2) and ( <Emphasis Type="Italic">T</Emphasis> 0, <Emphasis Type="Italic">T</Emphasis> 3) in its local similarity matrix. Rows that do not contain elements in both of its partitions are discarded. Therefore, in Figure <InternalRef RefID="F4">4</InternalRef> , the hash table row ( <Emphasis Type="Italic">ABD</Emphasis> | <Emphasis Type="Italic">CE</Emphasis> , ( <Emphasis Type="Italic">T</Emphasis> 1 ||)) is discarded. Each increment to entry ( <Emphasis Type="Italic">i</Emphasis> , <Emphasis Type="Italic">j</Emphasis> ) in the similarity matrix represents that the pair of trees <Emphasis Type="Italic">T</Emphasis>
														<Subscript>
															<Emphasis Type="Italic">i</Emphasis>
														</Subscript> and <Emphasis Type="Italic">T</Emphasis>
														<Subscript>
															<Emphasis Type="Italic">j</Emphasis>
														</Subscript> share a bipartition.</Para>
													<Para>Once, a mapper has finished processing its hash table, it emits its similarity matrix for processing in the reduce stage. The key is the row id and the value is the contents of that row id. Thus, in Figure <InternalRef RefID="F4">4</InternalRef> , the first mapper emits the (key, value) pairs ( <Emphasis Type="Italic">T</Emphasis> 0, (1, 1)), and ( <Emphasis Type="Italic">T</Emphasis> 1, (0, 0)).</Para>
												</Section4>
												<Section4 ID="Sec_86879">
													<Heading>The second reduce stage</Heading>
													<Para>Here, the input is a similarity matrix row identifier, <Emphasis Type="Italic">i</Emphasis> , and a list of rows that contain the local similarity scores found by each of the mappers. For a particular key, the number of rows within the list of rows received by a reducer is equal to the number of mappers, <Emphasis Type="Italic">m</Emphasis> . In Figure <InternalRef RefID="F4">4</InternalRef> , the first reducer receives the following (key, value) pair for similarity identifier, <Emphasis Type="Italic">T</Emphasis> 0: ( <Emphasis Type="Italic">T</Emphasis> 0, (1, 1), (1, 0)). The reducer sums up the columns of each of the lists to produce ( <Emphasis Type="Italic">T</Emphasis> 0, (2, 1)). To produce the RF distance for row <Emphasis Type="Italic">T</Emphasis>
														<Subscript>
															<Emphasis Type="Italic">i</Emphasis>
														</Subscript> , each column in the final similarity matrix is subtracted from <Emphasis Type="Italic">n</Emphasis> - 3, the maximum possible RF distance where <Emphasis Type="Italic">n</Emphasis> represents the number of taxa in the <Emphasis Type="Italic">t</Emphasis> trees of interest. Together, the output from the reduce stage yields a final submatrix. For each node <Emphasis Type="Italic">N</Emphasis>
														<Subscript>
															<Emphasis Type="Italic">i</Emphasis>
														</Subscript> , the resulting submatrix is written to a file. These files can then be combined to form a final RF matrix, or be kept in their partitioned form for easier handling.</Para>
												</Section4>
											</Section3>
										</Section2>
										<Section2 ID="Sec_18619">
											<Heading>Analysis of MrsRF</Heading>
											<Para>MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) is where all of the computation for the MrsRF algorithm lies. At least one node will require <Emphasis Type="Italic">O</Emphasis> ( <Emphasis Type="Italic">t</Emphasis> ) time to obtain the trees for itsandsets. The first map phase of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm, which is based on HashRF's first phase, requires, where <Emphasis Type="Italic">n</Emphasis> is the number of taxa and <Emphasis Type="Italic">m</Emphasis> is the number of mappers. <Emphasis Type="Italic">O</Emphasis> ( <Emphasis Type="Italic">n</Emphasis> ( <Emphasis Type="Italic">p</Emphasis> + <Emphasis Type="Italic">q</Emphasis> )) is the total number of bipartitions that must be processed across the <Emphasis Type="Italic">p</Emphasis> + <Emphasis Type="Italic">q</Emphasis> trees and inserted into the hash table. Suppose <Emphasis Type="Italic">b</Emphasis> unique bipartitions are found. In the worst case, a bipartition has a length of <Emphasis Type="Italic">p</Emphasis> + <Emphasis Type="Italic">q</Emphasis> , which re ects the fact that it appears in all <Emphasis Type="Italic">p</Emphasis> + <Emphasis Type="Italic">q</Emphasis> trees. Hence, the complexity isfor the first reduce phase, where <Emphasis Type="Italic">r</Emphasis> is the number of reducers. For the second phase of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm, in the worst case, each mapper requiresto produce its local similarity matrix. Each reducer requirestime. Hence, if <Emphasis Type="Italic">p</Emphasis> and <Emphasis Type="Italic">q</Emphasis> are large enough, phase 2 is more time-consuming than phase 1 in the MrsRF algorithm. Our analysis does not incorporate communication costs as there is not an explicit model of communication for the MapReduce framework.</Para>
										</Section2>
										<Section2 ID="Sec_86903">
											<Heading>Biological trees</Heading>
											<Para>Below, we describe the biological trees used in this study were obtained from two recent Bayesian analysis.</Para>
											<Para>1. 20,000 trees obtained from a Bayesian analysis of an alignment of 150 taxa (23 desert taxa and 127 others from freshwater, marine, and oil habitats) with 1,651 aligned sites <CitationRef CitationID="B13">13</CitationRef> . Two independent runs consisting of 25 million generations (with trees sampled every 1,000 generations) were performed with four independent chains in MrBayes using the GTR+I+Γ model.</Para>
											<Para>2. 33,306 trees obtained from an analysis of a three-gene, 567 taxa (560 angiosperms, seven outgroups) dataset with 4,621 aligned characters, which is one of the largest Bayesian analysis done to date <CitationRef CitationID="B14">14</CitationRef> . Twelve runs, each with four chains, ran for at least 10 million generations in MrBayes using the GTR+I+Γ model. Trees were sampled every 1,000 generations.</Para>
											<Para>Table <InternalRef RefID="T1">1</InternalRef> presents statistical data on our collection of biological trees. Both collections contain unique trees as each tree appears once. The total number of bipartitions in a collection of <Emphasis Type="Italic">t</Emphasis> binary trees is <Emphasis Type="Italic">t</Emphasis> ( <Emphasis Type="Italic">n</Emphasis> - 3), where <Emphasis Type="Italic">n</Emphasis> is the number of taxa. This is the number of bipartitions that must be processed by MrsRF in order to compute the <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix. Many of the these bipartitions are shared across the trees. There are 1,168 and 2,444 unique bipartitions among the 150 and 567 taxa trees, respectively. The hash table size is the result of the first reduce stage of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) algorithm. The RF matrix data is the output of the second reduce stage of the algorithm. When MrsRF is executing for speed in our experiments, the hash table from the first reduce stage of MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) is kept in memory.</Para>
											<Table Float="No" ID="T1">
												<Caption Language="En">
													<CaptionNumber>Table 1</CaptionNumber>
													<CaptionContent>
														<SimplePara>Statistics for our Bayesian tree collections</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="7">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">number of taxa</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Bold">total trees ( <Emphasis Type="Italic">t</Emphasis> )</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>
																	<Emphasis Type="Bold">total bipartitions</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">unique bipartitions</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">hash table size</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">RF matrix cells ( <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> )</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>
																	<Emphasis Type="Bold">RF matrix size</Emphasis>
																</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>150</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>20,000</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>2.9 × 10 <Superscript>6</Superscript>
																</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>1168</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>16 MB</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>4 × 10 <Superscript>8</Superscript>
																</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>1.2 GB</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>567</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>33,306</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>18.8 × 10 <Superscript>6</Superscript>
																</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>2444</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>102 MB</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>1.1 × 10 <Superscript>9</Superscript>
																</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>3.3 GB</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
										</Section2>
										<Section2 ID="Sec_85326">
											<Heading>Implementation and platform</Heading>
											<Para>All experiments were run on a multi-core cluster with configurations ranging from 1 to 32 nodes. Each node consists of a PowerEdge 1950 1U server, with two Intel Xeon E5420 2.5 GHz quad-core processors, resulting in a total of eight cores. Each node also consists of 16 GB DDR2 667 MHz fully-buffered DRAM and 160 GB of hard-disk. The nodes are connected together with a gigabit ethernet switch. We modified the Phoenix runtime system (Original Release version) to work on 64-bit Linux platforms, as the cluster runs the CentOS 5.2 64-bit operating system on all nodes. HashRF and HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) <CitationRef CitationID="B11">11</CitationRef> are written in C++ and MrsRF and Phoenix are implemented in C. All programs are compiled with gcc 4.1.2 with the -03 compiler option.</Para>
										</Section2>
									</Section1>
									<Section1 ID="Sec_56114">
										<Heading>Results and discussion</Heading>
										<Section2 ID="Sec_20434">
											<Heading>Establishing the fastest sequential algorithm</Heading>
											<Para>We evaluate the performance of MrsRF on our computational platform as we vary the number of cores, the number of nodes, and the problem size of interest. First, we establish the fastest sequential algorithm in order to compute the speedup of our approach. Speedup is defined aswhere, <Emphasis Type="Italic">T</Emphasis>
												<Subscript>1</Subscript> is the time required by the fastest sequential program and <Emphasis Type="Italic">T</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis>
												</Subscript> is the time required by MrsRF run on <Emphasis Type="Italic">N</Emphasis> nodes and <Emphasis Type="Italic">c</Emphasis> cores. Previous experiments established HashRF and HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) as the fastest sequential algorithms for computing the RF matrix <CitationRef CitationID="B11">11</CitationRef>
												<CitationRef CitationID="B15">15</CitationRef> .</Para>
											<Para>Figure <InternalRef RefID="F5">5</InternalRef> compares the sequential running time of HashRF, HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ), and MrsRF. Each data point represents the average of five runs of the algorithm for each dataset. Surprisingly, our experiments showed that our MrsRF algorithm using 1 core is up to 2.8 times faster than HashRF on larger tree sets. This corresponds to an average time of 680.9 seconds for MrsRF compared to a running time of 1913.22 for HashRF, and a running time of 1657.75 for HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ). The difference in performance is due to language-specific implementation decisions. MrsRF is written in C to match the implementation language of Phoenix. HashRF and HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ), on the other hand, are C++ implementations that employ the Standard Template Library (STL) and classes, which introduces extra overhead when compared to MrsRF's ANSI C implementation. Thus, all our speedup results are in relation to MrsRF run on a single core.</Para>
											<Figure Category="Standard" Float="No" ID="F5">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Sequential running time and speedup of HashRF, HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ), and MrsRF (1 core) algorithms on 20,000 and 33,306 trees on 150 and 567 taxa, respectively</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-5" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Sequential running time and speedup of HashRF, HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ), and MrsRF (1 core) algorithms on 20,000 and 33,306 trees on 150 and 567 taxa, respectively</Emphasis> . (a) denotes the running time for each algorithm. (b) denotes the speedup of HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) and MrsRF (1-core) over HashRF.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
										</Section2>
										<Section2 ID="Sec_13405">
											<Heading>Multi-core performance of MrsRF</Heading>
											<Para>Figure <InternalRef RefID="F6">6</InternalRef> shows the speedup of MrsRF on our 20,000 and 33,306 tree sets over the total number of CPUs utilized, ranging from 1 to 32 cores. Every dataset was run five times using various cluster configurations. The numbers reported are the average between each set of five runs. Speedup is calculated with respect to MrsRF run serially on one core.</Para>
											<Figure Category="Standard" Float="No" ID="F6">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Speedup of MrsRF algorithm on various <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> multi-core cluster configurations</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-6" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Speedup of MrsRF algorithm on various <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> multi-core cluster configurations</Emphasis> . (a) shows the speedup of MrsRF on various <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> configurations over MrsRF (1-core) for 150 taxa and 20,000 trees. (b) shows the speedup of MrsRF on various <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> configurations over MrsRF (1-core) for 567 taxa and 33,306 trees.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Section3 ID="Sec_77930">
												<Heading>Cluster configurations</Heading>
												<Para>To test how much a factor architecture is to speedup, we used different system configurations to measure performance. Let <Emphasis Type="Italic">N</Emphasis> denote the number of nodes used, and <Emphasis Type="Italic">c</Emphasis> denote the number of cores used on each node. For any <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> configuration, there are <Emphasis Type="Italic">Nc</Emphasis> total cores being utilized. Thus, for 8 total cores to be used, we run our algorithm using 1 × 8, 2 × 4, 4 × 2 and 8 × 1 configurations. Each curve denotes the number of cores utilized per node. Therefore, if <Emphasis Type="Italic">c</Emphasis> = 4 and the total number of cores is 8, then this data point reflects a 2 × 4 configuration. Likewise, if <Emphasis Type="Italic">c</Emphasis> = 1 and the total number of cores is 32, then the data point reflects a 32 × 1 configuration.</Para>
											</Section3>
											<Section3 ID="Sec_69558">
												<Heading>150 taxa trees</Heading>
												<Para>Figure <InternalRef RefID="F6">6</InternalRef> shows that as the number of bipartitions increase, so does the performance of MrsRF. While the curves for <Emphasis Type="Italic">c</Emphasis> = 2 and <Emphasis Type="Italic">c</Emphasis> = 4 performs the best, the <Emphasis Type="Italic">c</Emphasis> = 1 and <Emphasis Type="Italic">c</Emphasis> = 8 configurations performs the poorest. Performance differences across various cluster configurations are underscored as the total number of cores increases. This is due to overhead in partitioning the data ( <Emphasis Type="Italic">c</Emphasis> = 1) and inter-node resource contention for memory bandwidth ( <Emphasis Type="Italic">c</Emphasis> = 8). Despite this, an increase in total cores results in an increase in performance, and we see our best performance when 16 total nodes is utilized, with a maximum speedup of 7.25 for this dataset. This corresponds to an average running time of 8.73 seconds for MrsRF on 32 cores.</Para>
											</Section3>
											<Section3 ID="Sec_65483">
												<Heading>567 taxa trees</Heading>
												<Para>We see a similar trend in architectural performance in the 567 taxa case, thus underscoring the importance of managing resource contention and communication overhead in relation to performance. However, with the increased bipartitions present in the 567 taxa case, we see an markedly increased amount of speedup, with a maximum amount of speedup of 18.4 attainable with 32 cores using a 16 × 2 cluster configuration. The maximum speedup corresponds to an average running time of 36.93 seconds. In comparison, it took the serial execution of MrsRF an average of 680.9 seconds to compute the RF matrix, while it took HashRF and HashRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) an average of 1913.22 and 1657.75 seconds respectively. Our results show that MrsRF is a very scalable approach for computing the all-to-all RF Matrix, with performance increasing with large problem sizes. Figure <InternalRef RefID="F7">7</InternalRef> shows that Phase 2 of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) approach exhibits linear speedup. Overall speedup of MrsRF increases (decreases) when Phase 2 (Phase 1) of MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) dominates the computation time. Once again, the differences in speedup that we observe with different <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> configurations suggest that multiple cluster configurations should be run to achieve the maximum speedup.</Para>
												<Figure Category="Standard" Float="No" ID="F7">
													<Caption Language="En">
														<CaptionContent>
															<SimplePara>Performance of Phase 1 and Phase 2 of MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) on 567 taxa and 33,306 trees</SimplePara>
														</CaptionContent>
													</Caption>
													<MediaObject>
														<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-7" Format="GIF" Rendition="Preview" Type="Linedraw"/>
														<TextObject>
															<Para>
																<Emphasis Type="Bold">Performance of Phase 1 and Phase 2 of MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ) on 567 taxa and 33,306 trees</Emphasis> . Here, the 567 taxa - 33,306 tree set is of interest. (a) shows the speedup of Phase 1 of the MrsRF algorithm run on different <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> configurations over Phase 1 of MrsRF (1-core). (b) shows the speedup of Phase 2 of the MrsRF algorithm run on different <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> configurations over Phase 2 of MrsRF (1-core).</Para>
														</TextObject>
													</MediaObject>
												</Figure>
											</Section3>
										</Section2>
										<Section2 ID="Sec_10668">
											<Heading>RF matrix application: Visually summarizing tree collections</Heading>
											<Para>The fundamental question we address here is "what do the gathered trees tell us about the Bayesian analyses that produced them?" To answer this, we partitioned our <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix based on the MrBayes run that generated the tree. Figure <InternalRef RefID="F8">8</InternalRef> shows a heatmap of our 20, 000 × 20, 000 RF matrix broken up into a 2 × 2 matrix, where each entry ( <Emphasis Type="Italic">i</Emphasis> , <Emphasis Type="Italic">j</Emphasis> ) shows the average RF rate between the trees from run <Emphasis Type="Italic">i</Emphasis> and run <Emphasis Type="Italic">j</Emphasis> of MrBayes. For this dataset, two MrBayes runs were used to create the entire collection of 20, 000 trees, where each run consisted of 10, 000 trees. Figure <InternalRef RefID="F9">9</InternalRef> shows the 33, 306 × 33, 306 RF matrix broken up into a 12 × 12 matrix. For this dataset, twelve MrBayes runs were used to create the entire collection of 33, 306 trees, where each run consisted of 2, 000 to 3, 000 trees.</Para>
											<Figure Category="Standard" Float="No" ID="F8">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>A heatmap illustrating the clustering of the biological trees across MrBayes runs for the 150 taxa and 20,000 tree set</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-8" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">A heatmap illustrating the clustering of the biological trees across MrBayes runs for the 150 taxa and 20,000 tree set</Emphasis> .</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Figure Category="Standard" Float="No" ID="F9">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>A heatmap illustrating the clustering of the biological trees across MrBayes runs for the 567 taxa and 33,306 tree set</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-11-S1-S15-9" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">A heatmap illustrating the clustering of the biological trees across MrBayes runs for the 567 taxa and 33,306 tree set</Emphasis> .</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>In Figures <InternalRef RefID="F8">8</InternalRef> and <InternalRef RefID="F9">9</InternalRef> , heatmap cell ( <Emphasis Type="Italic">i</Emphasis> , <Emphasis Type="Italic">j</Emphasis> ) is colored according to how similar (lower RF rates) the trees are across runs <Emphasis Type="Italic">i</Emphasis> and <Emphasis Type="Italic">j</Emphasis> . Hot regions, colored in shades of red, denote highly similar trees. Cool regions, colored in shades of green, denote dissimilar trees. Cell (run1, run1) in Figure <InternalRef RefID="F8">8</InternalRef> shows an average RF rate of about 20% while (run1, run2) show an average RF rate of around 24%. In Figure <InternalRef RefID="F9">9</InternalRef> , cell (run6, run10) shows an average RF rate of about 18% while these runs compared to themselves (i.e., cells (run6, run6) and cells (run10, run10)) show higher levels of similarity with an average RF rate of around 11%. Finally, the histogram in the color key represent the number of cells with a particular RF value.</Para>
											<Para>Hierarchical clustering, using the hclust function in R, is used to cluster highly similar cells in the heatmap to each other. For the 150 taxa tree collection, the trees from one run are not similar to trees in the other run suggesting that two different summaries are required to encapsulate the evolutionary relationships among the trees (see Figure <InternalRef RefID="F8">8</InternalRef> ). For the 567 taxa trees, the heatmap (in Figure <InternalRef RefID="F9">9</InternalRef> ) shows regions of high similarity among the trees within a run, and regions of dissimilarity across runs. The clustering also shows that runs 0, 6, 10 exhibit trees with high levels of similarity among them. One conclusion is that these runs converged to similar areas of tree space in the phylogenetic search and the trees from those runs can be summarized by a single tree (such as a consensus tree). This is also true for runs 1, 5, 7, and 8. The clustering of the other runs (runs 2 and 3) and (runs 0, 6, 11) have lower levels of similarity.</Para>
											<Para>Overall, the clusterings in Figures <InternalRef RefID="F8">8</InternalRef> and <InternalRef RefID="F9">9</InternalRef> suggest that there exist several well-supported partitions of the trees and that each partition should be summarized separately in order to minimize information loss. Moreover, the data suggests that the various Bayesian runs among the 150 and 567 taxa trees did not converge to the same place in tree space. One of the greatest benefits of convergence is reliability of the trees found by a phylogenetic heuristic. Hence, RF matrices could be used as a method to detect convergence between runs.</Para>
										</Section2>
									</Section1>
									<Section1 ID="Sec_93624">
										<Heading>Conclusion</Heading>
										<Para>In this paper, we evaluate the applicability of the MapReduce framework for developing multi-core phylogenetic applications. We design a new algorithm called MrsRF for computing the all-to-all RF matrix using the MapReduce framework. An open-source implementation of our MrsRF algorithm is available from the web <CitationRef CitationID="B9">9</CitationRef> . One of the novelties of computing an RF matrix in a MapReduce context is that the size of the input ( <Emphasis Type="Italic">t</Emphasis> evolutionary trees) is much smaller than the size of the output ( <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix). Our results show that we achieve a significant speedup using MrsRF over the fastest sequential algorithm. On our largest problem size (567 taxa and 33,306 trees), we attain a maximum speedup of 18.4 on 32 cores. Our results suggest that MrsRF is a very scalable approach with increased performance resulting from larger collections of trees. Furthermore, our results show the usefulness of running an algorithm on multiple <Emphasis Type="Italic">N</Emphasis> × <Emphasis Type="Italic">c</Emphasis> multi-core cluster configurations to ensure that the best performance is attained. Finally, we show an application for RF matrices related to summarizing a collection of trees and detecting convergence of a phylogenetic analysis.</Para>
										<Para>Additionally, our results suggest that the storing the hash table is a better alternative to storing the large RF matrices, which are not sparse. Since the hash table contains a list of shared, unique bipartitions, it is much smaller than the final <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix. For example, storing the hash table of the 33, 306 × 33, 306 RF matrix takes only 102 MB of storage compared to the 3.3 GB necessary to store the full matrix (see Table <InternalRef RefID="T1">1</InternalRef> ). Given the speed of the MrsRF( <Emphasis Type="Italic">p</Emphasis> , <Emphasis Type="Italic">q</Emphasis> ), especially in Phase 2, one could store the hash table and compute the resulting <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> RF matrix on the fly as needed.</Para>
										<Para>Overall, our results show that MapReduce is an exciting approach for developing multi-core phylogenetic applications. Future work includes studying the performance of MrsRF on larger clusters and tree collections. Finally, we intend to design additional MapReduce phylogenetic applications—especially as it relates to reconstructing more accurate phylogenetic trees efficiently.</Para>
									</Section1>
									<Section1 ID="Sec_77082">
										<Heading>Competing interests</Heading>
										<Para>The authors declare that they have no competing interests.</Para>
									</Section1>
									<Section1 ID="Sec_93279">
										<Heading>Authors' contributions</Heading>
										<Para>SM and TW both designed and implemented the MrsRF algorithm. SM created all of the figures. Both authors contributed to writing the manuscript and have approved its final contents.</Para>
									</Section1>
								</Body>
								<BodyRef FileRef="http://www.biomedcentral.com/1471-2105/11/S1/S15" TargetType="Manuscript"/>
								<ArticleBackmatter>
									<Bibliography ID="Bib1">
										<Heading>References</Heading>
										<Citation ID="CR1">
											<CitationNumber>1.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Dean</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Ghemawat</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">MapReduce: Simplified Data Processing on Large Clusters</ArticleTitle>
												<JournalTitle>Commun ACM</JournalTitle>
												<VolumeID>51</VolumeID>
												<FirstPage>107</FirstPage>
												<LastPage>113</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1145/1327452.1327492</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR2">
											<CitationNumber>2.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>MC</Initials>
													<FamilyName>Schatz</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">CloudBurst: Highly Sensitive Read Mapping with MapReduce</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>1754</FirstPage>
												<LastPage>1760</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btp324</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR3">
											<CitationNumber>3.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>DM</Initials>
													<FamilyName>Hillis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TA</Initials>
													<FamilyName>Heath</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KS</Initials>
													<FamilyName>John</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">Analysis and Visualization of Tree Space</ArticleTitle>
												<JournalTitle>Syst Biol</JournalTitle>
												<VolumeID>54</VolumeID>
												<FirstPage>471</FirstPage>
												<LastPage>482</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1080/10635150590946961</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR4">
											<CitationNumber>4.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Sul</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Matthews</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TL</Initials>
													<FamilyName>Williams</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Using Tree Diversity to Compare Phylogenetic Heuristics</ArticleTitle>
												<JournalTitle>BMC Bioinformatics</JournalTitle>
												<VolumeID>10</VolumeID>
												<FirstPage>S3</FirstPage>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR5">
											<CitationNumber>5.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Stockham</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>LS</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Warnow</FamilyName>
												</BibAuthorName>
												<Year>2002</Year>
												<ArticleTitle Language="En">Statistically Based Postprocessing of Phylogenetic Analysis by Clustering</ArticleTitle>
												<JournalTitle>Proceedings of 10th Int'l Conf. on Intelligent Systems for Molecular Biology (ISMB'02)</JournalTitle>
												<VolumeID/>
												<FirstPage>285</FirstPage>
												<LastPage>293</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR6">
											<CitationNumber>6.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Ranger</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Raghuraman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Penmetsa</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Bradski</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Kozyrakis</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">Evaluating MapReduce for Multi-core and Multiprocessor Systems</ArticleTitle>
												<JournalTitle>High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on</JournalTitle>
												<VolumeID/>
												<FirstPage>13</FirstPage>
												<LastPage>24</LastPage>
												<Occurrence Type="DOI">
													<Handle>full_text</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR7">
											<CitationNumber>7.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Gabriel</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>GE</Initials>
													<FamilyName>Fagg</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Bosilca</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Angskun</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JJ</Initials>
													<FamilyName>Dongarra</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JM</Initials>
													<FamilyName>Squyres</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>V</Initials>
													<FamilyName>Sahay</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Kambadur</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Barrett</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Lumsdaine</FamilyName>
												</BibAuthorName>
												<Year>2004</Year>
												<ArticleTitle Language="En">Open MPI: Goals, Concept, and Design of a Next Generation MPI Implementation</ArticleTitle>
												<JournalTitle>Proceedings, 11th European PVM/MPI Users' Group Meeting, Budapest, Hungary</JournalTitle>
												<VolumeID/>
												<FirstPage>97</FirstPage>
												<LastPage>104</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR8">
											<CitationNumber>8.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials/>
													<FamilyName/>
												</BibAuthorName>
												<Year/>
												<ArticleTitle Language="En">Hadoop</ArticleTitle>
												<JournalTitle>Internet Website, last accessed May 2009</JournalTitle>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR9">
											<CitationNumber>9.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Matthews</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TL</Initials>
													<FamilyName>Williams</FamilyName>
												</BibAuthorName>
												<Year/>
												<ArticleTitle Language="En">MrsRF</ArticleTitle>
												<JournalTitle>Internet Website, last accessed October 2009</JournalTitle>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR10">
											<CitationNumber>10.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>DF</Initials>
													<FamilyName>Robinson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>LR</Initials>
													<FamilyName>Foulds</FamilyName>
												</BibAuthorName>
												<Year>1981</Year>
												<ArticleTitle Language="En">Comparison of Phylogenetic Trees</ArticleTitle>
												<JournalTitle>Mathematical Biosciences</JournalTitle>
												<VolumeID>53</VolumeID>
												<FirstPage>131</FirstPage>
												<LastPage>147</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1016/0025-5564(81)90043-2</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR11">
											<CitationNumber>11.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Sul</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TL</Initials>
													<FamilyName>Williams</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">An Experimental Analysis of Robinson-Foulds Distance Matrix Algorithms</ArticleTitle>
												<JournalTitle>European Symposium of Algorithms (ESA'08)</JournalTitle>
												<VolumeID>5193</VolumeID>
												<FirstPage>793</FirstPage>
												<LastPage>804</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR12">
											<CitationNumber>12.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Sul</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TL</Initials>
													<FamilyName>Williams</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">A Randomized Algorithm for Comparing Sets of Phylogenetic Trees</ArticleTitle>
												<JournalTitle>Proc. Fifth Asia Pacific Bioinformatics Conference (APBC'07)</JournalTitle>
												<VolumeID/>
												<FirstPage>121</FirstPage>
												<LastPage>130</LastPage>
												<Occurrence Type="DOI">
													<Handle>full_text</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR13">
											<CitationNumber>13.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>LA</Initials>
													<FamilyName>Lewis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>PO</Initials>
													<FamilyName>Lewis</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">Unearthing the Molecular Phylodiversity of Desert Soil Green Algae (Chlorophyta)</ArticleTitle>
												<JournalTitle>Syst Bio</JournalTitle>
												<VolumeID>54</VolumeID>
												<FirstPage>936</FirstPage>
												<LastPage>947</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1080/10635150500354852</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR14">
											<CitationNumber>14.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>DE</Initials>
													<FamilyName>Soltis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MA</Initials>
													<FamilyName>Gitzendanner</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>PS</Initials>
													<FamilyName>Soltis</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">A 567-Taxon Data Set for Angiosperms: The Challenges Posed by Bayesian Analyses of Large Data Sets</ArticleTitle>
												<JournalTitle>Int J Plant Sci</JournalTitle>
												<VolumeID>168</VolumeID>
												<FirstPage>137</FirstPage>
												<LastPage>157</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1086/509788</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR15">
											<CitationNumber>15.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Sul</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Brammer</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TL</Initials>
													<FamilyName>Williams</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Efficiently Computing Arbitrarily-Sized Robinson-Foulds Distance Matrices</ArticleTitle>
												<JournalTitle>Workshop on Algorithms in Bioinformatics (WABI'08)</JournalTitle>
												<VolumeID>5251</VolumeID>
												<FirstPage>123</FirstPage>
												<LastPage>134</LastPage>
												<Occurrence Type="DOI">
													<Handle>full_text</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
									</Bibliography>
								</ArticleBackmatter>
							</Article>
						</Issue>
					</Volume>
				</Journal>
				<meta:Info xmlns:meta="http://www.springer.com/app/meta">
					<meta:DateLoaded>2010-05-13T20:42:11.167926+02:00</meta:DateLoaded>
					<meta:Authors>
						<meta:Author>Matthews, Suzanne</meta:Author>
						<meta:Author>Williams, Tiffani</meta:Author>
					</meta:Authors>
					<meta:Institutions/>
					<meta:Date>2010-01-18</meta:Date>
					<meta:Type>Article</meta:Type>
					<meta:DOI>10.1186/1471-2105-11-S1-S15</meta:DOI>
					<meta:Title>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</meta:Title>
					<meta:ISXN>1471-2105</meta:ISXN>
					<meta:Journal>BMC Bioinformatics</meta:Journal>
					<meta:PubName>BioMed Central</meta:PubName>
					<meta:ArticleFirstPage>S15</meta:ArticleFirstPage>
					<meta:Publication>BMC Bioinformatics</meta:Publication>
					<meta:PublicationType>Journal</meta:PublicationType>
					<meta:SubjectGroup>
						<meta:Subject Type="Primary">Life Sciences</meta:Subject>
						<meta:Subject Priority="1" Type="Secondary">Bioinformatics</meta:Subject>
						<meta:Subject Priority="2" Type="Secondary">Microarrays</meta:Subject>
						<meta:Subject Priority="3" Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>
						<meta:Subject Priority="4" Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>
						<meta:Subject Priority="5" Type="Secondary">Combinatorial Libraries</meta:Subject>
						<meta:Subject Priority="6" Type="Secondary">Algorithms</meta:Subject>
					</meta:SubjectGroup>
				</meta:Info>
			</Publisher>
			<Images>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-0" xml:lang="en" language="en">
					<Caption>
						<p>Word count example using the MapReduce paradigm</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Figure 1 gives an overview of how the MapReduce paradigm
operates in order to count the number of words in a file.</p>
						</p><p>
In Figure 1 , the fifth reducer takes in as input the keyfishand
the associated list1-1-1-1, as "fish" occurred four times in the
input file.
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-1.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-1" xml:lang="en" language="en">
					<Caption>
						<p>Global partitioning scheme of the MrsRF algorithm</p>
					</Caption>
					<FullText>
						<p>
For N = 4, the N nodes are partitioned into a 2 × 2 grid (see
Figure 2 ).
						</p><p>
For example, consider node N <sub xmlns="http://www.w3.org/1999/xhtml">2</sub> in Figure 2 .
						</p><p>
In Figure 2 ,= { T <sub xmlns="http://www.w3.org/1999/xhtml">t /2</sub> , ..., T <sub xmlns="http://www.w3.org/1999/xhtml">t -1</sub> } and=
{ T <sub xmlns="http://www.w3.org/1999/xhtml">0</sub> , ..., T <sub xmlns="http://www.w3.org/1999/xhtml">t /2 - 1</sub> } on node N
							<sub xmlns="http://www.w3.org/1999/xhtml">2</sub> .
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-2.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-4" xml:lang="en" language="en">
					<Caption>
						<p>Sequential running time and speedup of HashRF, HashRF( p , q ), and MrsRF (1 core) algorithms on 20,000 and 33,306 trees on 150 and 567 taxa, respectively</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Figure 5 compares the sequential running time of HashRF, HashRF(
p , q ), and MrsRF.</p>
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-5.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-5" xml:lang="en" language="en">
					<Caption>
						<p>Speedup of MrsRF algorithm on various N × c multi-core cluster configurations</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Figure 6 shows the speedup of MrsRF on our 20,000 and 33,306
tree sets over the total number of CPUs utilized, ranging from 1 to
32 cores.</p>
						</p><p>
							<p xmlns="http://www.w3.org/1999/xhtml">Figure 6 shows that as the number of bipartitions increase, so
does the performance of MrsRF.</p>
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-6.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-6" xml:lang="en" language="en">
					<Caption>
						<p>Performance of Phase 1 and Phase 2 of MrsRF( p , q ) on 567 taxa and 33,306 trees</p>
					</Caption>
					<FullText>
						<p>
Figure 7 shows that Phase 2 of the MrsRF( p , q ) approach exhibits
linear speedup.
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-7.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-2" xml:lang="en" language="en">
					<Caption>
						<p>Phase 1 of the MrsRF( p , q ) algorithm</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">In Figure 3 , there are two input files, each containing two
trees each.</p>
						</p><p>
For example, in Figure 3 , the first mapper emits the following
(key, value) pairs: ( AB | CDE , (1, T 0, T 1)), ( ABC | DE , (1, T
0)), and ( ABD | CE , (1, T 1)).
						</p><p>
Continuing with our example from Figure 3 , the first reducer
processes the lists associated with keys ABD | CE , and ABC | DE .
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-3.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-3" xml:lang="en" language="en">
					<Caption>
						<p>Phase 2 of the MrsRF( p , q ) algorithm</p>
					</Caption>
					<FullText>
						<p>
In Figure 4 , key ( ABC | DE ) has as its list of values ( T 0|| T
2, T 3).
						</p><p>
							<p xmlns="http://www.w3.org/1999/xhtml">Consider Figure 4 .</p>
						</p><p>
Therefore, in Figure 4 , the hash table row ( ABD | CE , ( T 1 ||))
is discarded.
						</p><p>
Thus, in Figure 4 , the first mapper emits the (key, value) pairs (
T 0, (1, 1)), and ( T 1, (0, 0))..
						</p><p>
In Figure 4 , the first reducer receives the following (key, value)
pair for similarity identifier, T 0: ( T 0, (1, 1), (1, 0)).
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-4.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-7" xml:lang="en" language="en">
					<Caption>
						<p>A heatmap illustrating the clustering of the biological trees across MrBayes runs for the 150 taxa and 20,000 tree set</p>
					</Caption>
					<FullText>
						<p>
Figure 8 shows a heatmap of our 20, 000 × 20, 000 RF matrix broken
up into a 2 × 2 matrix, where each entry ( i , j ) shows the
average RF rate between the trees from run i and run j of MrBayes.
						</p><p>
							<p xmlns="http://www.w3.org/1999/xhtml">In Figures 8 and 9 , heatmap cell ( i , j ) is colored according
to how similar (lower RF rates) the trees are across runs i and j
.</p>
						</p><p>
Cell (run1, run1) in Figure 8 shows an average RF rate of about 20%
while (run1, run2) show an average RF rate of around 24%.
						</p><p>
For the 150 taxa tree collection, the trees from one run are not
similar to trees in the other run suggesting that two different
summaries are required to encapsulate the evolutionary
relationships among the trees (see Figure 8 ).
						</p><p>
							<p xmlns="http://www.w3.org/1999/xhtml">Overall, the clusterings in Figures 8 and 9 suggest that there
exist several well-supported partitions of the trees and that each
partition should be summarized separately in order to minimize
information loss.</p>
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-8.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-8" xml:lang="en" language="en">
					<Caption>
						<p>A heatmap illustrating the clustering of the biological trees across MrBayes runs for the 567 taxa and 33,306 tree set</p>
					</Caption>
					<FullText>
						<p>
Figure 9 shows the 33, 306 × 33, 306 RF matrix broken up into a 12
× 12 matrix.
						</p><p>
							<p xmlns="http://www.w3.org/1999/xhtml">In Figures 8 and 9 , heatmap cell ( i , j ) is colored according
to how similar (lower RF rates) the trees are across runs i and j
.</p>
						</p><p>
In Figure 9 , cell (run6, run10) shows an average RF rate of about
18% while these runs compared to themselves (i.e., cells (run6,
run6) and cells (run10, run10)) show higher levels of similarity
with an average RF rate of around 11%.
						</p><p>
For the 567 taxa trees, the heatmap (in Figure 9 ) shows regions of
high similarity among the trees within a run, and regions of
dissimilarity across runs.
						</p><p>
							<p xmlns="http://www.w3.org/1999/xhtml">Overall, the clusterings in Figures 8 and 9 suggest that there
exist several well-supported partitions of the trees and that each
partition should be summarized separately in order to minimize
information loss.</p>
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-11-S1-S15-9.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-11-S1-S15-9" xml:lang="en" language="en">
					<Caption>
						<p>Statistics for our Bayesian tree collections</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Table 1 presents statistical data on our collection of
biological trees.</p>
						</p><p>
For example, storing the hash table of the 33, 306 × 33, 306 RF
matrix takes only 102 MB of storage compared to the 3.3 GB
necessary to store the full matrix (see Table 1 ).
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T1">
							<Caption Language="En">
								<CaptionNumber>Table 1</CaptionNumber>
								<CaptionContent>
									<SimplePara>Statistics for our Bayesian tree collections</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="7">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<colspec colname="c3" colnum="3"/>
								<colspec colname="c4" colnum="4"/>
								<colspec colname="c5" colnum="5"/>
								<colspec colname="c6" colnum="6"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">number of taxa</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>
												<Emphasis Type="Bold">total trees ( <Emphasis Type="Italic">t</Emphasis> )</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>
												<Emphasis Type="Bold">total bipartitions</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>
												<Emphasis Type="Bold">unique bipartitions</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c4">
											<SimplePara>
												<Emphasis Type="Bold">hash table size</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c5">
											<SimplePara>
												<Emphasis Type="Bold">RF matrix cells ( <Emphasis Type="Italic">t</Emphasis> × <Emphasis Type="Italic">t</Emphasis> )</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c6">
											<SimplePara>
												<Emphasis Type="Bold">RF matrix size</Emphasis>
											</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>150</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>20,000</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>2.9 × 10 <Superscript>6</Superscript>
											</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>1168</SimplePara>
										</entry>
										<entry colname="c4">
											<SimplePara>16 MB</SimplePara>
										</entry>
										<entry colname="c5">
											<SimplePara>4 × 10 <Superscript>8</Superscript>
											</SimplePara>
										</entry>
										<entry colname="c6">
											<SimplePara>1.2 GB</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>567</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>33,306</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>18.8 × 10 <Superscript>6</Superscript>
											</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>2444</SimplePara>
										</entry>
										<entry colname="c4">
											<SimplePara>102 MB</SimplePara>
										</entry>
										<entry colname="c5">
											<SimplePara>1.1 × 10 <Superscript>9</Superscript>
											</SimplePara>
										</entry>
										<entry colname="c6">
											<SimplePara>3.3 GB</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
						</Table>
					</Table>
					<Authors>
						<Author>Matthews, Suzanne</Author>
						<Author>Williams, Tiffani</Author>
					</Authors>
					<Institutions>
						<Institution>Department of Computer Science and Engineering, Texas A&amp;M University, College Station, TX</Institution>
					</Institutions>
					<ArticleTitle>MrsRF: an efficient MapReduce algorithm for analyzing large collections of evolutionary trees</ArticleTitle>
					<DOI>10.1186/1471-2105-11-S1-S15</DOI>
					<PubDate>2010-01-18</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>11</VolumeId>
					<IssueId>Suppl 1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>MapReduce</Keyword>
						<Keyword>analyzing</Keyword>
						<Keyword>MrsRF</Keyword>
						<Keyword>evolutionary</Keyword>
						<Keyword>trees</Keyword>
						<Keyword>collections</Keyword>
						<Keyword>large</Keyword>
						<Keyword>efficient</Keyword>
						<Keyword>algorithm</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Matthews and Williams; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-S1-S15.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2010-08-20T19:12:18.062844+02:00</DateLoaded>
				</Image>
			</Images>
		</result>
		<result>
			<Publisher xml:lang="en">
				<PublisherInfo>
					<PublisherName>BMC</PublisherName>
					<PublisherLocation/>
				</PublisherInfo>
				<Journal>
					<JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="Unnumbered">
						<JournalID>1471-2105</JournalID>
						<JournalPrintISSN>1471-2105</JournalPrintISSN>
						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>
						<JournalTitle>BMC Bioinformatics</JournalTitle>
						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>
						<JournalSubjectGroup>
							<JournalSubject Type="Primary">Life Sciences</JournalSubject>
							<JournalSubject Priority="1" Type="Secondary">Bioinformatics</JournalSubject>
							<JournalSubject Priority="2" Type="Secondary">Microarrays</JournalSubject>
							<JournalSubject Priority="3" Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>
							<JournalSubject Priority="4" Type="Secondary">Computer Appl. in Life Sciences </JournalSubject>
							<JournalSubject Priority="5" Type="Secondary">Combinatorial Libraries</JournalSubject>
							<JournalSubject Priority="6" Type="Secondary">Algorithms</JournalSubject>
						</JournalSubjectGroup>
					</JournalInfo>
					<Volume>
						<VolumeInfo TocLevels="0" VolumeType="Regular">
							<VolumeIDStart>10</VolumeIDStart>
							<VolumeIDEnd>10</VolumeIDEnd>
							<VolumeIssueCount/>
						</VolumeInfo>
						<Issue IssueType="Regular">
							<IssueInfo TocLevels="0">
								<IssueIDStart>1</IssueIDStart>
								<IssueIDEnd>1</IssueIDEnd>
								<IssueArticleCount/>
								<IssueHistory>
									<PrintDate>
										<Year>2009</Year>
										<Month>2</Month>
										<Day>03</Day>
									</PrintDate>
								</IssueHistory>
								<IssueCopyright>
									<CopyrightHolderName>Lin; licensee BioMed Central Ltd.</CopyrightHolderName>
									<CopyrightYear>2009</CopyrightYear>
								</IssueCopyright>
							</IssueInfo>
							<Article ID="Art1">
								<ArticleInfo ArticleType="Abstract" ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">
									<ArticleID>1471-2105-10-46</ArticleID>
									<ArticleDOI>10.1186/1471-2105-10-46</ArticleDOI>
									<ArticleSequenceNumber/>
									<ArticleTitle Language="En">Is searching full text more effective than searching abstracts?</ArticleTitle>
									<ArticleSubTitle Language="En"/>
									<ArticleCategory>Research article</ArticleCategory>
									<ArticleFirstPage>46</ArticleFirstPage>
									<ArticleLastPage>46</ArticleLastPage>
									<ArticleHistory>
										<Received>
											<Year>2008</Year>
											<Month>10</Month>
											<Day>02</Day>
										</Received>
										<Revised>
											<Year/>
											<Month/>
											<Day/>
										</Revised>
										<Accepted>
											<Year>2009</Year>
											<Month>2</Month>
											<Day>03</Day>
										</Accepted>
									</ArticleHistory>
									<ArticleEditorialResponsibility/>
									<ArticleCopyright>
										<CopyrightHolderName>Lin; licensee BioMed Central Ltd.</CopyrightHolderName>
										<CopyrightYear>2009</CopyrightYear>
									</ArticleCopyright>
									<ArticleGrants Type="OpenChoice">
										<MetadataGrant Grant="OpenAccess"/>
										<AbstractGrant Grant="OpenAccess"/>
										<BodyPDFGrant Grant="Restricted"/>
										<BodyHTMLGrant Grant="Restricted"/>
										<BibliographyGrant Grant="Restricted"/>
										<ESMGrant Grant="Restricted"/>
									</ArticleGrants>
									<ArticleContext>
										<JournalID/>
										<VolumeIDStart>10</VolumeIDStart>
										<VolumeIDEnd>10</VolumeIDEnd>
										<IssueIDStart>1</IssueIDStart>
										<IssueIDEnd>1</IssueIDEnd>
									</ArticleContext>
								</ArticleInfo>
								<ArticleHeader>
									<AuthorGroup>
										<Author AffiliationIDS="I1 I2">
											<AuthorName DisplayOrder="Western">
												<GivenName>Jimmy</GivenName>
												<FamilyName>Lin</FamilyName>
											</AuthorName>
											<Contact>
												<Email>jimmylin@umd.edu</Email>
											</Contact>
										</Author>
										<Affiliation ID="I1">
											<OrgName>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</OrgName>
											<OrgAddress>
												<Postcode/>
												<City/>
												<State/>
											</OrgAddress>
										</Affiliation>
										<Affiliation ID="I2">
											<OrgName>The iSchool, University of Maryland, College Park, Maryland, USA</OrgName>
											<OrgAddress>
												<Postcode/>
												<City/>
												<State/>
											</OrgAddress>
										</Affiliation>
									</AuthorGroup>
									<Abstract ID="Abs1" Language="En">
										<Heading>Abstract</Heading>
										<AbstractSection>
											<Heading>Background</Heading>
											<Para>With the growing availability of full-text articles online, scientists and other consumers of the life sciences literature now have the ability to go beyond searching bibliographic records (title, abstract, metadata) to directly access full-text content. Motivated by this emerging trend, I posed the following question: is searching full text more effective than searching abstracts? This question is answered by comparing text retrieval algorithms on MEDLINE <Superscript>®</Superscript> abstracts, full-text articles, and spans (paragraphs) within full-text articles using data from the TREC 2007 genomics track evaluation. Two retrieval models are examined: <Emphasis Type="Italic">bm25</Emphasis> and the ranking algorithm implemented in the open-source Lucene search engine.</Para>
										</AbstractSection>
										<AbstractSection>
											<Heading>Results</Heading>
											<Para>Experiments show that treating an entire article as an indexing unit does not consistently yield higher effectiveness compared to abstract-only search. However, retrieval based on spans, or paragraphs-sized segments of full-text articles, consistently outperforms abstract-only search. Results suggest that highest overall effectiveness may be achieved by combining evidence from spans and full articles.</Para>
										</AbstractSection>
										<AbstractSection>
											<Heading>Conclusion</Heading>
											<Para>Users searching full text are more likely to find relevant articles than searching only abstracts. This finding affirms the value of full text collections for text retrieval and provides a starting point for future work in exploring algorithms that take advantage of rapidly-growing digital archives. Experimental results also highlight the need to develop distributed text retrieval algorithms, since full-text articles are significantly longer than abstracts and may require the computational resources of multiple machines in a cluster. The MapReduce programming model provides a convenient framework for organizing such computations.</Para>
										</AbstractSection>
									</Abstract>
									<KeywordGroup Language="En">
										<Keyword>abstracts?</Keyword>
										<Keyword>text</Keyword>
										<Keyword>more</Keyword>
										<Keyword>full</Keyword>
										<Keyword>searching</Keyword>
										<Keyword>than</Keyword>
										<Keyword>effective</Keyword>
									</KeywordGroup>
								</ArticleHeader>
								<Body>
									<Section1 ID="Sec_28397">
										<Heading>Background</Heading>
										<Para>The exponential growth of peer-reviewed literature and the breakdown of disciplinary boundaries heralded by genome-scale instruments have made it harder than ever for scientists to find and assimilate all the publications important to their research <CitationRef CitationID="B1">1</CitationRef> . Thus, tools such as search engines are becoming indispensable. Motivated by the desire to develop more effective text retrieval algorithms for consumers of the life sciences literature, this study poses a straightforward question: Is searching full text more effective than searching abstracts? That is, given a particular information need expressed as a query, is the user more likely to find relevant articles searching the full text or searching only the abstracts?</Para>
										<Para>This question is highly relevant today due to the growing availability of full-text articles. In the United States, the NIH Public Access Policy now requires that all research published with NIH funding be made publicly accessible. More broadly, the Open Access movement for the dissemination of scientific knowledge has gained significant traction worldwide. These trends have contributed to the accumulation of full-text articles in public archives such as PubMed Central and on the websites of Open Access journal publishers. The growth of freely-available content represents a significant opportunity for scientists, clinicians, and other users of online retrieval systems – it is now possible to go beyond searching bibliographic data (abstract, title, metadata) in sources such as MEDLINE to directly search contents of the full text. Indeed, Zweigenbaum et al. <CitationRef CitationID="B2">2</CitationRef>
											<CitationRef CitationID="B3">3</CitationRef> identified analysis of full-text articles as one of the frontiers in biomedical text mining.</Para>
										<Para>To be clear, I refer to full-text search as the ability to perform retrieval on the entire textual content of an article (including its title and abstract), whereas abstract search refers to retrieval based only on abstract and title (similar to what is available in PubMed <Superscript>®</Superscript> today). This work focuses on content-based searching only; experiments do not take advantage of metadata (e.g., authors, journal titles, MeSH <Superscript>®</Superscript> descriptors), since they would presumably be available for both full-text and abstract searches.</Para>
										<Para>Not surprisingly, there isn't a straightforward answer to the question posed in this study. I describe experiments with MEDLINE abstracts, full-text articles, and spans (paragraphs) within full-text articles using data from the TREC 2007 genomics track evaluation. The experiments examined two retrieval models: <Emphasis Type="Italic">bm25</Emphasis> and the ranking algorithm implemented in the open-source Lucene search engine. Results show that treating an entire article as an indexing unit does not consistently yield higher effectiveness compared to abstract search. However, retrieval based on spans, or paragraphs-sized segments of full-text articles, consistently outperforms abstract search. Results suggest that highest overall effectiveness may be achieved by combining evidence from spans and full articles. These findings affirm the value of full text collections for text retrieval and provide a starting point for future work in exploring algorithms that take advantage of full text.</Para>
										<Para>One important implication of this work is the need to develop scalable text retrieval algorithms, since full-text articles are significantly longer than abstracts. In the near future, the amount of available content will likely outgrow the capabilities of individual machines, thus requiring the use of clusters for basic processing tasks. I explore this issue with Ivory, a toolkit for distributed text retrieval built on top of Hadoop, an open-source implementation of the MapReduce programming model <CitationRef CitationID="B4">4</CitationRef> .</Para>
										<Section2 ID="Sec_61749">
											<Heading>Related Work</Heading>
											<Para>Despite much optimism among researchers about the potential of full-text articles, it is not completely clear how present techniques for mining the biomedical literature, primarily developed for title, abstract, and metadata (e.g., MeSH descriptors), can be adapted to full text. For some tasks, full text does not appear to offer much beyond title and abstract. For example, consider the Medical Text Indexer (MTI), a production tool at the National Library of Medicine (NLM) that assists human indexers in assigning MeSH descriptors to MEDLINE citations based on title and abstract. Gay et al. <CitationRef CitationID="B5">5</CitationRef> reported their experiences on extending MTI to base MeSH recommendations on full text. After much tuning and careful selection of article sections, a modest gain in effectiveness was achieved. However, the improvements were most likely "not worth the extra effort" (personal communication, Alan Aronson) and there are currently no plans to integrate full-text capabilities into the NLM article processing pipeline. Furthermore, it is unclear whether the particular treatment of article sections is generalizable beyond the relatively small set of articles examined in the study.</Para>
											<Para>Other researchers have also tried to exploit information in full-text articles for various text mining tasks. For example, Yu et al. <CitationRef CitationID="B6">6</CitationRef> used surface patterns to extract synonyms for gene and protein names, and reported higher precision on full text than on abstracts. More recently, Seki and Mostafa <CitationRef CitationID="B7">7</CitationRef> explored an "inference network" approach to mining gene-disease associations. Taking advantage of full text yields a small gain in effectiveness (consistent with the MTI findings). However, the results were based on a small collection of articles and can only be characterized as preliminary. As with the MTI experiments, there remain questions about the generalizability of the results beyond those articles examined.</Para>
											<Para>There is no doubt that a significant amount of information is contained in well-written abstracts. Journal requirements for structured abstracts assist authors in crafting highly-informative prose that covers the key aspects of their work. In the clinical domain, for example, Demner-Fushman et al. <CitationRef CitationID="B8">8</CitationRef> showed that information in abstracts is sufficient to identify articles that are potentially useful for clinical decision support. In the biomedical domain, Yu <CitationRef CitationID="B9">9</CitationRef> examined text associated with images in articles and concluded that sentences in the abstract suffice to summarize image content. In fact, she suggested that abstract sentences may actually be superior, since associated text in the article typically describes only experimental procedures and often does not include the findings or conclusions of an experiment. In a related study, Yu and Lee <CitationRef CitationID="B10">10</CitationRef> discovered that 88% of figures and 85% of tables can be summarized by sentences in the abstract, and that 67% of abstract sentences correspond to images in the full-text articles. These results demonstrate that, at least for some tasks, it is unclear what additional value full-text content adds beyond title, abstract, and metadata.</Para>
											<Para>Additionally, researchers have compared the term content of abstracts with that of full text. Shah et al. <CitationRef CitationID="B11">11</CitationRef> examined a collection of articles and found that abstracts were the richest source of relevant keywords. This finding was echoed by Schuemie et al. <CitationRef CitationID="B12">12</CitationRef> , who concluded that the density of useful information is highest in the abstract, but information coverage in full text is greater than that of abstracts. However, the practicality of mining full-text articles is unclear due to increased computational requirements. Both these articles focused on characterizing texts and did not examine the impact of abstract vs. full text on a text-mining task.</Para>
											<Para>There has been much work on processing full-text content, for example, the automatic extraction of gene names and protein interactions in the BioCreative evaluations <CitationRef CitationID="B13">13</CitationRef>
												<CitationRef CitationID="B14">14</CitationRef> . However, that body of research differs significantly from the goals of this study in that I am primarily interested in differences between full text and abstracts, and the impact of these differences on effectiveness in text retrieval. In gene identification and related information extraction tasks, the text collection is fixed, while researchers attempt to develop effective algorithms (as determined by some standard metric such as F-measure). Specifically, the tasks are designed in a manner such that the source of the text is not a relevant factor. Experiments in this article, on the other hand, focus both on algorithms <Emphasis Type="Italic">and</Emphasis> data.</Para>
											<Para>The problem of retrieving sub-document segments of text has previously been explored in the information retrieval literature <CitationRef CitationID="B15">15</CitationRef>
												<CitationRef CitationID="B16">16</CitationRef>
												<CitationRef CitationID="B17">17</CitationRef>
												<CitationRef CitationID="B18">18</CitationRef>
												<CitationRef CitationID="B19">19</CitationRef>
												<CitationRef CitationID="B20">20</CitationRef>
												<CitationRef CitationID="B21">21</CitationRef>
												<CitationRef CitationID="B22">22</CitationRef>
												<CitationRef CitationID="B23">23</CitationRef>
												<CitationRef CitationID="B24">24</CitationRef> ; the task is often called passage retrieval. Recently, there has been substantial interest in XML retrieval, which shares many of the same issues as passage retrieval since XML documents naturally support retrieval at different granularities (i.e., every XML element is a potential result). The locus of research activity resides in the INitiative for Evaluation of XML Retrieval (INEX), an annual evaluation forum that began in 2002 <CitationRef CitationID="B25">25</CitationRef> . For several years, the evaluation used a collection of XML-encoded journal articles from the IEEE Computer Society. Most relevant to this work, the genomics "track" at the 2007 Text Retrieval Conference (TREC) has explored passage retrieval in the life sciences domain. The overview paper <CitationRef CitationID="B26">26</CitationRef> provides an entry point into that literature (also see Section 5.1). Experiments reported in this article used the test collection from the TREC 2007 evaluation.</Para>
											<Para>There is one important difference between this work and the research discussed above. Whereas I ask the question "Is searching full text more effective than searching abstracts?", previous work simply assumes that the answer is <Emphasis Type="Italic">yes</Emphasis> . Research papers on passage retrieval often take for granted that retrieving passages provides value to the user. The INEX evaluations implicitly assume that retrieving XML fragments is desirable (although there have been studies that explore the validity of this assumption <CitationRef CitationID="B27">27</CitationRef>
												<CitationRef CitationID="B28">28</CitationRef> ). Since widespread availability of full text is a relatively recent phenomenon in the life sciences, and searching abstracts has been available for decades (and shown to be reasonably effective), it is worth questioning the premise that searching full text is inherently superior. Focusing on this question, however, certainly does not diminish the contributions of previous work: to the extent that full text is shown to be valuable for text retrieval in this specific domain, findings from related work can be leveraged to inform the development of full-text retrieval algorithms for the life sciences literature.</Para>
										</Section2>
										<Section2 ID="Sec_05486">
											<Heading>Comparison of Full Text and Abstracts</Heading>
											<Para>In comparing search effectiveness on full-text articles and abstracts, it makes sense to begin by discussing their characteristics and enumerating potential advantages and disadvantages. Such an analysis could guide the interpretation of experimental results.</Para>
											<Para>Length is the most obvious difference between full-text articles and abstracts – the former provides systems with significantly more text to process. Most information retrieval algorithms today build on "bag of words" representations, where text is captured as a weighted feature vector with each term as a feature <CitationRef CitationID="B29">29</CitationRef> , or as a probability distribution over terms in the case of language modeling approaches <CitationRef CitationID="B30">30</CitationRef>
												<CitationRef CitationID="B31">31</CitationRef> . These models derive from statistical properties such as term frequency and document frequency. More text could yield a more robust characterization of these statistics (for example, in the language modeling framework, longer documents require less smoothing). However, the potential downside is that full text may introduce noise. For example, an article may contain an elaborate discussion about related work that does not directly pertain to the article's main focus. Articles often contain conjectures (that turn out to be incorrect) or future work (which may or may not be fruitful lines of inquiry). In these cases, term statistics may be misleading due to the presence of "distractors" that dilute the impact of important terms. In contrast, abstracts focus on the key ideas presented in the articles and little else. Similar observations have been made with respect to collections of newswire documents (e.g., <CitationRef CitationID="B17">17</CitationRef> ). Many news stories are written about one central topic, but contain many aspects that are only loosely connected – in these cases, a global characterization of the entire document may not accurately capture what the article is "about".</Para>
											<Para>Another challenge with full-text articles is the variety of formats in which they are distributed. Whereas there is a small number of formats for encoding MEDLINE abstracts, full-text collections can be found in XML, SGML, PDF, and even raw HTML (e.g., the result of Web crawls). Each format is associated with idiosyncrasies that make pre-processing a challenge. Just to give one example, there are numerous ways to encode Greek symbols ( <Emphasis Type="Italic">α</Emphasis> , <Emphasis Type="Italic">β</Emphasis> , ...) – sometimes, they are directly encoded in extended character sets (e.g., Unicode); often, they are "written-out" (e.g., alpha, beta, ...); in other cases, they are encoded as HTML entities (e.g., &amp;alpha;). The prevalence of special characters in the literature complicates seemingly simple tasks such as tokenization. The problem is further exacerbated when one tries to combine full-text collections from different sources (in different formats).</Para>
											<Para>Access to full text is necessary for certain fine-grained retrieval tasks, e.g., image search <CitationRef CitationID="B3">3</CitationRef> . Recently, there has been substantial interest in this problem <CitationRef CitationID="B32">32</CitationRef>
												<CitationRef CitationID="B33">33</CitationRef>
												<CitationRef CitationID="B34">34</CitationRef> , based on both image features and features derived from text associated with images, e.g., captions and sentences in which the figures are referenced. Full text is obviously essential for extracting these features. For other tasks, access to full text may be desirable, e.g., question answering. Unlike a search engine, a question answering system attempts to return a response that directly answers the user's question. Due to their specificity, users' questions sometimes may not be sufficiently addressed by abstracts – often, useful nuggets of information may be found in parts of articles that are not well reflected in the abstract (e.g., in the related work or discussion sections).</Para>
											<Para>However, it is also important to take into account other considerations in information seeking. Since many users are uncomfortable reading large amounts of text on screen, they often print out journal articles first before reading them in detail. In these cases, finer-grained passage and image access capabilities are most useful in helping users decide what articles they want to read, rather than directly answering users' questions. Often, complex information needs such as those faced by biologists cannot be answered with a paragraph or an image – instead, information returned by the system must be considered in the context of the <Emphasis Type="Italic">entire</Emphasis> article from which the segment was extracted (for example, so that the scientist can verify the experimental procedure, consider alternative hypotheses mentioned in the discussion section, etc.). For this reason, I focus on a system's ability to identify relevant articles. This corresponds to <Emphasis Type="Italic">ad hoc</Emphasis> retrieval, a task that has been well studied by information retrieval researchers in large-scale evaluations such as TREC.</Para>
											<Para>Based on this discussion, it is clear that there are advantages and disadvantages to full-text retrieval. However, the extent to which these various factors balance out and affect search effectiveness can only be determined empirically. The next section presents a series of experiments that explore these issues.</Para>
										</Section2>
									</Section1>
									<Section1 ID="Sec_51400">
										<Heading>Results</Heading>
										<Section2 ID="Sec_30884">
											<Heading>Test Collection</Heading>
											<Para>Retrieval experiments in this article were conducted with data from the TREC 2007 genomics track evaluation <CitationRef CitationID="B26">26</CitationRef> , which used a collection of 162,259 full-text articles from Highwire Press. MEDLINE records that correspond to the full-text articles were also provided as supplementary data. To standardize system output, the organizers of the evaluation divided up the entire collection into 12.6 million "legal spans" (i.e., paragraphs) that represent the basic units of retrieval. The test collection contains 36 information needs, called "topics", and relevance judgments, which are lists of legal spans that were assessed by humans to be relevant for each topic. Despite the availability of relevance judgments at the level of spans, I focused on article-level relevance in order to support a meaningful comparison of abstract and full-text search. Section 5.1 describes this test collection in more detail.</Para>
										</Section2>
										<Section2 ID="Sec_41930">
											<Heading>Retrieval Conditions</Heading>
											<Para>A matrix experiment was devised with two different retrieval models (in three separate implementations) and three different data conditions. The goal was to compare the effectiveness of different experimental settings, as quantified by standard retrieval metrics (see Section 2.4). The retrieval models examined were:</Para>
											<Para>• The Okapi <Emphasis Type="Italic">bm25</Emphasis> ranking algorithm <CitationRef CitationID="B35">35</CitationRef>
												<CitationRef CitationID="B36">36</CitationRef> (described in Section 5.2), as implemented in Ivory, a toolkit for distributed text retrieval. Ivory was developed with Hadoop, an open-source implementation of the MapReduce programming model (see Section 5.3 for details).</Para>
											<Para>• The ranking algorithm implemented in the open-source search engine Lucene, which represents a modified <Emphasis Type="Italic">tf.idf</Emphasis> retrieval model (described in Section 5.2). Due to its popularity and ease of use, Lucene provides a good baseline for comparison. This ranking algorithm was also implemented in Ivory, primarily for evaluation of efficiency and scalability (see Section 2.6).</Para>
											<Para>The matrix design consisted of three different data conditions. The following indexes were built (one set of indexes for Lucene, another set of indexes for Ivory):</Para>
											<Para>• Abstract index, built on the abstracts and titles of articles in the Highwire collection, taken from the MEDLINE records. Each abstract was considered a "document" for retrieval purposes.</Para>
											<Para>• Article index, built on the full-text articles in the Highwire collection (which include abstracts and titles). The entire text of each article was considered a "document" for retrieval purposes.</Para>
											<Para>• Span index, built on the legal spans in the Highwire collection. In this experimental condition, each of the 12.6 million spans in the collection was treated as if it were a "document".</Para>
											<Para>For each cell in the matrix experiment, I performed a retrieval run with 36 queries, taken verbatim from the TREC 2007 genomics track test data (see Section 5.1 for examples). With the abstract and article indexes, retrieval results consisted of 1000 article ids in rank order. For the span index, the ranked results consisted of span ids. As a post-processing step, I applied a script to create a ranking of articles from a ranking of spans, using two different methods described by Hearst and Plaunt <CitationRef CitationID="B17">17</CitationRef> :</Para>
											<Para>• Maximum of supporting spans (max): the score for an article is computed as the maximum of scores for all spans contained in that article. Article ids were sorted by this score in descending order. This method favors articles that have a single high-scoring span.</Para>
											<Para>• Sum of supporting spans (sum): the score for an article is computed as the sum of scores for all spans contained in that article. Article ids were sorted by this score in descending order. This method favors articles that have many potentially-relevant spans.</Para>
											<Para>In order to ensure that the post-processing script generated a ranked list of 1000 articles (same as the abstract and article conditions), 5000 spans were retrieved initially.</Para>
										</Section2>
										<Section2 ID="Sec_35708">
											<Heading>Evidence Combination</Heading>
											<Para>An obvious extension to the matrix experiment discussed in the previous section is to integrate evidence from multiple sources. As an initial exploration, I conducted a series of experiments that combined the results of span retrieval with either article or abstract retrieval. In both cases, runs were combined by first normalizing each set of scores and then averaging scores from the different runs. The goal of this experiment was to examine the effect of combining content representations at different granularities. Note that in principle a simple average of the scores could be replaced with weighted linear interpolation (or some other evidence combination technique), but in absence of a principled approach to determining parameters, I opted not to explore this option for fear of overtraining on limited data.</Para>
										</Section2>
										<Section2 ID="Sec_56943">
											<Heading>Evaluation Metrics</Heading>
											<Para>For all experimental conditions, I arrived at a ranking of 1000 articles, which supported a meaningful comparison across all data conditions. To evaluate effectiveness, three different metrics were collected:</Para>
											<Para>• Mean average precision (MAP), the single-point metric of effectiveness most widely accepted by the information retrieval community. The standard cutoff of 1000 hits was used.</Para>
											<Para>• Precision at 20 (P20), the fraction of articles in the top twenty results that are relevant. The cutoff of twenty equals the number of hits on a result page in the present PubMed interface. In a Web environment, many searchers only focus on the first page of results.</Para>
											<Para>• Interpolated precision at recall of 50% (IP@R50), which attempts to capture the experience of a dedicated, recall-oriented searcher (e.g., scientist conducting a literature search). This metric quantifies the amount of irrelevant material a user must tolerate in order to find half of the relevant articles – the higher the IP@R50, the less "junk" a user must sort through. In this study, I characterize IP@R50 as a "recall-oriented" metric. The recall level of 50% was arbitrarily selected.</Para>
											<Para>Section 2.5 focuses on retrieval effectiveness with Ivory, comparing <Emphasis Type="Italic">bm25</Emphasis> and the implementation of Lucene's ranking algorithm. Section 2.6 focuses on efficiency, comparing Lucene with the implementation of its ranking algorithm in Ivory.</Para>
										</Section2>
										<Section2 ID="Sec_57972">
											<Heading>Retrieval Effectiveness</Heading>
											<Para>Results of the matrix experiment described in Section 2.2 with Ivory are presented in Table <InternalRef RefID="T1">1</InternalRef> . The three parts of the table show effectiveness in terms of MAP, P20, and IP@R50. The columns report figures for the two retrieval models: <Emphasis Type="Italic">bm25</Emphasis> and the Ivory implementation of Lucene's ranking algorithm. Abstract retrieval (i.e., retrieval using the abstract index) is taken as the baseline, and the table provides relative differences with respect to this condition. The Wilcoxon signed-rank test was applied in all cases to assess the statistical significance of the results.</Para>
											<Table Float="No" ID="T1">
												<Caption Language="En">
													<CaptionNumber>Table 1</CaptionNumber>
													<CaptionContent>
														<SimplePara>Effectiveness of <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm on abstracts, full-text articles, and spans from full text.</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="3">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">MAP</Emphasis>
																</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.163</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.129</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.146 (-11%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.235 (+82%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.240 (+47%)**</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.206 (+60%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (sum)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.192 (+18%)*</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.198 (+54%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">P20</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.322</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.293</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.158 (-51%)**</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.353 (+20%)*</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.357 (+11%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.332 (+13%)°</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (sum)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.314 (-3%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.317 (+8%)*</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">IP@R50</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.110</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.090</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.163 (+48%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.222 (+146%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.212 (+93%)**</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.189 (+109%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (sum)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.149 (+36%)*</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.159 (+77%)**</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
												<tfooter>
													<SimplePara>For all metrics, relative improvements over baseline are shown; ** = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01); * = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.05); ° = not significant.</SimplePara>
												</tfooter>
											</Table>
											<Para>Results show that for abstract retrieval, <Emphasis Type="Italic">bm25</Emphasis> is significantly more effective in terms of MAP than the Lucene ranking algorithm ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01). Comparing abstract retrieval with article retrieval (i.e., retrieval using the article index), MAP is significantly higher ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01) for Lucene but differences are not statistically significant for <Emphasis Type="Italic">bm25</Emphasis> . For the Lucene ranking algorithm, article retrieval significantly outperforms abstract retrieval for the other two metrics as well. On the other hand, for <Emphasis Type="Italic">bm25</Emphasis> , article retrieval either hurts (P20), or doesn't have a significant impact (IP@P50). Article retrieval, which simply treats full-text articles as longer "documents", does not appear to yield consistent gains in effectiveness.</Para>
											<Para>For retrieval using the span index, the "max" strategy for generating article rankings appears to be more effective than the "sum" strategy. In terms of MAP and IP@R50, both span retrieval strategies significantly outperform retrieval with the abstract index, for both <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm. For IP@R50, in particular, the gains are quite substantial. However, span retrieval does not appear to have a significant impact on P20 compared to the abstract retrieval baseline.</Para>
											<Para>Table <InternalRef RefID="T2">2</InternalRef> shows the results of significance testing between article retrieval and span retrieval with the "max" strategy (the more effective, and thus more interesting, of the two strategies). For <Emphasis Type="Italic">bm25</Emphasis> , article retrieval is significantly worse in terms of MAP and P20. None of the differences are significant for the Lucene ranking algorithm. Note that for these comparisons, there are often large differences in per-topic scores, but in many cases one run does not consistently outperform another – for this reason, substantial differences in the mean may not necessarily yield statistical significance. These results suggest that span retrieval is at least as effective as treating the entire article as an indexing unit, and at least in a few cases, span retrieval is superior.</Para>
											<Table Float="No" ID="T2">
												<Caption Language="En">
													<CaptionNumber>Table 2</CaptionNumber>
													<CaptionContent>
														<SimplePara>Results of significance testing comparing article retrieval with span retrieval ("max" strategy).</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="3">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>MAP</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">p</Emphasis> &lt; 0.01</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">n.s.</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>P20</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">p</Emphasis> &lt; 0.01</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">n.s.</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>IP@R50</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">n.s.</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">n.s.</Emphasis>
																</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
											<Para>Results for the evidence combination experiments are shown in Table <InternalRef RefID="T3">3</InternalRef> , where span retrieval is combined with abstract and article retrieval. In the interest of space, effectiveness metrics are only shown for the "max" strategy. Relative differences are shown with respect to the span retrieval baseline. As with before, the Wilcoxon signed-rank test was applied in all cases to assess the statistical significance of the results. For the Lucene ranking algorithm, combining span retrieval with article retrieval yields significant gains for all three metrics. Other differences are not statistically significant.</Para>
											<Table Float="No" ID="T3">
												<Caption Language="En">
													<CaptionNumber>Table 3</CaptionNumber>
													<CaptionContent>
														<SimplePara>Effectiveness of <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm combining evidence from spans with evidence from abstracts and articles.</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="3">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">MAP</Emphasis>
																</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.240</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.206</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max) + Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.257 (+7%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.216 (+5%)°</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max) + Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.257 (+7%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.262 (+27%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">P20</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.357</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.332</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max) + Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.382 (+7%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.349 (+5%)°</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max) + Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.343 (-4%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.404 (+22%)**</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Bold">IP@R50</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.212</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.189</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max) + Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.215 (+1%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.190 (+1%)°</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (max) + Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>0.257 (+21%)°</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>0.244 (+29%)**</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
												<tfooter>
													<SimplePara>For all metrics, relative improvements over baseline are shown; ** = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01); * = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.05); ° = not significant.</SimplePara>
												</tfooter>
											</Table>
											<Para>Table <InternalRef RefID="T4">4</InternalRef> attempts to summarize findings from all these experiments by establishing a partial rank order of different experimental conditions (based on significance testing), in terms of each effectiveness metric and retrieval model. The "sum" strategy for span retrieval is not considered since the alternative "max" strategy appears to be more effective. In all cases, integrating span-level evidence with article-level evidence either yields the highest effectiveness or is not significantly worse than the condition that yields the highest effectiveness. This suggests that combining article content and span-level analysis is an effective approach to exploiting full text in the life sciences.</Para>
											<Table Float="No" ID="T4">
												<Caption Language="En">
													<CaptionNumber>Table 4</CaptionNumber>
													<CaptionContent>
														<SimplePara>Comparison of different experimental conditions for <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm.</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="3">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara>Model</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>Metric</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Comparison</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>
																	<Emphasis Type="Italic">bm25</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>MAP</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Span (max) + Article, Span (max) &gt;&gt; Abstract, Article</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>P20</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Span (max) + Article, Span (max), Abstract &gt;&gt; Article</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>IP@R50</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Span (max) + Article &gt;&gt; Abstract, Article; Span (max) &gt;&gt; Abstract</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Lucene</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>MAP</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Span (max) + Article &gt;&gt; Span (max), Article &gt;&gt; Abstract</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>P20</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Span (max) + Article &gt; Span (max), Article; Article &gt; Abstract</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>IP@R50</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Span (max) + Article &gt; Span (max), Article &gt;&gt; Abstract</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
												<tfooter>
													<SimplePara>
														<Emphasis Type="Italic">A</Emphasis> &gt;&gt; <Emphasis Type="Italic">B</Emphasis> indicates that <Emphasis Type="Italic">A</Emphasis> is significantly better than <Emphasis Type="Italic">B</Emphasis> ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01); <Emphasis Type="Italic">A</Emphasis> &gt; <Emphasis Type="Italic">B</Emphasis> indicates that <Emphasis Type="Italic">A</Emphasis> is significantly better than <Emphasis Type="Italic">B</Emphasis> ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.05);</SimplePara>
												</tfooter>
											</Table>
										</Section2>
										<Section2 ID="Sec_90572">
											<Heading>Retrieval Efficiency</Heading>
											<Para>Given that there is value in searching full text, the question of efficiency must be addressed. The potential effectiveness gains of full text come at a significant cost in terms of dataset size. In its compressed form as distributed, the collection of 162,259 full-text articles from Highwire Press occupies 3.28 GB (12.6 GB uncompressed). The corresponding MEDLINE abstracts, also in compressed form, take up only 139 MB (502 MB uncompressed). The difference in size is more than an order of magnitude – which means that algorithms must process significantly more data to realize the potential gains of full-text content. Articles in the Highwire collection contain on average 4148 terms (after stopword removal), while abstracts contain on average only 142 terms. For reference, spans average 66 terms, or a bit less than half the length of abstracts.</Para>
											<Para>Open-source text retrieval systems available today, designed to run on individual machines, would have no difficulty handling the Highwire collection and even collections that are much larger. However, these articles represent only a small fraction of the material already available or soon to be available. Based on recent estimates, records are added to MEDLINE at a rate of approximately 65 k per month. A lower bound on the growth of available full-text content can be estimated by examining the growth of PubMed Central. Over the past two years, the digital archive has grown by approximately 40 k articles per month. However, the growth rate is uneven due to retrospective conversion; more recently, this figure is closer to 20 k articles per month. Nevertheless, full-text collections will inevitably outgrow the capabilities of individual machines – the only practical recourse is to distribute computations across multiple machines in a cluster.</Para>
											<Para>I have been exploring the requirements of scaling to larger datasets with Ivory, a toolkit for distributed text retrieval implemented in Java using Hadoop, which is an open-source implementation of the MapReduce framework <CitationRef CitationID="B4">4</CitationRef> . Section 5.3 describes Ivory in more detail, but an evaluation of its efficiency is presented here. Specifically, I compare the original implementation of Lucene with the Ivory implementation of the Lucene ranking algorithm (to factor out the effects of different retrieval algorithms).</Para>
											<Para>All experiments were conducted with Amazon's Elastic Compute Cloud (EC2) service, which allows one to dynamically provision clusters of different sizes. EC2 is an example of a "utility computing" service, where anyone can "rent" computing cycles at a reasonable cost. For this work, EC2 provided a homogeneous computing environment that supports easy comparison of different cluster configurations. The basic unit of computing resource in EC2 is the small instance-hour, the virtualized equivalent of a processor core with 1.7 GB of memory, running for an hour. I experimented with the following configurations:</Para>
											<Para>• Lucene (version 2.0), running on a single EC2 instance. Default settings "out of the box" were used for all experiments.</Para>
											<Para>• Ivory (with Hadoop version 0.17.0), running on an EC2 cluster with 10 slave instances (plus 1 instance for the master). This is comparable to a cluster with 10 cores.</Para>
											<Para>• Same as above, except with 20 slave instances, comparable to a cluster with 20 cores.</Para>
											<Para>As an aside, note that physical equivalents of these clusters are quite modest by today's standards. Quad-core processors are widely available in server-class machines, and with dual processor packages, a 20-core cluster is within the means of most research groups.</Para>
											<Para>Running times for index construction for the three different configurations are shown in Table <InternalRef RefID="T5">5</InternalRef> . Lucene, which was not designed to run on a cluster, takes over a day to build the span index (containing 12.6 million spans). With either cluster configurations, indexing takes less than an hour. These figures should be interpreted with the caveat that Lucene builds a richer index that supports complex query operators and that I am comparing a single core to clusters. However, the point is that Lucene cannot be easily adapted to run on multiple machines and thus indexing speed is fundamentally bound by the disk bandwidth of one machine. A cluster can take advantage of the aggregate disk bandwidth of many machines, and MapReduce provides a convenient model for organizing these disk operations (see Section 5.3).</Para>
											<Table Float="No" ID="T5">
												<Caption Language="En">
													<CaptionNumber>Table 5</CaptionNumber>
													<CaptionContent>
														<SimplePara>Time required for index construction, comparing Lucene to different Ivory configurations.</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="4">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Lucene (1 core)</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory (10 cores)</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Ivory (20 cores)</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Abstract</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>1 h 00 m 58 s</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>1 m 32 s</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>1 m 07 s</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Article</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>19 h 09 m 23 s</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>17 m 21 s</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>9 m 57 s</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>27 h 10 m 46 s</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>39 m 58 s</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>24 m 56 s</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
											<Para>The speedup demonstrated by Ivory is important because time for inverted index construction places an upper bound on how fast a researcher can explore the solution space for algorithms that require manipulating the index. Since research in information retrieval is fundamentally empirical in nature, progress is driven by iterative experimentation. Thus, exceedingly long experimental cycles represent a potential impediment to advances in the state of the art.</Para>
											<Para>In terms of retrieval, running times on the entire set of 36 topics from the TREC 2007 genomics track are shown in Table <InternalRef RefID="T6">6</InternalRef> . The gains in efficiency are not quite as dramatic, but still substantial. In its present implementation, Ivory was designed for batch-style experiments, not real-time retrieval (see Section 5.3 for more discussion). These numbers are therefore only presented for reference, and should not be taken as indicative of efficiency in operational settings (where techniques such as caching can greatly reduce retrieval latency). My experiments primarily focus on indexing efficiency, which is more important for the issues explored in this study.</Para>
											<Table Float="No" ID="T6">
												<Caption Language="En">
													<CaptionNumber>Table 6</CaptionNumber>
													<CaptionContent>
														<SimplePara>Time required for retrieval runs, comparing Lucene to different Ivory configurations.</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="4">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>Lucene (1 core)</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Ivory (10 cores)</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Ivory (20 cores)</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Abstract (1000 hits)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>1 m 42 s</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>51 s</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>40 s</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Article (1000 hits)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>7 m 00 s</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>1 m 51 s</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>1 m 09 s</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Span (5000 hits)</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>21 m 32 s</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>11 m 57 s</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>8 m 25 s</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
											<Para>Taking advantage of full-text content requires more computational resources to cope with the increased quantities of data. Inevitably, full-text collections will outgrow retrieval systems designed to run on single machines – necessitating the development of distributed algorithms. The MapReduce framework provides a practical solution for distributed text retrieval.</Para>
										</Section2>
									</Section1>
									<Section1 ID="Sec_90994">
										<Heading>Discussion</Heading>
										<Para>Is searching full text more effective than searching abstracts? The answer appears to be <Emphasis Type="Italic">yes</Emphasis> . Furthermore, experimental results suggest that span-level analysis provides a promising strategy for taking advantage of full-text content. Whereas simply treating entire articles as indexing units yields mixed results, span retrieval consistently outperforms abstract retrieval. Combining span- and article-level evidence yields the highest effectiveness across a range of experimental conditions.</Para>
										<Para>Why does span retrieval work? Further analysis of results in Section 2.5 reveals some interesting observations. Focusing on the "max" strategy, Table <InternalRef RefID="T1">1</InternalRef> shows that, overall, span retrieval has a relatively small effect on precision (seen in the P20 scores), but a large impact on recall (seen in the IP@R50 scores). This makes sense: key ideas in an article are likely reinforced multiple times, often in slightly different ways. This potentially alleviates mismatches between query terms and terms used by authors – in essence, span indexing gives a retrieval algorithm multiple opportunities to identify a relevant article. This enhanced recall leads to higher overall effectiveness in terms of MAP.</Para>
										<Para>In general, the "max" strategy for generating article rankings from span rankings appears to be more effective than the "sum" strategy. Why is this so? One possibility is the issue of length normalization. In the current implementation, longer articles tend to have higher scores simply because they contain more spans; thus, there is an inherent bias in the "sum" strategy. Length normalization plays an important role in text retrieval <CitationRef CitationID="B37">37</CitationRef>
											<CitationRef CitationID="B38">38</CitationRef> , but I leave a thorough exploration of this issue for future work.</Para>
										<Para>The findings in this article pave the way for future advances in full-text retrieval algorithms for the life sciences, which can draw from a wealth of previous work in the information retrieval literature on passage retrieval, XML retrieval, etc. In fact, the effectiveness of span retrieval confirms a well-known finding: ranking algorithms benefit from techniques that exploit document structure, particularly for longer documents.</Para>
										<Para>Remaining focused on the problem of using full-text content to improve article ranking, how in general can article structure be exploited? Within the space of "bag of words" models, strategies can be organized in terms of two questions:</Para>
										<Para>• At what levels of granularity should retrieval algorithms build representations of full-text content?</Para>
										<Para>• How should evidence from multiple representations be combined to rank articles?</Para>
										<Para>These two questions provide context for future work. As a start, I have experimented with two different indexing granularities (full articles and spans), but alternative approaches include sliding windows <CitationRef CitationID="B18">18</CitationRef>
											<CitationRef CitationID="B20">20</CitationRef> , multi-paragraph segments <CitationRef CitationID="B19">19</CitationRef> , hierarchically-overlapping segments <CitationRef CitationID="B38">38</CitationRef>
											<CitationRef CitationID="B39">39</CitationRef> , and segments based on topic shifts <CitationRef CitationID="B17">17</CitationRef> . There are many strategies for integrating evidence from multiple content representations and representations at different granularities (e.g., <CitationRef CitationID="B40">40</CitationRef> ). I have begun to examine some of these strategies, but there are many more possibilities yet to be explored. For example, differential treatment of article sections may improve effectiveness since some sections are more important than others, i.e., more likely to contain relevant information. Earlier work on a smaller collection of documents from the Federal Register illustrated the potential of assigning weights to different section types <CitationRef CitationID="B19">19</CitationRef> . More recently, Tbahriti et al. <CitationRef CitationID="B41">41</CitationRef> found section-specific weights to be helpful for retrieval in the context of structured abstracts in the life sciences. However, one challenge that must be overcome for this strategy to work on a large scale is the lack of standardized section headings – both across journals and different types of articles (e.g., research vs. review articles).</Para>
										<Para>In this work I have focused on exploiting full-text content to better rank articles. Alternatively, one could leverage full text to directly return relevant information, i.e., with passage retrieval techniques. This was, in fact, the original design of the TREC 2007 genomics track evaluation. Of course, this begs the question: How are they related? In the information retrieval literature, a distinction is made between passage retrieval and document retrieval that exploits passage-level evidence. This exactly parallels the present discussion about retrieving segments of full-text content versus leveraging full-text content to enhance article retrieval. However, I argue that the two are complementary from a user interface point of view.</Para>
										<Para>Leaving aside non-traditional search interfaces, a retrieval system must ultimately present users with lists of results. Consider the two approaches to exploiting full text in this context:</Para>
										<Para>Even if the primary goal of a system is to leverage full-text content to enhance article retrieval, results have to be presented in a manner that suggests the relevance of an article. This necessarily involves creating some type of surrogate for the article, which can either be indicative or informative. Common techniques for generating such surrogates include displaying titles and metadata (as with the current PubMed interface) and short keyword-in-context extracts (as with Google Scholar). The first is primarily indicative, while the second aims to be informative. Extraction of informative text segments from articles is essentially a passage retrieval task – and in some cases, this information may already be available as a natural byproduct of the article ranking process. For example, in algorithms that integrate evidence from multiple spans within an article, those salient spans might form the basis of generating article surrogates.</Para>
										<Para>Even if the primary goal of a system is to directly retrieve relevant passages, the passages must still be couched within the context of the article containing the passages (to provide users with pointers back to the original content). In addition, there will be cases where a passage retrieval algorithm suggests multiple passages extracted from the same article (unless this is explicitly suppressed, which may lead to loss of potentially-important information). To facilitate result presentation, it would be desirable to group passages by the articles that contain them – which essentially involves article ranking.</Para>
										<Para>In other words, the distinction between retrieving passages and retrieving articles becomes blurred when one considers elements of the user interface. Both approaches must grapple with the same issues, thus creating synergies where algorithms specifically developed for one purpose may be useful for the other.</Para>
									</Section1>
									<Section1 ID="Sec_10939">
										<Heading>Conclusion</Heading>
										<Para>Experiments in this article with the TREC 2007 genomics track test collection illustrate that there is significant value in searching full-text articles. Given the rapidly growing availability of full text in online digital archives, this is a positive development for scientists who depend on access to the literature for their research. Results show that retrieval at the level of paragraphs within full text is significantly more effective than searching abstracts only. Combining span- and article-level evidence appears to yield the best results. However, much work remains in developing effective full-text retrieval algorithms for the life sciences literature: toward that end, this work presents a first step.</Para>
										<Para>One important issue in moving from searching abstracts to searching full text is that of scalability. Gains in effectiveness come at a cost – algorithms must process significantly more text. Although currently-available tools designed to run on single machines suffice to handle present test collections, it is clear that future systems must distribute computations across multiple machines to cope with ever-growing quantities of text. As illustrated by Ivory, MapReduce provides a convenient framework for distributed text retrieval. The combination of greater effectiveness enabled by full text and greater efficiency enabled by cluster computing paves the way for exciting future developments in information access tools for the life sciences.</Para>
									</Section1>
									<Section1 ID="Sec_95277">
										<Heading>Methods</Heading>
										<Section2 ID="Sec_01663">
											<Heading>Test Collection</Heading>
											<Para>Experiments reported in this article were conducted with the test collection from the TREC 2007 genomics track evaluation <CitationRef CitationID="B26">26</CitationRef> . A test collection is a standard laboratory tool for evaluating text retrieval systems, which consists of three components:</Para>
											<Para>• a collection – documents on which retrieval is performed,</Para>
											<Para>• a set of information needs – written statements describing the desired information (called "topics"), which are usually provided as queries to the system, and</Para>
											<Para>• relevance judgments – records specifying the documents that should be retrieved in response to each information need (i.e., which documents are relevant to each topic).</Para>
											<Para>The use of test collections to assess the effectiveness of text retrieval algorithms is a well-established methodology in the information retrieval literature, dating back to the Cranfield experiments in the 60's <CitationRef CitationID="B42">42</CitationRef> . These tools enable rapid, reproducible experiments in a controlled setting without requiring manual assessment. In modern information retrieval research, test collections are created through large-scale evaluations, such as the Text Retrieval Conferences (TRECs) sponsored by the U.S. National Institute of Standards and Technology (NIST) <CitationRef CitationID="B43">43</CitationRef> . TREC is an annual evaluation forum that draws together researchers from around the world to work on shared problems in different "tracks". Over the years, TREC has explored a wide variety of problems ranging from multimedia retrieval to spam detection. The genomics track was dedicated to exploring biomedical text retrieval.</Para>
											<Para>The TREC 2007 genomics track used a collection of 162,259 full-text articles assembled in 2006. These articles came from the electronic distribution of 49 genomics-related journals from Highwire Press. The articles were distributed in HTML, which preserved formatting, structure, table and figure legends, etc. In addition, the organizers gathered MEDLINE records corresponding to each of the full-text articles, which were also made available to participants in the evaluation.</Para>
											<Para>The test collection contains 36 official topics in the form of questions that asked for specific entities such as proteins and drugs – the first five topics are shown in Table <InternalRef RefID="T7">7</InternalRef> . Entities of interest are denoted in square brackets and correspond to controlled terminologies from various sources (e.g., MeSH). The topics were created after surveying biologists about recent information needs, and hence can be considered representative for an important group of users who regularly depend on access to the literature.</Para>
											<Table Float="No" ID="T7">
												<Caption Language="En">
													<CaptionNumber>Table 7</CaptionNumber>
													<CaptionContent>
														<SimplePara>Sample topics from the TREC 2007 genomics track.</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="2">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara>200</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>What serum [PROTEINS] change expression in association with high disease activity in lupus?</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara>201</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>What [MUTATIONS] in the Raf gene are associated with cancer?</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>202</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>What [DRUGS] are associated with lysosomal abnormalities in the nervous system?</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>203</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>What [CELL OR TISSUE TYPES] express receptor binding sites for vasoactive intestinal peptide (VIP) on their cell surface?</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>204</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>What nervous system [CELL OR TISSUE TYPES] synthesize neurosteroids in the brain?</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>205</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>What [SIGNS OR SYMPTOMS] of anxiety disorder are related to coronary artery disease?</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
											<Para>Relevance judgments consist of lists of legal spans that were determined to contain an answer, based on the opinion of human assessors with significant domain knowledge (Ph.D. in the life sciences). A legal span is a prescribed unit of retrieval that corresponds to a paragraph in the full-text article. The notion of a legal span evolved out of an attempt to standardize system output. Since systems varied in their processing of article text (in terms of segmentation, tokenization, etc.), a prescriptively-defined unit of retrieval made results easier to compare. In total, there are 12.6 million legal spans in the collection; a list of all legal spans was distributed alongside the full-text articles. In the context of this study, an article is considered relevant if it contains at least one relevant legal span.</Para>
											<Para>As a final note, organizers of the TREC 2007 genomics track were unable to gather corresponding MEDLINE records for approximately 1% of the full-text articles in the Highwire collection. This was due to inconsistencies between the document identifiers used by Highwire Press and PMIDs in MEDLINE. According to my analysis, this resulted in the abstract index having 13 fewer relevant articles than the full-text index (out of a total of 2477) for the entire test set of 36 topics. To be consistent, the official relevance judgments were used in all experiments. However, I confirmed that the small number of missing abstracts had no significant impact on results.</Para>
										</Section2>
										<Section2 ID="Sec_04666">
											<Heading>Retrieval Models</Heading>
											<Para>Most modern text retrieval systems adopt a "bag of words" model, in which documents are treated as unordered collections of terms. Although such a model ignores the richness and complexity of natural language – disregarding syntax, semantics, and even word order – this simplification has proven to be effective in practice. In one standard formulation of the retrieval problem, a document <Emphasis Type="Italic">d</Emphasis> is represented as a vector <Emphasis Type="Italic">W</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">d</Emphasis>
												</Subscript> of term weights <Emphasis Type="Italic">w</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> , <Emphasis Type="Italic">d</Emphasis>
												</Subscript> , which reflect the importance of each term <Emphasis Type="Italic">t</Emphasis> in the document. A document vector has dimensions | <Emphasis Type="Italic">V</Emphasis> |, the size of the vocabulary in the entire collection. As a matter of convenience, "document" generically refers to the unit of indexing (which may in actuality be an abstract, a paragraph, etc.) A query <Emphasis Type="Italic">q</Emphasis> is represented in the same manner, and the score of a document with respect to the query is computed as follows:</Para>
											<Para>This inner-product formulation is sufficiently general to capture a wide range of retrieval models. Note that since queries are often very short, <Emphasis Type="Italic">w</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> , <Emphasis Type="Italic">q</Emphasis>
												</Subscript> is generally less important than <Emphasis Type="Italic">w</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> , <Emphasis Type="Italic">d</Emphasis>
												</Subscript> .</Para>
											<Para>Given a query, a text retrieval system returns a list of documents with respect to a particular retrieval model. In the inner-product formulation, ranking algorithms vary in how term weights are computed. This work explores two different ranking algorithms: the algorithm implemented in the open-source Lucene search engine and Okapi <Emphasis Type="Italic">bm25</Emphasis> .</Para>
											<Para>Lucene is best described as a modified <Emphasis Type="Italic">tf.idf</Emphasis> ranking algorithm. Given a query <Emphasis Type="Italic">q</Emphasis> , the score of a document is computed as the sum of contributions from individual query terms:</Para>
											<Para>For each term <Emphasis Type="Italic">t</Emphasis> that appears in the query <Emphasis Type="Italic">q</Emphasis> , <Emphasis Type="Italic">tf</Emphasis> is the term frequency in the document, <Emphasis Type="Italic">N</Emphasis> is the number of documents in the collection, <Emphasis Type="Italic">n</Emphasis> is the number of documents containing <Emphasis Type="Italic">t</Emphasis> (its document frequency), and <Emphasis Type="Italic">dl</Emphasis> is the document length. The first term inside the summation is the <Emphasis Type="Italic">tf</Emphasis> component, the second is the <Emphasis Type="Italic">idf</Emphasis> component, and the third is a length normalization component. On top of a standard inner-product formulation, Lucene introduces <Emphasis Type="Italic">c</Emphasis> , a "coordination factor", defined as the fraction of query terms found in the document. This factor rewards documents that have many matching terms.</Para>
											<Para>Okapi <Emphasis Type="Italic">bm25</Emphasis>
												<CitationRef CitationID="B35">35</CitationRef>
												<CitationRef CitationID="B36">36</CitationRef> models documents as a mixture of two Poisson processes. One process generates so-called <Emphasis Type="Italic">elite</Emphasis> terms, corresponding to those that an author uses in writing about the topic of a particular document. The other process generates <Emphasis Type="Italic">non-elite</Emphasis> terms, corresponding to those that the author uses in passing (i.e., words whose appearance is incidental to the topic of the document). Due to the complexity of parameter estimation for the full two-Poisson formulation, <Emphasis Type="Italic">bm25</Emphasis> uses an empirically-derived approximation <CitationRef CitationID="B44">44</CitationRef> . Given a query <Emphasis Type="Italic">q</Emphasis> , <Emphasis Type="Italic">bm25</Emphasis> computes the score of a document as the sum of contributions from individual query terms:</Para>
											<Para>For each term <Emphasis Type="Italic">t</Emphasis> , <Emphasis Type="Italic">tf</Emphasis> and <Emphasis Type="Italic">qtf</Emphasis> are term frequencies in the document and query, respectively; <Emphasis Type="Italic">N</Emphasis> is the number of documents in the collection; and <Emphasis Type="Italic">n</Emphasis> is the number of documents containing the term. <Emphasis Type="Italic">K</Emphasis> , a length normalization factor, is defined as follows:</Para>
											<Para>where <Emphasis Type="Italic">dl</Emphasis> is the document length and <Emphasis Type="Italic">avdl</Emphasis> is the average length of all documents. The constants <Emphasis Type="Italic">k</Emphasis>
												<Subscript>1</Subscript> , <Emphasis Type="Italic">b</Emphasis> , and <Emphasis Type="Italic">k</Emphasis>
												<Subscript>3</Subscript> are tunable parameters. In my experiments, I used <Emphasis Type="Italic">k</Emphasis>
												<Subscript>1</Subscript> = 1.2, <Emphasis Type="Italic">b</Emphasis> = 0.75, and <Emphasis Type="Italic">k</Emphasis>
												<Subscript>3</Subscript> = 1000, which are typical settings recommended in the literature.</Para>
										</Section2>
										<Section2 ID="Sec_44883">
											<Heading>Ivory: A Toolkit for Distributed Text Retrieval</Heading>
											<Para>Ivory is a toolkit for distributed text retrieval being developed at the University of Maryland to explore scalable algorithms <CitationRef CitationID="B45">45</CitationRef> . The software was created using Hadoop, which is an open-source Java implementation of the MapReduce programming model <CitationRef CitationID="B4">4</CitationRef> originally developed by Google. Ivory supports the large class of retrieval models that can be expressed as an inner product of term weights (see Section 5.2), and currently implements both <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm (with a special extension to handle the coordination factor).</Para>
											<Para>Certainly, distributed retrieval systems are not new – Web search engines have been in existence for over a decade. However, the exact architectures of these systems are guarded as commercial secrets. Even though outsiders are occasionally offered glimpses into their design <CitationRef CitationID="B46">46</CitationRef>
												<CitationRef CitationID="B47">47</CitationRef> , few details are available about important engineering tradeoffs. On the other hand, most open-source search engines were not specifically designed for multiple machines (or require tedious manual configuration to run on clusters). Although existing tools can easily support text retrieval experiments involving the Highwire collection (and indeed even much larger collections), the growth of available content will inevitably require transition to cluster-based environments. Ivory represents an initial attempt to develop a toolkit for distributed text retrieval using Hadoop; upon suitable maturity, it will be released as open-source software.</Para>
											<Para>MapReduce is an attractive framework for concurrent programming because it frees the software developer from having to explicitly worry about system-level issues such as fault tolerance, synchronization, inter-process communication, scheduling, etc. The abstraction simplifies the design of scalable, distributed algorithms. With the release of Hadoop, an open-source implementation of the MapReduce programming model led by Yahoo, this versatile framework is available to anyone.</Para>
											<Para>MapReduce draws inspiration from higher-order functions in functional programming and builds on the observation that many information processing tasks have the same basic structure: a computation is applied over a large number of records (e.g., Web pages, nodes in a graph) to generate partial results, which are then aggregated in some fashion. In MapReduce, the programmer defines a "mapper" and a "reducer" with the following signatures:</Para>
											<Para>Key-value pairs form the basic data structure in MapReduce. The mapper is applied to every input key-value pair to generate an arbitrary number of intermediate key-value pairs (I adopt the convention of [...] to denote a list). The reducer is applied to all values associated with the same intermediate key to generate output key-value pairs. This two-stage processing structure is illustrated in Figure <InternalRef RefID="F1">1</InternalRef> .</Para>
											<Figure Category="Standard" Float="No" ID="F1">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Illustration of the MapReduce framework: the "mapper" is applied to all input records, which generates results that are aggregated by the "reducer"</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-10-46-1" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Illustration of the MapReduce framework: the "mapper" is applied to all input records, which generates results that are aggregated by the "reducer"</Emphasis> . The runtime groups together values by keys.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>In MapReduce, a programmer need only provide implementations of the mapper and reducer. On top of a distributed file system <CitationRef CitationID="B48">48</CitationRef> , the runtime transparently handles all other aspects of execution, on clusters ranging from a few to a few thousand nodes. The runtime is responsible for scheduling map and reduce workers, detecting and handling faults, delivering input data, shuffling intermediate results, and gathering final output. The MapReduce abstraction allows many complex algorithms to be expressed concisely.</Para>
											<Para>The pseudo-code for Ivory's indexing algorithm is shown in Figure <InternalRef RefID="F2">2</InternalRef> . Like nearly all text retrieval systems, Ivory builds a data structure called an inverted index, which given a term provides access to the list of documents that contain the term. An inverted index consists of postings lists, one associated with each term in the collection. A postings list is comprised of individual postings, each of which represents a (document id, term frequency) pair. This information is used to compute term weights during retrieval.</Para>
											<Figure Category="Standard" Float="No" ID="F2">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Pseudo-code of Ivory's indexing algorithm in MapReduce</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-10-46-2" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Pseudo-code of Ivory's indexing algorithm in MapReduce</Emphasis> . The mapper processes each document and emits postings with the associated term as the key. The reducer gathers all postings for each term to create the inverted index.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>Input to the indexer consists of document ids (keys) and associated document content (values). In each mapper, the document text is tokenized and stemmed term occurrences are first stored in a histogram <Emphasis Type="Italic">H</Emphasis> (implemented as an associative array). After this histogram has been built, the mapper then iterates over all terms. For each term, a pair consisting of the document id ( <Emphasis Type="Italic">k</Emphasis> ) and the frequency of the term in the document ( <Emphasis Type="Italic">f</Emphasis> ) is created. Each pair, denoted by ⟨ <Emphasis Type="Italic">k</Emphasis> , <Emphasis Type="Italic">f</Emphasis> ⟩ in the pseudo-code, represents an individual posting.</Para>
											<Para>The mapper then emits an intermediate key-value pair with the term as the key and the posting as the value. MapReduce guarantees that all values associated with the same key will be sent to the same reducer; the reducer gathers up all postings, sorts them by descending term frequency, and emits the complete postings list, which is then written out to the distributed file system. The final key-value pairs (terms and associated postings lists) make up the inverted index.</Para>
											<Para>Typically, computing term weights requires information about document lengths. This is straightforwardly expressed as another MapReduce algorithm: each mapper counts up the number of terms in a document and emits the term count as a value with the associated document id as the key. In this case, there is no need for a reducer – document lengths are directly written to disk. The table of document lengths is relatively compact, and is read into memory by each mapper in the retrieval phase.</Para>
											<Para>Retrieval with an inverted index involves fetching postings lists that correspond to query terms, and then scoring documents based on term weights computed from postings information. For real-time applications, this requires low-latency access to the inverted index. However, Hadoop was primarily designed for high-throughput batch computations, not computations for which low latency is desired. Presently, Hadoop does not provide a mechanism for low-latency random access to the distributed file system. To work around this limitation, Ivory's retrieval algorithm was designed for parallel query execution.</Para>
											<Para>The pseudo-code for Ivory's retrieval algorithm is shown in Figure <InternalRef RefID="F3">3</InternalRef> . The input to each mapper is a term <Emphasis Type="Italic">t</Emphasis> (the key) and its associated postings list <Emphasis Type="Italic">P</Emphasis> (the value). The mapper loads up <Emphasis Type="Italic">all</Emphasis> the queries at once and processes each query in turn. If the query does not contain <Emphasis Type="Italic">t</Emphasis> , no action is performed. If the query contains <Emphasis Type="Italic">t</Emphasis> , then the corresponding postings must be traversed to compute the partial contributions to the query-document score. For each posting element, the partial contribution to the score ( <Emphasis Type="Italic">w</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> , <Emphasis Type="Italic">q</Emphasis>
												</Subscript> · <Emphasis Type="Italic">w</Emphasis>
												<Subscript>
													<Emphasis Type="Italic">t</Emphasis> , <Emphasis Type="Italic">d</Emphasis>
												</Subscript> ) is computed based on the actual ranking algorithm. For example, <Emphasis Type="Italic">bm25</Emphasis> requires the document frequency of <Emphasis Type="Italic">t</Emphasis> (known by the length of the postings list), term frequency of <Emphasis Type="Italic">t</Emphasis> in the document (stored in the posting), query frequency (loaded by the mapper), document length (stored separately and loaded by the mapper; see above), and average document length (same). With all the necessary components, computing the partial score contribution is simply a matter of arithmetic. Each partial score is stored in an associative array <Emphasis Type="Italic">H</Emphasis> , indexed by the document id <Emphasis Type="Italic">k</Emphasis> – this structure serves the same functionality as accumulators in a traditional retrieval engine. The mapper emits an intermediate key-value pair with the query number <Emphasis Type="Italic">i</Emphasis> as the key and <Emphasis Type="Italic">H</Emphasis> as the value. The result of each mapper is all partial query-document scores associated with term <Emphasis Type="Italic">t</Emphasis> for all queries that contain the term.</Para>
											<Figure Category="Standard" Float="No" ID="F3">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Pseudo-code of Ivory's retrieval algorithm in MapReduce</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="1471-2105-10-46-3" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>
															<Emphasis Type="Bold">Pseudo-code of Ivory's retrieval algorithm in MapReduce</Emphasis> . The mapper processes the postings lists in parallel. For each query term, the mapper initializes accumulators to hold partial score contributions from all documents containing the term. The reducer adds up partial scores to produce the final results.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>In the reduce phase, all associative arrays belonging to the same query are brought together by the runtime. The reducer performs an element-wise sum of all the associative arrays (denoted by the M <Emphasis Type="SmallCaps">ERGE</Emphasis> function in the pseudo-code): this adds up the contributions for each query term across all documents. The final result is an associative array containing scores for all documents that have at least one query term. This structure is then sorted (outside MapReduce by a separate process) to generate the final ranked list for fixed cutoff.</Para>
											<Para>Note that this retrieval algorithm replaces random access of the postings with a parallel scan of all postings – this modification was necessary due to limitations of Hadoop. However, since disk scans are distributed across the entire cluster, it is possible to exploit the aggregate disk bandwidth of all available machines. In processing a set of queries, each postings list is accessed only once – each mapper computes partial score contributions for <Emphasis Type="Italic">all</Emphasis> queries that contain the query term. It should be emphasized that this algorithm is not meant for real-time retrieval applications, but rather is intended for running batch-style experiments in a research context.</Para>
										</Section2>
									</Section1>
								</Body>
								<BodyRef FileRef="http://www.biomedcentral.com/1471-2105/10/46" TargetType="Manuscript"/>
								<ArticleBackmatter>
									<Bibliography ID="Bib1">
										<Heading>References</Heading>
										<Citation ID="CR1">
											<CitationNumber>1.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Hunter</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KB</Initials>
													<FamilyName>Cohen</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Biomedical Language Processing: What's Beyond PubMed?</ArticleTitle>
												<JournalTitle>Mol Cell</JournalTitle>
												<VolumeID>21</VolumeID>
												<FirstPage>589</FirstPage>
												<LastPage>594</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR2">
											<CitationNumber>2.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Zweigenbaum</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Demner-Fushman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KB</Initials>
													<FamilyName>Cohen</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">New Frontiers In Biomedical Text Mining</ArticleTitle>
												<JournalTitle>Pacific Symposium on Biocomputing 12</JournalTitle>
												<VolumeID/>
												<FirstPage>205</FirstPage>
												<LastPage>208</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR3">
											<CitationNumber>3.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Zweigenbaum</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Demner-Fushman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KB</Initials>
													<FamilyName>Cohen</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">Frontiers of Biomedical Text Mining: Current Progress</ArticleTitle>
												<JournalTitle>Brief Bioinform</JournalTitle>
												<VolumeID>8</VolumeID>
												<FirstPage>358</FirstPage>
												<LastPage>375</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR4">
											<CitationNumber>4.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Dean</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Ghemawat</FamilyName>
												</BibAuthorName>
												<Year>2004</Year>
												<ArticleTitle Language="En">MapReduce: Simplified Data Processing on Large Clusters</ArticleTitle>
												<JournalTitle>Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI 2004), San Francisco, California</JournalTitle>
												<VolumeID/>
												<FirstPage>137</FirstPage>
												<LastPage>150</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR5">
											<CitationNumber>5.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>CW</Initials>
													<FamilyName>Gay</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Kayaalp</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AR</Initials>
													<FamilyName>Aronson</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">Semi-Automatic Indexing of Full Text Biomedical Articles</ArticleTitle>
												<JournalTitle>AMIA Annu Symp Proc</JournalTitle>
												<VolumeID/>
												<FirstPage>271</FirstPage>
												<LastPage>275</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR6">
											<CitationNumber>6.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>V</Initials>
													<FamilyName>Hatzivassiloglou</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Friedman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Rzhetsky</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WJ</Initials>
													<FamilyName>Wilbur</FamilyName>
												</BibAuthorName>
												<Year>2002</Year>
												<ArticleTitle Language="En">Automatic Extraction of Gene and Protein Synonyms from MEDLINE and Journal Articles</ArticleTitle>
												<JournalTitle>Proc AMIA Symp</JournalTitle>
												<VolumeID/>
												<FirstPage>919</FirstPage>
												<LastPage>923</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR7">
											<CitationNumber>7.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Seki</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Mostafa</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">Discovering Implicit Associations Between Genes and Hereditary Diseases</ArticleTitle>
												<JournalTitle>Pac Symp Biocomput</JournalTitle>
												<VolumeID/>
												<FirstPage>316</FirstPage>
												<LastPage>327</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR8">
											<CitationNumber>8.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Demner-Fushman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Hauser</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Thoma</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">The Role of Title, Metadata and Abstract in Identifying Clinically Relevant Journal Articles</ArticleTitle>
												<JournalTitle>AMIA Annu Symp Proc</JournalTitle>
												<VolumeID/>
												<FirstPage>191</FirstPage>
												<LastPage>195</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR9">
											<CitationNumber>9.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Towards Answering Biological Questions with Experimental Evidence: Automatically Identifying Text that Summarize Image Content in Full-Text Articles</ArticleTitle>
												<JournalTitle>AMIA Annu Symp Proc</JournalTitle>
												<VolumeID/>
												<FirstPage>834</FirstPage>
												<LastPage>838</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR10">
											<CitationNumber>10.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Accessing Bioscience Images from Abstract Sentences</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>22</VolumeID>
												<FirstPage>e547</FirstPage>
												<LastPage>e556</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR11">
											<CitationNumber>11.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>PK</Initials>
													<FamilyName>Shah</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Perez-Iratxeta</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Bork</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MA</Initials>
													<FamilyName>Andrade</FamilyName>
												</BibAuthorName>
												<Year>2003</Year>
												<ArticleTitle Language="En">Information Extraction from Full Text Scientific Articles: Where are the Keywords?</ArticleTitle>
												<JournalTitle>BMC Bioinformatics</JournalTitle>
												<VolumeID>4</VolumeID>
												<FirstPage>20</FirstPage>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR12">
											<CitationNumber>12.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>MJ</Initials>
													<FamilyName>Schuemie</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Weeber</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>BJA</Initials>
													<FamilyName>Schijvenaars</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>EM</Initials>
													<FamilyName>van Mulligen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CC</Initials>
													<FamilyName>Eijk</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Jelier</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Mons</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JA</Initials>
													<FamilyName>Kors</FamilyName>
												</BibAuthorName>
												<Year>2004</Year>
												<ArticleTitle Language="En">Distribution of Information in Biomedical Abstracts and Full-Text Publications</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>20</VolumeID>
												<FirstPage>2597</FirstPage>
												<LastPage>2604</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR13">
											<CitationNumber>13.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>WJ</Initials>
													<FamilyName>Wilbur</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Smith</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Tanabe</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">BioCreative 2. Gene Mention Task</ArticleTitle>
												<JournalTitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop, Madrid, Spain</JournalTitle>
												<VolumeID/>
												<FirstPage>7</FirstPage>
												<LastPage>16</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR14">
											<CitationNumber>14.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Krallinger</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>F</Initials>
													<FamilyName>Leitner</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Valencia</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">Assessment of the Second BioCreative PPI Task: Automatic Extraction of Protein-Protein Interactions</ArticleTitle>
												<JournalTitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop, Madrid, Spain</JournalTitle>
												<VolumeID/>
												<FirstPage>41</FirstPage>
												<LastPage>54</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR15">
											<CitationNumber>15.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Salton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Buckley</FamilyName>
												</BibAuthorName>
												<Year>1991</Year>
												<ArticleTitle Language="En">Automatic Text Structuring and Retrieval – Experiments in Automatic Encyclopedia searching</ArticleTitle>
												<JournalTitle>Proceedings of the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1991), Chicago, Illinois</JournalTitle>
												<VolumeID/>
												<FirstPage>21</FirstPage>
												<LastPage>30</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR16">
											<CitationNumber>16.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Salton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Allan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Buckley</FamilyName>
												</BibAuthorName>
												<Year>1993</Year>
												<ArticleTitle Language="En">Approaches to Passage Retrieval in Full Text Information Systems</ArticleTitle>
												<JournalTitle>Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993), Pittsburgh, Pennsylvania</JournalTitle>
												<VolumeID/>
												<FirstPage>49</FirstPage>
												<LastPage>58</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR17">
											<CitationNumber>17.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>MA</Initials>
													<FamilyName>Hearst</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Plaunt</FamilyName>
												</BibAuthorName>
												<Year>1993</Year>
												<ArticleTitle Language="En">Subtopic Structuring for Full-Length Document Access</ArticleTitle>
												<JournalTitle>Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1993), Pittsburgh, Pennsylvania</JournalTitle>
												<VolumeID/>
												<FirstPage>56</FirstPage>
												<LastPage>68</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR18">
											<CitationNumber>18.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>JP</Initials>
													<FamilyName>Callan</FamilyName>
												</BibAuthorName>
												<Year>1994</Year>
												<ArticleTitle Language="En">Passage-Level Evidence in Document Retrieval</ArticleTitle>
												<JournalTitle>Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994), Dublin, Ireland</JournalTitle>
												<VolumeID/>
												<FirstPage>302</FirstPage>
												<LastPage>310</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR19">
											<CitationNumber>19.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Wilkinson</FamilyName>
												</BibAuthorName>
												<Year>1994</Year>
												<ArticleTitle Language="En">Effective Retrieval of Structured Documents</ArticleTitle>
												<JournalTitle>Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994), Dublin, Ireland</JournalTitle>
												<VolumeID/>
												<FirstPage>311</FirstPage>
												<LastPage>317</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR20">
											<CitationNumber>20.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Kaszkiel</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Zobel</FamilyName>
												</BibAuthorName>
												<Year>1997</Year>
												<ArticleTitle Language="En">Passage Retrieval Revisited</ArticleTitle>
												<JournalTitle>Proceedings of the 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1997), Philadelphia, Pennsylvania</JournalTitle>
												<VolumeID/>
												<FirstPage>178</FirstPage>
												<LastPage>185</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR21">
											<CitationNumber>21.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Clarke</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Cormack</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Tudhope</FamilyName>
												</BibAuthorName>
												<Year>2000</Year>
												<ArticleTitle Language="En">Relevance Ranking for One to Three Term Queries</ArticleTitle>
												<JournalTitle>Information Processing and Management</JournalTitle>
												<VolumeID>36</VolumeID>
												<FirstPage>291</FirstPage>
												<LastPage>311</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR22">
											<CitationNumber>22.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>X</Initials>
													<FamilyName>Liu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WB</Initials>
													<FamilyName>Croft</FamilyName>
												</BibAuthorName>
												<Year>2002</Year>
												<ArticleTitle Language="En">Passage Retrieval Based on Language Models</ArticleTitle>
												<JournalTitle>Proceedings of the Eleventh International Conference on Information and Knowledge Management (CIKM 2002), McLean, Virginia</JournalTitle>
												<VolumeID/>
												<FirstPage>375</FirstPage>
												<LastPage>382</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR23">
											<CitationNumber>23.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Tellex</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Katz</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Lin</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Marton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Fernandes</FamilyName>
												</BibAuthorName>
												<Year>2003</Year>
												<ArticleTitle Language="En">Quantitative Evaluation of Passage Retrieval Algorithms for Question Answering</ArticleTitle>
												<JournalTitle>Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003), Toronto, Canada</JournalTitle>
												<VolumeID/>
												<FirstPage>41</FirstPage>
												<LastPage>47</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR24">
											<CitationNumber>24.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Si</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Discriminative Probabilistic Models for Passage Based Retrieval</ArticleTitle>
												<JournalTitle>In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008), Singapore</JournalTitle>
												<VolumeID/>
												<FirstPage>419</FirstPage>
												<LastPage>426</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR25">
											<CitationNumber>25.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Lalmas</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Tombros</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">INEX 2002–2006: Understanding XML Retrieval Evaluation</ArticleTitle>
												<JournalTitle>Digital Libraries: Research and Development – First International DELOS Conference, Revised Selected Papers, Pisa, Italy</JournalTitle>
												<VolumeID/>
												<FirstPage>187</FirstPage>
												<LastPage>196</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR26">
											<CitationNumber>26.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>WR</Initials>
													<FamilyName>Hersh</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Cohen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Ruslen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Roberts</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">TREC 2007 Genomics Track Overview</ArticleTitle>
												<JournalTitle>Proceedings of the Sixteenth Text REtrieval Conference (TREC 2007), Gaithersburg, Maryland</JournalTitle>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR27">
											<CitationNumber>27.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Trotman</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">Wanted: Element Retrieval Users</ArticleTitle>
												<JournalTitle>Proceedings of the INEX 2005 Workshop on Element Retrieval Methodology, Glasgow, Scotland</JournalTitle>
												<VolumeID/>
												<FirstPage>63</FirstPage>
												<LastPage>69</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR28">
											<CitationNumber>28.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Larsen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Tombros</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Malik</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Is XML Retrieval Meaningful to Users? Searcher Preferences for Full Documents vs. Elements</ArticleTitle>
												<JournalTitle>Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2006), Seattle, Washington</JournalTitle>
												<VolumeID/>
												<FirstPage>663</FirstPage>
												<LastPage>664</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR29">
											<CitationNumber>29.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Salton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Wong</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CS</Initials>
													<FamilyName>Yang</FamilyName>
												</BibAuthorName>
												<Year>1975</Year>
												<ArticleTitle Language="En">A Vector Space Model for Automatic Indexing</ArticleTitle>
												<JournalTitle>Communications of the ACM</JournalTitle>
												<VolumeID>18</VolumeID>
												<FirstPage>613</FirstPage>
												<LastPage>620</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR30">
											<CitationNumber>30.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>JM</Initials>
													<FamilyName>Ponte</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WB</Initials>
													<FamilyName>Croft</FamilyName>
												</BibAuthorName>
												<Year>1998</Year>
												<ArticleTitle Language="En">A Language Modeling Approach to Information Retrieval</ArticleTitle>
												<JournalTitle>Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998), Melbourne, Australia</JournalTitle>
												<VolumeID/>
												<FirstPage>275</FirstPage>
												<LastPage>281</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR31">
											<CitationNumber>31.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Metzler</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WB</Initials>
													<FamilyName>Croft</FamilyName>
												</BibAuthorName>
												<Year>2004</Year>
												<ArticleTitle Language="En">Combining the Language Model and Inference Network Approaches to Retrieval</ArticleTitle>
												<JournalTitle>Information Processing and Management</JournalTitle>
												<VolumeID>40</VolumeID>
												<FirstPage>735</FirstPage>
												<LastPage>750</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR32">
											<CitationNumber>32.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Rafkind</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SF</Initials>
													<FamilyName>Chang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Exploring Text and Image Features to Classify Images in Bioscience Literature</ArticleTitle>
												<JournalTitle>Proceedings of the HLT/NAACL 2006 Workshop on Biomedical Natural Language Processing (BioNLP'06), New York, New York</JournalTitle>
												<VolumeID/>
												<FirstPage>73</FirstPage>
												<LastPage>80</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR33">
											<CitationNumber>33.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Shatkay</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>N</Initials>
													<FamilyName>Chen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Blostein</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Integrating Image Data into Biomedical Text Categorization</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>22</VolumeID>
												<FirstPage>e446</FirstPage>
												<LastPage>e453</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR34">
											<CitationNumber>34.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Kou</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WW</Initials>
													<FamilyName>Cohen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RF</Initials>
													<FamilyName>Murphy</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">A Stacked Graphical Model for Associating Sub-Images with Sub-Captions</ArticleTitle>
												<JournalTitle>Pacific Symposium on Biocomputing 12</JournalTitle>
												<VolumeID/>
												<FirstPage>257</FirstPage>
												<LastPage>268</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR35">
											<CitationNumber>35.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SE</Initials>
													<FamilyName>Robertson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Walker</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Hancock-Beaulieu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Gatford</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Payne</FamilyName>
												</BibAuthorName>
												<Year>1995</Year>
												<ArticleTitle Language="En">Okapi at TREC-4</ArticleTitle>
												<JournalTitle>Proceedings of the Fourth Text REtrieval Conference (TREC-4), Gaithersburg, Maryland</JournalTitle>
												<VolumeID/>
												<FirstPage>73</FirstPage>
												<LastPage>96</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR36">
											<CitationNumber>36.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Sparck Jones</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Walker</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SE</Initials>
													<FamilyName>Robertson</FamilyName>
												</BibAuthorName>
												<Year>2000</Year>
												<ArticleTitle Language="En">A Probabilistic Model of Information Retrieval: Development and Comparative Experiments (Parts 1 and 2)</ArticleTitle>
												<JournalTitle>Information Processing and Management</JournalTitle>
												<VolumeID>36</VolumeID>
												<FirstPage>779</FirstPage>
												<LastPage>840</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR37">
											<CitationNumber>37.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Singhal</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Buckley</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Mitra</FamilyName>
												</BibAuthorName>
												<Year>1996</Year>
												<ArticleTitle Language="En">Pivoted Document Length Normalization</ArticleTitle>
												<JournalTitle>Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1996), Zürich, Switzerland</JournalTitle>
												<VolumeID/>
												<FirstPage>21</FirstPage>
												<LastPage>29</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR38">
											<CitationNumber>38.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Kamps</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>de Rijke</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Sigurbjörnsson</FamilyName>
												</BibAuthorName>
												<Year>2004</Year>
												<ArticleTitle Language="En">Length Normalization in XML Retrieval</ArticleTitle>
												<JournalTitle>Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2004), Sheffield, United Kingdom</JournalTitle>
												<VolumeID/>
												<FirstPage>80</FirstPage>
												<LastPage>87</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR39">
											<CitationNumber>39.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Ogilvie</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Callan</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">Parameter Estimation for a Simple Hierarchical Generative Model for XML Retrieval</ArticleTitle>
												<JournalTitle>Proceedings of the 2005 Initiative for the Evaluation of XML Retrieval Workshop (INEX 2005), Dagstuhl, Germany</JournalTitle>
												<VolumeID/>
												<FirstPage>211</FirstPage>
												<LastPage>224</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR40">
											<CitationNumber>40.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Sigurbjörnsson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Kamps</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>de Rijke</FamilyName>
												</BibAuthorName>
												<Year>2003</Year>
												<ArticleTitle Language="En">An Element-based Approach to XML Retrieval</ArticleTitle>
												<JournalTitle>Proceedings of the 2003 Initiative for the Evaluation of XML Retrieval Workshop (INEX 2005), Dagstuhl, Germany</JournalTitle>
												<VolumeID/>
												<FirstPage>19</FirstPage>
												<LastPage>26</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR41">
											<CitationNumber>41.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>I</Initials>
													<FamilyName>Tbahriti</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Chichester</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>F</Initials>
													<FamilyName>Lisacek</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Ruch</FamilyName>
												</BibAuthorName>
												<Year>2006</Year>
												<ArticleTitle Language="En">Using Argumentation to Retrieve Articles with Similar Citations: An Inquiry into Improving Related Articles Search in the MEDLINE Digital Library</ArticleTitle>
												<JournalTitle>Int J Med Inform</JournalTitle>
												<VolumeID>75</VolumeID>
												<FirstPage>488</FirstPage>
												<LastPage>495</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR42">
											<CitationNumber>42.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>CW</Initials>
													<FamilyName>Cleverdon</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Mills</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>EM</Initials>
													<FamilyName>Keen</FamilyName>
												</BibAuthorName>
												<Year>1968</Year>
												<ArticleTitle Language="En">Factors Determining the Performance of Indexing Systems</ArticleTitle>
												<JournalTitle/>
												<VolumeID>Two</VolumeID>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR43">
											<CitationNumber>43.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>DK</Initials>
													<FamilyName>Harman</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">The TREC Test Collections</ArticleTitle>
												<JournalTitle>TREC: Experiment and Evaluation in Information Retrieval</JournalTitle>
												<VolumeID/>
												<FirstPage>21</FirstPage>
												<LastPage>52</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR44">
											<CitationNumber>44.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SE</Initials>
													<FamilyName>Robertson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Walker</FamilyName>
												</BibAuthorName>
												<Year>1994</Year>
												<ArticleTitle Language="En">Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval</ArticleTitle>
												<JournalTitle>Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1994), Dublin, Ireland</JournalTitle>
												<VolumeID/>
												<FirstPage>232</FirstPage>
												<LastPage>241</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR45">
											<CitationNumber>45.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Elsayed</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Lin</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Oard</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Pairwise Document Similarity in Large Collections with MapReduce</ArticleTitle>
												<JournalTitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL 2008), Companion Volume, Columbus, Ohio</JournalTitle>
												<VolumeID/>
												<FirstPage>265</FirstPage>
												<LastPage>268</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR46">
											<CitationNumber>46.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Brin</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Page</FamilyName>
												</BibAuthorName>
												<Year>1998</Year>
												<ArticleTitle Language="En">The Anatomy of a Large-Scale Hypertextual Web Search Engine</ArticleTitle>
												<JournalTitle>Proceedings of the Seventh International World Wide Web Conference (WWW 7), Brisbane, Australia</JournalTitle>
												<VolumeID/>
												<FirstPage>107</FirstPage>
												<LastPage>117</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR47">
											<CitationNumber>47.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>LA</Initials>
													<FamilyName>Barroso</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Dean</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>U</Initials>
													<FamilyName>Hölzle</FamilyName>
												</BibAuthorName>
												<Year>2003</Year>
												<ArticleTitle Language="En">Web Search for a Planet: The Google Cluster Architecture</ArticleTitle>
												<JournalTitle>IEEE Micro</JournalTitle>
												<VolumeID>23</VolumeID>
												<FirstPage>22</FirstPage>
												<LastPage>28</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR48">
											<CitationNumber>48.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Ghemawat</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Gobioff</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>ST</Initials>
													<FamilyName>Leung</FamilyName>
												</BibAuthorName>
												<Year>2003</Year>
												<ArticleTitle Language="En">The Google File System</ArticleTitle>
												<JournalTitle>Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP 2003), Bolton Landing, New York</JournalTitle>
												<VolumeID/>
												<FirstPage>29</FirstPage>
												<LastPage>43</LastPage>
											</BibArticle>
										</Citation>
									</Bibliography>
								</ArticleBackmatter>
							</Article>
						</Issue>
					</Volume>
				</Journal>
				<meta:Info xmlns:meta="http://www.springer.com/app/meta">
					<meta:Authors>
						<meta:Author>Lin, Jimmy</meta:Author>
					</meta:Authors>
					<meta:Institutions>
						<meta:Institution geo="-76.9322420,38.9946680,0">
							<meta:OrgName>University of Maryland</meta:OrgName>
							<meta:GeoOrg>-76.9322420,38.9946680,0#University of Maryland</meta:GeoOrg>
							<meta:Country>United States</meta:Country>
						</meta:Institution>
					</meta:Institutions>
					<meta:Date>2009-02-03</meta:Date>
					<meta:Type>Article</meta:Type>
					<meta:DOI>10.1186/1471-2105-10-46</meta:DOI>
					<meta:Title>Is searching full text more effective than searching abstracts?</meta:Title>
					<meta:ISXN>1471-2105</meta:ISXN>
					<meta:Journal>BMC Bioinformatics</meta:Journal>
					<meta:PubName>BioMed Central</meta:PubName>
					<meta:ArticleFirstPage>46</meta:ArticleFirstPage>
					<meta:Publication>BMC Bioinformatics</meta:Publication>
					<meta:PublicationType>Journal</meta:PublicationType>
					<meta:SubjectGroup>
						<meta:Subject Type="Primary">Life Sciences</meta:Subject>
						<meta:Subject Priority="1" Type="Secondary">Bioinformatics</meta:Subject>
						<meta:Subject Priority="2" Type="Secondary">Microarrays</meta:Subject>
						<meta:Subject Priority="3" Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>
						<meta:Subject Priority="4" Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>
						<meta:Subject Priority="5" Type="Secondary">Combinatorial Libraries</meta:Subject>
						<meta:Subject Priority="6" Type="Secondary">Algorithms</meta:Subject>
					</meta:SubjectGroup>
				</meta:Info>
			</Publisher>
			<Images>
				<Image Id="5-10.1186_1471-2105-10-46-0" xml:lang="en" language="en">
					<Caption>
						<p>Illustration of the MapReduce framework: the "mapper" is applied to all input records, which generates results that are aggregated by the "reducer"</p>
					</Caption>
					<FullText>
						<p>
This two-stage processing structure is illustrated in Figure 1 ..
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-10-46-1.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-1" xml:lang="en" language="en">
					<Caption>
						<p>Pseudo-code of Ivory's indexing algorithm in MapReduce</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">The pseudo-code for Ivory's indexing algorithm is shown in
Figure 2 .</p>
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-10-46-2.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-2" xml:lang="en" language="en">
					<Caption>
						<p>Pseudo-code of Ivory's retrieval algorithm in MapReduce</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">The pseudo-code for Ivory's retrieval algorithm is shown in
Figure 3 .</p>
						</p>
					</FullText>
					<File>
						<Color>true</Color>
						<Format>JPG</Format>
						<Path>/Images/BMC/MEDIUM_1471-2105-10-46-3.jpg</Path>
						<Type>Linedraw</Type>
					</File>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<ImageType>Line</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>BioMed Central</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-4" xml:lang="en" language="en">
					<Caption>
						<p>Results of significance testing comparing article retrieval with span retrieval ("max" strategy).</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Table 2 shows the results of significance testing between
article retrieval and span retrieval with the "max" strategy (the
more effective, and thus more interesting, of the two
strategies).</p>
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T2">
							<Caption Language="En">
								<CaptionNumber>Table 2</CaptionNumber>
								<CaptionContent>
									<SimplePara>Results of significance testing comparing article retrieval with span retrieval ("max" strategy).</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="3">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>MAP</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>
												<Emphasis Type="Italic">p</Emphasis> &lt; 0.01</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>
												<Emphasis Type="Italic">n.s.</Emphasis>
											</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>P20</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>
												<Emphasis Type="Italic">p</Emphasis> &lt; 0.01</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>
												<Emphasis Type="Italic">n.s.</Emphasis>
											</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>IP@R50</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>
												<Emphasis Type="Italic">n.s.</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>
												<Emphasis Type="Italic">n.s.</Emphasis>
											</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-3" xml:lang="en" language="en">
					<Caption>
						<p>Effectiveness of bm25 and the Lucene ranking algorithm on abstracts, full-text articles, and spans from full text.</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Results of the matrix experiment described in Section 2.2 with
Ivory are presented in Table 1 .</p>
						</p><p>
Focusing on the "max" strategy, Table 1 shows that, overall, span
retrieval has a relatively small effect on precision (seen in the
P20 scores), but a large impact on recall (seen in the IP@R50
scores).
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T1">
							<Caption Language="En">
								<CaptionNumber>Table 1</CaptionNumber>
								<CaptionContent>
									<SimplePara>Effectiveness of <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm on abstracts, full-text articles, and spans from full text.</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="3">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">MAP</Emphasis>
											</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.163</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.129</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.146 (-11%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.235 (+82%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.240 (+47%)**</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.206 (+60%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (sum)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.192 (+18%)*</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.198 (+54%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara/>
										</entry>
										<entry colname="c2">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">P20</Emphasis>
											</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.322</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.293</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.158 (-51%)**</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.353 (+20%)*</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.357 (+11%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.332 (+13%)°</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (sum)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.314 (-3%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.317 (+8%)*</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara/>
										</entry>
										<entry colname="c2">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">IP@R50</Emphasis>
											</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.110</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.090</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.163 (+48%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.222 (+146%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.212 (+93%)**</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.189 (+109%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (sum)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.149 (+36%)*</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.159 (+77%)**</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
							<tfooter>
								<SimplePara>For all metrics, relative improvements over baseline are shown; ** = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01); * = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.05); ° = not significant.</SimplePara>
							</tfooter>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-5" xml:lang="en" language="en">
					<Caption>
						<p>Effectiveness of bm25 and the Lucene ranking algorithm combining evidence from spans with evidence from abstracts and articles.</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Results for the evidence combination experiments are shown in
Table 3 , where span retrieval is combined with abstract and
article retrieval.</p>
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T3">
							<Caption Language="En">
								<CaptionNumber>Table 3</CaptionNumber>
								<CaptionContent>
									<SimplePara>Effectiveness of <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm combining evidence from spans with evidence from abstracts and articles.</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="3">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">MAP</Emphasis>
											</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.240</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.206</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max) + Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.257 (+7%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.216 (+5%)°</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max) + Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.257 (+7%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.262 (+27%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara/>
										</entry>
										<entry colname="c2">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">P20</Emphasis>
											</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.357</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.332</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max) + Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.382 (+7%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.349 (+5%)°</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max) + Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.343 (-4%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.404 (+22%)**</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara/>
										</entry>
										<entry colname="c2">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Bold">IP@R50</Emphasis>
											</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Ivory ( <Emphasis Type="Italic">bm25</Emphasis> )</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory ( <Emphasis Type="Italic">Lucene</Emphasis> )</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.212</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.189</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max) + Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.215 (+1%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.190 (+1%)°</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (max) + Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>0.257 (+21%)°</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>0.244 (+29%)**</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
							<tfooter>
								<SimplePara>For all metrics, relative improvements over baseline are shown; ** = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01); * = statistically significant ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.05); ° = not significant.</SimplePara>
							</tfooter>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-6" xml:lang="en" language="en">
					<Caption>
						<p>Comparison of different experimental conditions for bm25 and the Lucene ranking algorithm.</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Table 4 attempts to summarize findings from all these
experiments by establishing a partial rank order of different
experimental conditions (based on significance testing), in terms
of each effectiveness metric and retrieval model.</p>
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T4">
							<Caption Language="En">
								<CaptionNumber>Table 4</CaptionNumber>
								<CaptionContent>
									<SimplePara>Comparison of different experimental conditions for <Emphasis Type="Italic">bm25</Emphasis> and the Lucene ranking algorithm.</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="3">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara>Model</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>Metric</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Comparison</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>
												<Emphasis Type="Italic">bm25</Emphasis>
											</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>MAP</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Span (max) + Article, Span (max) &gt;&gt; Abstract, Article</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>P20</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Span (max) + Article, Span (max), Abstract &gt;&gt; Article</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>IP@R50</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Span (max) + Article &gt;&gt; Abstract, Article; Span (max) &gt;&gt; Abstract</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Lucene</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>MAP</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Span (max) + Article &gt;&gt; Span (max), Article &gt;&gt; Abstract</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>P20</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Span (max) + Article &gt; Span (max), Article; Article &gt; Abstract</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>IP@R50</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Span (max) + Article &gt; Span (max), Article &gt;&gt; Abstract</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
							<tfooter>
								<SimplePara>
									<Emphasis Type="Italic">A</Emphasis> &gt;&gt; <Emphasis Type="Italic">B</Emphasis> indicates that <Emphasis Type="Italic">A</Emphasis> is significantly better than <Emphasis Type="Italic">B</Emphasis> ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.01); <Emphasis Type="Italic">A</Emphasis> &gt; <Emphasis Type="Italic">B</Emphasis> indicates that <Emphasis Type="Italic">A</Emphasis> is significantly better than <Emphasis Type="Italic">B</Emphasis> ( <Emphasis Type="Italic">p</Emphasis> &lt; 0.05);</SimplePara>
							</tfooter>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-7" xml:lang="en" language="en">
					<Caption>
						<p>Time required for index construction, comparing Lucene to different Ivory configurations.</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">Running times for index construction for the three different
configurations are shown in Table 5 .</p>
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T5">
							<Caption Language="En">
								<CaptionNumber>Table 5</CaptionNumber>
								<CaptionContent>
									<SimplePara>Time required for index construction, comparing Lucene to different Ivory configurations.</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="4">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<colspec colname="c3" colnum="3"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Lucene (1 core)</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory (10 cores)</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>Ivory (20 cores)</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Abstract</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>1 h 00 m 58 s</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>1 m 32 s</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>1 m 07 s</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Article</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>19 h 09 m 23 s</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>17 m 21 s</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>9 m 57 s</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>27 h 10 m 46 s</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>39 m 58 s</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>24 m 56 s</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-8" xml:lang="en" language="en">
					<Caption>
						<p>Time required for retrieval runs, comparing Lucene to different Ivory configurations.</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">In terms of retrieval, running times on the entire set of 36
topics from the TREC 2007 genomics track are shown in Table 6 .</p>
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T6">
							<Caption Language="En">
								<CaptionNumber>Table 6</CaptionNumber>
								<CaptionContent>
									<SimplePara>Time required for retrieval runs, comparing Lucene to different Ivory configurations.</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="4">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<colspec colname="c2" colnum="2"/>
								<colspec colname="c3" colnum="3"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
										<entry colname="c1">
											<SimplePara>Lucene (1 core)</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>Ivory (10 cores)</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>Ivory (20 cores)</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara/>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Abstract (1000 hits)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>1 m 42 s</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>51 s</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>40 s</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Article (1000 hits)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>7 m 00 s</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>1 m 51 s</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>1 m 09 s</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>Span (5000 hits)</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>21 m 32 s</SimplePara>
										</entry>
										<entry colname="c2">
											<SimplePara>11 m 57 s</SimplePara>
										</entry>
										<entry colname="c3">
											<SimplePara>8 m 25 s</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
				<Image Id="5-10.1186_1471-2105-10-46-9" xml:lang="en" language="en">
					<Caption>
						<p>Sample topics from the TREC 2007 genomics track.</p>
					</Caption>
					<FullText>
						<p>
							<p xmlns="http://www.w3.org/1999/xhtml">The test collection contains 36 official topics in the form of
questions that asked for specific entities such as proteins and
drugs – the first five topics are shown in Table 7 .</p>
						</p>
					</FullText>
					<Table>
						<Table Float="No" ID="T7">
							<Caption Language="En">
								<CaptionNumber>Table 7</CaptionNumber>
								<CaptionContent>
									<SimplePara>Sample topics from the TREC 2007 genomics track.</SimplePara>
								</CaptionContent>
							</Caption>
							<tgroup cols="2">
								<colspec colname="c0" colnum="0"/>
								<colspec colname="c1" colnum="1"/>
								<thead>
									<row>
										<entry colname="c0">
											<SimplePara>200</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>What serum [PROTEINS] change expression in association with high disease activity in lupus?</SimplePara>
										</entry>
									</row>
								</thead>
								<tbody>
									<row>
										<entry colname="c0">
											<SimplePara>201</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>What [MUTATIONS] in the Raf gene are associated with cancer?</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>202</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>What [DRUGS] are associated with lysosomal abnormalities in the nervous system?</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>203</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>What [CELL OR TISSUE TYPES] express receptor binding sites for vasoactive intestinal peptide (VIP) on their cell surface?</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>204</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>What nervous system [CELL OR TISSUE TYPES] synthesize neurosteroids in the brain?</SimplePara>
										</entry>
									</row>
									<row>
										<entry colname="c0">
											<SimplePara>205</SimplePara>
										</entry>
										<entry colname="c1">
											<SimplePara>What [SIGNS OR SYMPTOMS] of anxiety disorder are related to coronary artery disease?</SimplePara>
										</entry>
									</row>
								</tbody>
							</tgroup>
						</Table>
					</Table>
					<Authors>
						<Author>Lin, Jimmy</Author>
					</Authors>
					<Institutions>
						<Institution>National Center for Biotechnology Information, National Library of Medicine, Bethesda, Maryland, USA</Institution>
						<Institution>The iSchool, University of Maryland, College Park, Maryland, USA</Institution>
					</Institutions>
					<ArticleTitle>Is searching full text more effective than searching abstracts?</ArticleTitle>
					<DOI>10.1186/1471-2105-10-46</DOI>
					<PubDate>2009-02-03</PubDate>
					<SourceType>Article</SourceType>
					<SourceTitle>BMC Bioinformatics</SourceTitle>
					<JournalId>1471-2105</JournalId>
					<VolumeId>10</VolumeId>
					<IssueId>1</IssueId>
					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>
					<SubjectCollection>Life Sciences</SubjectCollection>
					<Subjects>
						<Subject Type="Primary">Life Sciences</Subject>
						<Subject Type="Secondary" Priority="1">Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="2">Microarrays</Subject>
						<Subject Type="Secondary" Priority="3">Computational Biology/Bioinformatics</Subject>
						<Subject Type="Secondary" Priority="4">Computer Appl. in Life Sciences</Subject>
						<Subject Type="Secondary" Priority="5">Combinatorial Libraries</Subject>
						<Subject Type="Secondary" Priority="6">Algorithms</Subject>
					</Subjects>
					<Keywords>
						<Keyword>abstracts?</Keyword>
						<Keyword>text</Keyword>
						<Keyword>more</Keyword>
						<Keyword>full</Keyword>
						<Keyword>searching</Keyword>
						<Keyword>than</Keyword>
						<Keyword>effective</Keyword>
					</Keywords>
					<OpenAccess>true</OpenAccess>
					<CopyrightHolder>Lin; licensee BioMed Central Ltd.</CopyrightHolder>
					<ImageType>Table</ImageType>
					<ArticleURI>/Images/BMC/10.1186@1471-2105-10-46.xml</ArticleURI>
					<Provider>Springer</Provider>
					<DateLoaded>2009-11-26T18:09:18.308Z</DateLoaded>
				</Image>
			</Images>
		</result>
		<result>
			<Publisher xml:lang="en">
				<PublisherInfo>
					<PublisherName>Springer US</PublisherName>
					<PublisherLocation>Boston</PublisherLocation>
					<PublisherURL>http://www.springer-ny.com</PublisherURL>
				</PublisherInfo>
				<Journal OutputMedium="All">
					<JournalInfo JournalProductType="NonStandardArchiveJournal" NumberingStyle="ContentOnly">
						<JournalID>11227</JournalID>
						<JournalPrintISSN>0920-8542</JournalPrintISSN>
						<JournalElectronicISSN>1573-0484</JournalElectronicISSN>
						<JournalTitle>The Journal of Supercomputing</JournalTitle>
						<JournalSubTitle>An International Journal of High-Performance Computer Design, Analysis, and Use</JournalSubTitle>
						<JournalAbbreviatedTitle>J Supercomput</JournalAbbreviatedTitle>
						<JournalSubjectGroup>
							<JournalSubject Type="Primary">Computer Science</JournalSubject>
							<JournalSubject Type="Secondary">Computer Science, general</JournalSubject>
							<JournalSubject Type="Secondary">Processor Architectures</JournalSubject>
							<JournalSubject Type="Secondary">Programming Languages, Compilers, Interpreters</JournalSubject>
						</JournalSubjectGroup>
					</JournalInfo>
					<JournalOnlineFirst>
						<Article ID="s11227-010-0503-2">
							<ArticleInfo ArticleCitation="ArticleFirstPage" ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
								<ArticleID>503</ArticleID>
								<ArticleDOI>10.1007/s11227-010-0503-2</ArticleDOI>
								<ArticleSequenceNumber>0</ArticleSequenceNumber>
								<ArticleTitle Language="En">The <Emphasis Type="Italic">Nornir</Emphasis> run-time system for parallel programs using Kahn process networks on multi-core machines—a flexible alternative to MapReduce</ArticleTitle>
								<ArticleFirstPage>1</ArticleFirstPage>
								<ArticleLastPage>27</ArticleLastPage>
								<ArticleHistory>
									<RegistrationDate>
										<Year>2010</Year>
										<Month>10</Month>
										<Day>14</Day>
									</RegistrationDate>
									<OnlineDate>
										<Year>2010</Year>
										<Month>11</Month>
										<Day>13</Day>
									</OnlineDate>
								</ArticleHistory>
								<ArticleCopyright>
									<CopyrightHolderName>The Author(s)</CopyrightHolderName>
									<CopyrightYear>2010</CopyrightYear>
								</ArticleCopyright>
								<ArticleGrants Type="OpenChoice">
									<MetadataGrant Grant="OpenAccess"/>
									<AbstractGrant Grant="OpenAccess"/>
									<BodyPDFGrant Grant="OpenAccess"/>
									<BodyHTMLGrant Grant="OpenAccess"/>
									<BibliographyGrant Grant="OpenAccess"/>
									<ESMGrant Grant="OpenAccess"/>
								</ArticleGrants>
								<ArticleContext>
									<JournalID>11227</JournalID>
									<VolumeIDStart>
										<?InsertByIssueBuilding VolumeIDStart?>
									</VolumeIDStart>
									<VolumeIDEnd>
										<?InsertByIssueBuilding VolumeIDEnd?>
									</VolumeIDEnd>
									<IssueIDStart>
										<?InsertByIssueBuilding IssueIDStart?>
									</IssueIDStart>
									<IssueIDEnd>
										<?InsertByIssueBuilding IssueIDEnd?>
									</IssueIDEnd>
								</ArticleContext>
							</ArticleInfo>
							<ArticleHeader>
								<AuthorGroup>
									<Author AffiliationIDS="Aff1 Aff2" CorrespondingAffiliationID="Aff2">
										<AuthorName DisplayOrder="Western">
											<GivenName>Željko</GivenName>
											<FamilyName>Vrba</FamilyName>
										</AuthorName>
										<Contact>
											<Email>zvrba@ifi.uio.no</Email>
										</Contact>
									</Author>
									<Author AffiliationIDS="Aff1 Aff2">
										<AuthorName DisplayOrder="Western">
											<GivenName>Pål</GivenName>
											<FamilyName>Halvorsen</FamilyName>
										</AuthorName>
										<Contact>
											<Email>paalh@ifi.uio.no</Email>
										</Contact>
									</Author>
									<Author AffiliationIDS="Aff1 Aff2">
										<AuthorName DisplayOrder="Western">
											<GivenName>Carsten</GivenName>
											<FamilyName>Griwodz</FamilyName>
										</AuthorName>
										<Contact>
											<Email>griff@ifi.uio.no</Email>
										</Contact>
									</Author>
									<Author AffiliationIDS="Aff1 Aff2">
										<AuthorName DisplayOrder="Western">
											<GivenName>Paul</GivenName>
											<FamilyName>Beskow</FamilyName>
										</AuthorName>
										<Contact>
											<Email>paulbb@ifi.uio.no</Email>
										</Contact>
									</Author>
									<Author AffiliationIDS="Aff1 Aff2">
										<AuthorName DisplayOrder="Western">
											<GivenName>Håvard</GivenName>
											<FamilyName>Espeland</FamilyName>
										</AuthorName>
										<Contact>
											<Email>haavares@ifi.uio.no</Email>
										</Contact>
									</Author>
									<Author AffiliationIDS="Aff3">
										<AuthorName DisplayOrder="Western">
											<GivenName>Dag</GivenName>
											<FamilyName>Johansen</FamilyName>
										</AuthorName>
										<Contact>
											<Email>dag@cs.uit.no</Email>
										</Contact>
									</Author>
									<Affiliation ID="Aff1">
										<OrgName>Simula Research Laboratory</OrgName>
										<OrgAddress>
											<City>Oslo</City>
											<Country>Norway</Country>
										</OrgAddress>
									</Affiliation>
									<Affiliation ID="Aff2">
										<OrgDivision>Department of Informatics</OrgDivision>
										<OrgName>University of Oslo</OrgName>
										<OrgAddress>
											<City>Oslo</City>
											<Country>Norway</Country>
										</OrgAddress>
									</Affiliation>
									<Affiliation ID="Aff3">
										<OrgDivision>Department of Computer Science</OrgDivision>
										<OrgName>University of Tromsø</OrgName>
										<OrgAddress>
											<City>Tromsø</City>
											<Country>Norway</Country>
										</OrgAddress>
									</Affiliation>
								</AuthorGroup>
								<Abstract ID="Abs1" Language="En" OutputMedium="All">
									<Heading>Abstract</Heading>
									<Para>Even though shared-memory concurrency is a paradigm frequently used for developing parallel applications on small- and middle-sized machines, experience has shown that it is hard to use. This is largely caused by synchronization primitives which are low-level, inherently non-deterministic, and, consequently, non-intuitive to use. In this paper, we present the <Emphasis Type="Italic">Nornir</Emphasis> run-time system. Nornir is comparable to well-known frameworks such as MapReduce and Dryad that are recognized for their efficiency and simplicity. Unlike these frameworks, Nornir also supports process structures containing branches and cycles. Nornir is based on the formalism of Kahn process networks, which is a shared-nothing, message-passing model of concurrency. We deem this model a simple and deterministic alternative to shared-memory concurrency. Experiments with real and synthetic benchmarks on up to 8 CPUs show that performance in most cases scales almost linearly with the number of CPUs, when not limited by data dependencies. We also show that the modeling flexibility allows Nornir to outperform its MapReduce counterparts using well-known benchmarks.</Para>
								</Abstract>
								<KeywordGroup Language="En">
									<Heading>Keywords</Heading>
									<Keyword>Parallel processing</Keyword>
									<Keyword>Kahn process networks</Keyword>
								</KeywordGroup>
							</ArticleHeader>
							<BodyRef TargetType="OnlinePDF" FileRef="BodyRef/PDF/11227_2010_Article_503.pdf"/>
							<BodyRef TargetType="TEX" FileRef="BodyRef/PDF/11227_2010_503_TEX.zip"/>
							<ArticleBackmatter>
								<Bibliography ID="Bib1">
									<Heading>References</Heading>
									<Citation ID="CR1">
										<CitationNumber>1.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>G</Initials>
												<FamilyName>Allen</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Zucknick</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Evans</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">A distributed deadlock detection and resolution algorithm for process networks</ChapterTitle>
											<BookTitle>IEEE international conference on acoustics, speech and signal processing</BookTitle>
											<ConfEventLocation>(ICASSP) 2</ConfEventLocation>
											<ConfEventDate>April 2007</ConfEventDate>
											<FirstPage>II-33</FirstPage>
											<LastPage>II-36</LastPage>
										</BibChapter>
										<BibUnstructured>
Allen G, Zucknick P, Evans B (2007) A distributed deadlock detection and resolution algorithm for process networks. In: IEEE international conference on acoustics, speech and signal processing, (ICASSP) 2, April 2007, pp II-33–II-36
										</BibUnstructured>
									</Citation>
									<Citation ID="CR2">
										<CitationNumber>2.</CitationNumber>
										<BibUnstructured>
Apache Hadoop, Accessed July 2009. <ExternalRef>
												<RefSource>http://hadoop.apache.org/</RefSource>
												<RefTarget Address="http://hadoop.apache.org/" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR3">
										<CitationNumber>3.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Armstrong</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">A history of Erlang</ChapterTitle>
											<BookTitle>HOPL III: Proceedings of the 3rd ACM SIGPLAN conference on history of programming languages</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>6-1</FirstPage>
											<LastPage>6-26</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1238844.1238850</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Armstrong J (2007) A history of Erlang. In: HOPL III: Proceedings of the 3rd ACM SIGPLAN conference on history of programming languages, pp 6-1–6-26. ACM, New York
										</BibUnstructured>
									</Citation>
									<Citation ID="CR4">
										<CitationNumber>4.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>NS</Initials>
												<FamilyName>Arora</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>RD</Initials>
												<FamilyName>Blumofe</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>CG</Initials>
												<FamilyName>Plaxton</FamilyName>
											</BibAuthorName>
											<Year>1998</Year>
											<ChapterTitle Language="En">Thread scheduling for multiprogrammed multiprocessors</ChapterTitle>
											<BookTitle>Proceedings of ACM symposium on parallel algorithms and architectures (SPAA)</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>119</FirstPage>
											<LastPage>129</LastPage>
										</BibChapter>
										<BibUnstructured>
Arora NS, Blumofe RD, Plaxton CG (1998) Thread scheduling for multiprogrammed multiprocessors. In: Proceedings of ACM symposium on parallel algorithms and architectures (SPAA). ACM, New York, pp 119–129
										</BibUnstructured>
									</Citation>
									<Citation ID="CR5">
										<CitationNumber>5.</CitationNumber>
										<BibUnstructured>
Brooks C, Lee EA, Liu X, Neuendorffer S, Zhao Y, Zheng H (2008) Heterogeneous concurrent modeling and design in Java (vol 1: Introduction to Ptolemy II). Tech rep UCB/EECS-2008-28, EECS Department, University of California, Berkeley, Apr 2008
										</BibUnstructured>
									</Citation>
									<Citation ID="CR6">
										<CitationNumber>6.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>PA</Initials>
												<FamilyName>Buhr</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>RA</Initials>
												<FamilyName>Stroobosscher</FamilyName>
											</BibAuthorName>
											<Year>1990</Year>
											<ArticleTitle Language="En">The <Emphasis Type="Italic">μ</Emphasis> system: providing light-weight concurrency on shared-memory multiprocessor computers running UNIX</ArticleTitle>
											<JournalTitle>Softw Pract Exp</JournalTitle>
											<VolumeID>20</VolumeID>
											<IssueID>9</IssueID>
											<FirstPage>929</FirstPage>
											<LastPage>964</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1002/spe.4380200906</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>
Buhr PA, Stroobosscher RA (1990) The <Emphasis Type="Italic">μ</Emphasis> system: providing light-weight concurrency on shared-memory multiprocessor computers running UNIX. Softw Pract Exp 20(9):929–964
										</BibUnstructured>
									</Citation>
									<Citation ID="CR7">
										<CitationNumber>7.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>U</Initials>
												<FamilyName>Catalyurek</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>E</Initials>
												<FamilyName>Boman</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>K</Initials>
												<FamilyName>Devine</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Bozdag</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Heaphy</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>L</Initials>
												<FamilyName>Riesen</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">Hypergraph-based dynamic load balancing for adaptive scientific computations</ChapterTitle>
											<BookTitle>Proc of 21st international parallel and distributed processing symposium (IPDPS’07)</BookTitle>
											<PublisherName>IEEE Press</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<BibComments>Also available as Sandia National Labs Tech Report SAND2006-6450C</BibComments>
										</BibChapter>
										<BibUnstructured>
Catalyurek U, Boman E, Devine K, Bozdag D, Heaphy R, Riesen L (2007) Hypergraph-based dynamic load balancing for adaptive scientific computations. In: Proc of 21st international parallel and distributed processing symposium (IPDPS’07). IEEE Press, New York. Also available as Sandia National Labs Tech Report SAND2006-6450C
										</BibUnstructured>
									</Citation>
									<Citation ID="CR8">
										<CitationNumber>8.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Chaiken</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Jenkins</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P-Å</Initials>
												<FamilyName>Larson</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Ramsey</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Shakib</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Weaver</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Zhou</FamilyName>
											</BibAuthorName>
											<Year>2008</Year>
											<ArticleTitle Language="En">Scope: easy and efficient parallel processing of massive data sets</ArticleTitle>
											<JournalTitle>Proc VLDB Endow</JournalTitle>
											<VolumeID>1</VolumeID>
											<IssueID>2</IssueID>
											<FirstPage>1265</FirstPage>
											<LastPage>1276</LastPage>
										</BibArticle>
										<BibUnstructured>
Chaiken R, Jenkins B, Larson P-Å, Ramsey B, Shakib D, Weaver S, Zhou J (2008) Scope: easy and efficient parallel processing of massive data sets. Proc VLDB Endow 1(2):1265–1276
										</BibUnstructured>
									</Citation>
									<Citation ID="CR9">
										<CitationNumber>9.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Chih Yang</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Dasdan</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R-L</Initials>
												<FamilyName>Hsiao</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>DS</Initials>
												<FamilyName>Parker</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">Map-Reduce-Merge: simplified relational data processing on large clusters</ChapterTitle>
											<BookTitle>Proceedings of ACM international conference on management of data (SIGMOD)</BookTitle>
											<FirstPage>1029</FirstPage>
											<LastPage>1040</LastPage>
										</BibChapter>
										<BibUnstructured>
Chih Yang H, Dasdan A, Hsiao R-L, Parker DS (2007) Map-Reduce-Merge: simplified relational data processing on large clusters. In: Proceedings of ACM international conference on management of data (SIGMOD), pp 1029–1040
										</BibUnstructured>
									</Citation>
									<Citation ID="CR10">
										<CitationNumber>10.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>E</Initials>
												<FamilyName>Kock</FamilyName>
												<Particle>de</Particle>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>G</Initials>
												<FamilyName>Essink</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>WJM</Initials>
												<FamilyName>Smits</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Wolf</FamilyName>
												<Particle>van der</Particle>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>J-Y</Initials>
												<FamilyName>Brunei</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>W</Initials>
												<FamilyName>Kruijtzer</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Lieverse</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>KA</Initials>
												<FamilyName>Vissers</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>K</Initials>
												<FamilyName>Yapi</FamilyName>
											</BibAuthorName>
											<Year>2000</Year>
											<ChapterTitle Language="En">Application modeling for signal processing systems</ChapterTitle>
											<BookTitle>Proceedings of design automation conference</BookTitle>
											<FirstPage>402</FirstPage>
											<LastPage>405</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/DAC.2000.855344</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
de Kock E, Essink G, Smits WJM, van der Wolf R, Brunei J-Y, Kruijtzer W, Lieverse P, Vissers KA, Yapi K (2000) Application modeling for signal processing systems. In: Proceedings of design automation conference, pp 402–405
										</BibUnstructured>
									</Citation>
									<Citation ID="CR11">
										<CitationNumber>11.</CitationNumber>
										<BibUnstructured>
de Kruijf M, Sankaralingam K (2007) MapReduce for the Cell BE architecture. University of Wisconsin Computer Sciences technical report CS-TR-2007 1625
										</BibUnstructured>
									</Citation>
									<Citation ID="CR12">
										<CitationNumber>12.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Dean</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Ghemawat</FamilyName>
											</BibAuthorName>
											<Year>2004</Year>
											<ChapterTitle Language="En">MapReduce: simplified data processing on large clusters</ChapterTitle>
											<BookTitle>Proceedings of symposium on operating systems design &amp; implementation (OSDI)</BookTitle>
											<PublisherName>USENIX Association</PublisherName>
											<PublisherLocation>Berkeley</PublisherLocation>
											<FirstPage>10</FirstPage>
										</BibChapter>
										<BibUnstructured>
Dean J, Ghemawat S (2004) MapReduce: simplified data processing on large clusters. In: Proceedings of symposium on operating systems design &amp; implementation (OSDI). USENIX Association, Berkeley, p 10
										</BibUnstructured>
									</Citation>
									<Citation ID="CR13">
										<CitationNumber>13.</CitationNumber>
										<BibUnstructured>
Dean J, Ghemawat S (2010) System and method for efficient large-scale data processing. US Patent No 7650331, Jan 2010
										</BibUnstructured>
									</Citation>
									<Citation ID="CR14">
										<CitationNumber>14.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Gedik</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Andrade</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>K-L</Initials>
												<FamilyName>Wu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>PS</Initials>
												<FamilyName>Yu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Doo</FamilyName>
											</BibAuthorName>
											<Year>2008</Year>
											<ChapterTitle Language="En">Spade: the system s declarative stream processing engine</ChapterTitle>
											<BookTitle>SIGMOD ’08: proceedings of the 2008 ACM SIGMOD international conference on management of data</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>1123</FirstPage>
											<LastPage>1134</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1376616.1376729</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Gedik B, Andrade H, Wu K-L, Yu PS, Doo M (2008) Spade: the system s declarative stream processing engine. In: SIGMOD ’08: proceedings of the 2008 ACM SIGMOD international conference on management of data. ACM, New York, pp 1123–1134
										</BibUnstructured>
									</Citation>
									<Citation ID="CR15">
										<CitationNumber>15.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Geilen</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Basten</FamilyName>
											</BibAuthorName>
											<Year>2003</Year>
											<ChapterTitle Language="En">Requirements on the execution of Kahn process networks</ChapterTitle>
											<BookTitle>Programming languages and systems, European symposium on programming (ESOP)</BookTitle>
											<PublisherName>Springer</PublisherName>
											<PublisherLocation>Berlin</PublisherLocation>
											<FirstPage>319</FirstPage>
											<LastPage>334</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1007/3-540-36575-3_22</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Geilen M, Basten T (2003) Requirements on the execution of Kahn process networks. In: Programming languages and systems, European symposium on programming (ESOP). Springer, Berlin, pp 319–334
										</BibUnstructured>
									</Citation>
									<Citation ID="CR16">
										<CitationNumber>16.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Giacomoni</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Moseley</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Vachharajani</FamilyName>
											</BibAuthorName>
											<Year>2008</Year>
											<ChapterTitle Language="En">FastForward for efficient pipeline parallelism: a cache-optimized concurrent lock-free queue</ChapterTitle>
											<BookTitle>PPoPP: proceedings of the ACM SIGPLAN symposium on principles and practice of parallel programming</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>43</FirstPage>
											<LastPage>52</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1345206.1345215</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Giacomoni J, Moseley T, Vachharajani M (2008) FastForward for efficient pipeline parallelism: a cache-optimized concurrent lock-free queue. In: PPoPP: proceedings of the ACM SIGPLAN symposium on principles and practice of parallel programming. ACM, New York, pp 43–52
										</BibUnstructured>
									</Citation>
									<Citation ID="CR17">
										<CitationNumber>17.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>MI</Initials>
												<FamilyName>Gordon</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>W</Initials>
												<FamilyName>Thies</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Amarasinghe</FamilyName>
											</BibAuthorName>
											<Year>2006</Year>
											<ChapterTitle Language="En">Exploiting coarse-grained task, data, and pipeline parallelism in stream programs</ChapterTitle>
											<BookTitle>ASPLOS-XII: proceedings of the 12th international conference on architectural support for programming languages and operating systems</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>151</FirstPage>
											<LastPage>162</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1168857.1168877</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Gordon MI, Thies W, Amarasinghe S (2006) Exploiting coarse-grained task, data, and pipeline parallelism in stream programs. In: ASPLOS-XII: proceedings of the 12th international conference on architectural support for programming languages and operating systems. ACM, New York, pp 151–162
										</BibUnstructured>
									</Citation>
									<Citation ID="CR18">
										<CitationNumber>18.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>He</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>W</Initials>
												<FamilyName>Fang</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>Q</Initials>
												<FamilyName>Luo</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>NK</Initials>
												<FamilyName>Govindaraju</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Wang</FamilyName>
											</BibAuthorName>
											<Year>2008</Year>
											<ChapterTitle Language="En">Mars: a MapReduce framework on graphics processors</ChapterTitle>
											<BookTitle>PACT ’08: proceedings of the 17th international conference on parallel architectures and compilation techniques</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>260</FirstPage>
											<LastPage>269</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1454115.1454152</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
He B, Fang W, Luo Q, Govindaraju NK, Wang T (2008) Mars: a MapReduce framework on graphics processors. In: PACT ’08: proceedings of the 17th international conference on parallel architectures and compilation techniques. ACM, New York, pp 260–269
										</BibUnstructured>
									</Citation>
									<Citation ID="CR19">
										<CitationNumber>19.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Hudak</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Hughes</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>SP</Initials>
												<FamilyName>Jones</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Wadler</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">A history of Haskell: being lazy with class</ChapterTitle>
											<BookTitle>HOPL III: proceedings of the 3rd ACM SIGPLAN conference on history of programming languages</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>12-1</FirstPage>
											<LastPage>12-55</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1238844.1238856</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Hudak P, Hughes J, Jones SP, Wadler P (2007) A history of Haskell: being lazy with class. In: HOPL III: proceedings of the 3rd ACM SIGPLAN conference on history of programming languages, pp 12-1–12-55. ACM, New York
										</BibUnstructured>
									</Citation>
									<Citation ID="CR20">
										<CitationNumber>20.</CitationNumber>
										<BibUnstructured>
Intel Corporation, Threading building blocks. <ExternalRef>
												<RefSource>http://www.threadingbuildingblocks.org</RefSource>
												<RefTarget Address="http://www.threadingbuildingblocks.org" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR21">
										<CitationNumber>21.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Isard</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Budiu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>Y</Initials>
												<FamilyName>Yu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Birrell</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Fetterly</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">Dryad: distributed data-parallel programs from sequential building blocks</ChapterTitle>
											<BookTitle>Proc of the ACM SIGOPS/EuroSys European conference on computer systems</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>59</FirstPage>
											<LastPage>72</LastPage>
										</BibChapter>
										<BibUnstructured>
Isard M, Budiu M, Yu Y, Birrell A, Fetterly D (2007) Dryad: distributed data-parallel programs from sequential building blocks. In: Proc of the ACM SIGOPS/EuroSys European conference on computer systems. ACM, New York, pp 59–72
										</BibUnstructured>
									</Citation>
									<Citation ID="CR22">
										<CitationNumber>22.</CitationNumber>
										<BibUnstructured>
Kahn G (1974) The semantics of a simple language for parallel programming. Inf Process 74
										</BibUnstructured>
									</Citation>
									<Citation ID="CR23">
										<CitationNumber>23.</CitationNumber>
										<BibBook>
											<BibAuthorName>
												<Initials>DE</Initials>
												<FamilyName>Knuth</FamilyName>
											</BibAuthorName>
											<Year>1997</Year>
											<BookTitle>Fundamental Algorithms. The Art of Computer Programming</BookTitle>
											<NumberInSeries>1</NumberInSeries>
											<PublisherName>Addison–Wesley</PublisherName>
											<PublisherLocation>Reading</PublisherLocation>
										</BibBook>
										<BibUnstructured>
Knuth DE (1997) Fundamental Algorithms. The Art of Computer Programming, vol 1. Addison–Wesley, Reading
										</BibUnstructured>
									</Citation>
									<Citation ID="CR24">
										<CitationNumber>24.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Lämmel</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ArticleTitle Language="En">Google’s MapReduce programming model—revisited</ArticleTitle>
											<JournalTitle>Sci Comput Program</JournalTitle>
											<VolumeID>68</VolumeID>
											<IssueID>3</IssueID>
											<FirstPage>208</FirstPage>
											<LastPage>237</LastPage>
										</BibArticle>
										<BibUnstructured>
Lämmel R (2007) Google’s MapReduce programming model—revisited. Sci Comput Program 68(3):208–237
										</BibUnstructured>
									</Citation>
									<Citation ID="CR25">
										<CitationNumber>25.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>EA</Initials>
												<FamilyName>Lee</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Parks</FamilyName>
											</BibAuthorName>
											<Year>1995</Year>
											<ArticleTitle Language="En">Dataflow process networks</ArticleTitle>
											<JournalTitle>Proc IEEE</JournalTitle>
											<VolumeID>83</VolumeID>
											<IssueID>5</IssueID>
											<FirstPage>773</FirstPage>
											<LastPage>801</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/5.381846</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>
Lee EA, Parks T (1995) Dataflow process networks. Proc IEEE 83(5):773–801
										</BibUnstructured>
									</Citation>
									<Citation ID="CR26">
										<CitationNumber>26.</CitationNumber>
										<BibUnstructured>
Message passing interface forum, Accessed July 2009. <ExternalRef>
												<RefSource>http://www.mpi-forum.org/</RefSource>
												<RefTarget Address="http://www.mpi-forum.org/" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR27">
										<CitationNumber>27.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Olson</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Evans</FamilyName>
											</BibAuthorName>
											<Year>2005</Year>
											<ChapterTitle Language="En">Deadlock detection for distributed process networks</ChapterTitle>
											<BookTitle>ICASSP: Proc of IEEE international conference on acoustics, speech, and signal processing</BookTitle>
											<ConfEventDate>March 2005</ConfEventDate>
											<NumberInSeries>5</NumberInSeries>
											<FirstPage>73</FirstPage>
											<LastPage>76</LastPage>
										</BibChapter>
										<BibUnstructured>
Olson A, Evans B (2005) Deadlock detection for distributed process networks. In: ICASSP: Proc of IEEE international conference on acoustics, speech, and signal processing, March 2005, vol 5, pp 73–76
										</BibUnstructured>
									</Citation>
									<Citation ID="CR28">
										<CitationNumber>28.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Olston</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Reed</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>U</Initials>
												<FamilyName>Srivastava</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Kumar</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Tomkins</FamilyName>
											</BibAuthorName>
											<Year>2008</Year>
											<ChapterTitle Language="En">Pig latin: a not-so-foreign language for data processing</ChapterTitle>
											<BookTitle>SIGMOD ’08: proceedings of the 2008 ACM SIGMOD international conference on management of data</BookTitle>
											<PublisherName>ACM</PublisherName>
											<PublisherLocation>New York</PublisherLocation>
											<FirstPage>1099</FirstPage>
											<LastPage>1110</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1145/1376616.1376726</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Olston C, Reed B, Srivastava U, Kumar R, Tomkins A (2008) Pig latin: a not-so-foreign language for data processing. In: SIGMOD ’08: proceedings of the 2008 ACM SIGMOD international conference on management of data. ACM, New York, pp 1099–1110
										</BibUnstructured>
									</Citation>
									<Citation ID="CR29">
										<CitationNumber>29.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Pike</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Dorward</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Griesemer</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Quinlan</FamilyName>
											</BibAuthorName>
											<Year>2005</Year>
											<ArticleTitle Language="En">Interpreting the data: parallel analysis with Sawzall</ArticleTitle>
											<JournalTitle>Sci Program</JournalTitle>
											<VolumeID>13</VolumeID>
											<IssueID>4</IssueID>
											<FirstPage>277</FirstPage>
											<LastPage>298</LastPage>
										</BibArticle>
										<BibUnstructured>
Pike R, Dorward S, Griesemer R, Quinlan S (2005) Interpreting the data: parallel analysis with Sawzall. Sci Program 13(4):277–298
										</BibUnstructured>
									</Citation>
									<Citation ID="CR30">
										<CitationNumber>30.</CitationNumber>
										<BibUnstructured>
PVM (Parallel Virtual Machine), Accessed August 2010. <ExternalRef>
												<RefSource>http://www.csm.ornl.gov/pvm/</RefSource>
												<RefTarget Address="http://www.csm.ornl.gov/pvm/" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR31">
										<CitationNumber>31.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Ranger</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Raghuraman</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Penmetsa</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>G</Initials>
												<FamilyName>Bradski</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Kozyrakis</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">Evaluating MapReduce for multi-core and multiprocessor systems</ChapterTitle>
											<BookTitle>Proceedings of the IEEE international symposium on high performance computer architecture (HPCA)</BookTitle>
											<PublisherName>IEEE Computer Society</PublisherName>
											<PublisherLocation>Washington</PublisherLocation>
											<FirstPage>13</FirstPage>
											<LastPage>24</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/HPCA.2007.346181</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Ranger C, Raghuraman R, Penmetsa A, Bradski G, Kozyrakis C (2007) Evaluating MapReduce for multi-core and multiprocessor systems. In: Proceedings of the IEEE international symposium on high performance computer architecture (HPCA). IEEE Computer Society, Washington, pp 13–24
										</BibUnstructured>
									</Citation>
									<Citation ID="CR32">
										<CitationNumber>32.</CitationNumber>
										<BibUnstructured>
Richardson IEG H.264/MPEG-4 part 10 white paper. Available online. <ExternalRef>
												<RefSource>http://www.vcodex.com/files/h264_overview_orig.pdf</RefSource>
												<RefTarget Address="http://www.vcodex.com/files/h264_overview_orig.pdf" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR33">
										<CitationNumber>33.</CitationNumber>
										<BibUnstructured>
The OpenMP API specification for parallel programming, Accessed July 2009. <ExternalRef>
												<RefSource>http://openmp.org/wp/</RefSource>
												<RefTarget Address="http://openmp.org/wp/" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR34">
										<CitationNumber>34.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Thompson</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Pimentel</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ChapterTitle Language="En">Towards multi-application workload modeling in sesame for system-level design space exploration</ChapterTitle>
											<BookTitle>Embedded computer systems: architectures, modeling, and simulation</BookTitle>
											<NumberInSeries>4599/2007</NumberInSeries>
											<FirstPage>222</FirstPage>
											<LastPage>232</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1007/978-3-540-73625-7_24</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Thompson M, Pimentel A (2007) Towards multi-application workload modeling in sesame for system-level design space exploration. In: Embedded computer systems: architectures, modeling, and simulation, vol 4599/2007, pp 222–232
										</BibUnstructured>
									</Citation>
									<Citation ID="CR35">
										<CitationNumber>35.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>SV</Initials>
												<FamilyName>Valvåg</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Johansen</FamilyName>
											</BibAuthorName>
											<Year>2008</Year>
											<ChapterTitle Language="En">Oivos: Simple and efficient distributed data processing</ChapterTitle>
											<BookTitle>Proceedings of IEEE international conference on high performance computing and communications (HPCC)</BookTitle>
											<FirstPage>113</FirstPage>
											<LastPage>122</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/HPCC.2008.105</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Valvåg SV, Johansen D (2008) Oivos: Simple and efficient distributed data processing. In: Proceedings of IEEE international conference on high performance computing and communications (HPCC), pp 113–122
										</BibUnstructured>
									</Citation>
									<Citation ID="CR36">
										<CitationNumber>36.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>SV</Initials>
												<FamilyName>Valvåg</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Johansen</FamilyName>
											</BibAuthorName>
											<Year>2009</Year>
											<ChapterTitle Language="En">Cogset: A unified engine for reliable storage and parallel processing</ChapterTitle>
											<BookTitle>Proceedings of IFIP international conference on network and parallel computing workshops (NPC)</BookTitle>
											<FirstPage>174</FirstPage>
											<LastPage>181</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/NPC.2009.23</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Valvåg SV, Johansen D (2009) Cogset: A unified engine for reliable storage and parallel processing. In: Proceedings of IFIP international conference on network and parallel computing workshops (NPC), pp 174–181
										</BibUnstructured>
									</Citation>
									<Citation ID="CR37">
										<CitationNumber>37.</CitationNumber>
										<BibUnstructured>
Vrba Ž (2009) Implementation and performance aspects of Kahn process networks. PhD thesis, Department of Informatics, University of Oslo, Norway, Dec 2009. Dissertation No 903
										</BibUnstructured>
									</Citation>
									<Citation ID="CR38">
										<CitationNumber>38.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>Ž</Initials>
												<FamilyName>Vrba</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Halvorsen</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Griwodz</FamilyName>
											</BibAuthorName>
											<Year>2009</Year>
											<ChapterTitle Language="En">Evaluating the run-time performance of Kahn process network implementation techniques on shared-memory multiprocessors</ChapterTitle>
											<BookTitle>International conference on complex, intelligent and software intensive systems (CISIS)—international workshop on multi-core computing systems (MuCoCoS)</BookTitle>
											<FirstPage>639</FirstPage>
											<LastPage>644</LastPage>
										</BibChapter>
										<BibUnstructured>
Vrba Ž, Halvorsen P, Griwodz C (2009) Evaluating the run-time performance of Kahn process network implementation techniques on shared-memory multiprocessors. In: International conference on complex, intelligent and software intensive systems (CISIS)—international workshop on multi-core computing systems (MuCoCoS), pp 639–644
										</BibUnstructured>
									</Citation>
									<Citation ID="CR39">
										<CitationNumber>39.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>Ž</Initials>
												<FamilyName>Vrba</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Halvorsen</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Griwodz</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Beskow</FamilyName>
											</BibAuthorName>
											<Year>2009</Year>
											<ChapterTitle Language="En">Kahn process networks are a flexible alternative to MapReduce</ChapterTitle>
											<BookTitle>IEEE international conference on high performance computing and communications (HPCC)</BookTitle>
											<FirstPage>154</FirstPage>
											<LastPage>162</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/HPCC.2009.46</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Vrba Ž, Halvorsen P, Griwodz C, Beskow P (2009) Kahn process networks are a flexible alternative to MapReduce. In: IEEE international conference on high performance computing and communications (HPCC), pp 154–162
										</BibUnstructured>
									</Citation>
									<Citation ID="CR40">
										<CitationNumber>40.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>Ž</Initials>
												<FamilyName>Vrba</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Halvorsen</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Griwodz</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Beskow</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Johansen</FamilyName>
											</BibAuthorName>
											<Year>2009</Year>
											<ChapterTitle Language="En">The Nornir run-time system for parallel programs using Kahn process networks</ChapterTitle>
											<BookTitle>6th international conference on network and parallel computing</BookTitle>
											<ConfEventLocation>(NPC)</ConfEventLocation>
											<ConfEventDate>October 2009</ConfEventDate>
											<PublisherName>IEEE Computer Society</PublisherName>
											<PublisherLocation>Los Alamitos</PublisherLocation>
											<FirstPage>1</FirstPage>
											<LastPage>8</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/NPC.2009.19</Handle>
											</Occurrence>
										</BibChapter>
										<BibUnstructured>
Vrba Ž, Halvorsen P, Griwodz C, Beskow P, Johansen D (2009) The Nornir run-time system for parallel programs using Kahn process networks. In: 6th international conference on network and parallel computing (NPC), October 2009. IEEE Computer Society, Los Alamitos, pp 1–8
										</BibUnstructured>
									</Citation>
									<Citation ID="CR41">
										<CitationNumber>41.</CitationNumber>
										<BibChapter>
											<BibAuthorName>
												<Initials>Ž</Initials>
												<FamilyName>Vrba</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Halvorsen</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Griwodz</FamilyName>
											</BibAuthorName>
											<Year>2010</Year>
											<ChapterTitle Language="En">A simple improvement of the work-stealing scheduling algorithm</ChapterTitle>
											<BookTitle>International conference on complex, intelligent and software intensive systems (CISIS)—international workshop on multi-core computing systems (MuCoCoS)</BookTitle>
											<FirstPage>925</FirstPage>
											<LastPage>930</LastPage>
										</BibChapter>
										<BibUnstructured>
Vrba Ž, Halvorsen P, Griwodz C (2010) A simple improvement of the work-stealing scheduling algorithm. In: International conference on complex, intelligent and software intensive systems (CISIS)—international workshop on multi-core computing systems (MuCoCoS), pp 925–930
										</BibUnstructured>
									</Citation>
								</Bibliography>
							</ArticleBackmatter>
						</Article>
					</JournalOnlineFirst>
				</Journal>
				<meta:Info xmlns:meta="http://www.springer.com/app/meta">
					<meta:DateLoaded>2010-11-16T02:05:17.776919+01:00</meta:DateLoaded>
					<meta:Authors>
						<meta:Author>Vrba, Željko</meta:Author>
						<meta:Author>Halvorsen, Pål</meta:Author>
						<meta:Author>Griwodz, Carsten</meta:Author>
						<meta:Author>Beskow, Paul</meta:Author>
						<meta:Author>Espeland, Håvard</meta:Author>
						<meta:Author>Johansen, Dag</meta:Author>
					</meta:Authors>
					<meta:Institutions>
						<meta:Institution geo="10.7460924,59.9127263">
							<meta:OrgName>Simula Research Laboratory</meta:OrgName>
							<meta:GeoOrg>10.7460924,59.9127263#Simula Research Laboratory</meta:GeoOrg>
							<meta:Country>Norway</meta:Country>
						</meta:Institution>
						<meta:Institution geo="10.7217560,59.9420946">
							<meta:OrgName>University of Oslo</meta:OrgName>
							<meta:GeoOrg>10.7217560,59.9420946#University of Oslo</meta:GeoOrg>
							<meta:Country>Norway</meta:Country>
						</meta:Institution>
						<meta:Institution geo="18.9118937,69.6348384">
							<meta:OrgName>University of Tromsø</meta:OrgName>
							<meta:GeoOrg>18.9118937,69.6348384#University of Tromsø</meta:GeoOrg>
							<meta:Country>Norway</meta:Country>
						</meta:Institution>
					</meta:Institutions>
					<meta:Date>2010-11-13</meta:Date>
					<meta:Type>Article</meta:Type>
					<meta:DOI>10.1007/s11227-010-0503-2</meta:DOI>
					<meta:Title>The <Emphasis Type="Italic">Nornir</Emphasis> run-time system for parallel programs using Kahn process networks on multi-core machines—a flexible alternative to MapReduce</meta:Title>
					<meta:ISXN>1573-0484</meta:ISXN>
					<meta:PubName>Springer</meta:PubName>
					<meta:Journal>The Journal of Supercomputing</meta:Journal>
					<meta:Publication>The Journal of Supercomputing</meta:Publication>
					<meta:PublicationType>Journal</meta:PublicationType>
					<meta:SubjectGroup>
						<meta:Subject Type="Primary">Computer Science</meta:Subject>
						<meta:Subject Type="Secondary">Computer Science, general</meta:Subject>
						<meta:Subject Type="Secondary">Processor Architectures</meta:Subject>
						<meta:Subject Type="Secondary">Programming Languages, Compilers, Interpreters</meta:Subject>
					</meta:SubjectGroup>
				</meta:Info>
			</Publisher>
			<Images/>
		</result>
		<result>
			<Publisher xml:lang="en">
				<PublisherInfo>
					<PublisherName>Springer US</PublisherName>
					<PublisherLocation>Boston</PublisherLocation>
					<PublisherURL>http://www.springer-ny.com</PublisherURL>
				</PublisherInfo>
				<Journal OutputMedium="All">
					<JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
						<JournalID>11042</JournalID>
						<JournalPrintISSN>1380-7501</JournalPrintISSN>
						<JournalElectronicISSN>1573-7721</JournalElectronicISSN>
						<JournalTitle>Multimedia Tools and Applications</JournalTitle>
						<JournalSubTitle>An International Journal</JournalSubTitle>
						<JournalAbbreviatedTitle>Multimed Tools Appl</JournalAbbreviatedTitle>
						<JournalSubjectGroup>
							<JournalSubject Type="Primary">Computer Science</JournalSubject>
							<JournalSubject Type="Secondary">Special Purpose and Application-Based Systems</JournalSubject>
							<JournalSubject Type="Secondary">Data Structures, Cryptology and Information Theory</JournalSubject>
							<JournalSubject Type="Secondary">Computer Communication Networks</JournalSubject>
							<JournalSubject Type="Secondary">Multimedia Information Systems</JournalSubject>
						</JournalSubjectGroup>
					</JournalInfo>
					<JournalOnlineFirst>
						<Article ID="s11042-011-0727-z">
							<ArticleInfo ArticleCitation="ArticleFirstPage" ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
								<ArticleID>727</ArticleID>
								<ArticleDOI>10.1007/s11042-011-0727-z</ArticleDOI>
								<ArticleSequenceNumber>0</ArticleSequenceNumber>
								<ArticleTitle Language="En">Event retrieval in video archives using rough set theory and partially supervised learning</ArticleTitle>
								<ArticleFirstPage>1</ArticleFirstPage>
								<ArticleLastPage>29</ArticleLastPage>
								<ArticleHistory>
									<RegistrationDate>
										<Year>2011</Year>
										<Month>1</Month>
										<Day>5</Day>
									</RegistrationDate>
									<OnlineDate>
										<Year>2011</Year>
										<Month>1</Month>
										<Day>20</Day>
									</OnlineDate>
								</ArticleHistory>
								<ArticleCopyright>
									<CopyrightHolderName>The Author(s)</CopyrightHolderName>
									<CopyrightYear>2011</CopyrightYear>
								</ArticleCopyright>
								<ArticleGrants Type="OpenChoice">
									<MetadataGrant Grant="OpenAccess"/>
									<AbstractGrant Grant="OpenAccess"/>
									<BodyPDFGrant Grant="OpenAccess"/>
									<BodyHTMLGrant Grant="OpenAccess"/>
									<BibliographyGrant Grant="OpenAccess"/>
									<ESMGrant Grant="OpenAccess"/>
								</ArticleGrants>
								<ArticleContext>
									<JournalID>11042</JournalID>
									<VolumeIDStart>
										<?InsertByIssueBuilding VolumeIDStart?>
									</VolumeIDStart>
									<VolumeIDEnd>
										<?InsertByIssueBuilding VolumeIDEnd?>
									</VolumeIDEnd>
									<IssueIDStart>
										<?InsertByIssueBuilding IssueIDStart?>
									</IssueIDStart>
									<IssueIDEnd>
										<?InsertByIssueBuilding IssueIDEnd?>
									</IssueIDEnd>
								</ArticleContext>
							</ArticleInfo>
							<ArticleHeader>
								<AuthorGroup>
									<Author AffiliationIDS="Aff1" CorrespondingAffiliationID="Aff1">
										<AuthorName DisplayOrder="Western">
											<GivenName>Kimiaki</GivenName>
											<FamilyName>Shirahama</FamilyName>
										</AuthorName>
										<Contact>
											<Phone>+81-78-803-6834</Phone>
											<Fax>+81-78-803-6834</Fax>
											<Email>shirahama@econ.kobe-u.ac.jp</Email>
										</Contact>
										<Biography>
											<FormalPara OutputMedium="All" RenderingStyle="Style1">
												<Heading>Kimiaki Shirahama</Heading>
												<Para>received his B.E., M.E. and Ph.D. degrees in Engineering from Kobe University, Japan in 2003, 2005 and 2011, respectively. Currently, he is an assistant professor in the Graduate School of Economics at Kobe University. His research interests include multimedia data processing, data mining and virtual reality. He is a member of ACM SIGKDD, the Institute of Image Information and Television Engineers in Japan (ITE), Information Processing Society of Japan (IPSJ) and the Institute of Electronics, Information and Communication Engineering in Japan (IEICE).
													<Figure Category="Standard" Float="No" ID="Figa">
														<MediaObject>
															<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Figa_HTML.gif"/>
														</MediaObject>
													</Figure>
												</Para>
											</FormalPara>
										</Biography>
									</Author>
									<Author AffiliationIDS="Aff2">
										<AuthorName DisplayOrder="Western">
											<GivenName>Yuta</GivenName>
											<FamilyName>Matsuoka</FamilyName>
										</AuthorName>
										<Contact>
											<Email>matuoka@ai.cs.scitec.kobe-u.ac.jp</Email>
										</Contact>
										<Biography>
											<FormalPara OutputMedium="All" RenderingStyle="Style1">
												<Heading>Yuta Matsuoka</Heading>
												<Para>received his B.E. degree from Kobe University, Japan in 2009. He is currently pursuing his M.E. degree in the Graduate School of Engineering at Kobe University. His research interests include image/video data retrieval and machine learning. He is a member of the Institute of Electronics, Information and Communication Engineering in Japan (IEICE).
													<Figure Category="Standard" Float="No" ID="Figb">
														<MediaObject>
															<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Figb_HTML.gif"/>
														</MediaObject>
													</Figure>
												</Para>
											</FormalPara>
										</Biography>
									</Author>
									<Author AffiliationIDS="Aff3">
										<AuthorName DisplayOrder="Western">
											<GivenName>Kuniaki</GivenName>
											<FamilyName>Uehara</FamilyName>
										</AuthorName>
										<Contact>
											<Email>uehara@kobe-u.ac.jp</Email>
										</Contact>
										<Biography>
											<FormalPara OutputMedium="All" RenderingStyle="Style1">
												<Heading>Kuniaki Uehara</Heading>
												<Para>received his B.E., M.E. and Ph.D. degrees in Information and Computer Sciences from Osaka University, Japan in 1978, 1980 and 1984, respectively. He was an assistant professor in the Institute of Scientific and Industrial Research, Osaka University from 1984 to 1990, an associate professor in the Department of Computer Science and Systems Engineering, Kobe University from 1990 to 1997, a professor in the Research Center for Urban Safety and Security, Kobe University from 1997 to 2002 and a professor in the Graduate School of Science and Technology, Kobe University from 2002 to 2009. Currently, he is a professor in the Graduate School of System Informatics, Kobe University. From 1989 to 1990, he was also a visiting assistant professor at Oregon State University. He has published a number of books, articles and conference papers in wide areas of artificial intelligence, especially in machine learning, natural language processing and intelligent software engineering. He is a member of AAAI, Information Processing Society of Japan (IPSJ), the Japan Society for Artificial Intelligence (JSAI), the Institute of Electronics, Information and Communication Engineering in Japan (IEICE), the Mathematical Linguistic Society of Japan and Japan Society for Software Science and Technology (JSSST).
													<Figure Category="Standard" Float="No" ID="Figc">
														<MediaObject>
															<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Figc_HTML.gif"/>
														</MediaObject>
													</Figure>
												</Para>
											</FormalPara>
										</Biography>
									</Author>
									<Affiliation ID="Aff1">
										<OrgDivision>Graduate School of Economics</OrgDivision>
										<OrgName>Kobe University</OrgName>
										<OrgAddress>
											<Street>2-1, Rokkodai, Nada</Street>
											<City>Kobe</City>
											<Postcode>657-8501</Postcode>
											<Country>Japan</Country>
										</OrgAddress>
									</Affiliation>
									<Affiliation ID="Aff2">
										<OrgDivision>Graduate School of Engineering</OrgDivision>
										<OrgName>Kobe University</OrgName>
										<OrgAddress>
											<Street>1-1, Rokkodai, Nada</Street>
											<City>Kobe</City>
											<Postcode>657-8501</Postcode>
											<Country>Japan</Country>
										</OrgAddress>
									</Affiliation>
									<Affiliation ID="Aff3">
										<OrgDivision>Graduate School of System Informatics</OrgDivision>
										<OrgName>Kobe University</OrgName>
										<OrgAddress>
											<Street>1-1, Rokkodai, Nada</Street>
											<City>Kobe</City>
											<Postcode>657-8501</Postcode>
											<Country>Japan</Country>
										</OrgAddress>
									</Affiliation>
								</AuthorGroup>
								<Abstract ID="Abs1" Language="En">
									<Heading>Abstract</Heading>
									<Para>This paper develops a query-by-example method for retrieving shots of an event (event shots) using example shots provided by a user. The following three problems are mainly addressed. Firstly, event shots cannot be retrieved using a single model as they contain significantly different features due to varied camera techniques, settings and so forth. This is overcome by using rough set theory to extract multiple classification rules with each rule specialized to retrieve a portion of event shots. Secondly, since a user can only provide a small number of example shots, the amount of event shots retrieved by extracted rules is inevitably limited. We thus incorporate bagging and the random subspace method. Classifiers characterize significantly different event shots depending on example shots and feature dimensions. However, this can result in the potential retrieval of many unnecessary shots. Rough set theory is used to combine classifiers into rules which provide greater retrieval accuracy. Lastly, counter example shots, which are a necessity for rough set theory, are not provided by the user. Hence, a partially supervised learning method is used to collect these from shots other than example shots. Counter example shots, which are as similar to example shots as possible, are collected because they are useful for characterizing the boundary between event shots and the remaining shots. The proposed method is tested on TRECVID 2009 video data.</Para>
								</Abstract>
								<KeywordGroup Language="En">
									<Heading>Keywords</Heading>
									<Keyword>Query by example</Keyword>
									<Keyword>Rough set theory</Keyword>
									<Keyword>Bagging</Keyword>
									<Keyword>Random subspace</Keyword>
									<Keyword>Small sample size problem</Keyword>
									<Keyword>Partially supervised learning</Keyword>
								</KeywordGroup>
							</ArticleHeader>
							<Body>
								<Section1 ID="Sec1" Type="Introduction">
									<Heading>Introduction</Heading>
									<Para>In recent times, a large number of videos have been stacked as video archives. Viewing these is a time consuming process since videos are essentially temporal media. Thus, there is great demand to develop a method which can efficiently retrieve interesting shots (or video segments) from the archive. Users are usually interested in retrieving shots which match semantic content rather than physical content such as RGB pixel values. In this paper, the term <Emphasis Type="Italic">event</Emphasis> is used to denote a query regarding semantic content. The shots matching the event are termed <Emphasis Type="Italic">event shots</Emphasis> while <Emphasis Type="Italic">non-event shots</Emphasis> refer to the remaining shots. The retrieval of event shots is termed <Emphasis Type="Italic">event retrieval</Emphasis>.</Para>
									<Para>The proper representation of an event is very important. Existing approaches can be roughly classified into two types, namely, the <Emphasis Type="Italic">Query-By-Keyword</Emphasis> (QBK) and <Emphasis Type="Italic">Query-By-Example</Emphasis> (QBE) approaches. With QBK, a user represents an event by using keywords and shots are subsequently retrieved by matching with defined keywords. With QBE, a user provides example shots to represent an event and shots are then retrieved based on their similarity to example shots in terms of features. For example, consider the event ‘people appear with computers’ depicted in Fig. <InternalRef RefID="Fig1">1</InternalRef>. Assume that an event shot, <Emphasis Type="Italic">Shot 1</Emphasis>, is annotated with the words ‘people’ and ‘computer’. To retrieve <Emphasis Type="Italic">Shot 1</Emphasis> using QBK, a user needs to enter the keywords ‘people’ and ‘computer’. However, the keywords ‘programmer’ or ‘internet user’ might be entered instead, which would not match the annotated words for <Emphasis Type="Italic">Shot 1</Emphasis>, despite matching the actual event. This ambiguity relating to semantic content makes it difficult for the user to appropriately represent events using keywords. Alternatively, with QBE, the event is defined using features contained in example shots, such as <Emphasis Type="Italic">Ex. 1</Emphasis> in Fig. <InternalRef RefID="Fig1">1</InternalRef>. This eliminates the ambiguity associated with semantic content found in the QBK approach.
										<Figure Category="Standard" Float="Yes" ID="Fig1">
											<Caption Language="En">
												<CaptionNumber>Fig. 1</CaptionNumber>
												<CaptionContent>
													<SimplePara>Comparison between the QBK and QBE approaches for the event ‘people appear with computers’</SimplePara>
												</CaptionContent>
											</Caption>
											<MediaObject>
												<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig1_HTML.gif"/>
											</MediaObject>
										</Figure>
									</Para>
									<Para>The QBE approach offers the added advantage of not requiring predefined retrieval models. In traditional QBK methods, a retrieval model needs to be prepared for each event [<CitationRef CitationID="CR15">15</CitationRef>, <CitationRef CitationID="CR30">30</CitationRef>], and recent QBK methods prepare classifiers for assessing the relevance of keywords<Footnote ID="Fn1">
											<Para>These keywords are frequently called <Emphasis Type="Italic">concepts</Emphasis>. However, some readers may confuse them with concepts which are hierarchically organized in an ontology. In light of this, we do not use the term <Emphasis Type="Italic">concept</Emphasis>.</Para>
										</Footnote> defining each event [<CitationRef CitationID="CR22">22</CitationRef>, <CitationRef CitationID="CR31">31</CitationRef>]. However, it is impractical to prepare retrieval models and classifiers for all possible events. In comparison, a retrieval model is constructed on the fly from example shots in the QBE approach. In other words, as long as example shots are provided, QBE can perform retrieval for any event. In this paper, we describe the development of a QBE method.</Para>
									<Para>The following three problems in the QBE method have been addressed.</Para>
									<Para>
										<Emphasis Type="Bold">1. Large variety of event shots</Emphasis>  Event shots are taken using different camera techniques and settings. For example, in Fig. <InternalRef RefID="Fig2">2</InternalRef>, <Emphasis Type="Italic">Shot 1</Emphasis> depicts the user’s hands with the computer monitor in a tight shot. <Emphasis Type="Italic">Shot 2</Emphasis> shows the face of a person with the front of the computer monitor while <Emphasis Type="Italic">Shot 3</Emphasis> shows a computer monitor from the side. <Emphasis Type="Italic">Shot 4</Emphasis> captures the back of a person facing the computer screen. This illustrates that objects related to the same event can be depicted in several ways. Furthermore, event shots show numerous objects unrelated to the event. For example, among four shots in Fig. <InternalRef RefID="Fig2">2</InternalRef>, the background and peoples’ clothing are different and a caption is visible in <Emphasis Type="Italic">Shot 2</Emphasis>, but not in any other shots. Thus, even for the same event, event shots not only contain significantly different features, but also many redundant features. Therefore, a single retrieval model is not capable of retrieving a large variety of event shots.
										<Figure Category="Standard" Float="Yes" ID="Fig2">
											<Caption Language="En">
												<CaptionNumber>Fig. 2</CaptionNumber>
												<CaptionContent>
													<SimplePara>An example of a variety of event shots for the event ‘people appear with computers’</SimplePara>
												</CaptionContent>
											</Caption>
											<MediaObject>
												<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig2_HTML.gif"/>
											</MediaObject>
										</Figure>
									</Para>
									<Para>
										<Emphasis Type="Italic">Rough set theory</Emphasis> (RST) is a set-theoretic classification method for extracting rough descriptions of a class from imprecise (or noisy) data, and is used for retrieving a variety of event shots [<CitationRef CitationID="CR13">13</CitationRef>]. RST can be used to extract multiple classification rules, which can correctly identify different subsets of example shots. In other words, each classification rule is specialized for retrieving event shots characterized by certain features. Hence, by accumulating event shots retrieved with various classification rules, we can retrieve a variety of event shots.</Para>
									<Para>
										<Emphasis Type="Bold">2. Small sample size</Emphasis>  In QBE, a user can only provide a small number of example shots for an event. Since QBE, by definition, retrieves shots similar to example shots, a small number of example shots will inevitably lead to a small range of event shots. We use <Emphasis Type="Italic">bagging</Emphasis> and the <Emphasis Type="Italic">random subspace method</Emphasis> to overcome this problem. Specifically, various classifiers are built using randomly selected example shots and feature dimensions. When only a small number of example shots are available, classifiers output significantly different classification results depending on example shots [<CitationRef CitationID="CR32">32</CitationRef>]. In addition, classification results differ depending on feature dimensions [<CitationRef CitationID="CR8">8</CitationRef>]. Thus, by building various classifiers using bagging and the random subspace method, we can extend the range of event shots that can be retrieved. However, this also results in many non-event shots potentially being retrieved. To overcome this, RST is used to extract classification rules as combinations of classifiers, which can accurately retrieve event shots.</Para>
									<Para>
										<Emphasis Type="Bold">3. Lack of negative examples</Emphasis>  RST extracts classification rules to enable the discrimination of event shots from non-event shots. This requires two types of example shots, <Emphasis Type="Italic">positive examples</Emphasis> (p-examples), provided by a user to serve as representatives of event shots, and <Emphasis Type="Italic">negative examples</Emphasis> (n-examples), which are not provided by the user and serve as representatives of non-event shots. To solve the lack of n-examples, we formulate the QBE approach as <Emphasis Type="Italic">partially supervised learning</Emphasis> (PSL). Classification rules are extracted using p-examples and <Emphasis Type="Italic">unlabeled examples</Emphasis> (u-examples), which refer to shots other than p-examples. N-examples are collected from these u-examples. It can be considered that n-examples similar to p-examples are informative, because they allow the characterization of the boundary between event and non-event shots. To collect such n-examples, we devise a PSL method based on the coarse-to-fine approach. Firstly, all u-examples are regarded as n-examples, because the number of event shots included in u-examples is usually very small. Subsequently, n-examples which are dissimilar to p-examples, are iteratively filtered using a classifier built on p-examples and the remaining n-examples.</Para>
									<Para>Our proposed QBE method is summarized in Fig. <InternalRef RefID="Fig3">3</InternalRef>. It should be noted that our main research objective is to develop a method which can retrieve a variety of event shots only by using a small number of p-examples. Given p-examples, n-examples are collected using the PSL method. Subsequently, various classifiers are built based on bagging and the random subspace method. Lastly, RST is used to extract combinations of classifiers as classification rules, and shots matching many rules are retrieved.
										<Figure Category="Standard" Float="Yes" ID="Fig3">
											<Caption Language="En">
												<CaptionNumber>Fig. 3</CaptionNumber>
												<CaptionContent>
													<SimplePara>An overview of our event retrieval method based on the QBE approach</SimplePara>
												</CaptionContent>
											</Caption>
											<MediaObject>
												<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig3_HTML.gif"/>
											</MediaObject>
										</Figure>
									</Para>
								</Section1>
								<Section1 ID="Sec5">
									<Heading>Related works and the innovation of our research</Heading>
									<Section2 ID="Sec6">
										<Heading>Rough set theory</Heading>
										<Para>Firstly, RST determines the indiscernibility relation, which relates to whether a p-example and n-example can be discerned with respect to available features. Thereafter, multiple classification rules are extracted by combining indiscernibility relations among examples based on set theory. RST can extract classification rules without any assumption or parameter as long as indiscernibility relations can be defined.</Para>
										<Para>Although methods other than RST can be used to retrieve a variety of event shots, they have inappropriate limitations. For example, a Gaussian Mixture Model (GMM) can extract multiple feature distributions of event shots [<CitationRef CitationID="CR34">34</CitationRef>]. However, such distributions cannot be appropriately estimated only from a small number of p-examples. In addition, without any a priori knowledge, the number of Gaussian distributions in the GMM cannot be effectively determined. The Genetic Algorithm (GA) can be used to extract multiple classification rules [<CitationRef CitationID="CR7">7</CitationRef>]. Each rule is encoded as a bit string (chromosome), where one bit indicates whether or not a feature is utilized. The GA combines bit strings based on the principles of biological evolution, such as crossover and mutation, to extract accurate rules. However, with no a priori knowledge, parameters in the GA, such as the number of bit strings, the probability of crossover and the probability of mutation, cannot be effectively determined.</Para>
										<Para>Decision tree learning methods extract multiple classification rules in a tree-based approach [<CitationRef CitationID="CR2">2</CitationRef>]. Each rule is represented as a path in a tree, where p-examples and n-examples are recursively classified using a feature associated with each node. Sequential covering methods extract multiple rules in a sequential approach [<CitationRef CitationID="CR2">2</CitationRef>]. Each rule is sequentially extracted from p-examples, which are not characterized (covered) by already extracted rules. But, the tree-based and sequential approaches only extract one classification rule for each p-example. As a result, the number of extracted rules is inevitably small, which is insufficient for retrieving a variety of event shots. In comparison to the above methods, without any parameter setting, RST can extract various rules as minimal sets of features, which can correctly identifying different subsets of p-examples.</Para>
										<Para>Traditional RST can only deal with categorical features where the indiscernibility relation between two examples can be easily defined according to whether they have the same value. In contrast, in our case, examples are represented by non-categorical and high-dimensional features. For instance, the bag-of-visual-words representation involves thousands of dimensions, each of which indicates the frequency of a local edge shape (visual word). Thus, when applying RST to QBE, the most important issue becomes the definition of indiscernibility relations among examples, that is, the categorization of non-categorical high-dimensional features. With respect to this issue, existing approaches can be classified into the following three types:</Para>
										<Para>
											<Emphasis Type="Bold">1. Clustering-based</Emphasis>  This approach groups examples into a small number of clusters. The indiscernibility relation between two examples is then defined by examining whether or not they belong to the same cluster [<CitationRef CitationID="CR5">5</CitationRef>, <CitationRef CitationID="CR20">20</CitationRef>].</Para>
										<Para>
											<Emphasis Type="Bold">2. Similarity-based</Emphasis>  This approach does not categorize a feature, but rather defines the indiscernibility relation between two examples by measuring their similarity for the feature [<CitationRef CitationID="CR28">28</CitationRef>, <CitationRef CitationID="CR36">36</CitationRef>].</Para>
										<Para>
											<Emphasis Type="Bold">3. Classifier-based</Emphasis>  This approach builds a classifier on a feature, and defines the indiscernibility relation by examining whether or not two examples are classified into the same class [<CitationRef CitationID="CR25">25</CitationRef>].</Para>
										<Para>In our research, we have previously developed the clustering-based and similarity-based RSTs, however, they had performance limitations. In [<CitationRef CitationID="CR20">20</CitationRef>], we developed a clustering-based RST using k-means clustering and tested it on TRECVID 2008 video data. TRECVID is an annual international competition where a large video data is used to benchmark state-of-the-art video analysis methods [<CitationRef CitationID="CR29">29</CitationRef>]. However, for most of events, inferred average precisions of the clustering-based RST were nearly equal to zero. One main reason for this was the coarseness in categorizing a feature into a small number of clusters, which led to semantically different shots frequently being included in the same cluster.</Para>
										<Para>We also developed a similarity-based RST in [<CitationRef CitationID="CR28">28</CitationRef>]. Although its performance was better than that of the clustering-based RST, it was far from the satisfactory. Table <InternalRef RefID="Tab1">1</InternalRef> provides a performance comparison between the similarity-based RST and classifier-based RST. We use the same examples, and the same features as will be described in Section <InternalRef RefID="Sec12">3</InternalRef>. In the similarity-based RST, cosine similarity is used as a similarity measure, while the classifier-based RST categorizes features using Support Vector Machines (SVMs) with the Radial Basis Function (RBF) kernel. For each event, the retrieval performance is evaluated as the number of event shots within 1,000 retrieved shots. As seen in Table <InternalRef RefID="Tab1">1</InternalRef>, the similarity-based RST is significantly outperformed by the classifier-based RST. The reason for the low performance of similarity-based RST is that it is difficult to appropriately measure similarities among examples for high-dimensional features.
											<Table Float="Yes" ID="Tab1">
												<Caption Language="En">
													<CaptionNumber>Table 1</CaptionNumber>
													<CaptionContent>
														<SimplePara>Comparison of the similarity-based RST and classifier-based RST</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup align="left" cols="6">
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<thead>
														<row>
															<entry colname="c1">
																<SimplePara>Event</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Tall building</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Flame</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>Computer</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>Helicopter/plane</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>Talking</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c1">
																<SimplePara>Similarity-based RST</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>46</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>14</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>54</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>17</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>38</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>Classifier-based RST</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>144</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>100</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>150</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>34</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>67</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
										</Para>
										<Para>Table <InternalRef RefID="Tab1">1</InternalRef> demonstrates the effectiveness of the classifier-based RST. In the following paragraphs, we discuss which classifiers are effective for the QBE approach, where high-dimensional features have to be categorized using only a small number of examples. The existing classifier-based RST uses different types of classifiers, such as the decision tree, nearest neighbor, naive Bayes and maximum entropy [<CitationRef CitationID="CR25">25</CitationRef>]. However, they are ineffective for the following two reasons. Firstly, a nearest neighbor is ineffective because the result of the similarity-based RST in Table <InternalRef RefID="Tab1">1</InternalRef> implies that similarity measures do not work well for high-dimensional features. Secondly, the other classifiers rely on probabilistic distributions, such as information gains in decision tree, conditional probabilities in naive Bayes, and entropy models in maximum entropy. These are ineffective because probabilistic distributions estimated from a small number of examples tend to deviate from the true distributions [<CitationRef CitationID="CR6">6</CitationRef>].</Para>
										<Para>In contrast, SVMs are effective when only a small number of examples are available as the margin maximization does not require any probability estimation [<CitationRef CitationID="CR6">6</CitationRef>]. Additionally, Vapnik [<CitationRef CitationID="CR33">33</CitationRef>] theorized that if the number of feature dimensions is large, the generalization error of an SVM is defined by the margin size and properties of examples, such as the diameter of the sphere enclosing examples and the number of examples. That is, from the theoretical perspective, the generalization error of the SVM is independent of the number of feature dimensions if this number is sufficiently large. Furthermore, as examples are generally not linearly separable, a kernel function is used to transform a high-dimensional feature into a higher-dimensional feature, with the above theory allowing a well generalized SVM to be built independent of the number of feature dimensions. Thus, we develop a classifier-based RST using SVMs, specifically using SVMs with the RBF kernel as it is known to be the most general kernel [<CitationRef CitationID="CR9">9</CitationRef>].</Para>
									</Section2>
									<Section2 ID="Sec10">
										<Heading>The problem of small sample size</Heading>
										<Para>As described in Section <InternalRef RefID="Sec1">1</InternalRef>, bagging and the random subspace method are useful for extending the range of event shots that can be retrieved. They are additionally useful for alleviating two important problems related to small sample sizes. The first is the <Emphasis Type="Italic">class imbalance problem</Emphasis>, which refers to the imbalance between the number of p-examples and n-examples, significantly degrading the classification performance [<CitationRef CitationID="CR1">1</CitationRef>]. In our technique, numerous n-examples can be collected using the partially supervised learning method. However, the SVM built using a small number of p-examples and all collected n-examples cannot appropriately classify shots into event and non-event shots. To address this, we use bagging which combines classifiers built on different sets of randomly selected examples [<CitationRef CitationID="CR3">3</CitationRef>]. In other words, for a small number of p-examples, an appropriate number of n-examples are randomly selected to build an SVM. However, due to the insufficiency of n-examples, the SVM wrongly classifies many non-event shots as event shots. Thus, we combine SVMs built on different sets of randomly selected n-examples in order to consider a variety of n-examples.</Para>
										<Para>The second problem is <Emphasis Type="Italic">overfitting</Emphasis> that a classifier can perfectly classify p-examples and n-examples, but cannot appropriately classify unseen examples. Generally, as the number of feature dimensions increases, the number of examples required to construct a well generalized classifier exponentially increases [<CitationRef CitationID="CR10">10</CitationRef>]. This is due to the fact that a class needs to be determined for each combination of values along different dimensions. In our case, we can only use a small number of examples (at most, a hundred of p- and n-examples in total). On the other hand, based on the bag-of-visual-words model, we represent each example as a vector with more than 1,000 dimensions. As a result, the SVM is overfit to feature dimensions which are specific to p-examples (or n-examples), but are ineffective for characterizing event shots (or non-event shots). Thus, we use the random subspace method, which combines classifiers built on randomly selected feature dimensions [<CitationRef CitationID="CR8">8</CitationRef>]. The original high-dimensional feature is transformed into a lower-dimensional feature, which alleviates building an overfit SVM. By combining such SVMs, a large number of feature dimensions can be considered.</Para>
										<Para>Numerous methods have been proposed to overcome the class imbalance and overfitting. For the class imbalance problem, Japkowicz [<CitationRef CitationID="CR11">11</CitationRef>] tested various sampling approaches, such as over-sampling of examples belonging to the minority class, under-sampling of examples belonging to the majority class, and sampling based on a classifier for examining the association of an example to the minority (or majority) class. Akbani et al. [<CitationRef CitationID="CR1">1</CitationRef>] used the Synthetic Minority Oversampling TEchnique (SMOTE), which synthetically generates new examples for the minority class based on their similarities with existing examples. Liu [<CitationRef CitationID="CR17">17</CitationRef>] presented various feature dimension reduction methods to overcome the problem of overfitting. For example, one method selects feature dimensions by measuring their relative importance based on an ensemble of decision trees, while a second method selects feature dimensions which are both maximally relevant and minimally redundant based on the mutual information between examples and classes. Guo and Dyer [<CitationRef CitationID="CR6">6</CitationRef>] proposed a method which simultaneously achieves dimension reduction and margin maximization in classifier training. As the above methods only select the best subset among examples or feature dimensions, they are not useful for extending the range of event shots that can be retrieved, unlike bagging and the random subspace method.</Para>
										<Para>The application of bagging and the random subspace method in this paper is crucially different to that in the previous research [<CitationRef CitationID="CR32">32</CitationRef>]. In particular, Tao et al. [<CitationRef CitationID="CR32">32</CitationRef>] performed simple majority voting using SVMs built by bagging and the random subspace method. We refer to this majority voting as <Emphasis Type="Italic">Simple_MV</Emphasis>. On the other hand, our method involves majority voting using classification rules, which are extracted as combinations of SVMs by RST. We refer to this as <Emphasis Type="Italic">RST_MV</Emphasis>. Table <InternalRef RefID="Tab2">2</InternalRef> shows a performance comparison between <Emphasis Type="Italic">Simple_MV</Emphasis> and <Emphasis Type="Italic">RST_MV</Emphasis>. For each event, 10 retrieval results are obtained using <Emphasis Type="Italic">Simple_MV</Emphasis> or <Emphasis Type="Italic">RST_MV</Emphasis>. To perform an appropriate comparison, retrieval results are obtained by using the same set of 60 SVMs for both <Emphasis Type="Italic">Simple_MV</Emphasis> and <Emphasis Type="Italic">RST_MV</Emphasis>. The second and fourth rows represent average numbers of event shots retrieved by <Emphasis Type="Italic">Simple_MV</Emphasis> and <Emphasis Type="Italic">RST_MV</Emphasis>, respectively. These rows demonstrate that <Emphasis Type="Italic">RST_MV</Emphasis> outperforms <Emphasis Type="Italic">Simple_MV</Emphasis> for all events. A key reason for this is the difference between SVMs and classification rules. The third and fifth rows represent average numbers of event shots which are correctly classified by SVMs and classification rules, respectively, showing that classification rules are much more accurate than SVMs. This allows <Emphasis Type="Italic">RST_MV</Emphasis> to achieve more accurate event retrieval than <Emphasis Type="Italic">Simple_MV</Emphasis>.
											<Table Float="Yes" ID="Tab2">
												<Caption Language="En">
													<CaptionNumber>Table 2</CaptionNumber>
													<CaptionContent>
														<SimplePara>Performance of SVM combination by majority voting and RST</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup align="left" cols="7">
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<colspec colname="c7" colnum="7"/>
													<thead>
														<row>
															<entry colname="c1">
																<SimplePara> </SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Event</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Tall building</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>Flame</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>Computer</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>Helicopter/plane</SimplePara>
															</entry>
															<entry colname="c7">
																<SimplePara>Talking</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c1" morerows="1">
																<SimplePara>Majority voting</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>Retrieval performance</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>136.0</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>122.6</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>160.5</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>36.1</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>175.6</SimplePara>
															</entry>
														</row>
														<row>
															<entry align="center" colname="c2">
																<SimplePara>SVM performance</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>84.3</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>67.4</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>90.4</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>22.6</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>98.4</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1" morerows="1">
																<SimplePara>RST</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>Retrieval performance</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>140.6</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>145.3</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>164.3</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>41.5</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>189.9</SimplePara>
															</entry>
														</row>
														<row>
															<entry align="center" colname="c2">
																<SimplePara>Rule performance</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>109.6</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>98.6</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>118.8</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>32.4</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>139.5</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
										</Para>
									</Section2>
									<Section2 ID="Sec11">
										<Heading>Negative example selection</Heading>
										<Para>Traditional QBE methods only use p-examples and retrieve shots similar to them [<CitationRef CitationID="CR12">12</CitationRef>, <CitationRef CitationID="CR23">23</CitationRef>]. But, only considering similarities to p-examples cannot yield accurate event retrieval. For example, consider the event ‘a car moves’. If a p-example showing a moving red car is provided, it may seem more similar to a shot where a person wears a red piece of clothing than a shot showing a moving white car. Compared to this, if a QBE method uses n-examples and compares them to the p-example, it can be found that the edge feature characterizing a car shape is important while the color feature is not. Thus, n-examples are required to construct a retrieval model (in our case, classification rules), which characterizes features specific to p-examples.</Para>
										<Para>Since event shots form only a small portion of all shots, several methods use an approach which considers randomly selected shots as n-examples [<CitationRef CitationID="CR21">21</CitationRef>, <CitationRef CitationID="CR31">31</CitationRef>]. However, this approach is associated with the following problem. The random selection of n-examples does not consider the type of n-examples that are required to construct an accurate retrieval model. For example, if all of collected n-examples are significantly dissimilar to p-examples, we cannot identify features that are relevant to p-examples because p-examples and n-examples can be classified by using any features.</Para>
										<Para>Numerous partially supervised learning (PSL) methods have been proposed to build a classifier using p-examples and unlabeled examples (u-examples). Most the existing methods adopt the two-step approach, wherein n-examples are first collected from u-examples and a classifier is subsequently built on p-examples and collected n-examples [<CitationRef CitationID="CR4">4</CitationRef>, <CitationRef CitationID="CR16">16</CitationRef>, <CitationRef CitationID="CR37">37</CitationRef>]. In such a PSL approach, one of main research issues is how to collect n-examples from u-examples. For instance, Liu et al. [<CitationRef CitationID="CR16">16</CitationRef>] proposed a method which selects some p-examples as ‘spy’ examples and adds them to u-examples. A naive Bayesian classifier is then built using p-examples and u-examples, where spy examples are used to set the probabilistic threshold for evaluating whether or not u-examples can be regarded as negative. Yu et al. [<CitationRef CitationID="CR37">37</CitationRef>] proposed a method which iteratively collects n-examples. In each iteration, the method builds an SVM from p-examples and already selected n-examples, and subsequently selects n-examples as u-examples, which are classified as negative by the SVM. Fung et al. [<CitationRef CitationID="CR4">4</CitationRef>] proposed an iterative method where u-examples which are more similar to already selected n-examples rather than p-examples are selected as n-examples.</Para>
										<Para>However, most of existing PSL methods only collect a large number of n-examples. Due to the class imbalance problem, the use of all collected n-examples does not guarantee the construction of an accurate classifier. To overcome this, we propose a new PSL method which can select a small number of n-examples useful for building an accurate classifier like SVM. Our method is inspired by the method proposed in [<CitationRef CitationID="CR38">38</CitationRef>]. It was originally developed to solve the class imbalance problem using under-sampling examples of the majority class, i.e., n-examples. Yuan et al. [<CitationRef CitationID="CR38">38</CitationRef>] contends that since n-examples similar to p-examples characterize the class boundary, they are useful for building an accurate SVM. N-examples are iteratively filtered to collect those n-examples that are similar to p-examples. In each iteration, the method filters out n-examples that are distant from the decision boundary of the SVM, which is built on p-examples and the remaining n-examples. As a result, the method selects a small number of n-examples that are as similar to p-examples as possible. We apply this method to PSL by regarding all u-examples as n-examples, because almost all of u-examples are non-event shots.</Para>
									</Section2>
								</Section1>
								<Section1 ID="Sec12">
									<Heading>Event retrieval method</Heading>
									<Para>In this section, the proposed QBE method is described. We assume that the representative semantic content in each shot is shown in the middle video frame, called <Emphasis Type="Italic">keyframe</Emphasis>. The following six types of features are extracted from the keyframe in each shot using the color descriptor software [<CitationRef CitationID="CR26">26</CitationRef>]: <Emphasis Type="Italic">1. SIFT</Emphasis>, <Emphasis Type="Italic">2. Opponent SIFT</Emphasis>, <Emphasis Type="Italic">3. RBG SIFT</Emphasis>, <Emphasis Type="Italic">4. Hue SIFT</Emphasis>, <Emphasis Type="Italic">5. Color histogram</Emphasis> and <Emphasis Type="Italic">6. Dense SIFT</Emphasis>. The first three types of SIFT features are defined in different color spaces and have different invariance properties for lighting conditions. By using these three SIFT features, we aim to characterize local shapes of objects (e.g. corners of buildings, vehicles, human eyes etc.), regardless of changes in illumination. Since they only compute derivatives in color spaces, they do not consider absolute color values which are effective for characterizing certain objects, such as faces, flames, vegetation and so on. So, we use <Emphasis Type="Italic">Hue SIFT</Emphasis> and <Emphasis Type="Italic">Color histogram</Emphasis>. <Emphasis Type="Italic">Hue SIFT</Emphasis> represents a SIFT feature concatenated with a Hue histogram in HSV color space, and <Emphasis Type="Italic">Color histogram</Emphasis> represents a pure RGB color histogram. All of the above five types of features are extracted at interest points computed by Harris-Laplace detector, where pixel values largely change in multiple directions. But, small changes of pixel values are critical when considering certain objects such as sky, water, roads and so on. In addition, many interest points in the background may be missed due to low contrast. Thus, <Emphasis Type="Italic">Dense SIFT</Emphasis> is used, wherein SIFT features are extracted at interest points sampled with a fixed interval. In this way, interest points missed by Harris-Laplace detector can be compensated. Due to space limitations, the above features are not discussed any further and can be found in [<CitationRef CitationID="CR26">26</CitationRef>] in greater detail.</Para>
									<Para>The above six types of features are represented using the bag-of-visual-words representation. For each type of feature, 1,000 visual words are extracted by clustering features at 200,000 interest points, sampled from keyframes in TRECVID 2009 development videos (219 videos) [<CitationRef CitationID="CR29">29</CitationRef>]. As depicted in Fig. <InternalRef RefID="Fig4">4</InternalRef>, a shot is represented as a 6,000-dimensional vector where each type of feature is represented as a 1,000-dimensional vector. Based on this high-dimensional representation, we perform rough set theory extended by bagging and the random subspace method, and partially supervised learning. It should be noted that for the random subspace method, it is unreasonable to build an SVM using dimensions randomly sampled across different types of features. Thus, we build one SVM by randomly selecting dimensions in the same type of feature.
										<Figure Category="Standard" Float="Yes" ID="Fig4">
											<Caption Language="En">
												<CaptionNumber>Fig. 4</CaptionNumber>
												<CaptionContent>
													<SimplePara>The representation of a shot as a 6,000-dimensional vector</SimplePara>
												</CaptionContent>
											</Caption>
											<MediaObject>
												<ImageObject Color="BlackWhite" Format="GIF" Rendition="HTML" Type="Linedraw" FileRef="MediaObjects/11042_2011_727_Fig4_HTML.gif"/>
											</MediaObject>
										</Figure>
									</Para>
									<Section2 ID="Sec13">
										<Heading>Rough set theory extended by bagging and the random subspace method</Heading>
										<Para>Given p-examples and n-examples, RST is used to extract classification rules, known as <Emphasis Type="Italic">decision rules</Emphasis>, for distinguishing event shots from non-event shots. Firstly, various SVMs are built using bagging and the random subspace method. Using each of these SVMs, the indiscernibility relation between a p-example and n-example is defined by determining whether or not the p-example and n-example are classified into the same class. Lastly, by combining such indiscernibility relations among examples, decision rules, which can discriminate subsets of p-examples from the entire set of n-examples, are extracted. The proposed RST is explained in detail in the following paragraphs.</Para>
										<Para>First of all, the number of p-examples and that of n-examples are too small to characterize the distribution of event shots and that of non-event shots in a high-dimensional feature space, respectively. So, the decision boundary of an SVM tends to be inaccurate. Generally, an SVM determines the class of an example based on the binary criterion, i.e., whether the example is located on the positive or negative side of the decision boundary. However, this classification is erroneous since the decision boundary is inaccurate. To overcome this, a continuous-valued criterion is employed. Specifically, the <Emphasis Type="Italic">probabilistic output</Emphasis> of the SVM, which approximates the distance between an example and the decision boundary using a sigmoid function, is used [<CitationRef CitationID="CR18">18</CitationRef>]. Based on this, the class of an example is determined as follows. Firstly, examples are ranked in descending order of probabilistic outputs of the SVM. If an example is ranked within the top <Emphasis Type="Italic">M</Emphasis> positions, where <Emphasis Type="Italic">M</Emphasis> is the number of p-examples, its class is determined as positive, or otherwise as negative. Thus, although the decision boundary is inaccurate, examples can be robustly classified.</Para>
										<Para>Classification results of SVMs can be summarized in a <Emphasis Type="Italic">decision table</Emphasis>, as shown in Fig. <InternalRef RefID="Fig5">5</InternalRef>. Each row represents the <Emphasis Type="Italic">i</Emphasis>-th p-example, <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> (1 ≤ <Emphasis Type="Italic">i</Emphasis> ≤ <Emphasis Type="Italic">M</Emphasis>) or <Emphasis Type="Italic">j</Emphasis>-th n-example, <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript> (1 ≤ <Emphasis Type="Italic">j</Emphasis> ≤ <Emphasis Type="Italic">N</Emphasis>). Each column <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">k</Emphasis>
											</Subscript> (1 ≤ <Emphasis Type="Italic">k</Emphasis> ≤ <Emphasis Type="Italic">K</Emphasis>) represents the classification result of the <Emphasis Type="Italic">k</Emphasis>-th SVM, where + 1 indicates that an example is classified as positive and − 1 indicates that it is classified as negative. The classification result of the <Emphasis Type="Italic">k</Emphasis>-th SVM for <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript> are represented by <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">k</Emphasis>
											</Subscript>(<Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript>) and <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">k</Emphasis>
											</Subscript>(<Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript>), respectively. That is, each classification result can be regarded as one feature in RST. Lastly, the rightmost column indicates whether an example is positive (<Emphasis Type="Italic">P</Emphasis>) or negative (<Emphasis Type="Italic">N</Emphasis>).
											<Figure Category="Standard" Float="Yes" ID="Fig5">
												<Caption Language="En">
													<CaptionNumber>Fig. 5</CaptionNumber>
													<CaptionContent>
														<SimplePara>Example of a decision table formed by applying RST extended by bagging and the random subspace method</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="BlackWhite" Format="GIF" Rendition="HTML" Type="Linedraw" FileRef="MediaObjects/11042_2011_727_Fig5_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
										<Para>Before discussing the proposed decision rule extraction method, a conceptual explanation in relation to decision rules is provided using Fig. <InternalRef RefID="Fig6">6</InternalRef>. In this figure, one p-example <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> and two n-examples <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript> are given for the event ‘tall buildings are shown’. Let <InlineEquation ID="IEq1">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq1.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{k_{1}}$</EquationSource>
											</InlineEquation> and <InlineEquation ID="IEq2">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq2.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{k_{2}}$</EquationSource>
											</InlineEquation> represent the classification of the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>1</Subscript>-th SVM built on <Emphasis Type="Italic">SIFT</Emphasis> feature and that of the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>2</Subscript>-th SVM built on <Emphasis Type="Italic">Hue SIFT</Emphasis> feature, respectively. Since <Emphasis Type="Italic">SIFT</Emphasis> feature of <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> is similar to the one of <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript>, the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>1</Subscript>-th SVM incorrectly classifies both as positive. On the other hand, since <Emphasis Type="Italic">Hue SIFT</Emphasis> feature of <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> is dissimilar to that of <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript>, the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>2</Subscript>-th SVM correctly classifies <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript> as positive and negative, respectively. The dissimilarity between <Emphasis Type="Italic">SIFT</Emphasis> features of <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript>’s and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript>’s enables them to be correctly classified by the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>1</Subscript>-th SVM. On the other hand, <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript> cannot be correctly classified by the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>2</Subscript>-th SVM due to their similar <Emphasis Type="Italic">Hue SIFT</Emphasis> features. Thus, in order to discriminate <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> from <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript>, a decision rule consisting of <InlineEquation ID="IEq3">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq3.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{k_{1}}$</EquationSource>
											</InlineEquation> and <InlineEquation ID="IEq4">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq4.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{k_{2}}$</EquationSource>
											</InlineEquation> is required. This rule indicates the combination of the <Emphasis Type="Italic">k</Emphasis>
											<Subscript>1</Subscript>-th and <Emphasis Type="Italic">k</Emphasis>
											<Subscript>2</Subscript>-th SVMs. In the following paragraphs, the extraction of such decision rules based on a logical operation is described.
											<Figure Category="Standard" Float="Yes" ID="Fig6">
												<Caption Language="En">
													<CaptionNumber>Fig. 6</CaptionNumber>
													<CaptionContent>
														<SimplePara>The concept of decision rules extracted by RST</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig6_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
										<Para>In order to extract decision rules, for each pair of <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript>, we first determine <Emphasis Type="Italic">discriminative features</Emphasis> which are useful for discriminating them. Specifically, the following set of discriminative features <Emphasis Type="Italic">f</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>,<Emphasis Type="Italic">j</Emphasis>
											</Subscript> are extracted between <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript>.
											<Equation ID="Equ1">
												<EquationNumber>1</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ1.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ f_{i, j} = \{ a_{k} | a_{k}(p_{i}) = +1 \wedge a_{k}(p_{i}) \neq a_{k}(n_{\!j}) \} $$</EquationSource>
											</Equation>This signifies that <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">k</Emphasis>
											</Subscript> is included in <Emphasis Type="Italic">f</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>,<Emphasis Type="Italic">j</Emphasis>
											</Subscript>, if the <Emphasis Type="Italic">k</Emphasis>-th SVM correctly classifies <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript> as positive and negative, respectively. In other words, <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> can be discriminated from <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript> when at least one feature in <Emphasis Type="Italic">f</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>,<Emphasis Type="Italic">j</Emphasis>
											</Subscript> is utilized.</Para>
										<Para>Next, in order to discriminate <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> from all n-examples, the discriminative features of <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript>’s are combined. This is achieved by using at least one discriminative feature in <Emphasis Type="Italic">f</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>, <Emphasis Type="Italic">j</Emphasis>
											</Subscript> for all n-examples. The <Emphasis Type="Italic">discernibility function</Emphasis>
											<Emphasis Type="Italic">df</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript>, which takes a conjunction of ∨ <Emphasis Type="Italic">f</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>, <Emphasis Type="Italic">j</Emphasis>
											</Subscript>, is computed as follows:
											<Equation ID="Equ2">
												<EquationNumber>2</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ2.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ df_{i} = \wedge \{ \vee f_{i,j} | \; 1 \leq j \leq N \} $$</EquationSource>
											</Equation>Let us consider the discernibility function <Emphasis Type="Italic">df</Emphasis>
											<Subscript>1</Subscript> for one p-example <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> and two n-examples <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript>. Let the sets of discriminative features between <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript> and between <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript> be <Emphasis Type="Italic">f</Emphasis>
											<Subscript>1,1</Subscript> = { <Emphasis Type="Italic">a</Emphasis>
											<Subscript>1</Subscript>, <Emphasis Type="Italic">a</Emphasis>
											<Subscript>3</Subscript>, <Emphasis Type="Italic">a</Emphasis>
											<Subscript>5</Subscript> } and <Emphasis Type="Italic">f</Emphasis>
											<Subscript>1,2</Subscript> = { <Emphasis Type="Italic">a</Emphasis>
											<Subscript>1</Subscript>, <Emphasis Type="Italic">a</Emphasis>
											<Subscript>2</Subscript> }, respectively. Under this condition, <Emphasis Type="Italic">df</Emphasis>
											<Subscript>1</Subscript> can be computed as (<Emphasis Type="Italic">a</Emphasis>
											<Subscript>1</Subscript> ∨ <Emphasis Type="Italic">a</Emphasis>
											<Subscript>3</Subscript> ∨ <Emphasis Type="Italic">a</Emphasis>
											<Subscript>5</Subscript> ) ∧ (<Emphasis Type="Italic">a</Emphasis>
											<Subscript>1</Subscript> ∨ <Emphasis Type="Italic">a</Emphasis>
											<Subscript>2</Subscript>) . This is simplified as <InlineEquation ID="IEq5">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq5.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$df^{*}_{1} = (a_{1}) \vee (a_{2} \wedge a_{3}) \vee (a_{2} \wedge a_{5})$</EquationSource>
											</InlineEquation>.<Footnote ID="Fn2">
												<Para>This simplification is achieved by using the distributive law <Emphasis Type="Italic">A</Emphasis> ∧ (<Emphasis Type="Italic">B</Emphasis> ∨ <Emphasis Type="Italic">C</Emphasis>) = (<Emphasis Type="Italic">A</Emphasis> ∧ <Emphasis Type="Italic">B</Emphasis>) ∨ (<Emphasis Type="Italic">A</Emphasis> ∧ <Emphasis Type="Italic">C</Emphasis>) and the absorption law <Emphasis Type="Italic">A</Emphasis> ∨ (<Emphasis Type="Italic">A</Emphasis> ∧ <Emphasis Type="Italic">B</Emphasis>) = <Emphasis Type="Italic">A</Emphasis>. Although the simplification of a Boolean function is NP-hard, we can obtain an approximate solution by using the genetic algorithm [<CitationRef CitationID="CR13">13</CitationRef>].</Para>
											</Footnote> Thus, <Emphasis Type="Italic">p</Emphasis>
											<Subscript>1</Subscript> can be distinguished from <Emphasis Type="Italic">n</Emphasis>
											<Subscript>1</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>2</Subscript>, by using <Emphasis Type="Italic">a</Emphasis>
											<Subscript>1</Subscript>, the set of <Emphasis Type="Italic">a</Emphasis>
											<Subscript>2</Subscript> and <Emphasis Type="Italic">a</Emphasis>
											<Subscript>3</Subscript>, or the set of <Emphasis Type="Italic">a</Emphasis>
											<Subscript>2</Subscript> and <Emphasis Type="Italic">a</Emphasis>
											<Subscript>5</Subscript>. Similarly, each conjunction term in <InlineEquation ID="IEq6">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq6.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$df^{*}_{i}$</EquationSource>
											</InlineEquation> represents a <Emphasis Type="Italic">reduct</Emphasis> which is the minimal set of features required to discriminate <Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> from all n-examples.</Para>
										<Para>From each reduct, we construct a decision rule in the form of an <Emphasis Type="Italic">IF-THEN</Emphasis> rule. A reduct <Emphasis Type="Italic">r</Emphasis>, consisting of <Emphasis Type="Italic">L</Emphasis> features, can be represented as follows.
											<Equation ID="Equ3">
												<EquationNumber>3</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ3.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ r = a_{1^{*}} \wedge a_{2^{*}} \wedge \cdots \wedge a_{L^{*}} $$</EquationSource>
											</Equation>where <InlineEquation ID="IEq8">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq8.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{l^{*}}$</EquationSource>
											</InlineEquation> (1 ≤ <Emphasis Type="Italic">l</Emphasis>
											<Superscript>*</Superscript> ≤ <Emphasis Type="Italic">L</Emphasis>) denotes a single feature among the total <Emphasis Type="Italic">K</Emphasis> features <Emphasis Type="Italic">a</Emphasis>
											<Subscript>1</Subscript>, ⋯ , <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">K</Emphasis>
											</Subscript>. Recall that in (<InternalRef RefID="Equ1">1</InternalRef>), <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">k</Emphasis>
											</Subscript> is selected as a discriminative feature only when <Emphasis Type="Italic">a</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">k</Emphasis>
											</Subscript> (<Emphasis Type="Italic">p</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript>) = + 1. Thus, the decision rule, <Emphasis Type="Italic">rule</Emphasis>, constructed from <Emphasis Type="Italic">r</Emphasis> has a conditional part, where <InlineEquation ID="IEq9">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq9.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{l^{*}} (x)$</EquationSource>
											</InlineEquation> has to be + 1 for a shot (i.e. unseen example) <Emphasis Type="Italic">x</Emphasis>. It is represented as follows.
											<Equation ID="Equ4">
												<EquationNumber>4</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ4.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ rule: IF \; (a_{1^{*}}(x) = +1) \wedge \cdots \wedge (a_{L^{*}}(x) = +1), \; THEN \; Class = P $$</EquationSource>
											</Equation>For example, from the reduct (<Emphasis Type="Italic">a</Emphasis>
											<Subscript>2</Subscript> ∧ <Emphasis Type="Italic">a</Emphasis>
											<Subscript>3</Subscript>), we can construct the decision rule <Emphasis Type="Italic">IF</Emphasis> (<Emphasis Type="Italic">a</Emphasis>
											<Subscript>2</Subscript>(<Emphasis Type="Italic">x</Emphasis>) = + 1) ∧ (<Emphasis Type="Italic">a</Emphasis>
											<Subscript>3</Subscript>(<Emphasis Type="Italic">x</Emphasis>) = + 1), <Emphasis Type="Italic">THEN</Emphasis>
											<Emphasis Type="Italic">Class</Emphasis> = <Emphasis Type="Italic">P</Emphasis>. This rule indicates that if both the 2-nd and 3-rd SVMs classify <Emphasis Type="Italic">x</Emphasis> as positive, then <Emphasis Type="Italic">x</Emphasis> is an event shot. Like this, a decision rule describes how to combine SVMs built by bagging and the random subspace method for retrieving event shots.</Para>
										<Para>When matching <Emphasis Type="Italic">x</Emphasis> with <Emphasis Type="Italic">rule</Emphasis>, we regard decision boundaries of SVMs as inaccurate, as described above. Decision rule matching is conducted based on probabilistic outputs of SVMs. To explain this, <InlineEquation ID="IEq10">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq10.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$SVM_{l^{*}}$</EquationSource>
											</InlineEquation> is used to represent the <Emphasis Type="Italic">l</Emphasis>
											<Superscript>*</Superscript>-th SVM corresponding to each feature <InlineEquation ID="IEq11">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq11.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$a_{l^{*}}$</EquationSource>
											</InlineEquation> in <Emphasis Type="Italic">rule</Emphasis> (1 ≤ <Emphasis Type="Italic">l</Emphasis>
											<Superscript>*</Superscript> ≤ <Emphasis Type="Italic">L</Emphasis>). In addition, the probabilistic output of the <InlineEquation ID="IEq12">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq12.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$SVM_{l^{*}}$</EquationSource>
											</InlineEquation> for <Emphasis Type="Italic">x</Emphasis> is denoted by <InlineEquation ID="IEq13">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq13.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$Prob_{SVM_{l^{*}}} (x)$</EquationSource>
											</InlineEquation>. Based on the above notations, we calculate <Emphasis Type="Italic">Match</Emphasis>(<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">rule</Emphasis>) which is an evaluation value of matching <Emphasis Type="Italic">x</Emphasis> with <Emphasis Type="Italic">rule</Emphasis>.
											<Equation ID="Equ5">
												<EquationNumber>5</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ5.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ Match(x, rule) = \prod\limits_{l^{*}=1}^{L} Prob_{SVM_{l^{*}}} (x) $$</EquationSource>
											</Equation>
											<Emphasis Type="Italic">Match</Emphasis>(<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">rule</Emphasis>) is the joint probability of <Emphasis Type="Italic">L</Emphasis> SVMs in <Emphasis Type="Italic">rule</Emphasis>. It should be noted that a threshold is required to determine whether or not <Emphasis Type="Italic">x</Emphasis> matches <Emphasis Type="Italic">rule</Emphasis>. In view of this, the following two considerations must be made. Firstly, feature numbers (i.e. <Emphasis Type="Italic">L</Emphasis>) differ depending on decision rules. Secondly, the distribution of SVM’s probabilistic outputs differs depending on the sigmoid function estimated for the SVM. For example, the mean of some SVM’s probabilistic outputs is 0.5, while the one of a different SVM’s probabilistic outputs is 0.3. Thus, it is unreasonable to use the same threshold for all decision rules.</Para>
										<Para>Instead of actual values of <Emphasis Type="Italic">Match</Emphasis>(<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">rule</Emphasis>), the following ranking approach is used to determine whether or not <Emphasis Type="Italic">x</Emphasis> matches <Emphasis Type="Italic">rule</Emphasis>. First, all shots are ranked in descending order of <Emphasis Type="Italic">Match</Emphasis>(<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">rule</Emphasis>). A shot ranked within the top <Emphasis Type="Italic">T</Emphasis>-th position is considered to match <Emphasis Type="Italic">rule</Emphasis> (<Emphasis Type="Italic">T</Emphasis> is set to 1,000 in all experiments). In this way, shots are matched with all decision rules based on the same criterion of the ranking position. Finally, our method outputs a retrieval result consisting of <Emphasis Type="Italic">T</Emphasis>′ shots which match the largest numbers of decision rules (<Emphasis Type="Italic">T</Emphasis>′ = 1,000 in our experiments).</Para>
									</Section2>
									<Section2 ID="Sec14">
										<Heading>Effectiveness of bagging and the random subspace method</Heading>
										<Para>In this section, we discuss whether bagging and the random subspace method are effective in extending the range of event shots that can be retrieved. That is, we examine differences among classification results of SVMs, which are built using different examples and feature dimensions.</Para>
										<Para>Table <InternalRef RefID="Tab3">3</InternalRef> shows experimental results obtained for five events that will be used in Section <InternalRef RefID="Sec16">4</InternalRef>. In particular, the objective is to examine whether classification results of SVMs change when using different examples and dimensions, even for the same type of feature. Thus, SVMs are built only by using <Emphasis Type="Italic">SIFT</Emphasis> feature. In the second to fourth rows of Table <InternalRef RefID="Tab3">3</InternalRef>, we only use bagging where SVMs are built using the same number of randomly selected examples. On the other hand, in the fifth to seventh rows of Table <InternalRef RefID="Tab3">3</InternalRef>, we use both bagging and the random subspace method where SVMs are built using the same number of randomly selected examples and <Emphasis Type="Italic">SIFT</Emphasis> dimensions. A comparison is drawn among classification results of 10 SVMs by examining 1,000 shots with the highest probabilistic outputs among the total 97,150 shots. We define the first classification result as the <Emphasis Type="Italic">baseline</Emphasis>, and examined the difference between the baseline and the remaining nine results, called <Emphasis Type="Italic">comparison results</Emphasis>. The second (or fifth) row represents the number of event shots included in the baseline. The third (or sixth) row represents the average number of shots that are only included in comparison results. Lastly, the fourth (or seventh) row represents the average number of event shots included only in comparison results.
											<Table Float="Yes" ID="Tab3">
												<Caption Language="En">
													<CaptionNumber>Table 3</CaptionNumber>
													<CaptionContent>
														<SimplePara>Differences among classification results of SVMs built using bagging and the random subspace method</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup align="left" cols="7">
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<colspec colname="c7" colnum="7"/>
													<thead>
														<row>
															<entry nameend="c2" namest="c1">
																<SimplePara>Event</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Event 1</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>Event 2</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>Event 3</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>Event 4</SimplePara>
															</entry>
															<entry colname="c7">
																<SimplePara>Event 5</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c1">
																<SimplePara>Bagging</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">Baseline:</Emphasis> # of event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>124</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>99</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>111</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>35</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>146</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">Comparison:</Emphasis> # of different shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>466.9</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>579.7</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>470.1</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>396.6</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>512.8</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">Comparison:</Emphasis> # of different event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>39.4</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>16.7</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>34.3</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>7.3</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>42.2</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>Bagging &amp; random subspace</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">Baseline:</Emphasis> # of event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>129</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>97</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>115</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>22</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>150</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">Comparison:</Emphasis> # of different shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>461.3</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>695.4</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>656.4</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>428.4</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>693</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">Comparison:</Emphasis> # of different event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>27.1</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>16.7</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>37.2</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>11.9</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>53.7</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
										</Para>
										<Para>As can be seen from Table <InternalRef RefID="Tab3">3</InternalRef>, by changing examples with bagging, comparison results include approximately 397 to 580 shots that differ from the baseline. In addition, compared to event shots in the baseline, about 16 to 32% of different event shots are included in comparison results. Furthermore, by changing both examples and feature dimensions with bagging and the random subspace method, comparison results include about 428 to 695 shots that are different from the baseline, and approximately 17 to 36% of event shots in comparison results are different from event shots in the baseline. This indicates that bagging and the random subspace method are effective in extending the range of event shots that can be retrieved. However, Table <InternalRef RefID="Tab3">3</InternalRef> indicates that as the number of event shots which can be retrieved increases, the number of non-event shots also increases. It can be seen that the ratio of event shots to those shots included only in comparison results is less than 10%. Thus, in order to accurately retrieve event shots, decision rules need to be extracted as combinations of SVMs by using RST.</Para>
									</Section2>
									<Section2 ID="Sec15">
										<Heading>Partially supervised learning</Heading>
										<Para>Since the proposed RST relies on SVMs built using bagging and the random subspace method, it is necessary to collect n-examples that are useful for building accurate SVMs. In particular, considering the class imbalance problem, the proposed partially supervised learning (PSL) method should be able to collect a small number of <Emphasis Type="Italic">informative</Emphasis> n-examples from unlabeled examples (u-examples). N-examples similar to p-examples are considered as informative, because they are useful for characterizing the boundary between event and non-event shots. The procedure involved in selecting a small number of informative n-examples is described below.</Para>
										<Para>The proposed PSL method is summarized in Algorithm 1. Firstly, since the number of event shots included in u-examples is very small, all u-examples are assumed to be n-examples. The set of p-examples and the set of n-examples are denoted by <Emphasis Type="Italic">P</Emphasis> and <Emphasis Type="Italic">N</Emphasis>, respectively. The number of p-examples and the one of n-examples are represented by |<Emphasis Type="Italic">P</Emphasis>| and |<Emphasis Type="Italic">N</Emphasis>|, respectively. Based on <Emphasis Type="Italic">P</Emphasis> and <Emphasis Type="Italic">N</Emphasis>, an SVM, which examines whether or not n-examples are informative, is built. However, only a small number of n-examples can be used due to the class imbalance problem. If n-examples are randomly selected from <Emphasis Type="Italic">N</Emphasis>, n-examples located in certain regions of the feature space may not be selected. As a result, the decision boundary of the SVM is wrongly estimated, and it is not possible to appropriately evaluate the informativeness of n-examples. Thus, we need to collect a set of <Emphasis Type="Italic">representative</Emphasis> n-examples, which characterize the distribution of all n-examples. To this end, n-examples are grouped into clusters using the k-means clustering algorithm and the Euclidian distance measure. As shown in the line 1 of Algorithm 1, n-examples are grouped into <Emphasis Type="Italic">N</Emphasis> / <Emphasis Type="Italic">β</Emphasis> clusters. <Emphasis Type="Italic">β</Emphasis> is a pre-defined parameter used to control the number of clusters relative to the number of n-examples. Since various semantic contents are presented in n-examples, their features are very diverse. This necessitates many clusters of n-examples. In our experiment, <Emphasis Type="Italic">β</Emphasis> is set to 10, so that when |<Emphasis Type="Italic">N</Emphasis>| = 30,000, one can obtain 3,000 clusters. In addition, since it is difficult to appropriately measure similarities among n-examples in the 6,000-dimensional feature space defined in Fig. <InternalRef RefID="Fig4">4</InternalRef>, our PSL method is conducted on 1,000-dimensional <Emphasis Type="Italic">SIFT</Emphasis> feature (the SVM in the line 3 of Algorithm 1 is also built on <Emphasis Type="Italic">SIFT</Emphasis> feature).
											<MediaObject>
												<ImageObject Color="BlackWhite" Format="GIF" Rendition="HTML" Type="Linedraw" FileRef="MediaObjects/11042_2011_727_Figd_HTML.gif"/>
											</MediaObject>
										</Para>
										<Para>After clustering, for each cluster <Emphasis Type="Italic">c</Emphasis>, the most centrally located n-example is selected as the representative n-example <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">c</Emphasis>
											</Subscript>.
											<Equation ID="Equ6">
												<EquationNumber>6</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ6.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ n_{c} = \min_{n_{i} \in N_{c}} \sum\limits_{n_{\!j} \in N_{c}} dist(n_{i}, n_{\!j}) $$</EquationSource>
											</Equation>where <Emphasis Type="Italic">N</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">c</Emphasis>
											</Subscript> is the set of n-examples in <Emphasis Type="Italic">c</Emphasis>, and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript> are n-examples in <Emphasis Type="Italic">N</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">c</Emphasis>
											</Subscript>. <Emphasis Type="Italic">dist</Emphasis>(<Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript>, <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript>) represents the Euclidian distance between <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">i</Emphasis>
											</Subscript> and <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">j</Emphasis>
											</Subscript>. Thus, <Emphasis Type="Italic">n</Emphasis>
											<Subscript>
												<Emphasis Type="Italic">c</Emphasis>
											</Subscript> is selected as the n-example having the minimum sum of Euclidian distances to the other n-examples in <Emphasis Type="Italic">c</Emphasis>. A set of representative n-examples for all clusters is denoted by <Emphasis Type="Italic">RN</Emphasis>.</Para>
										<Para>By using an SVM built on <Emphasis Type="Italic">P</Emphasis> and <Emphasis Type="Italic">RN</Emphasis>, it can be determined whether each n-example <Emphasis Type="Italic">n</Emphasis> in <Emphasis Type="Italic">N</Emphasis> is informative based on its distance from the decision boundary of the SVM. N-examples distant from the decision boundary are uninformative for defining the boundary between event and non-event shots. The above test is conducted using the following criterion:
											<Equation ID="Equ7">
												<EquationNumber>7</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ7.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ | w^{T} n + b{\kern-.5pt} | &gt; \gamma $$</EquationSource>
											</Equation>Using <Emphasis Type="Italic">x</Emphasis> as an arbitrary example, <Emphasis Type="Italic">w</Emphasis>
											<Superscript>
												<Emphasis Type="Italic">T</Emphasis>
											</Superscript>
											<Emphasis Type="Italic">x</Emphasis> + <Emphasis Type="Italic">b</Emphasis> = 0 represents the decision boundary (hyperplane) of the SVM. Based on this, | <Emphasis Type="Italic">w</Emphasis>
											<Superscript>
												<Emphasis Type="Italic">T</Emphasis>
											</Superscript>
											<Emphasis Type="Italic">n</Emphasis> + <Emphasis Type="Italic">b</Emphasis> | can characterize the distance between <Emphasis Type="Italic">n</Emphasis> and the decision boundary. Specifically, the distance is |<Emphasis Type="Italic">w</Emphasis>
											<Superscript>
												<Emphasis Type="Italic">T</Emphasis>
											</Superscript>
											<Emphasis Type="Italic">n</Emphasis> + <Emphasis Type="Italic">b</Emphasis> | / ||<Emphasis Type="Italic">w</Emphasis>||, but since ||<Emphasis Type="Italic">w</Emphasis>|| is constant, it can be omitted. If the distance between <Emphasis Type="Italic">n</Emphasis> and the decision boundary is larger than the threshold <Emphasis Type="Italic">γ</Emphasis>, <Emphasis Type="Italic">n</Emphasis> is regarded as uninformative and subsequently removed from <Emphasis Type="Italic">N</Emphasis>. This removal of uninformative n-examples is iterated until the number of n-examples is less than the pre-defined number of n-examples <Emphasis Type="Italic">α</Emphasis> or no further n-examples are removed from <Emphasis Type="Italic">N</Emphasis>.</Para>
										<Para>An example illustrating the above iteration is shown in Fig. <InternalRef RefID="Fig7">7</InternalRef>, with circles and crosses representing p-examples and n-examples, respectively. N-examples are initially collected as shown in Fig. <InternalRef RefID="Fig7">7</InternalRef>a, and the first iteration is then performed as shown in Fig. <InternalRef RefID="Fig7">7</InternalRef>b. N-examples are grouped into four clusters and an SVM is built using representative n-examples from these clusters. As a result, two p-examples and two representative n-examples are extracted as support vectors, as depicted by the solid lines. The dashed line represents the decision boundary of the SVM. Based on this, n-examples located on the left side of the bold line are regarded as uninformative and are subsequently discarded. The second iteration is then performed using the remaining n-examples, as shown in Fig. <InternalRef RefID="Fig7">7</InternalRef>c. In this iteration, an SVM is built using three representative n-examples. Based on the decision boundary of this SVM, n-examples located on the left side of the bold line are discarded. In this way, a small number of n-examples, which are highly similar to p-examples, can be obtained. Such n-examples are useful for characterizing the boundary between event and non-event shots.
											<Figure Category="Standard" Float="Yes" ID="Fig7">
												<Caption Language="En">
													<CaptionNumber>Fig. 7</CaptionNumber>
													<CaptionContent>
														<SimplePara>An example of the proposed partially supervised learning method</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="BlackWhite" Format="GIF" Rendition="HTML" Type="Linedraw" FileRef="MediaObjects/11042_2011_727_Fig7_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
									</Section2>
								</Section1>
								<Section1 ID="Sec16">
									<Heading>Experimental results</Heading>
									<Para>In this section, the proposed QBE method is tested on TRECVID 2009 video data [<CitationRef CitationID="CR29">29</CitationRef>]. This data consists of 219 development and 619 test videos in various genres, like cultural, news magazine, documentary and education programming. Each video is already divided into shots by using an automatic shot boundary detection method, where development and test videos include 36,106 and 97,150 shots, respectively (the detailed video data specification can be found on TRECVID 2009 Web site<Footnote ID="Fn3">
											<Para>
												<ExternalRef>
													<RefSource>http://www-nlpir.nist.gov/projects/tv2009/tv2009.html</RefSource>
													<RefTarget Address="http://www-nlpir.nist.gov/projects/tv2009/tv2009.html" TargetType="URL"/>
												</ExternalRef>
											</Para>
										</Footnote>). The proposed method is evaluated based on the following five events:
										<DefinitionList>
											<DefinitionListEntry>
												<Term>
													<Emphasis Type="Italic">Event 1:</Emphasis>
												</Term>
												<Description>
													<Para>A view of one or more tall buildings and the top story visible</Para>
												</Description>
											</DefinitionListEntry>
											<DefinitionListEntry>
												<Term>
													<Emphasis Type="Italic">Event 2:</Emphasis>
												</Term>
												<Description>
													<Para>Something burning with flames visible</Para>
												</Description>
											</DefinitionListEntry>
											<DefinitionListEntry>
												<Term>
													<Emphasis Type="Italic">Event 3:</Emphasis>
												</Term>
												<Description>
													<Para>One or more people, each at a table or desk with a computer visible</Para>
												</Description>
											</DefinitionListEntry>
											<DefinitionListEntry>
												<Term>
													<Emphasis Type="Italic">Event 4:</Emphasis>
												</Term>
												<Description>
													<Para>An airplane or helicopter on the ground, seen from outside</Para>
												</Description>
											</DefinitionListEntry>
											<DefinitionListEntry>
												<Term>
													<Emphasis Type="Italic">Event 5:</Emphasis>
												</Term>
												<Description>
													<Para>One or more people, each sitting in a chair, talking</Para>
												</Description>
											</DefinitionListEntry>
										</DefinitionList>Each event retrieval is conducted as follows. Firstly, p-examples are manually collected from development videos. Based on p-examples, n-examples are then collected using the proposed PSL method. Considering the class imbalance problem, the number of collected n-examples is set to be equal to five times the number of p-examples. Afterward, the proposed RST method extended by bagging and the random subspace method, is run where two libraries, LIBSVM [<CitationRef CitationID="CR9">9</CitationRef>] and ROSETTA [<CitationRef CitationID="CR13">13</CitationRef>], are used for SVM learning and for the reduct extraction in RST, respectively. SVM parameters are determined using 3-fold cross validation. Lastly, the retrieval performance is evaluated based on the number of event shots within 1,000 retrieved shots.</Para>
									<Section2 ID="Sec17">
										<Heading>Effectiveness of rough set theory extended by bagging and the random subspace method</Heading>
										<Para>To examine the effectiveness of RST extended by bagging and the random subspace method, a comparison is drawn among the four types of retrieval described below.
											<DefinitionList>
												<DefinitionListEntry>
													<Term>
														<Emphasis Type="Italic">Baseline:</Emphasis>
													</Term>
													<Description>
														<Para>For each of six features, an SVM is built using all examples and dimensions, and a search of test videos is conducted. The SVM which yields the best result is then manually selected. In other words, <Emphasis Type="Italic">Baseline</Emphasis> represents a favorable retrieval result under the ideal condition in which the best feature for the event can be selected.</Para>
													</Description>
												</DefinitionListEntry>
												<DefinitionListEntry>
													<Term>
														<Emphasis Type="Italic">RST_only:</Emphasis>
													</Term>
													<Description>
														<Para>RST, which uses neither bagging nor the random subspace method, is executed. One SVM is built for each feature using all examples and dimensions.</Para>
													</Description>
												</DefinitionListEntry>
												<DefinitionListEntry>
													<Term>
														<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG:</Emphasis>
													</Term>
													<Description>
														<Para>RST is executed only by using bagging. For each feature, three SVMs are built using different subsets of examples and all dimensions. Each subset is constructed by randomly sampling 75% of examples.</Para>
													</Description>
												</DefinitionListEntry>
												<DefinitionListEntry>
													<Term>
														<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS:</Emphasis>
													</Term>
													<Description>
														<Para>RST which incorporates both bagging and the random subspace method is executed. For each feature, 10 SVMs are built using different subsets of examples and dimensions. Each subset of examples is formed by randomly sampling 75% of examples, while each subset of dimensions is formed by randomly sampling 50% of dimensions.</Para>
													</Description>
												</DefinitionListEntry>
											</DefinitionList>
										</Para>
										<Para>Table <InternalRef RefID="Tab4">4</InternalRef> shows performances of the above four types of retrieval. For each event, the second row presents the number of p-examples. For the performance evaluation, we consider that retrieval results of the proposed method differ due to the following two random factors. The first is attributed to the fact that, the PSL method often terminates before the number of n-examples is reduced to the specified number, because there are no n-examples which can be filtered out (refer to the stopping criterion in Algorithm 1). In such a case, from the remaining n-examples, five times as many n-examples as p-examples are randomly selected. The second random factor is associated with bagging and the random subspace method, where examples and feature dimensions are randomly selected. Thus, in Table <InternalRef RefID="Tab4">4</InternalRef>, each row labeled ‘# of event shots’ indicates the mean number of event shots in 10 retrieval results. Similarly, rows labeled ‘# of decision rules’ and ‘# of average precision’ indicate the mean number of decision rules extracted by RST and the mean of average precisions, respectively.
											<Table Float="Yes" ID="Tab4">
												<Caption Language="En">
													<CaptionNumber>Table 4</CaptionNumber>
													<CaptionContent>
														<SimplePara>Performance comparison of <Emphasis Type="Italic">Baseline</Emphasis>, <Emphasis Type="Italic">RST_only</Emphasis>, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis>
														</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup align="left" cols="7">
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<colspec colname="c7" colnum="7"/>
													<thead>
														<row>
															<entry colname="c1">
																<SimplePara> </SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>Event</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Event 1</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>Event 2</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>Event 3</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>Event 4</SimplePara>
															</entry>
															<entry colname="c7">
																<SimplePara>Event 5</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara># of p-examples</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>100</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>46</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>61</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>40</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>124</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">Baseline</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara># of event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>161.1</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>98.9</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>163.0</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>42.7</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>151.0</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">RST_only</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara># of event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">165.1</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">111.0</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>161.5</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>34.1</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>146.9</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara># of decision rules</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>5.5</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>5.2</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>5.4</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>4.5</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>3.8</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>Average precision</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>0.0931</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>0.1449</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>0.0871</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>0.0094</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>0.0365</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara># of event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">170.1</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">137.3</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">176.7</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>38.2</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>
																	<Emphasis Type="Bold">196.4</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara># of decision rules</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>252.9</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>159.7</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>151.9</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>137.4</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>354.0</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>Average precision</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>0.1148</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>0.1827</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>0.0877</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>0.0154</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>0.0571</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara># of event shots</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">172.0</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">147.5</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">176.9</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>41.6</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>
																	<Emphasis Type="Bold">194.6</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara># of decision rules</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>6159.2</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>1797.6</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>2286</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>1822.9</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>11455.6</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>Average precision</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>0.1170</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>0.1942</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>0.0992</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>0.0159</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>0.0604</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
										</Para>
										<Para>In Table <InternalRef RefID="Tab4">4</InternalRef>, the numbers in bold fonts indicate that <Emphasis Type="Italic">RST_only</Emphasis>, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> or <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> can retrieve more event shots than <Emphasis Type="Italic">Baseline</Emphasis>. Since the performance of <Emphasis Type="Italic">RST_only</Emphasis> for <Emphasis Type="Italic">Event 3</Emphasis>, <Emphasis Type="Italic">Event 4</Emphasis> and <Emphasis Type="Italic">Event 5</Emphasis> is lower than that of <Emphasis Type="Italic">Baseline</Emphasis>, it is not necessarily effective. One main reason for this ineffectiveness of <Emphasis Type="Italic">RST_only</Emphasis> is that the number of extracted decision rules for each event is very small. On the other hand, except for <Emphasis Type="Italic">Event 4</Emphasis> (which will be discussed later), both <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> outperform <Emphasis Type="Italic">Baseline</Emphasis> where a greater number of decision rules are extracted compared to <Emphasis Type="Italic">RST_only</Emphasis>. Thus, bagging and the random subspace method are useful for building various SVMs, which enables the extraction of decision rules covering a large variety of event shots. Lastly, when making a comparison between <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis>, numbers of retrieved event shots are not significantly different from each other. For all events, average precisions of <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> are higher than those of <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>, implying that event shots are ranked at higher positions in a retrieval result by <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> than by <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>. Thus, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> is considered to be superior to <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>.</Para>
										<Para>For <Emphasis Type="Italic">Event 4</Emphasis>, one main reason for the ineffectiveness of <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> is the difficulty of accurately recognizing airplanes and helicopters. Specifically, SVMs built by bagging and the random subspace method wrongly classify many shots showing cars, trains, ships etc. as positive, because their shapes are relatively similar to those of airplanes and helicopters. Thus, combining such inaccurate SVMs into decision rules degrades the retrieval performance.</Para>
									</Section2>
									<Section2 ID="Sec18">
										<Heading>Effectiveness of partially supervised learning</Heading>
										<Para>In this section, we examine the effectiveness of our PSL method. To this end, the performance using n-examples collected by our PSL method is compared to the one using n-examples which are randomly collected from all of u-examples. For the simplicity, we call the former and the latter types of n-examples <Emphasis Type="Italic">PSL</Emphasis> n-examples and <Emphasis Type="Italic">random</Emphasis> n-examples, respectively. In Table <InternalRef RefID="Tab5">5</InternalRef>, we run <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> using the same p-examples in Table <InternalRef RefID="Tab4">4</InternalRef>. PSL n-examples are used in the second and third rows while random n-examples are used in the fourth and fifth rows. Each performance in Table <InternalRef RefID="Tab5">5</InternalRef> is evaluated as the average number of event shots in 10 retrieval results.
											<Table Float="Yes" ID="Tab5">
												<Caption Language="En">
													<CaptionNumber>Table 5</CaptionNumber>
													<CaptionContent>
														<SimplePara>Comparison between the retrieval performance using our PSL method and the one using the random n-example selection</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup align="left" cols="7">
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<colspec colname="c7" colnum="7"/>
													<thead>
														<row>
															<entry nameend="c2" namest="c1">
																<SimplePara>Event</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>Event 1</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>Event 2</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>Event 3</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>Event 4</SimplePara>
															</entry>
															<entry colname="c7">
																<SimplePara>Event 5</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">PSL</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">170.1</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">137.3</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">176.7</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>38.2</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>
																	<Emphasis Type="Bold">196.4</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">165.1</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">147.5</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">176.9</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>41.6</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>
																	<Emphasis Type="Bold">194.6</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1">
																<SimplePara>
																	<Emphasis Type="Italic">Random</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>157.9</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>136.0</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>168.5</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>46.6</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>187.5</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c1"/>
															<entry align="center" colname="c2">
																<SimplePara>
																	<Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis>
																</SimplePara>
															</entry>
															<entry align="center" colname="c3">
																<SimplePara>159.6</SimplePara>
															</entry>
															<entry align="center" colname="c4">
																<SimplePara>144.3</SimplePara>
															</entry>
															<entry align="center" colname="c5">
																<SimplePara>171.0</SimplePara>
															</entry>
															<entry align="center" colname="c6">
																<SimplePara>47.0</SimplePara>
															</entry>
															<entry align="center" colname="c7">
																<SimplePara>187.8</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
											</Table>
										</Para>
										<Para>As can be seen from Table <InternalRef RefID="Tab5">5</InternalRef>, except for <Emphasis Type="Italic">Event 4</Emphasis>, event retrieval using PSL n-examples is more accurate than the one using random n-examples. The reason for the ineffective performance in <Emphasis Type="Italic">Event 4</Emphasis> is as follows. Due to the difficulty of accurately recognizing helicopters and airplanes, few edges which characterize the sky and runways in the background, are important for <Emphasis Type="Italic">Event 4</Emphasis>. But, since PSL n-examples are highly similar to p-examples, most of them are characterized by few edges. So, several event shots with few edges are inevitably excluded from the retrieval result. Thus, it can be said that our PSL method works well for events involving objects, which can be relatively accurately recognized. For example, for <Emphasis Type="Italic">Event 1</Emphasis>, the net of a soccer goal, iron bars, closet etc. are shown in PSL n-examples, because shapes of these objects are similar to buildings. By comparing such PSL n-examples to p-examples, a precise boundary between event and non-event shots can be extracted. Finally, since random n-examples are currently used in almost all of state-of-the-art methods [<CitationRef CitationID="CR21">21</CitationRef>, <CitationRef CitationID="CR22">22</CitationRef>, <CitationRef CitationID="CR31">31</CitationRef>], we believe that our PSL method is novel in the sense that it can outperform the random n-example selection.</Para>
									</Section2>
									<Section2 ID="Sec19">
										<Heading>Effectiveness for the small sample size problem</Heading>
										<Para>The performance of the proposed method is also tested in the case where only a small number of p-examples are available. Figure <InternalRef RefID="Fig8">8</InternalRef> illustrates the difference in the retrieval performance of <Emphasis Type="Italic">Baseline</Emphasis>, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis>, depending on p-example numbers. For a specified p-example number, we construct three different sets of examples in the following way. Firstly, the specified number of p-examples are randomly collected from all available p-examples, as shown in Table <InternalRef RefID="Tab4">4</InternalRef>. Then, n-examples are collected using our PSL method. The performance is evaluated as the average number of event shots in retrieval results using the above three sets of examples.
											<Figure Category="Standard" Float="Yes" ID="Fig8">
												<Caption Language="En">
													<CaptionNumber>Fig. 8</CaptionNumber>
													<CaptionContent>
														<SimplePara>Retrieval performances for different available p-example numbers</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig8_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
										<Para>In Fig. <InternalRef RefID="Fig8">8</InternalRef>, for <Emphasis Type="Italic">Event 2</Emphasis> and <Emphasis Type="Italic">Event 3</Emphasis>, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> always outperform <Emphasis Type="Italic">Baseline</Emphasis>. For <Emphasis Type="Italic">Event 1</Emphasis> and <Emphasis Type="Italic">Event 5</Emphasis>, when only 10 p-examples are available, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> are outperformed by <Emphasis Type="Italic">Baseline</Emphasis>. In this case, most of SVMs are inaccurate and as a consequence, decision rules as combinations of these SVMs are also inaccurate. Finally, for <Emphasis Type="Italic">Event 4</Emphasis>, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> are always outperformed by <Emphasis Type="Italic">Baseline</Emphasis>, due to the difficulty of accurately recognizing airplanes and helicopters described in the previous section. To summarize the overall performance, except for <Emphasis Type="Italic">Event 4</Emphasis>, <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> and <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> become more appropriate than <Emphasis Type="Italic">Baseline</Emphasis> when greater than 20 p-examples are available.</Para>
										<Para>Finally, it is easy to collect more than 20 p-examples for <Emphasis Type="Italic">Event 1</Emphasis> and <Emphasis Type="Italic">Event 5</Emphasis> as event shots of these are often seen in videos. However, event shots of <Emphasis Type="Italic">Event 2</Emphasis> are rarely seen and there are only a limited number of videos containing these event shots. This increases the difficulty of collecting more than 20 p-examples for <Emphasis Type="Italic">Event 2</Emphasis>. A sufficient number of p-examples may be obtained by retrieving images and videos on the web using online image/video search engines such as Flickr and YouTube.</Para>
									</Section2>
									<Section2 ID="Sec20">
										<Heading>Performance comparison</Heading>
										<Para>A comparison is made between the performance of the proposed method and those of state-of-the-art methods. <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> is specially compared with methods developed in TRECVID 2009 search task [<CitationRef CitationID="CR29">29</CitationRef>]. This task consists of three categories, namely, the fully automatic, manually-assisted and interactive categories. Given the textual description of an event and some p-examples, methods in the fully automatic category retrieve event shots without any user intervention. In the manually-assisted category, a user intervention is allowed only prior to the start of the test video search. The interactive category allows an interactive user intervention based on retrieval results. Although our proposed method belongs to the manually-assisted category, only three retrieval results are submitted to this category. This is clearly insufficient for achieving a meaningful comparison. Thus, the retrieval result of the proposed method is compared with 88 results in the fully automatic category.</Para>
										<Para>Figure <InternalRef RefID="Fig9">9</InternalRef> shows the maximum (solid arrow) and average (dashed arrow) numbers of event shots among 10 retrieval results, obtained using <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis>+<Emphasis Type="Italic">RS</Emphasis> in Table <InternalRef RefID="Tab4">4</InternalRef>. As can be seen from Fig. <InternalRef RefID="Fig9">9</InternalRef>, the overall performance of the proposed method is ranked in the top quartile. It can be noted that almost all methods in the top quartile use classifiers to assess the relevance of each shot to keywords like <Emphasis Type="Italic">Person</Emphasis>, <Emphasis Type="Italic">Building</Emphasis>, <Emphasis Type="Italic">Cityspace</Emphasis> and so on. In this case, for each keyword, a classifier is built using a large number of training examples. For example, in the method proposed by researchers at City University of Hong Kong, classifiers for 374 keywords are built using 61,901 manually annotated shots [<CitationRef CitationID="CR22">22</CitationRef>]. Also, in the method developed by researchers at University of Amsterdam, classifiers for 64 keywords are built using more than 10,000 manually annotated shots, such as 39,674 shots for <Emphasis Type="Italic">Bus</Emphasis>, 21,532 shots for <Emphasis Type="Italic">Car</Emphasis> and so on [<CitationRef CitationID="CR31">31</CitationRef>]. Like this, methods in the top quartile require tremendous manual effort. Compared to these methods, our method only uses p-examples which are provided by a user in an off-the-cuff manner. Thus, it can be concluded that the proposed method is very effective in the sense that it requires neither manual shot annotation nor classifier preparation.
											<Figure Category="Standard" Float="Yes" ID="Fig9">
												<Caption Language="En">
													<CaptionNumber>Fig. 9</CaptionNumber>
													<CaptionContent>
														<SimplePara>Performance comparison between our method and methods in TRECVID 2009</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig9_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
									</Section2>
									<Section2 ID="Sec21">
										<Heading>Reducing computation cost by parallelization</Heading>
										<Para>In this section, we examine the computation cost of our QBE method and reduce it by parallelizing several processes. Our method consists of two main phases, PSL and RST. Figure <InternalRef RefID="Fig10">10</InternalRef> illustrates processes in PSL (a) and RST (b) phases. Roughly speaking, the input of the PSL phase is a set of p-examples, and its output is a set of n-examples which are as similar to p-examples as possible. To this end, we first regard all of u-examples as n-examples and group them into clusters. Secondly, we find the representative n-example for each cluster. An SVM is then built by using p-examples and representative n-examples. Subsequently, n-examples which are distant from the decision boundary of the SVM are removed. Finally, the above processes are iterated until the number of n-examples is less than the pre-defined threshold or no n-example can be removed.
											<Figure Category="Standard" Float="Yes" ID="Fig10">
												<Caption Language="En">
													<CaptionNumber>Fig. 10</CaptionNumber>
													<CaptionContent>
														<SimplePara>Illustrations of processes in PSL (<Emphasis Type="Bold">a</Emphasis>) and RST (<Emphasis Type="Bold">b</Emphasis>) phases</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="BlackWhite" Format="GIF" Rendition="HTML" Type="Linedraw" FileRef="MediaObjects/11042_2011_727_Fig10_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
										<Para>When a large number of n-examples remain, the following two processes require expensive computation costs. The first is the similarity calculation for calculating the similarity between an n-example and each cluster center. The second computationally expensive process is cross validation where, for each parameter candidate, an SVM is built and evaluated by computing the error rate in classifying p-examples and n-examples. Thus, we parallelize the above two processes by using a multicore PC. For the similarity calculation process, each core is used to calculate the similarity between one cluster center and n-examples. For the cross validation process, each core is used to compute the error rate of an SVM with one parameter candidate. These are implemented using Matlab Parallel Computing Toolbox [<CitationRef CitationID="CR19">19</CitationRef>].</Para>
										<Para>As shown in Fig. <InternalRef RefID="Fig10">10</InternalRef>b, the input of the RST phase is a set of p-examples and n-examples, and its output is the set of 1,000 shots which match the largest numbers of decision rules. The RST phase is summarized as follows. Firstly, various SVMs are built by using bagging and the random subspace method. Decision rules are then extracted based on SVMs’ classification results of p-examples and n-examples. Subsequently, in order to match shots with extracted decision rules, the following four processes are performed. The first one is SVM’s probabilistic output calculation to compute the probabilistic output of an SVM for each shot. The second process is the evaluation value calculation which computes the evaluation value of matching a shot with each decision rule. The third is shot ranking with evaluation values where, for each decision rule, shots are ranked in the descending order of evaluation values. Based on this, 1,000 shots with the largest evaluation values are regarded to match the decision rule. The final process is shot ranking with matched rules to determine 1,000 shots matching the largest numbers of decision rules.</Para>
										<Para>The RST phase has a high computation cost due to a large number of shots and decision rules. We address this by using two types of parallelizations on a multicore PC. The first one is applied to the process of SVM’s probabilistic output calculation, where each core is used to compute SVM’s probabilistic output for a distributed set of shots. The other three processes are implemented by using <Emphasis Type="Italic">MapReduce</Emphasis>, which is a parallel programming model that provides a simple and powerful interface [<CitationRef CitationID="CR24">24</CitationRef>]. We use ‘Phoenix’ which is a MapReduce library for multicore PCs, in order to save a significant amount of time on I/O operations [<CitationRef CitationID="CR24">24</CitationRef>]. In MapReduce, the basic data structure is a (<Emphasis Type="Italic">key</Emphasis>, <Emphasis Type="Italic">value</Emphasis>) pair. Based on this, the <Emphasis Type="Italic">Map</Emphasis> function constructs input (<Emphasis Type="Italic">key</Emphasis>, <Emphasis Type="Italic">value</Emphasis>) pairs from a distributed data, and produces intermediate (<Emphasis Type="Italic">key</Emphasis>, <Emphasis Type="Italic">value</Emphasis>) pairs by conducting a user-defined task. Subsequently, the <Emphasis Type="Italic">Reduce</Emphasis> function conducts a user-defined merge operation on intermediate (<Emphasis Type="Italic">key</Emphasis>, <Emphasis Type="Italic">value</Emphasis>) pairs with the same <Emphasis Type="Italic">key</Emphasis> and outputs a final result. In this manner, MapReduce divides a large-scale data into small pieces of (<Emphasis Type="Italic">key</Emphasis>, <Emphasis Type="Italic">value</Emphasis>) pairs, which are efficiently processed in parallel by the Map and Reduce functions.</Para>
										<Para>Three processes in Fig. <InternalRef RefID="Fig10">10</InternalRef>b are implemented by utilizing MapReduce twice. The first MapReduce performs two processes, the evaluation value calculation and shot ranking with evaluation values. Specifically, the objective is to determine 1,000 shots which have the largest evaluation values for each rule. To do this, the following Map and Reduce functions are designed:
											<Equation ID="Equ8">
												<EquationNumber>8</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ8.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ \begin{array}{rll} map_{1} &amp; : &amp; \big(x, \big[rule, Prob_{SVM}^{all} (x)\big]\big) \rightarrow List(rule, Match(x, rule)) \nonumber \\ reduce_{1} &amp; : &amp; (rule, List(Match(x, rule))) \rightarrow \big(rule, SList_{rule}^{1{\rm ,}000}\big) \label{eqn:first_mr} \end{array}$$</EquationSource>
											</Equation>where <Emphasis Type="Italic">x</Emphasis> and <Emphasis Type="Italic">rule</Emphasis> are a shot and decision rule, respectively. <InlineEquation ID="IEq14">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq14.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$Prob_{SVM}^{all} (x)$</EquationSource>
											</InlineEquation> represents the set of probabilistic outputs of all SVMs for <Emphasis Type="Italic">x</Emphasis>. By using <InlineEquation ID="IEq15">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq15.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$Prob_{SVM}^{all} (x)$</EquationSource>
											</InlineEquation>, <Emphasis Type="Italic">map</Emphasis>
											<Subscript>1</Subscript> computes <Emphasis Type="Italic">Match</Emphasis>(<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">rule</Emphasis>) which is the evaluation value of matching <Emphasis Type="Italic">x</Emphasis> with <Emphasis Type="Italic">rule</Emphasis> as defined in (<InternalRef RefID="Equ5">5</InternalRef>). The output of <Emphasis Type="Italic">map</Emphasis>
											<Subscript>1</Subscript> is a list including evaluation values of distributed shots for all rules. Subsequently, <Emphasis Type="Italic">reduce</Emphasis>
											<Subscript>1</Subscript> merges such lists produced by <Emphasis Type="Italic">map</Emphasis>
											<Subscript>1</Subscript> on different cores so that for the same <Emphasis Type="Italic">rule</Emphasis>, evaluation values of all shots are combined into a list, namely, <Emphasis Type="Italic">List</Emphasis>(<Emphasis Type="Italic">Match</Emphasis>(<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">rule</Emphasis>)). By sorting this list, <Emphasis Type="Italic">reduce</Emphasis>
											<Subscript>1</Subscript> outputs <InlineEquation ID="IEq16">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq16.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$SList_{rule}^{1,000}$</EquationSource>
											</InlineEquation> which consists of 1,000 shots with the largest evaluation values. That is, shots in <InlineEquation ID="IEq17">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq17.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$SList_{rule}^{1,000}$</EquationSource>
											</InlineEquation> are regarded to match <Emphasis Type="Italic">rule</Emphasis>.</Para>
										<Para>The second MapReduce performs the process of shot ranking with numbers of matched rules. This process aims to obtain 1,000 shots which match the largest numbers of decision rules. The following Map and Reduce functions are designed:
											<Equation ID="Equ9">
												<EquationNumber>9</EquationNumber>
												<MediaObject>
													<ImageObject FileRef="11042_2011_727_Article_Equ9.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</MediaObject>
												<EquationSource Format="TEX">$$ \begin{array}{rll} map_{2} &amp; : &amp; (x, 1) \rightarrow List(x, MRules(x)) \nonumber \\ reduce_{2} &amp; : &amp; (x, List(MRules(x))) \rightarrow SList_{MRules}^{1,000} \label{eqn:sec_mr} \end{array}$$</EquationSource>
											</Equation> where (<Emphasis Type="Italic">x</Emphasis>, 1) is obtained by parsing <InlineEquation ID="IEq18">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq18.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$SList_{rule}^{1,000}$</EquationSource>
											</InlineEquation> and indicates that a shot <Emphasis Type="Italic">x</Emphasis> matches one rule. It should be noted that as we have only to count the number of decision rules matched with <Emphasis Type="Italic">x</Emphasis>, we do not need to know which rules are matched with <Emphasis Type="Italic">x</Emphasis>. The function <Emphasis Type="Italic">map</Emphasis>
											<Subscript>2</Subscript> constructs a list of (<Emphasis Type="Italic">x</Emphasis>, <Emphasis Type="Italic">MRules</Emphasis>(<Emphasis Type="Italic">x</Emphasis>))s, where <Emphasis Type="Italic">MRules</Emphasis>(<Emphasis Type="Italic">x</Emphasis>) represents the number of decision rules matched with <Emphasis Type="Italic">x</Emphasis>. Subsequently, <Emphasis Type="Italic">reduce</Emphasis>
											<Subscript>2</Subscript> merges <Emphasis Type="Italic">MRules</Emphasis>(<Emphasis Type="Italic">x</Emphasis>) for the same shot into <Emphasis Type="Italic">List</Emphasis>(<Emphasis Type="Italic">MRules</Emphasis>(<Emphasis Type="Italic">x</Emphasis>)), which represents the total number of matched decision rules. Finally, the function sorts all shots based on <Emphasis Type="Italic">List</Emphasis>(<Emphasis Type="Italic">MRules</Emphasis>(<Emphasis Type="Italic">x</Emphasis>)) and outputs <InlineEquation ID="IEq19">
												<InlineMediaObject>
													<ImageObject FileRef="11042_2011_727_Article_IEq19.gif" Format="GIF" Color="BlackWhite" Type="Linedraw" Rendition="HTML"/>
												</InlineMediaObject>
												<EquationSource Format="TEX">$SList_{MRules}^{1,000}$</EquationSource>
											</InlineEquation> which consists of 1,000 shots matching the largest numbers of decision rules.</Para>
										<Para>Figure <InternalRef RefID="Fig11">11</InternalRef> illustrates retrieval times for our QBE method where event shots for <Emphasis Type="Italic">Event 1</Emphasis> are retrieved by applying <Emphasis Type="Italic">RST</Emphasis>+<Emphasis Type="Italic">BG</Emphasis> to 100 p-examples and 500 n-examples. Figure <InternalRef RefID="Fig11">11</InternalRef>a and b show the change in computation times in the PSL and RST phases, respectively. In both figures, the four bars from the top to the bottom represent computation times by parallelizing our method with 1, 2, 4 and 8 cores, respectively. Bars named <Emphasis Type="Italic">Others</Emphasis> depict computation times of non-parallelized processes while the other bars depict computation times of parallelized processes. Specifically, in the PSL phase in Fig. <InternalRef RefID="Fig11">11</InternalRef>a, <Emphasis Type="Italic">Clustering</Emphasis> and <Emphasis Type="Italic">SVM building</Emphasis> include the similarity calculation and cross validation processes in Fig. <InternalRef RefID="Fig10">10</InternalRef>a, respectively. In the RST phase in Fig. <InternalRef RefID="Fig11">11</InternalRef>b, <Emphasis Type="Italic">SVM prob.</Emphasis> corresponds to SVM’s probabilistic output calculation process in Fig. <InternalRef RefID="Fig10">10</InternalRef>b. <Emphasis Type="Italic">Rule matching</Emphasis> includes three processes parallelized by MapReduce. In addition, numbers overlaid with bars present actual values of computation times. Both of Fig. <InternalRef RefID="Fig11">11</InternalRef>a and b indicate that, as the number of cores increases, the computation time can be significantly shortened. Specifically, when only one core is used, our QBE method takes a total of 26,312 seconds to complete event retrieval with the PSL and RST phases requiring 24,796 and 1,516 seconds, respectively. In comparison, when 8 cores are used, event retrieval is completed in 6,194 seconds with computation times of the PSL and RST phases being 5,531 and 663 seconds, respectively.
											<Figure Category="Standard" Float="Yes" ID="Fig11">
												<Caption Language="En">
													<CaptionNumber>Fig. 11</CaptionNumber>
													<CaptionContent>
														<SimplePara>Comparison among retrieval times by parallelizing our QBE method with 1, 2, 4 and 8 cores</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" Format="GIF" Rendition="HTML" Type="LinedrawHalftone" FileRef="MediaObjects/11042_2011_727_Fig11_HTML.gif"/>
												</MediaObject>
											</Figure>
										</Para>
										<Para>From the perspective of computation time, our current QBE method is far from the satisfactory. So, the retrieval time will need to be further reduced by improving currently parallelized processes and parallelizing the other processes. In relation to this concern, Fig. <InternalRef RefID="Fig11">11</InternalRef> reveals an important issue. For <Emphasis Type="Italic">Clustering</Emphasis> and <Emphasis Type="Italic">Rule matching</Emphasis> processes, as the number of cores increases, computation times are nearly linearly reduced. However, compared to these reductions, reductions of computation times for <Emphasis Type="Italic">SVM building</Emphasis> and <Emphasis Type="Italic">SVM prob.</Emphasis> are significantly less effective. For example, for <Emphasis Type="Italic">SVM building</Emphasis> process in the PSL phase, even if the number of cores is doubled from 4 to 8, the ratio between the computation time of 4 cores and the one of 8 cores is only 1.29 (i.e. 2,001 seconds versus 1,547 seconds). One consideration is that <Emphasis Type="Italic">SVM building</Emphasis> and <Emphasis Type="Italic">SVM prob.</Emphasis> are parallelized simply by distributing examples (or shots) to multiple cores. On each core, functions in LibSVM libraries [<CitationRef CitationID="CR9">9</CitationRef>] are called where memory allocations and releases are executed many times. This degrades the effectiveness of parallelization. Therefore, in order to achieve effective parallelization of a process, it should be divided into sub-processes involving few memory allocations or releases.</Para>
									</Section2>
								</Section1>
								<Section1 ID="Sec22">
									<Heading>Conclusion and future work</Heading>
									<Para>In this paper, we proposed a QBE method which can retrieve event shots using only a small number of p-examples. Due to camera techniques and settings, event shots are characterized by significantly different features. As such, RST is used to extract multiple decision rules which characterize different subsets of p-examples. A variety of event shots can be retrieved where each decision rule is specialized to retrieve a part of event shots. Additionally, in order to extend the range of event shots that can be retrieved, bagging and the random subspace method are incorporated into RST. Classifiers built using different examples and feature dimensions, are useful for covering a variety of event shots while many non-event shots are potentially retrieved. Thus, RST is used to combine classifiers into decision rules in order to accurately retrieve event shots. Furthermore, to overcome the lack of n-examples, PSL is used to collect n-examples from u-examples. In particular, taking the class imbalance problem into account, a method which can collect a small number of n-examples useful for building an accurate classifier, is developed. Experimental results demonstrated that our method successfully covers various event shots when more than 20 p-examples are available. In addition, our method can achieve very effective event retrieval without any shot annotation or classifier preparation.</Para>
									<Para>The following issues will be addressed in future works. Firstly, we aim to use temporal features such as 3DSIFT [<CitationRef CitationID="CR27">27</CitationRef>] and acoustic features such as Mel-Frequency Cepstrum Coefficient (MFCC), as opposed to our current method which only makes use of image features. Secondly, although majority voting is currently conducted using all of extracted decision rules, some rules may be inaccurate or very similar to other rules. Thus, in order to obtain the optimal set of decision rules, a method which examines the accuracy of each decision rule based on cross validation will be developed. The relationship among decision rules will be investigated using the diversity measures proposed in [<CitationRef CitationID="CR14">14</CitationRef>]. Lastly, to improve the computation time of our current method, we plan to build a cluster consisting of tens or hundreds of PCs. On this cluster, in addition to the process of matching shots with decision rules, we parallelize the other processes using Apache Hadoop [<CitationRef CitationID="CR35">35</CitationRef>], which implements MapReduce on a large-scale PC cluster.</Para>
								</Section1>
							</Body>
							<BodyRef TargetType="OnlinePDF" FileRef="BodyRef/PDF/11042_2011_Article_727.pdf"/>
							<ArticleBackmatter>
								<Acknowledgments>
									<Heading>Acknowledgements</Heading>
									<SimplePara>This research is supported in part by Strategic Information and Communications R&amp;D Promotion Programme (SCOPE) by the Ministry of Internal Affairs and Communications, Japan.</SimplePara>
									<FormalPara RenderingStyle="Style1">
										<Heading>
											<Emphasis Type="Bold">Open Access</Emphasis>
										</Heading>
										<Para>This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</Para>
									</FormalPara>
								</Acknowledgments>
								<Bibliography ID="Bib1">
									<Heading>References</Heading>
									<Citation ID="CR1">
										<CitationNumber>1.</CitationNumber>
										<BibUnstructured>Akbani R, Kwek S, Japkowicz N (2004) Applying support vector machines to imbalanced datasets. In: Proc. of ECML 2004, pp 39–50</BibUnstructured>
									</Citation>
									<Citation ID="CR2">
										<CitationNumber>2.</CitationNumber>
										<BibUnstructured>Alpaydin E (2004) Introduction to machine learning. MIT Press</BibUnstructured>
									</Citation>
									<Citation ID="CR3">
										<CitationNumber>3.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>L</Initials>
												<FamilyName>Breiman</FamilyName>
											</BibAuthorName>
											<Year>1996</Year>
											<ArticleTitle Language="En">Bagging predictors</ArticleTitle>
											<JournalTitle>Mach Learn</JournalTitle>
											<VolumeID>24</VolumeID>
											<IssueID>2</IssueID>
											<FirstPage>123</FirstPage>
											<LastPage>140</LastPage>
											<Occurrence Type="ZLBID">
												<Handle>0858.68080</Handle>
											</Occurrence>
											<Occurrence Type="AMSID">
												<Handle>1425957</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Breiman L (1996) Bagging predictors. Mach Learn 24(2):123–140</BibUnstructured>
									</Citation>
									<Citation ID="CR4">
										<CitationNumber>4.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>G</Initials>
												<FamilyName>Fung</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Yu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Ku</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>P</Initials>
												<FamilyName>Yu</FamilyName>
											</BibAuthorName>
											<Year>2006</Year>
											<ArticleTitle Language="En">Text classification without negative examples revisit</ArticleTitle>
											<JournalTitle>IEEE Trans Knowl Data Eng</JournalTitle>
											<VolumeID>18</VolumeID>
											<IssueID>1</IssueID>
											<FirstPage>6</FirstPage>
											<LastPage>20</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TKDE.2006.16</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Fung G, Yu J, Ku H, Yu P (2006) Text classification without negative examples revisit. IEEE Trans Knowl Data Eng 18(1):6–20</BibUnstructured>
									</Citation>
									<Citation ID="CR5">
										<CitationNumber>5.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>Z</Initials>
												<FamilyName>Gang</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Kobayashi</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>Y</Initials>
												<FamilyName>Sakai</FamilyName>
											</BibAuthorName>
											<Year>2005</Year>
											<ArticleTitle Language="En">Group-based relevance feedback using reduct of rough set theory for interactive image retrieval</ArticleTitle>
											<JournalTitle>J ITEJ</JournalTitle>
											<VolumeID>56</VolumeID>
											<IssueID>6</IssueID>
											<FirstPage>884</FirstPage>
											<LastPage>893</LastPage>
										</BibArticle>
										<BibUnstructured>Gang Z, Kobayashi A, Sakai Y (2005) Group-based relevance feedback using reduct of rough set theory for interactive image retrieval. J ITEJ 56(6):884–893</BibUnstructured>
									</Citation>
									<Citation ID="CR6">
										<CitationNumber>6.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>G</Initials>
												<FamilyName>Guo</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Dyer</FamilyName>
											</BibAuthorName>
											<Year>2005</Year>
											<ArticleTitle Language="En">Learning from examples in the small sample case: face expression recognition</ArticleTitle>
											<JournalTitle>IEEE Trans Syst Man Cybern Part B</JournalTitle>
											<VolumeID>35</VolumeID>
											<IssueID>3</IssueID>
											<FirstPage>477</FirstPage>
											<LastPage>488</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TSMCB.2005.846658</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Guo G, Dyer C (2005) Learning from examples in the small sample case: face expression recognition. IEEE Trans Syst Man Cybern Part B 35(3):477–488</BibUnstructured>
									</Citation>
									<Citation ID="CR7">
										<CitationNumber>7.</CitationNumber>
										<BibUnstructured>Han J, Kamber M (2006) Data mining: concepts and techniques, 2nd edn. Morgan Kaufmann Publishers</BibUnstructured>
									</Citation>
									<Citation ID="CR8">
										<CitationNumber>8.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Ho</FamilyName>
											</BibAuthorName>
											<Year>1998</Year>
											<ArticleTitle Language="En">The random subspace method for constructing decision forests</ArticleTitle>
											<JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
											<VolumeID>20</VolumeID>
											<IssueID>8</IssueID>
											<FirstPage>832</FirstPage>
											<LastPage>844</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/34.709601</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Ho T (1998) The random subspace method for constructing decision forests. IEEE Trans Pattern Anal Mach Intell 20(8):832–844</BibUnstructured>
									</Citation>
									<Citation ID="CR9">
										<CitationNumber>9.</CitationNumber>
										<BibUnstructured>Hsu C, Chang C, Lin C (2003) A practical guide to support vector classification. <ExternalRef>
												<RefSource>http://www.csie.ntu.edu.tw/∼cjlin/papers/guide/guide.pdf</RefSource>
												<RefTarget Address="http://www.csie.ntu.edu.tw/∼cjlin/papers/guide/guide.pdf" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR10">
										<CitationNumber>10.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>A</Initials>
												<FamilyName>Jain</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Duin</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Mao</FamilyName>
											</BibAuthorName>
											<Year>2000</Year>
											<ArticleTitle Language="En">Statistical pattern recognition: a review</ArticleTitle>
											<JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
											<VolumeID>22</VolumeID>
											<IssueID>1</IssueID>
											<FirstPage>4</FirstPage>
											<LastPage>37</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/34.824819</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Jain A, Duin R, Mao J (2000) Statistical pattern recognition: a review. IEEE Trans Pattern Anal Mach Intell 22(1):4–37</BibUnstructured>
									</Citation>
									<Citation ID="CR11">
										<CitationNumber>11.</CitationNumber>
										<BibUnstructured>Japkowicz N (2000) The class imbalance problem: significance and strategies. In: Proc. of IC-AI 2000, pp 111–117</BibUnstructured>
									</Citation>
									<Citation ID="CR12">
										<CitationNumber>12.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>K</Initials>
												<FamilyName>Kashino</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Kurozumi</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Murase</FamilyName>
											</BibAuthorName>
											<Year>2003</Year>
											<ArticleTitle Language="En">A quick search method for audio and video signals based on histogram pruning</ArticleTitle>
											<JournalTitle>IEEE Trans Multimedia</JournalTitle>
											<VolumeID>5</VolumeID>
											<IssueID>3</IssueID>
											<FirstPage>348</FirstPage>
											<LastPage>357</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TMM.2003.813281</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Kashino K, Kurozumi T, Murase H (2003) A quick search method for audio and video signals based on histogram pruning. IEEE Trans Multimedia 5(3):348–357</BibUnstructured>
									</Citation>
									<Citation ID="CR13">
										<CitationNumber>13.</CitationNumber>
										<BibUnstructured>Komorowski J, Øhrn A, Skowron A (2002) The rosetta rough set software system. In: Klösgen W, Zytkow J (eds) Handbook of data mining and knowledge discovery, chap D.2.3. Oxford University Press</BibUnstructured>
									</Citation>
									<Citation ID="CR14">
										<CitationNumber>14.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>L</Initials>
												<FamilyName>Kuncheva</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Whitaker</FamilyName>
											</BibAuthorName>
											<Year>2003</Year>
											<ArticleTitle Language="En">Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy</ArticleTitle>
											<JournalTitle>Mach Learn</JournalTitle>
											<VolumeID>51</VolumeID>
											<IssueID>2</IssueID>
											<FirstPage>181</FirstPage>
											<LastPage>207</LastPage>
											<Occurrence Type="ZLBID">
												<Handle>1027.68113</Handle>
											</Occurrence>
											<Occurrence Type="DOI">
												<Handle>10.1023/A:1022859003006</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Kuncheva L, Whitaker C (2003) Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy. Mach Learn 51(2):181–207</BibUnstructured>
									</Citation>
									<Citation ID="CR15">
										<CitationNumber>15.</CitationNumber>
										<BibUnstructured>Lin W, Hauptmann A (2004) Modeling timing features in broadcast news video classification. In: Proc. of ICME 2004, pp 27–30</BibUnstructured>
									</Citation>
									<Citation ID="CR16">
										<CitationNumber>16.</CitationNumber>
										<BibUnstructured>Liu B, Lee W, Yu P, Li X (2002) Partially supervised classification of text documents. In: Proc. of ICML 2002, pp 387–394</BibUnstructured>
									</Citation>
									<Citation ID="CR17">
										<CitationNumber>17.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Liu</FamilyName>
											</BibAuthorName>
											<Year>2005</Year>
											<ArticleTitle Language="En">Evolving feature selection</ArticleTitle>
											<JournalTitle>IEEE Intell Syst</JournalTitle>
											<VolumeID>20</VolumeID>
											<IssueID>6</IssueID>
											<FirstPage>64</FirstPage>
											<LastPage>76</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/MIS.2005.105</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Liu H (2005) Evolving feature selection. IEEE Intell Syst 20(6):64–76</BibUnstructured>
									</Citation>
									<Citation ID="CR18">
										<CitationNumber>18.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Liu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Lin</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>R</Initials>
												<FamilyName>Weng</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ArticleTitle Language="En">A note on platt’s probabilistic outputs for support vector machines</ArticleTitle>
											<JournalTitle>Mach Learn</JournalTitle>
											<VolumeID>68</VolumeID>
											<IssueID>3</IssueID>
											<FirstPage>267</FirstPage>
											<LastPage>276</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1007/s10994-007-5018-6</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Liu H, Lin C, Weng R (2007) A note on platt’s probabilistic outputs for support vector machines. Mach Learn 68(3):267–276</BibUnstructured>
									</Citation>
									<Citation ID="CR19">
										<CitationNumber>19.</CitationNumber>
										<BibUnstructured>Luszczek P (2008) Enhancing multiple system performance using parallel computing with MATLAB. <ExternalRef>
												<RefSource>http://www.mathworks.com/mason/tag/proxy.html?dataid=10721&amp;fileid=55087</RefSource>
												<RefTarget Address="http://www.mathworks.com/mason/tag/proxy.html?dataid=10721&amp;fileid=55087" TargetType="URL"/>
											</ExternalRef>
										</BibUnstructured>
									</Citation>
									<Citation ID="CR20">
										<CitationNumber>20.</CitationNumber>
										<BibUnstructured>Mizui A, Shirahama K, Uehara K (2008) TRECVID 2008 NOTEBOOK PAPER: interactive search using multiple queries and rough set theory. In: Proc. of TRECVID 2008, pp 123–132</BibUnstructured>
									</Citation>
									<Citation ID="CR21">
										<CitationNumber>21.</CitationNumber>
										<BibUnstructured>Natsev A, Naphade M, Tešić J (2005) Learning the semantics of multimedia queries and concepts from a small number of examples. In: Proc. of ACM MM 2005, pp 598–607</BibUnstructured>
									</Citation>
									<Citation ID="CR22">
										<CitationNumber>22.</CitationNumber>
										<BibUnstructured>Ngo C, et al (2009) VIREO/DVM at TRECVID 2009: high-level feature extraction, automatic video search and content-based copy detection. In: Proc. of TRECVID2009, pp 415–432</BibUnstructured>
									</Citation>
									<Citation ID="CR23">
										<CitationNumber>23.</CitationNumber>
										<BibUnstructured>Peng Y, Ngo C (2005) EMD-based video clip retrieval by many-to-many matching. In: Proc. of CIVR 2005, pp 71–81</BibUnstructured>
									</Citation>
									<Citation ID="CR24">
										<CitationNumber>24.</CitationNumber>
										<BibUnstructured>Ranger C, et al (2007) Evaluating mapreduce for multi-core and multiprocessor systems. In: Proc. of HPCA 2007, pp 13–24</BibUnstructured>
									</Citation>
									<Citation ID="CR25">
										<CitationNumber>25.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Saha</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Murthy</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>S</Initials>
												<FamilyName>Pal</FamilyName>
											</BibAuthorName>
											<Year>2007</Year>
											<ArticleTitle Language="En">Rough set based ensemble classifier for web page classification</ArticleTitle>
											<JournalTitle>Fundam Math</JournalTitle>
											<VolumeID>76</VolumeID>
											<IssueID>1–2</IssueID>
											<FirstPage>171</FirstPage>
											<LastPage>187</LastPage>
											<Occurrence Type="AMSID">
												<Handle>2293056</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Saha S, Murthy C, Pal S (2007) Rough set based ensemble classifier for web page classification. Fundam Math 76(1–2):171–187</BibUnstructured>
									</Citation>
									<Citation ID="CR26">
										<CitationNumber>26.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>K</Initials>
												<FamilyName>Sande</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>T</Initials>
												<FamilyName>Gevers</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Snoek</FamilyName>
											</BibAuthorName>
											<Year>2010</Year>
											<ArticleTitle Language="En">Evaluating color descriptors for object and scene recognition</ArticleTitle>
											<JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
											<VolumeID>32</VolumeID>
											<IssueID>9</IssueID>
											<FirstPage>1582</FirstPage>
											<LastPage>1596</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TPAMI.2009.154</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Sande K, Gevers T, Snoek C (2010) Evaluating color descriptors for object and scene recognition. IEEE Trans Pattern Anal Mach Intell 32(9):1582–1596</BibUnstructured>
									</Citation>
									<Citation ID="CR27">
										<CitationNumber>27.</CitationNumber>
										<BibUnstructured>Scovanner P, Ali S, Shah M (2007) A 3-dimensional SIFT descriptor and its application to action recognition. In: Proc. of ACM MM 2007, pp 357–360</BibUnstructured>
									</Citation>
									<Citation ID="CR28">
										<CitationNumber>28.</CitationNumber>
										<BibUnstructured>Shirahama K, Sugihara C, Matsuoka Y, Matsumura K, Uehara K (2009) Kobe University at TRECVID 2009 search task. In: Proc. of TRECVID 2009, pp 76–84</BibUnstructured>
									</Citation>
									<Citation ID="CR29">
										<CitationNumber>29.</CitationNumber>
										<BibUnstructured>Smeaton A, Over P, Kraaij W (2006) Evaluation campaigns and TRECVid. In: Proc. of MIR 2006, pp 321–330</BibUnstructured>
									</Citation>
									<Citation ID="CR30">
										<CitationNumber>30.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>C</Initials>
												<FamilyName>Snoek</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>M</Initials>
												<FamilyName>Worring</FamilyName>
											</BibAuthorName>
											<Year>2005</Year>
											<ArticleTitle Language="En">Multimedia event-based video indexing using time intervals</ArticleTitle>
											<JournalTitle>IEEE Trans Multimedia</JournalTitle>
											<VolumeID>7</VolumeID>
											<IssueID>4</IssueID>
											<FirstPage>638</FirstPage>
											<LastPage>647</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TMM.2005.850966</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Snoek C, Worring M (2005) Multimedia event-based video indexing using time intervals. IEEE Trans Multimedia 7(4):638–647</BibUnstructured>
									</Citation>
									<Citation ID="CR31">
										<CitationNumber>31.</CitationNumber>
										<BibUnstructured>Snoek C, et al (2009) The mediamill TRECVID 2009 semantic video search engine. In: Proc. of TRECVID2009, pp 226–238</BibUnstructured>
									</Citation>
									<Citation ID="CR32">
										<CitationNumber>32.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>D</Initials>
												<FamilyName>Tao</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>X</Initials>
												<FamilyName>Tang</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>X</Initials>
												<FamilyName>Li</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>X</Initials>
												<FamilyName>Wu</FamilyName>
											</BibAuthorName>
											<Year>2006</Year>
											<ArticleTitle Language="En">Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval</ArticleTitle>
											<JournalTitle>IEEE Trans Pattern Anal Mach Intell</JournalTitle>
											<VolumeID>28</VolumeID>
											<IssueID>7</IssueID>
											<FirstPage>1088</FirstPage>
											<LastPage>1099</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TPAMI.2006.134</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Tao D, Tang X, Li X, Wu X (2006) Asymmetric bagging and random subspace for support vector machines-based relevance feedback in image retrieval. IEEE Trans Pattern Anal Mach Intell 28(7):1088–1099</BibUnstructured>
									</Citation>
									<Citation ID="CR33">
										<CitationNumber>33.</CitationNumber>
										<BibUnstructured>Vapnik V (1998) Statistical learning theory. Wiley-Interscience</BibUnstructured>
									</Citation>
									<Citation ID="CR34">
										<CitationNumber>34.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Verbeek</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>N</Initials>
												<FamilyName>Vlassis</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>B</Initials>
												<FamilyName>Kröse</FamilyName>
											</BibAuthorName>
											<Year>2003</Year>
											<ArticleTitle Language="En">Efficient greedy learning of gaussian mixture models</ArticleTitle>
											<JournalTitle>Neural Comput</JournalTitle>
											<VolumeID>15</VolumeID>
											<IssueID>2</IssueID>
											<FirstPage>469</FirstPage>
											<LastPage>485</LastPage>
											<Occurrence Type="ZLBID">
												<Handle>1047.68114</Handle>
											</Occurrence>
											<Occurrence Type="DOI">
												<Handle>10.1162/089976603762553004</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Verbeek J, Vlassis N, Kröse B (2003) Efficient greedy learning of gaussian mixture models. Neural Comput 15(2):469–485</BibUnstructured>
									</Citation>
									<Citation ID="CR35">
										<CitationNumber>35.</CitationNumber>
										<BibUnstructured>White T (2009) Hadoop: the definitive guide. O’Reilly Media</BibUnstructured>
									</Citation>
									<Citation ID="CR36">
										<CitationNumber>36.</CitationNumber>
										<BibUnstructured>Yao P (2009) Hybrid classifier using neighborhood rough set and svm for credit scoring. In: Proc. of BIFE 2009, pp 138–142</BibUnstructured>
									</Citation>
									<Citation ID="CR37">
										<CitationNumber>37.</CitationNumber>
										<BibArticle>
											<BibAuthorName>
												<Initials>H</Initials>
												<FamilyName>Yu</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>J</Initials>
												<FamilyName>Han</FamilyName>
											</BibAuthorName>
											<BibAuthorName>
												<Initials>K</Initials>
												<FamilyName>Chang</FamilyName>
											</BibAuthorName>
											<Year>2004</Year>
											<ArticleTitle Language="En">PEBL: web page classification without negative examples</ArticleTitle>
											<JournalTitle>IEEE Trans Knowl Data Eng</JournalTitle>
											<VolumeID>16</VolumeID>
											<IssueID>1</IssueID>
											<FirstPage>70</FirstPage>
											<LastPage>81</LastPage>
											<Occurrence Type="DOI">
												<Handle>10.1109/TKDE.2004.1264816</Handle>
											</Occurrence>
										</BibArticle>
										<BibUnstructured>Yu H, Han J, Chang K (2004) PEBL: web page classification without negative examples. IEEE Trans Knowl Data Eng 16(1):70–81</BibUnstructured>
									</Citation>
									<Citation ID="CR38">
										<CitationNumber>38.</CitationNumber>
										<BibUnstructured>Yuan J, Li J, Zhang B (2006) Learning concepts from large scale imbalanced data sets using support cluster machines. In: Proc. of ACM MM 2006, pp 441–450</BibUnstructured>
									</Citation>
								</Bibliography>
							</ArticleBackmatter>
						</Article>
					</JournalOnlineFirst>
				</Journal>
				<meta:Info xmlns:meta="http://www.springer.com/app/meta">
					<meta:DateLoaded>2011-01-26T12:13:24.90288+01:00</meta:DateLoaded>
					<meta:Authors>
						<meta:Author>Shirahama, Kimiaki</meta:Author>
						<meta:Author>Matsuoka, Yuta</meta:Author>
						<meta:Author>Uehara, Kuniaki</meta:Author>
					</meta:Authors>
					<meta:Institutions>
						<meta:Institution>
							<meta:OrgName>Kobe University</meta:OrgName>
							<meta:Country>Japan</meta:Country>
						</meta:Institution>
						<meta:Institution>
							<meta:OrgName>Kobe University</meta:OrgName>
							<meta:Country>Japan</meta:Country>
						</meta:Institution>
						<meta:Institution>
							<meta:OrgName>Kobe University</meta:OrgName>
							<meta:Country>Japan</meta:Country>
						</meta:Institution>
					</meta:Institutions>
					<meta:Date>2011-01-20</meta:Date>
					<meta:Type>Article</meta:Type>
					<meta:DOI>10.1007/s11042-011-0727-z</meta:DOI>
					<meta:Title>Event retrieval in video archives using rough set theory and partially supervised learning</meta:Title>
					<meta:ISXN>1573-7721</meta:ISXN>
					<meta:PubName>Springer</meta:PubName>
					<meta:Journal>Multimedia Tools and Applications</meta:Journal>
					<meta:Publication>Multimedia Tools and Applications</meta:Publication>
					<meta:PublicationType>Journal</meta:PublicationType>
					<meta:SubjectGroup>
						<meta:Subject Type="Primary">Computer Science</meta:Subject>
						<meta:Subject Type="Secondary">Special Purpose and Application-Based Systems</meta:Subject>
						<meta:Subject Type="Secondary">Data Structures, Cryptology and Information Theory</meta:Subject>
						<meta:Subject Type="Secondary">Computer Communication Networks</meta:Subject>
						<meta:Subject Type="Secondary">Multimedia Information Systems</meta:Subject>
					</meta:SubjectGroup>
				</meta:Info>
			</Publisher>
			<Images/>
		</result>
		<result>
			<Publisher xml:lang="en">
				<PublisherInfo>
					<PublisherName>Springer-Verlag</PublisherName>
					<PublisherLocation>London</PublisherLocation>
				</PublisherInfo>
				<Journal OutputMedium="All">
					<JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="ContentOnly">
						<JournalID>13174</JournalID>
						<JournalPrintISSN>1867-4828</JournalPrintISSN>
						<JournalElectronicISSN>1869-0238</JournalElectronicISSN>
						<JournalSPIN>32753929</JournalSPIN>
						<JournalTitle>Journal of Internet Services and Applications</JournalTitle>
						<JournalAbbreviatedTitle>J Internet Serv Appl</JournalAbbreviatedTitle>
						<JournalSubjectGroup>
							<JournalSubject Type="Primary">Computer Science</JournalSubject>
							<JournalSubject Type="Secondary">Processor Architectures</JournalSubject>
							<JournalSubject Type="Secondary">Computer Applications</JournalSubject>
							<JournalSubject Type="Secondary">Business Information Systems</JournalSubject>
							<JournalSubject Type="Secondary">Information Systems and Communication Service</JournalSubject>
							<JournalSubject Type="Secondary">Computer Communication Networks</JournalSubject>
							<JournalSubject Type="Secondary">Computer Systems Organization and Communication Networks</JournalSubject>
						</JournalSubjectGroup>
					</JournalInfo>
					<Volume OutputMedium="All">
						<VolumeInfo TocLevels="0" VolumeType="Regular">
							<VolumeIDStart>1</VolumeIDStart>
							<VolumeIDEnd>1</VolumeIDEnd>
							<VolumeIssueCount>3</VolumeIssueCount>
						</VolumeInfo>
						<Issue IssueType="Regular" OutputMedium="All">
							<IssueInfo IssueType="Regular" TocLevels="0">
								<IssueIDStart>1</IssueIDStart>
								<IssueIDEnd>1</IssueIDEnd>
								<IssueArticleCount>8</IssueArticleCount>
								<IssueHistory>
									<OnlineDate>
										<Year>2010</Year>
										<Month>5</Month>
										<Day>6</Day>
									</OnlineDate>
									<PrintDate>
										<Year>2010</Year>
										<Month>5</Month>
										<Day>5</Day>
									</PrintDate>
									<CoverDate>
										<Year>2010</Year>
										<Month>5</Month>
									</CoverDate>
									<PricelistYear>2010</PricelistYear>
								</IssueHistory>
								<IssueCopyright>
									<CopyrightHolderName>The Brazilian Computer Society</CopyrightHolderName>
									<CopyrightYear>2010</CopyrightYear>
								</IssueCopyright>
							</IssueInfo>
							<Article ID="s13174-010-0001-z" OutputMedium="All">
								<ArticleInfo ArticleCitation="ArticleFirstPage" ArticleType="OriginalPaper" ContainsESM="No" Language="En" NumberingStyle="ContentOnly" TocLevels="0">
									<ArticleID>1</ArticleID>
									<ArticleDOI>10.1007/s13174-010-0001-z</ArticleDOI>
									<ArticleSequenceNumber>5</ArticleSequenceNumber>
									<ArticleTitle Language="En">The unique strengths and storage access characteristics of discard-based search</ArticleTitle>
									<ArticleCategory>Original Paper</ArticleCategory>
									<ArticleFirstPage>31</ArticleFirstPage>
									<ArticleLastPage>44</ArticleLastPage>
									<ArticleHistory>
										<RegistrationDate>
											<Year>2010</Year>
											<Month>2</Month>
											<Day>2</Day>
										</RegistrationDate>
										<Received>
											<Year>2010</Year>
											<Month>1</Month>
											<Day>26</Day>
										</Received>
										<Accepted>
											<Year>2010</Year>
											<Month>2</Month>
											<Day>2</Day>
										</Accepted>
										<OnlineDate>
											<Year>2010</Year>
											<Month>2</Month>
											<Day>24</Day>
										</OnlineDate>
									</ArticleHistory>
									<ArticleCopyright>
										<CopyrightHolderName>The Author(s)</CopyrightHolderName>
										<CopyrightYear>2010</CopyrightYear>
									</ArticleCopyright>
									<ArticleGrants Type="OpenChoice">
										<MetadataGrant Grant="OpenAccess"/>
										<AbstractGrant Grant="OpenAccess"/>
										<BodyPDFGrant Grant="OpenAccess"/>
										<BodyHTMLGrant Grant="OpenAccess"/>
										<BibliographyGrant Grant="OpenAccess"/>
										<ESMGrant Grant="OpenAccess"/>
									</ArticleGrants>
								</ArticleInfo>
								<ArticleHeader>
									<AuthorGroup>
										<Author AffiliationIDS="Aff1" CorrespondingAffiliationID="Aff1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Mahadev</GivenName>
												<FamilyName>Satyanarayanan</FamilyName>
											</AuthorName>
											<Contact>
												<Email>satya@cs.cmu.edu</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="Aff2">
											<AuthorName DisplayOrder="Western">
												<GivenName>Rahul</GivenName>
												<FamilyName>Sukthankar</FamilyName>
											</AuthorName>
										</Author>
										<Author AffiliationIDS="Aff2">
											<AuthorName DisplayOrder="Western">
												<GivenName>Lily</GivenName>
												<FamilyName>Mummert</FamilyName>
											</AuthorName>
										</Author>
										<Author AffiliationIDS="Aff1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Adam</GivenName>
												<FamilyName>Goode</FamilyName>
											</AuthorName>
										</Author>
										<Author AffiliationIDS="Aff1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Jan</GivenName>
												<FamilyName>Harkes</FamilyName>
											</AuthorName>
										</Author>
										<Author AffiliationIDS="Aff3">
											<AuthorName DisplayOrder="Western">
												<GivenName>Steve</GivenName>
												<FamilyName>Schlosser</FamilyName>
											</AuthorName>
										</Author>
										<Affiliation ID="Aff1">
											<OrgName>Carnegie Mellon Univ.</OrgName>
											<OrgAddress>
												<City>Pittsburgh</City>
												<State>PA</State>
												<Country>USA</Country>
											</OrgAddress>
										</Affiliation>
										<Affiliation ID="Aff2">
											<OrgName>Intel Labs Pittsburgh</OrgName>
											<OrgAddress>
												<City>Pittsburgh</City>
												<State>PA</State>
												<Country>USA</Country>
											</OrgAddress>
										</Affiliation>
										<Affiliation ID="Aff3">
											<OrgName>Avere Systems</OrgName>
											<OrgAddress>
												<City>Pittsburgh</City>
												<State>PA</State>
												<Country>USA</Country>
											</OrgAddress>
										</Affiliation>
									</AuthorGroup>
									<Abstract ID="Abs1" Language="En" OutputMedium="All">
										<Heading>Abstract</Heading>
										<Para>
											<Emphasis Type="Italic">Discard-based search</Emphasis> is a new approach to searching the content of complex, unlabeled, nonindexed data such as digital photographs, medical images, and real-time surveillance data. The essence of this approach is <Emphasis Type="Italic">query-specific content-based computation, pipelined with human cognition</Emphasis>. In this approach, query-specific parallel computation shrinks a search task down to human scale, thus allowing the expertise, judgment, and intuition of an expert to be brought to bear on the specificity and selectivity of the search. In this paper, we report on the lessons learned in the <Emphasis Type="Italic">Diamond project</Emphasis> from applying discard-based search to a variety of applications in the health sciences. From the viewpoint of a user, discard-based search offers unique strengths. From the viewpoint of server hardware and software, it offers unique opportunities for optimization that contradict long-established tenets of storage design. Together, these distinctive end-to-end attributes herald a new genre of Internet applications.</Para>
									</Abstract>
									<KeywordGroup Language="En">
										<Heading>Keywords</Heading>
										<Keyword>Data-intensive computing</Keyword>
										<Keyword>Non-text search technology</Keyword>
										<Keyword>Medical image processing</Keyword>
										<Keyword>Interactive search</Keyword>
										<Keyword>Computer vision</Keyword>
										<Keyword>Pattern recognition</Keyword>
										<Keyword>Distributed systems</Keyword>
										<Keyword>ImageJ</Keyword>
										<Keyword>MATLAB</Keyword>
										<Keyword>Parallel processing</Keyword>
										<Keyword>Human-in-the-loop</Keyword>
										<Keyword>Diamond</Keyword>
										<Keyword>OpenDiamond</Keyword>
										<Keyword>Storage systems</Keyword>
										<Keyword>I/O workloads</Keyword>
										<Keyword>RAID</Keyword>
									</KeywordGroup>
								</ArticleHeader>
								<BodyRef TargetType="OnlinePDF" FileRef="BodyRef/PDF/13174_2010_Article_1.pdf"/>
								<BodyRef TargetType="TEX" FileRef="BodyRef/PDF/13174_2010_1_TEX.zip"/>
								<ArticleBackmatter>
									<Bibliography ID="Bib1">
										<Heading>References</Heading>
										<Citation ID="CR1">
											<CitationNumber>1.</CitationNumber>
											<BibUnstructured>
Acharya A, Uysal M, Saltz J (1998) Active disks: programming model, algorithms and evaluation. In: Proceedings of the international conference on architectural support for programming languages and operating systems
											</BibUnstructured>
										</Citation>
										<Citation ID="CR2">
											<CitationNumber>2.</CitationNumber>
											<BibUnstructured>
von Ahn L, Dabbish L (2004) Labeling images with a computer game. In: Proceedings of the SIGCHI conference on human factors in computing systems
											</BibUnstructured>
										</Citation>
										<Citation ID="CR3">
											<CitationNumber>3.</CitationNumber>
											<BibUnstructured>
Amiri K, Petrou D, Ganger G, Gibson G (2000) Dynamic function placement for data-intensive cluster computing. In: Proceedings of the USENIX technical conference
											</BibUnstructured>
										</Citation>
										<Citation ID="CR4">
											<CitationNumber>4.</CitationNumber>
											<BibUnstructured>
Arpaci-Dusseau R, Anderson E, Treuhaft N, Culler D, Hellerstein J, Patterson D, Yelick K (1999) Cluster I/O with river: making the fast case common. In: Proceedings of input/output for parallel and distributed systems
											</BibUnstructured>
										</Citation>
										<Citation ID="CR5">
											<CitationNumber>5.</CitationNumber>
											<BibUnstructured>
Avnur R, Hellerstein J (2000) Eddies: continuously adaptive query processing. In: Proceedings of SIGMOD
											</BibUnstructured>
										</Citation>
										<Citation ID="CR6">
											<CitationNumber>6.</CitationNumber>
											<BibUnstructured>
Dean J, Ghemawat S (2004) MapReduce: simplified data processing on large clusters. In: Proceedings of the USENIX symposium on operating systems design and implementation, San Francisco, CA
											</BibUnstructured>
										</Citation>
										<Citation ID="CR7">
											<CitationNumber>7.</CitationNumber>
											<BibUnstructured>
Dean J, Ghemawat S (2008) MapReduce: simplified data processing on large clusters. Commun ACM 51(1)
											</BibUnstructured>
										</Citation>
										<Citation ID="CR8">
											<CitationNumber>8.</CitationNumber>
											<BibBook>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Duda</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Hart</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Stork</FamilyName>
												</BibAuthorName>
												<Year>2001</Year>
												<BookTitle>Pattern classification</BookTitle>
												<PublisherName>Wiley</PublisherName>
												<PublisherLocation>New York</PublisherLocation>
												<Occurrence Type="ZLBID">
													<Handle>0968.68140</Handle>
												</Occurrence>
											</BibBook>
											<BibUnstructured>
Duda R, Hart P, Stork D (2001) Pattern classification. Wiley, New York
											</BibUnstructured>
										</Citation>
										<Citation ID="CR9">
											<CitationNumber>9.</CitationNumber>
											<BibUnstructured>
Flickner M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B, Gorkani M, Hafner J, Lee D, Petkovic D, Steele D, Yanker P (1995) Query by image and video content: the QBIC system. IEEE Comput 28(9)
											</BibUnstructured>
										</Citation>
										<Citation ID="CR10">
											<CitationNumber>10.</CitationNumber>
											<BibUnstructured>
Gibbons P, Mummert L, Sukthankar R, Satyanarayananan M (2007) Just-in-time indexing for interactive data exploration. Tech Rep CMU-CS-07-120, Computer Science Department, Carnegie Mellon University, Pittsburgh, PA
											</BibUnstructured>
										</Citation>
										<Citation ID="CR11">
											<CitationNumber>11.</CitationNumber>
											<BibUnstructured>
Goode A, Sukthankar R, Mummert L, Chen M, Saltzman J, Ross D, Szymanski S, Tarachandani A, Satyanarayanan M (2008) Distributed online anomaly detection in high-content screening. In: Proceedings of the 2008 5th IEEE international symposium on biomedical imaging, Paris, France
											</BibUnstructured>
										</Citation>
										<Citation ID="CR12">
											<CitationNumber>12.</CitationNumber>
											<BibUnstructured>
Goode A, Chen M, Tarachandani A, Mummert L, Sukthankar R, Helfrich C, Stefanni A, Fix L, Saltzmann J, Satyanarayanan M (2007) Interactive search of adipocytes in large collections of digital cellular images. In: Proceedings of the 2007 IEEE international conference on multimedia and expo (ICME07), Beijing, China
											</BibUnstructured>
										</Citation>
										<Citation ID="CR13">
											<CitationNumber>13.</CitationNumber>
											<BibUnstructured>
Goode A, Satyanarayanan M (2008) A vendor-neutral library and viewer for whole-slide images. Tech Rep CMU-CS-08-136, Computer Science Department, Carnegie Mellon University, Pittsburgh, PA
											</BibUnstructured>
										</Citation>
										<Citation ID="CR14">
											<CitationNumber>14.</CitationNumber>
											<BibUnstructured>
Hunt G, Scott M (1999) The Coign automatic distributed partitioning system. In: Proceedings of OSDI
											</BibUnstructured>
										</Citation>
										<Citation ID="CR15">
											<CitationNumber>15.</CitationNumber>
											<BibUnstructured>
Huston L, Sukthankar R, Wickremesinghe R, Satyanarayanan M, Ganger GR, Riedel E, Ailamaki A (2004) Diamond: a storage architecture for early discard in interactive search. In: Proceedings of the 3rd USENIX conference on file and storage technologies, San Francisco, CA
											</BibUnstructured>
										</Citation>
										<Citation ID="CR16">
											<CitationNumber>16.</CitationNumber>
											<BibUnstructured>
Keeton K, Patterson D, Hellerstein J (1998) A case for intelligent disks (IDISKs). SIGMOD Rec 27(3)
											</BibUnstructured>
										</Citation>
										<Citation ID="CR17">
											<CitationNumber>17.</CitationNumber>
											<BibUnstructured>
Kim E, Haseyama M, Kitajima H (2002) Fast and robust ellipse extraction from complicated images. In: Proceedings of IEEE information technology and applications
											</BibUnstructured>
										</Citation>
										<Citation ID="CR18">
											<CitationNumber>18.</CitationNumber>
											<BibUnstructured>
Lowe D (2004) Distinctive image features from scale-invariant keypoints. Int J Comput Vis
											</BibUnstructured>
										</Citation>
										<Citation ID="CR19">
											<CitationNumber>19.</CitationNumber>
											<BibUnstructured>
Merriam-Webster (2007) Merriam–Webster online search. <ExternalRef>
													<RefSource>http://mw1.merriam-webster.com/dictionary/tenet</RefSource>
													<RefTarget Address="http://mw1.merriam-webster.com/dictionary/tenet" TargetType="URL"/>
												</ExternalRef>
											</BibUnstructured>
										</Citation>
										<Citation ID="CR20">
											<CitationNumber>20.</CitationNumber>
											<BibUnstructured>
Minka T, Picard R (1997) Interactive learning using a society of models. Pattern Recognit 30
											</BibUnstructured>
										</Citation>
										<Citation ID="CR21">
											<CitationNumber>21.</CitationNumber>
											<BibUnstructured>
Mummert L, Schlosser S, Mesnier M, Satyanarayanan M (2007) Rethinking storage for discard-based search. Tech Rep CMU-CS-07-176, Computer Science Department, Carnegie Mellon University, Pittsburgh, PA
											</BibUnstructured>
										</Citation>
										<Citation ID="CR22">
											<CitationNumber>22.</CitationNumber>
											<BibUnstructured>
Patterson RH, Gibson GA, Ginting E, Stodolsky D, Zelenka J (1995) Informed prefetching and caching. In: Proceedings of the fifteenth ACM symposium on operating systems principles, Copper Mountain, CO
											</BibUnstructured>
										</Citation>
										<Citation ID="CR23">
											<CitationNumber>23.</CitationNumber>
											<BibUnstructured>
Riedel E, Gibson G, Faloutsos C (1998) Active storage for large-scale data mining and multimedia. In: Proceedings of the international conference on very large databases
											</BibUnstructured>
										</Citation>
										<Citation ID="CR24">
											<CitationNumber>24.</CitationNumber>
											<BibUnstructured>
Stonebreaker M (1981) Operating system support for database management. Commun ACM 24(7)
											</BibUnstructured>
										</Citation>
										<Citation ID="CR25">
											<CitationNumber>25.</CitationNumber>
											<BibUnstructured>
Wikipedia (2007) Conveyor belt sushi. Wikipedia, The Free Encyclopedia. [Online: accessed 3-September-2007]
											</BibUnstructured>
										</Citation>
										<Citation ID="CR26">
											<CitationNumber>26.</CitationNumber>
											<BibUnstructured>
Yang L, Jin R, Mummert L, Sukthankar R, Goode A, Zheng B, Hoi SC, Satyanarayanan M (2010) A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval. IEEE Trans Pattern Anal Mach Intell 32(1)
											</BibUnstructured>
										</Citation>
										<Citation ID="CR27">
											<CitationNumber>27.</CitationNumber>
											<BibUnstructured>
Yao A, Yao F (1985) A general approach to D-dimensional geometric queries. In: Proceedings of the annual ACM symposium on theory of computing
											</BibUnstructured>
										</Citation>
									</Bibliography>
								</ArticleBackmatter>
							</Article>
						</Issue>
					</Volume>
				</Journal>
				<meta:Info xmlns:meta="http://www.springer.com/app/meta">
					<meta:DateLoaded>2010-05-11T03:28:05.689455+02:00</meta:DateLoaded>
					<meta:Authors>
						<meta:Author>Satyanarayanan, Mahadev</meta:Author>
						<meta:Author>Sukthankar, Rahul</meta:Author>
						<meta:Author>Mummert, Lily</meta:Author>
						<meta:Author>Goode, Adam</meta:Author>
						<meta:Author>Harkes, Jan</meta:Author>
						<meta:Author>Schlosser, Steve</meta:Author>
					</meta:Authors>
					<meta:Institutions>
						<meta:Institution geo="-79.9430537,40.4431288,0">
							<meta:OrgName>Carnegie Mellon Univ.</meta:OrgName>
							<meta:GeoOrg>-79.9430537,40.4431288,0#Carnegie Mellon Univ.</meta:GeoOrg>
							<meta:Country>United States</meta:Country>
						</meta:Institution>
						<meta:Institution geo="-79.9958864,40.4406248,0">
							<meta:OrgName>Intel Labs Pittsburgh</meta:OrgName>
							<meta:GeoOrg>-79.9958864,40.4406248,0#Intel Labs Pittsburgh</meta:GeoOrg>
							<meta:Country>United States</meta:Country>
						</meta:Institution>
						<meta:Institution geo="-79.9958864,40.4406248,0">
							<meta:OrgName>Avere Systems</meta:OrgName>
							<meta:GeoOrg>-79.9958864,40.4406248,0#Avere Systems</meta:GeoOrg>
							<meta:Country>United States</meta:Country>
						</meta:Institution>
					</meta:Institutions>
					<meta:Date>2010-05-05</meta:Date>
					<meta:Type>Article</meta:Type>
					<meta:DOI>10.1007/s13174-010-0001-z</meta:DOI>
					<meta:Title>The unique strengths and storage access characteristics of discard-based search</meta:Title>
					<meta:ISXN>1869-0238</meta:ISXN>
					<meta:PubName>Springer</meta:PubName>
					<meta:Journal>Journal of Internet Services and Applications</meta:Journal>
					<meta:Publication>Journal of Internet Services and Applications</meta:Publication>
					<meta:PublicationType>Journal</meta:PublicationType>
					<meta:SubjectGroup>
						<meta:Subject Type="Primary">Computer Science</meta:Subject>
						<meta:Subject Type="Secondary">Processor Architectures</meta:Subject>
						<meta:Subject Type="Secondary">Computer Applications</meta:Subject>
						<meta:Subject Type="Secondary">Business Information Systems</meta:Subject>
						<meta:Subject Type="Secondary">Information Systems and Communication Service</meta:Subject>
						<meta:Subject Type="Secondary">Computer Communication Networks</meta:Subject>
						<meta:Subject Type="Secondary">Computer Systems Organization and Communication Networks</meta:Subject>
					</meta:SubjectGroup>
				</meta:Info>
			</Publisher>
			<Images/>
		</result>
		<result>
			<Publisher xml:lang="en">
				<PublisherInfo>
					<PublisherName>BMC</PublisherName>
					<PublisherLocation/>
				</PublisherInfo>
				<Journal>
					<JournalInfo JournalProductType="ArchiveJournal" NumberingStyle="Unnumbered">
						<JournalID>GBJ</JournalID>
						<JournalPrintISSN>1465-6906</JournalPrintISSN>
						<JournalElectronicISSN>1465-6906</JournalElectronicISSN>
						<JournalTitle>Genome Biology</JournalTitle>
						<JournalAbbreviatedTitle>Genome Biol</JournalAbbreviatedTitle>
						<JournalSubjectGroup>
							<JournalSubject Type="Primary">Life Sciences</JournalSubject>
							<JournalSubject Priority="1" Type="Secondary">Life Sciences, general</JournalSubject>
							<JournalSubject Priority="2" Type="Secondary">Biomedicine general</JournalSubject>
							<JournalSubject Priority="3" Type="Secondary">Human Genetics</JournalSubject>
							<JournalSubject Priority="4" Type="Secondary">Animal Genetics and Genomics</JournalSubject>
							<JournalSubject Priority="5" Type="Secondary">Microbial Genetics and Genomics</JournalSubject>
							<JournalSubject Priority="6" Type="Secondary">Plant Genetics &amp; Genomics</JournalSubject>
							<JournalSubject Priority="7" Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>
							<JournalSubject Priority="8" Type="Secondary">Mathematical Biology in General</JournalSubject>
						</JournalSubjectGroup>
					</JournalInfo>
					<Volume>
						<VolumeInfo TocLevels="0" VolumeType="Regular">
							<VolumeIDStart>10</VolumeIDStart>
							<VolumeIDEnd>10</VolumeIDEnd>
							<VolumeIssueCount/>
						</VolumeInfo>
						<Issue IssueType="Regular">
							<IssueInfo TocLevels="0">
								<IssueIDStart>9</IssueIDStart>
								<IssueIDEnd>9</IssueIDEnd>
								<IssueArticleCount/>
								<IssueHistory>
									<PrintDate>
										<Year>2009</Year>
										<Month>9</Month>
										<Day>17</Day>
									</PrintDate>
								</IssueHistory>
								<IssueCopyright>
									<CopyrightHolderName>Schneeberger et al.; licensee BioMed Central Ltd.</CopyrightHolderName>
									<CopyrightYear>2009</CopyrightYear>
								</IssueCopyright>
							</IssueInfo>
							<Article ID="Art1">
								<ArticleInfo ArticleType="Abstract" ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">
									<ArticleID>gb-2009-10-9-r98</ArticleID>
									<ArticleDOI>10.1186/gb-2009-10-9-r98</ArticleDOI>
									<ArticleSequenceNumber/>
									<ArticleTitle Language="En">Simultaneous alignment of short reads against multiple genomes</ArticleTitle>
									<ArticleSubTitle Language="En"/>
									<ArticleCategory>Software</ArticleCategory>
									<ArticleFirstPage>R98</ArticleFirstPage>
									<ArticleLastPage>R98</ArticleLastPage>
									<ArticleHistory>
										<Received>
											<Year>2009</Year>
											<Month>7</Month>
											<Day>20</Day>
										</Received>
										<Revised>
											<Year>2009</Year>
											<Month>9</Month>
											<Day>12</Day>
										</Revised>
										<Accepted>
											<Year>2009</Year>
											<Month>9</Month>
											<Day>17</Day>
										</Accepted>
									</ArticleHistory>
									<ArticleEditorialResponsibility/>
									<ArticleCopyright>
										<CopyrightHolderName>Schneeberger et al.; licensee BioMed Central Ltd.</CopyrightHolderName>
										<CopyrightYear>2009</CopyrightYear>
									</ArticleCopyright>
									<ArticleGrants Type="OpenChoice">
										<MetadataGrant Grant="OpenAccess"/>
										<AbstractGrant Grant="OpenAccess"/>
										<BodyPDFGrant Grant="Restricted"/>
										<BodyHTMLGrant Grant="Restricted"/>
										<BibliographyGrant Grant="Restricted"/>
										<ESMGrant Grant="Restricted"/>
									</ArticleGrants>
									<ArticleContext>
										<JournalID/>
										<VolumeIDStart>10</VolumeIDStart>
										<VolumeIDEnd>10</VolumeIDEnd>
										<IssueIDStart>9</IssueIDStart>
										<IssueIDEnd>9</IssueIDEnd>
									</ArticleContext>
								</ArticleInfo>
								<ArticleHeader>
									<AuthorGroup>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Korbinian</GivenName>
												<FamilyName>Schneeberger</FamilyName>
											</AuthorName>
											<Contact>
												<Email>korbinian.schneeberger@tuebingen.mpg.de</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Jörg</GivenName>
												<FamilyName>Hagmann</FamilyName>
											</AuthorName>
											<Contact>
												<Email>joerg.hagmann@tuebingen.mpg.de</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Stephan</GivenName>
												<FamilyName>Ossowski</FamilyName>
											</AuthorName>
											<Contact>
												<Email>stephan.ossowski@tuebingen.mpg.de</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Norman</GivenName>
												<FamilyName>Warthmann</FamilyName>
											</AuthorName>
											<Contact>
												<Email>norman@warthmann.com</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I2">
											<AuthorName DisplayOrder="Western">
												<GivenName>Sandra</GivenName>
												<FamilyName>Gesing</FamilyName>
											</AuthorName>
											<Contact>
												<Email>sandra.gesing@uni-tuebingen.de</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I2">
											<AuthorName DisplayOrder="Western">
												<GivenName>Oliver</GivenName>
												<FamilyName>Kohlbacher</FamilyName>
											</AuthorName>
											<Contact>
												<Email>oliver.kohlbacher@uni-tuebingen.de</Email>
											</Contact>
										</Author>
										<Author AffiliationIDS="I1">
											<AuthorName DisplayOrder="Western">
												<GivenName>Detlef</GivenName>
												<FamilyName>Weigel</FamilyName>
											</AuthorName>
											<Contact>
												<Email>weigel@weigelworld.org</Email>
											</Contact>
										</Author>
										<Affiliation ID="I1">
											<OrgName>Department of Molecular Biology, Max Planck Institute for Developmental Biology, Spemannstrasse 37-39, D-72076 Tübingen, Germany</OrgName>
											<OrgAddress>
												<Postcode/>
												<City/>
												<State/>
											</OrgAddress>
										</Affiliation>
										<Affiliation ID="I2">
											<OrgName>Center for Bioinformatics Tübingen (ZBIT), Eberhard Karls University Tübingen, Sand 14, 72076 Tübingen, Germany</OrgName>
											<OrgAddress>
												<Postcode/>
												<City/>
												<State/>
											</OrgAddress>
										</Affiliation>
									</AuthorGroup>
									<Abstract ID="Abs1" Language="En">
										<Heading>Abstract</Heading>
										<Para>Genome resequencing with short reads generally relies on alignments against a single reference. GenomeMapper supports simultaneous mapping of short reads against multiple genomes by integrating related genomes ( <Emphasis Type="Italic">e.g</Emphasis> ., individuals of the same species) into a single graph structure. It constitutes the first approach for handling multiple references and introduces representations for alignments against complex structures. Demonstrated benefits include access to polymorphisms that cannot be identified by alignments against the reference alone. Download GenomeMapper at <ExternalRef>
												<RefSource>http://1001genomes.org</RefSource>
												<RefTarget Address="http://1001genomes.org" TargetType="URl"/>
											</ExternalRef> .</Para>
									</Abstract>
									<KeywordGroup Language="En">
										<Heading>Keywords</Heading>
										<Keyword>reads</Keyword>
										<Keyword>short</Keyword>
										<Keyword>alignment</Keyword>
										<Keyword>Simultaneous</Keyword>
										<Keyword>against</Keyword>
										<Keyword>genomes</Keyword>
										<Keyword>multiple</Keyword>
									</KeywordGroup>
								</ArticleHeader>
								<Body>
									<Section1 ID="Sec_13704">
										<Heading>Rationale</Heading>
										<Para>Within the last few years, a variety of second- (or next-) generation sequencing technologies have been developed to enable analyses of small to medium-sized genomes within weeks or even days. The methods are now overcoming the disadvantages of short read length (currently the longest reads are obtained with the Titanium system produced by Roche/454 Life Sciences (Brandford, CT, USA) with Q20 at 400 bp) and a lower quality of individual reads with a dramatic increase in the total amount of data generated.</Para>
										<Para>The initial resequencing of <Emphasis Type="Italic">Caenorhabditis elegans</Emphasis> and <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> (Arabidopsis) strains with Illumina reads <CitationRef CitationID="B1">1</CitationRef>
											<CitationRef CitationID="B2">2</CitationRef> was recently complemented by genome sequences of several human individuals, generated with data derived from technologies from Illumina (San Diego, CA, USA), Applied Biosystems (Foster City, CA, USA), and Helicos (Cambridge, MA, USA) <CitationRef CitationID="B3">3</CitationRef>
											<CitationRef CitationID="B4">4</CitationRef>
											<CitationRef CitationID="B5">5</CitationRef>
											<CitationRef CitationID="B6">6</CitationRef>
											<CitationRef CitationID="B7">7</CitationRef>
											<CitationRef CitationID="B8">8</CitationRef>
											<CitationRef CitationID="B9">9</CitationRef>
											<CitationRef CitationID="B10">10</CitationRef> . Even partial <Emphasis Type="Italic">de novo</Emphasis> assemblies of targeted regions within large genomes have been attempted <CitationRef CitationID="B2">2</CitationRef> . However, short-read analysis of complex genomes is greatly aided by using a sequence backbone against which the short reads are aligned to find their genomic origin.</Para>
										<Para>Different approaches for fast mapping of short reads have been suggested, including methods for indexing substrings of either the short reads or the reference sequence with the use of <Emphasis Type="Italic">k</Emphasis> -mers or spaced seeds (academic tools such as Bowtie, BWA, CloudBurst, MAQ, MOM, MosaikAligner, mrFAST, mrsFAST, Pash, PASS, PatMaN, RazorS, RMAP, SeqMap, SHRiMP, SliderII, SOAP, SOAP2, ssaha2 <CitationRef CitationID="B2">2</CitationRef>
											<CitationRef CitationID="B11">11</CitationRef>
											<CitationRef CitationID="B12">12</CitationRef>
											<CitationRef CitationID="B13">13</CitationRef>
											<CitationRef CitationID="B14">14</CitationRef>
											<CitationRef CitationID="B15">15</CitationRef>
											<CitationRef CitationID="B16">16</CitationRef>
											<CitationRef CitationID="B17">17</CitationRef>
											<CitationRef CitationID="B18">18</CitationRef>
											<CitationRef CitationID="B19">19</CitationRef>
											<CitationRef CitationID="B20">20</CitationRef>
											<CitationRef CitationID="B21">21</CitationRef>
											<CitationRef CitationID="B22">22</CitationRef>
											<CitationRef CitationID="B23">23</CitationRef>
											<CitationRef CitationID="B24">24</CitationRef>
											<CitationRef CitationID="B25">25</CitationRef>
											<CitationRef CitationID="B26">26</CitationRef>
											<CitationRef CitationID="B27">27</CitationRef>
											<CitationRef CitationID="B28">28</CitationRef> , and commercial tools such as ZOOM <CitationRef CitationID="B29">29</CitationRef> ). It has been reported that the current high demand for rapid alignments, to accommodate the flood of data generated by efforts such as the 1000 Genomes Project, can be met with new indexing strategies <CitationRef CitationID="B16">16</CitationRef> . However, this is normally at the cost of not allowing complex alignments, including gaps.</Para>
										<Para>For natural inbred strains of Arabidopsis, the high level of individual differences constitutes a substantial challenge. It has been estimated that several percent of the reference genome are either missing or very divergent in other strains of this species, which features homozygous genomes that are 25 times smaller than a haploid human genome <CitationRef CitationID="B30">30</CitationRef>
											<CitationRef CitationID="B31">31</CitationRef> . This results in regions inaccessible to simple short-read alignments, in particular for alignment algorithms that do not accommodate many mismatches and gaps. New approaches supporting accurate alignments even in highly divergent regions are therefore sorely needed.</Para>
										<Para>We note that the information derived from resequenced individual genomes is in itself useful for subsequent resequencing efforts, especially when the latter are at lower sequence coverage than the earlier efforts. Incorporating known polymorphisms increases the genome space against which the sample reads are aligned, which should greatly improve the mapping results. For example, an alignment suggesting a string of deleted bases in the focal genome becomes much more reliable if this deletion is known to exist in the population. The incorporation of such missing or inserted bases in the target/reference sequence not only would decrease the complexity of the alignments, but also would reduce sequencing costs, as more reads can be placed on the genome.</Para>
										<Para>Apart from these practical reasons, aligning against only a single reference biases the analysis toward a comparison within the sequence space highly conserved with the reference. Taking into account all known genome variants would reduce this bias. Aligning reads against multiple genomes separately increases computation time and storage space and introduces new problems of merging and interpreting redundant results.</Para>
										<Para>Here we present a new short-read alignment algorithm, GenomeMapper, which performs simultaneous alignments of short reads against multiple genomes. GenomeMapper assures high alignment quality, while competing in runtime with other short-read alignment tools. This is achieved by representing multiple genomes with a novel hash-based graph data structure against which the reads are aligned. To our knowledge, this constitutes the first approach for aligning a sequence against a graph of sequences rather than aligning two linear sequences. We also propose the first standards to tackle the problems arising from multiple references. GenomeMapper is currently the tool of choice for the Arabidopsis 1001 Genomes Project <CitationRef CitationID="B32">32</CitationRef>
											<CitationRef CitationID="B33">33</CitationRef> , and the default alignment option of the short-read analysis pipeline SHORE <CitationRef CitationID="B2">2</CitationRef> . GenomeMapper has been used to analyze sequence reads derived from bacterial, plant, invertebrate, and mammalian genomes. To demonstrate the impact of adopting multiple genomes as the short-read alignment target, we describe the construction of a multiple genome sequence graph based on published polymorphisms of Arabidopsis <CitationRef CitationID="B2">2</CitationRef> . We present the alignment and consensus sequence analysis of the Est-1 strain by using this graph and compare the results with the conventional approach of aligning the same set of reads against a single reference. We discuss the implications of our work for the analysis of more-complex reference sequences.</Para>
									</Section1>
									<Section1 ID="Sec_48502">
										<Heading>GenomeMapper's indexing and alignment strategy</Heading>
										<Section2 ID="Sec_79127">
											<Heading>Multiple genomes in one index</Heading>
											<Para>One way to decrease runtime for the generation of sequence alignments is to build index structures of either the reads or the reference sequence. To allow simultaneous alignments against multiple genome sequences, all target sequences have to be combined into one data structure. GenomeMapper achieves this goal by building a joint index of all genomes that are alignment targets. This index will be persistently stored, and, once compiled, the index does not need to be rebuilt for future alignment tasks.</Para>
											<Para>The index is a simple hash-based mapping of <Emphasis Type="Italic">k</Emphasis> -mers (sequence signatures of 5 to 13 bp) to their locations within the target sequences. Each <Emphasis Type="Italic">k</Emphasis> -mer present in target sequences is unambiguously converted into a single integer, applying a two-bit representation of the four DNA nucleotides. Each hash key points to one hash value consisting of a list of all genome locations of the <Emphasis Type="Italic">k</Emphasis> -mer. Although this rather simplistic hash-indexing approach has some disadvantages compared with more recently developed strategies ( <Emphasis Type="Italic">e.g</Emphasis> ., Burrows-Wheeler indexing <CitationRef CitationID="B16">16</CitationRef> ), the latter are usually geared toward ungapped alignments and are not easily extendable to nonlinear structures imposed by multiple genomes. Further, spaced-seed approaches, implemented in tools such as SHRiMP or ZOOM, can be more sensitive <CitationRef CitationID="B34">34</CitationRef> . However, when these approaches are applied to real data, they do not result in a substantial increase in the number of alignments compared with an approach with contiguous seeds followed by a complex alignment, because contiguous seeds are usually chosen short enough ( <Emphasis Type="Italic">i.e</Emphasis> ., 9 to 12 bp) for anchoring and subsequent aligning of reads (see later for comparison with other mapping tools).</Para>
											<Para>Mapping indices tend to require a large amount of random access memory (RAM). Current computer servers usually allow multiple processors to share physical RAM. To avoid the unnecessary overhead of loading the same index multiple times, GenomeMapper makes use of memory-mapped files, allowing computer processes to share the same index structure within the memory. This reduces the overall memory footprint when running several instances of GenomeMapper in parallel.</Para>
										</Section2>
										<Section2 ID="Sec_52236">
											<Heading>Index graph creation</Heading>
											<Para>The input for GenomeMapper's index-creation step consists of the sequence of one of the genomes and a list of differences in the other genomes compared with the first one ( <Emphasis Type="Italic">i.e</Emphasis> ., one FASTA file and a list of single-nucleotide polymorphisms (SNPs) and indels of every additional genome). Each position not explicitly annotated as different is assumed to be identical in all of the genomes, and will therefore be stored only once. This is important to avoid redundant alignments to several genomes. Divergent sequences are stored separately for each of the genomes. Identical regions, which are represented once, must be connected with polymorphic regions, which are represented by branches in the index. Hence, the reference loses its linear/sequential characteristic, but rather forms a sequence graph. Note that none of the genomes represents "the reference" (Figure <InternalRef RefID="F1">1</InternalRef> ).</Para>
											<Figure Category="Standard" Float="No" ID="F1">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Efficient alignments against multiple genomes</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="gb-2009-10-9-r98-1" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>Efficient alignments against multiple genomes. <Emphasis Type="Bold">(a)</Emphasis> Only reads that are sufficiently similar can be aligned against a single reference. <Emphasis Type="Bold">(b)</Emphasis> Separate alignment against multiple genomes allows access to divergent regions, but results in redundant alignments of reads that match all targets (blue). <Emphasis Type="Bold">(c)</Emphasis> Alignments against a graph index representing multiple genomes provide access to divergent regions without redundant alignments.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>To store this information efficiently, each of the genomes is partitioned into non-overlapping sequence blocks of up to 256 bp, which represent the genomic sequence of all genomes. The connections of blocks to their neighbors allow continuous reconstruction of each genome. Invariant regions will be represented by one block only. Every variant, including all SNPs, will trigger the formation of branches, which constitute the parallel blocks that account for the nonlinearity of the genome graph (Figure <InternalRef RefID="F2">2a</InternalRef> and <InternalRef RefID="F2">2b</InternalRef> ). Because complex differences such as inversions or duplications can always be defined as combinations of deletions and insertions, they can be readily incorporated into a graph index.</Para>
											<Figure Category="Standard" Float="No" ID="F2">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>GenomeMapper's graph index structure</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="gb-2009-10-9-r98-2" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>GenomeMapper's graph index structure. <Emphasis Type="Bold">(a)</Emphasis> Examples of orthologous sequences in four divergent genomes. Sequences at the beginning and end of each fragment are shared (underlaid with green boxes). Divergent regions start <Emphasis Type="Italic">k</Emphasis> -1 positions (in this case, six positions) before the first true variable position, to account for the <Emphasis Type="Italic">k</Emphasis> -mer length used for the hash-key calculation. <Emphasis Type="Bold">(b)</Emphasis> Graph structure created by these sequences, with <Emphasis Type="Italic">k</Emphasis> -mer length 7, and maximal block length of 10 (instead of 256) for reasons of illustration. The number attached to each block is its unique identifier. Note that blocks do not occupy their maximal block length after an indel, exemplified by blocks 3 and 8. Blocks 1 and 12 correspond to sequences identical in all four genomes and are present only once in the index structure. Arrows between the blocks visualize the edges between the nodes in the genome graph as they are stored in the block table [see Table S1 in Additional data file 1]. <Emphasis Type="Bold">(c)</Emphasis> Alignment of a read against the most similar genome, Genome 3, with a 2-bp insertion. Although the insertion also is observed in Genome 2, the 4-bp deletion downstream in Genome 3 makes the read more similar to it than to Genome 2. The transformed alignment of the read against the original reference sequence (Ref. seq.) includes the 4-bp deletion (as supported by Genome 3) given in parentheses (green), whereas the 2-bp insertion (which is supported neither by Genome 3 nor by the reference sequence) is annotated like a mismatch by using square brackets.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>A unique identifier for each block allows a constant look-up time in a table that stores all relevant block information. In addition to referring to the genomes, of which it is a part, each block encodes for its sequence, the connections to its neighboring blocks, and the position within the genome. Each block thus harbors the genome sequence of all or a subset of genomes with identical sequences within the respective region. The block table is the implementation of a sequence graph, where the blocks represent the nodes, and the connections between them, the edges (Figure <InternalRef RefID="F2">2b</InternalRef> ). We refer to this table as <Emphasis Type="Italic">genome graph</Emphasis> . A comprehensive list of all features stored in the block table is given in Supplemental Table S1 [see Additional data file 1].</Para>
											<Para>Generating different genome graphs with a different number of diverged genomes shows that the increase of a new sequence, and thus additional blocks, decreases with every new genome added; thus, the genome graph is less memory expensive than storing the genomes separately [see Additional data file 1].</Para>
											<Para>Because all relevant information is stored in the genome graph, the positional information attached to each <Emphasis Type="Italic">k</Emphasis> -mer in the hash described earlier (linking each <Emphasis Type="Italic">k</Emphasis> -mer to its locations in the genome) must merely store the block identifier (represented by 3 bytes) and the position within the block (1 byte). Based on this information, the position of every base within each of the genomes can be inferred. The 4-byte encoding accommodates a combined length of all unique sequences of up to 4 Gb.</Para>
											<Para>Efficient read mapping requires that each <Emphasis Type="Italic">k</Emphasis> -mer generated from one of the sequences in the genome graph can be queried for its locations in constant time. This is achieved by building a hash table connecting the <Emphasis Type="Italic">k</Emphasis> -mer (hash key) to its positional information in the genomes (hash value). Each hash key refers to a list of entries. Each of these entries stores a block identifier and a block position, allowing a unique positioning of each <Emphasis Type="Italic">k</Emphasis> -mer.</Para>
										</Section2>
										<Section2 ID="Sec_45232">
											<Heading>Need for complex alignments</Heading>
											<Para>Earlier studies showed that, in a random comparison of two natural Arabidopsis strains, typically one SNP occurs for every 200 bp. In addition, by using early-generation Illumina single reads, more than 60,000 small indels (1 to 3 bp) and 10,000 indels of up to several hundred base pairs have been detected in two strains, presenting a lower boundary for the degree of polymorphism in this species <CitationRef CitationID="B2">2</CitationRef> .</Para>
											<Para>Mismatches in alignments result not only from sequence differences, but also from sequencing errors. The error probability of Illumina sequence reads has been shown to be less than 1% for most, but not all parts of the read <CitationRef CitationID="B2">2</CitationRef> . In comparison with the rate of natural variation in Arabidopsis, mismatches from errors in individual reads outnumber true SNPs approximately 17 to 1, whereas true gaps are almost as frequent as gaps resulting from sequencing errors [see Additional data file 1].</Para>
											<Para>To avoid misplacement of individual reads, some mapping tools favor alignments in which the cumulative base quality of mismatching bases is low <CitationRef CitationID="B21">21</CitationRef> . With respect to the high level of natural differences in Arabidopsis, such a strategy could bias alignments away from polymorphic regions. GenomeMapper instead performs, for each read, an alignment based on dynamic programming similar to the Needleman-Wunsch alignment algorithm (see <CitationRef CitationID="B35">35</CitationRef> and Additional data file 1 for modifications). Our method ensures that all alignments within a given number of mismatches and gaps are reported, provided that they share at least one identical substring of length <Emphasis Type="Italic">k</Emphasis> when using a <Emphasis Type="Italic">k</Emphasis> -mer index. No other constraints are imposed on the number of mismatches, gaps, or base call quality. By default, GenomeMapper aligns against all instances of a repeat, but it also can be instructed to align only against a subset of them.</Para>
											<Para>In our experience, resequencing projects of bacterial or medium-sized eukaryotic genomes such as those of Arabidopsis strains do not benefit from using alignments other than the optimal ones. Nonetheless, GenomeMapper can be configured to report not only the best scoring alignments, but also all hits within the specified range of mismatches and gaps (all-hits instead of best-hits strategy). As expected, this comes with an increase in runtime, especially for highly repetitive genomes.</Para>
										</Section2>
										<Section2 ID="Sec_61906">
											<Heading>Aligning sequences against the graph</Heading>
											<Para>GenomeMapper's alignment procedure is partitioned into three steps, including speed optimization. The optimization bypasses the costly calculation of alignment matrices without a decrease in sensitivity and is based on two observations: first, a dynamic programming alignment is required only if the best alignment involves gaps; and, second, the frequency of gaps is lower than that of mismatches. This is the case both for sequencing errors in Illumina reads and for true polymorphisms. To cope with this, GenomeMapper applies a higher penalty for gaps than for mismatches. Therefore, alignments with a penalty lower than the gap penalty do not require dynamic programming. The optimization cannot be applied in an all-hits strategy including gapped alignments and will not increase speed if the best alignment features gaps.</Para>
											<Para>In the first step of the alignment procedure, GenomeMapper scans the hash index for <Emphasis Type="Italic">k</Emphasis> -mers identical between read and genome graph to detect quickly all genomes and locations with nearly identical alignments. In the second step, GenomeMapper determines the location and sequence of <Emphasis Type="Italic">nearly identical maximal substrings</Emphasis> (NIMS) between read and genome graph. GenomeMapper will finally perform a <Emphasis Type="Italic">k</Emphasis> -banded alignment by applying dynamic programming to ensure a consistent gap placement [see Additional data file 1].</Para>
											<Para>In detail, GenomeMapper starts by calculating the hash keys of a predefined set of non-overlapping <Emphasis Type="Italic">k</Emphasis> -mers of the read sequence and retrieves their genomic positions from the hash index. The pair, consisting of a <Emphasis Type="Italic">k</Emphasis> -mer along with one of its positions in the genome, is referred to as <Emphasis Type="Italic">hit</Emphasis> . If the best alignment of a read contains up to one mismatch less than the number of non-overlapping k-mers fitting into the read, at least one hit within this alignment can be computed (see <CitationRef CitationID="B36">36</CitationRef> and Additional data file 1). Each hit serves as the seed for an ungapped alignment comparing the unmatched parts of the read with the target sequence.</Para>
											<Para>If the first step does not reveal a valid alignment, which is always optimal because of the prerequisite that one mismatch is less penalized than one gap, GenomeMapper starts calculating hits not only for a subset, but also for each of the <Emphasis Type="Italic">k</Emphasis> -mers within the read sequence. If two hits are adjacent in the read and in the genome graph, they will be merged, resulting in so-called <Emphasis Type="Italic">extended hits</Emphasis> . If a single mismatch between read and genome sequence is adjacent to extended hits on either side, GenomeMapper can bridge this mismatch by merging the extended hits now harboring this mismatch. Once all hits are maximally extended (they now constitute NIMSs), the read has to be aligned against the regions determined by each of the NIMS, aborting as soon as the best possible alignment will be worse than the mismatch and gap constraints [see Additional data file 1].</Para>
											<Para>To retrieve the genomic sequence for the alignments, GenomeMapper must follow the links between blocks. Starting from the block harboring the hit or NIMS, GenomeMapper follows the edges of the genome graph to generate a target sequence for the alignment. If multiple blocks reside next to one of these blocks, each of the branches will generate a separate target sequence for an independent alignment. Note that GenomeMapper will not concatenate sequences from different genomes. The alignment phase is implemented with an efficient parallelization, which substantially reduces runtime. It is distributed in a master-slave model on a shared-memory architecture. All alignment threads can access the genome data and the read data. The master thread distributes individual hits by signaling each alignment thread and collects the results. The number of threads used by the parallel implementation is a user-defined parameter that can be adjusted to the hardware.</Para>
											<Para>The parallel version of GenomeMapper relies on POSIX threads to manage the individual compute threads efficiently. POSIX threads are available for all relevant platforms (including Linux, Mac OS, and Windows).</Para>
										</Section2>
										<Section2 ID="Sec_46316">
											<Heading>Representation of the alignments</Heading>
											<Para>Independent of the algorithm used to detect the best alignments, GenomeMapper will report two different representations of the alignment. The first one constitutes the alignment of the read against the genome to which it is most similar (reference-free alignment). Because commonly used tools for alignment consensus analysis such as MAQ, Mosaik, SHORE, and VAAL <CitationRef CitationID="B1">1</CitationRef>
												<CitationRef CitationID="B2">2</CitationRef>
												<CitationRef CitationID="B18">18</CitationRef>
												<CitationRef CitationID="B37">37</CitationRef> report base calls based on the location relative to one reference sequence, GenomeMapper implements a second alignment representation, which transforms the strain alignment into an alignment against the reference sequence. This reference-based alignment can then be used as input for one of the tools mentioned earlier. Which of the genomes constitutes the 'reference sequence' is defined in the index creation. As the reference sequence is not necessarily the most similar sequence to the read, the reference-based alignment can feature more mismatches and gaps than the strain alignment and can exceed the user-defined constraints.</Para>
											<Para>This transformation generates two categories of mismatches in the reference-based alignment. The first category contains mismatches that are unique to the read sequence. The second consists of mismatches identical between the read and the strain it was aligned with, but different from the reference sequence. Such mismatches are more likely to represent true polymorphisms, because they have already been previously observed. GenomeMapper indicates the different types of mismatches by using round and square brackets (Figure <InternalRef RefID="F2">2c</InternalRef> ). We have updated SHORE's <CitationRef CitationID="B2">2</CitationRef> consensus analysis to exploit this additional information (see section, <Emphasis Type="Italic">Impact on Resequencing</Emphasis> ).</Para>
										</Section2>
										<Section2 ID="Sec_71273">
											<Heading>Position descriptors for reference-free and reference-based alignments</Heading>
											<Para>An alignment is typically anchored by the position of the 5' nucleotide in the target sequence at which the alignment starts. Because different genomes may feature indels of different lengths, however, even for identical sites, positional information can become ambiguous. The decision for one of the locations only ( <Emphasis Type="Italic">e.g</Emphasis> ., that of the reference genome) would overvalue the reference.</Para>
											<Para>Currently the sole community-wide accepted description of a genomic location is the corresponding nucleotide within the reference sequence, which easily accommodates gaps, but not insertions, relative to the reference. We therefore implemented two position descriptors into GenomeMapper. The first refers to the particular genome against which the alignment was performed (the strain alignment). The second represents the position of the alignment against the reference (the reference alignment). Insertions are annotated by using the upstream reference position followed by the position of the inserted nucleotide within the insertion, separated by a decimal point ( <Emphasis Type="Italic">e.g</Emphasis> ., "80359.12" describes the 12th nucleotide within the insertion after position 80359 of the reference). Strain alignments transformed to reference alignments lose their reference-free characteristic and therefore are immediately comparable with conventional mapping results.</Para>
										</Section2>
										<Section2 ID="Sec_21531">
											<Heading>Comparison with other mapping tools</Heading>
											<Para>GenomeMapper also can be used for alignments against a single target genome. This allowed us to compare runtime and sensitivity of GenomeMapper (version 0.3.1s) with those of four other popular mapping tools: SOAP (version 1.11 <CitationRef CitationID="B19">19</CitationRef> ), soap2 (version 2.01 <CitationRef CitationID="B20">20</CitationRef> ), bowtie (version 0.9.8 <CitationRef CitationID="B16">16</CitationRef> ), and MAQ (version 0.7.1 <CitationRef CitationID="B18">18</CitationRef> ). SOAP and MAQ were previously compared with bowtie <CitationRef CitationID="B16">16</CitationRef> , but with a human target. Here we aligned against the Arabidopsis Col-0 reference genome <CitationRef CitationID="B38">38</CitationRef> with seed length set to 12. All tests were performed on 10 independent read sets, each consisting of 500,000 reads randomly sampled from reads generated in this work for the Arabidopsis Est-1 strain (see later). We tried to run all alignment tools with optimal parameters to achieve the best possible sensitivity and runtime [see Supplemental Table S2 in Additional data file 1 for command lines of all runs]. To make them directly comparable with GenomeMapper, we set SOAP, soap2, and MAQ to report all repetitive best hits rather than a random subset of them, even though this comes with an additional investment in runtime. All tests were performed on a compute server with eight cores (two AMD Opteron quad core processors) and 32 GB RAM. Figure <InternalRef RefID="F3">3</InternalRef> compares average runtimes, measured as the wall clock, as well as sensitivity of all alignments and of gapped alignments, both measured as the number of reads that could be aligned. As this analysis is based on real data for which no gold-standard sequence information is available, nothing is known about the true origin of the DNA reads. We therefore took the fraction of aligned reads as a proxy for sensitivity.</Para>
											<Figure Category="Standard" Float="No" ID="F3">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Performance of GenomeMapper compared with that of other short-read alignment tools</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="gb-2009-10-9-r98-3" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>Performance of GenomeMapper compared with that of other short-read alignment tools. <Emphasis Type="Bold">(a)</Emphasis> Runtime, measured as wall clock time between invocation and termination of the program, averaged from 10 independent tests with different random sets of 500,000 short reads from Est-1. The worst test was excluded from average calculations. Error bars indicate standard deviation. <Emphasis Type="Italic">mm</Emphasis> , <Emphasis Type="Italic">gaps</Emphasis> , and <Emphasis Type="Italic">edit</Emphasis> refer to the maximal number of mismatches, gaps and edit operations allowed. GenomeMapper was run with four different parameter settings: the serial version; the parallel version on four cores; the serial version merely aligning NIMS of length 13 or longer; and the parallel version aligning only NIMS of length 13 or longer. SOAP was found running on up to four CPUs instead of only one CPU, as configured with the command line (option -p). <Emphasis Type="Bold">(b)</Emphasis> Average sensitivity, measured as the percentage of aligned reads. Only GenomeMapper and SOAP can perform gapped alignments. <Emphasis Type="Bold">(c)</Emphasis> Average sensitivity of alignments, allowing three gaps and four mismatches with a combined maximum of four edit operations, measured as number of reads with gapped alignments. Fractions refer to the number of all reads with gapped alignments.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>Without allowing any mismatches, little difference in runtime or in sensitivity was found between the alignment tools, with GenomeMapper being slower than bowtie and soap2, but faster than SOAP and MAQ. Allowing two mismatches caused similar increases in runtime for all tools. With respect to sensitivity, more than 99% of the differences in the reads that could be aligned with up to two mismatches resulted from different strategies in aligning ambiguous base calls (Ns). SOAP, for example, aligns Ns without an alignment penalty.</Para>
											<Para>Different from SOAP, GenomeMapper's runtime was drastically affected by allowing additional gaps (which are not accommodated by the other tools tested) (Figure <InternalRef RefID="F3">3a</InternalRef> ). The first reason for this disparity is the different alignment strategy. SOAP allows neither gaps combined with mismatches nor multiple gaps in the same alignment, whereas the dynamic programming alignment in GenomeMapper supports any combination of gaps and mismatches. Second, even though SOAP was set to run on one processor (option -p was set to 1), we found it running in parallel on up to four CPUs, and therefore using more computational power than the other tools.</Para>
											<Para>By applying GenomeMapper's parallelization set to run on four cores, runtime was reduced significantly. Parallelization is geared toward complex alignments and did not reduce runtime for ungapped alignments. Another way to reduce runtime is offered by skipping alignments triggered by NIMS/hits of length 12 (seeds that could not be extended by at least one base, option -l, indicated by "NIMS 13" in Figure <InternalRef RefID="F3">3a</InternalRef> ), but this came at a cost of sensitivity being reduced by 0.6%.</Para>
											<Para>Compared with SOAP, GenomeMapper's more accurate alignment method resulted in higher sensitivity (Figure <InternalRef RefID="F3">3b</InternalRef> ; compare results for 4 mm/1 gap and 4 mm/3 gaps). Considering only gapped alignments, GenomeMapper aligned more than 5 times as many reads as SOAP (Figure <InternalRef RefID="F3">3c</InternalRef> ), whereas only one of 500,000 reads was aligned by SOAP, but not by GenomeMapper. This difference showcases GenomeMapper's ability to combine multiple gaps with mismatches in the same alignment.</Para>
											<Para>Note that the reads used for benchmarking had been quality trimmed. This removes the common trend of read endings having increased chances of harboring mismatches because of higher error rates. Untrimmed reads with additional mismatches would have almost completely prohibited SOAP from performing gapped alignments. This is expected to be even more an issue with longer reads.</Para>
											<Para>GenomeMapper's relatively high runtime when allowing a large number of gaps and mismatches is explained mostly by the enormous number of alignments performed once optimizations could not reveal the best alignment. Nonetheless, accurate alignments are important for correct read placement in regions of high divergence and therefore justify the performance loss. Whereas aligning against a genome graph comes with additional computational costs, it greatly increases sensitivity. One can compensate for increased runtime with computing power, but reads that are never correctly aligned in the first place are lost for further analyses.</Para>
										</Section2>
										<Section2 ID="Sec_80617">
											<Heading>Impact on resequencing</Heading>
											<Para>To examine the practical relevance of graph-based alignments against multiple genomes, we compared performance with a conventional single-reference approach by using reads from the genome of Arabidopsis strain Estland-1 (Est-1) from Estonia, generated in the <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> 1001 Genomes Project [see Additional data file 1]. The 47.7 million alignable single-end high-quality reads were produced on an Illumina Genome Analyzer. After quality trimming of the reads to 36 to 42 bp, the average depth of genome coverage was 13 fold.</Para>
											<Para>We first used the reference Arabidopsis Col-0 sequence (TAIR8 <CitationRef CitationID="B38">38</CitationRef> ) as the alignment target. In the second analysis, we included two Arabidopsis genomes, Bur-0 and Tsu-1 (see Figure <InternalRef RefID="F4">4</InternalRef> ). Previous Illumina single-read sequencing and comparison against the Col-0 reference had revealed 570,100 and 502,036 SNPs, as well as 48,999 and 47,765 indels of up to 3 bp, respectively <CitationRef CitationID="B2">2</CitationRef> . In addition, 16,463 and 3,007 longer indels of up to 641 bp had been discovered from targeted <Emphasis Type="Italic">de novo</Emphasis> assembly of highly polymorphic regions <CitationRef CitationID="B2">2</CitationRef> . These two genomes differ from the reference by 0.5 to 0.6%, which reflects a lower bound of sequence divergence, given the limitations of short-read analyses.</Para>
											<Figure Category="Standard" Float="No" ID="F4">
												<Caption Language="En">
													<CaptionContent>
														<SimplePara>Alignments against a 17-bp insertion present in a nonreference genome</SimplePara>
													</CaptionContent>
												</Caption>
												<MediaObject>
													<ImageObject Color="Color" FileRef="gb-2009-10-9-r98-4" Format="GIF" Rendition="Preview" Type="Linedraw"/>
													<TextObject>
														<Para>Alignments against a 17-bp insertion present in a nonreference genome. <Emphasis Type="Bold">(a)</Emphasis> Alignments of Est-1 reads against the graph of Arabidopsis chromosome 1, reference positions 20,166,584 to 20,166,747. Alignments against both the Col-0 reference and the Bur-0 variant genomes are highlighted in dark gray; alignments of reads aligning best against a single genome are highlighted in light gray. Most reads align against the Bur-0 allele, suggesting that Est-1 is more similar to Bur-0 at this locus. In particular, the 17-bp insertion found in Bur-0 is supported by the Est-1 reads. Because of the alignment constraints (maximum of four edit operations), these alignments could not have been performed against the Col-0 sequence only. Within the second divergent region, indicated by a red arrow, Bur-0 has a complex change, ACC-&gt;T, relative to Col-0, with Est-1 featuring a third allele, ACC-&gt;TA. Because this change is near the 17-bp insertion, only a subset of the alignments would have been found with single reference alignments only. For simplicity, Tsu-1, which also is included in the graph target, is not shown here. <Emphasis Type="Bold">(b)</Emphasis> Annotation of this region with respect to the Col-0 reference genome.</Para>
													</TextObject>
												</MediaObject>
											</Figure>
											<Para>The Bur-0 and Tsu-1 genomes, together with the Col-0 reference genome, were used to build a multiple genome graph. To take advantage of the additional information produced by the graph-based alignments, and to make it comparable to a single reference analysis, we updated SHORE <CitationRef CitationID="B2">2</CitationRef> , our genome-resequencing analysis pipeline [see Additional data file 1]. This included incorporation of GenomeMapper's transformed alignment representation, different scoring schemes for previously known and newly discovered polymorphisms, and the support of indels up to any length, restricted only by the maximal indel length within the known genome space.</Para>
											<Para>More than 1% of all reads, 0.51 million reads, could be aligned to the genome graph, but not to the single reference. These additional alignments resemble highly divergent regions of Est-1, which are particularly interesting, but also constitute the regions that are least accessible to conventional methods. Compared with the "reference only" alignments, the graph alignments increased the number of recovered SNPs by 15%, of deletions by 22.6%, and of insertions by 37.2% (Table <InternalRef RefID="T1">1</InternalRef> ). In particular, 1,551 deletions and 1,841 insertions longer than 3 bp, with a maximum length of 641 bp and 281 bp, known from previous <Emphasis Type="Italic">de novo</Emphasis> assembly of larger indels in Bur-0 and Tsu-1 <CitationRef CitationID="B2">2</CitationRef> , were detected. Only a small subset of the long indels was represented in the "reference only" analysis (two 3-bp deletions can modify the sequence in the same way as one 6-bp deletion). Because of the limitation of three gapped positions per alignment, the vast majority of long indels could not be discovered with the conventional "reference only" alignment. These observations illustrate that indel detection is not limited by alignment constraints, but only by the data included in the genome graph.</Para>
											<Table Float="No" ID="T1">
												<Caption Language="En">
													<CaptionNumber>Table 1</CaptionNumber>
													<CaptionContent>
														<SimplePara>Recovery of Est-1 variants by using SHORE</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="6">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara>
																	<Emphasis Type="Bold">Predicted by both analyses <Superscript>a</Superscript>
																	</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">Private to genome graph analysis</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">Private to reference-only analysis</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">Total gain in genome graph analysis</Emphasis>
																</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>SNPs</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara>401,158</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>66,264</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>5,423</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>15.0%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Deletions</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>All</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>25,926</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>6,807</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>778</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>22.6%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>1-3 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>25,865</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>5,256</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>778</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>16.8%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>≥ 4 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>61</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>1,551</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>0</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>2,542%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Insertions</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>All</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>22,305</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>9,220</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>678</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>37.2%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>1-3 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>22,285</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>7,379</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>678</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>29.2%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>≥ 4 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>20</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>1,841</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>0</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>9,205%</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
												<tfooter>
													<SimplePara>
														<Superscript>a</Superscript> Includes variants predicted by graph analysis that have been found in the single-reference analysis in the same sequence context, although with a differing position, resulting from ambiguous alignments. Some of the variants longer than 3 bp could be reassembled in the single-reference analysis, by combining shorter indels.</SimplePara>
												</tfooter>
											</Table>
											<Para>The reliability of variant detection was improved as well, with 244,101 SNP calls made in the "reference only" analysis having additional support from one of the additional genomes in the graph (11,382 and 16,958 for deletions and insertions, respectively). Similarly, recall rates for 1 to 3 bp indels were drastically increased.</Para>
											<Para>Validation results for single-reference and genome-graph analysis based on 600 kb of dideoxy sequences distributed throughout the Est-1 genome <CitationRef CitationID="B39">39</CitationRef> are shown in Table <InternalRef RefID="T2">2</InternalRef> . In a typical Arabidopsis strain, about 85% of SNPs are accessible to analysis with 36-bp single-end short reads, with the remainder being located in repetitive regions <CitationRef CitationID="B2">2</CitationRef> . Of 2,316 SNPs in the validation set, 85.2% were called by using genome-graph analysis, an increase of more than 7% compared with the single-reference analysis at a similar error rate of less than 0.5%. Recall rates for indels were increased even more, by 14.8% for insertions and 8.4% for deletions.</Para>
											<Table Float="No" ID="T2">
												<Caption Language="En">
													<CaptionNumber>Table 2</CaptionNumber>
													<CaptionContent>
														<SimplePara>Validation of polymorphism predictions in Est-1</SimplePara>
													</CaptionContent>
												</Caption>
												<tgroup cols="7">
													<colspec colname="c0" colnum="0"/>
													<colspec colname="c1" colnum="1"/>
													<colspec colname="c2" colnum="2"/>
													<colspec colname="c3" colnum="3"/>
													<colspec colname="c4" colnum="4"/>
													<colspec colname="c5" colnum="5"/>
													<colspec colname="c6" colnum="6"/>
													<thead>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara/>
															</entry>
															<entry colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">Graph analysis</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">Single-reference analysis</Emphasis>
																</SimplePara>
															</entry>
														</row>
													</thead>
													<tbody>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara/>
															</entry>
															<entry colname="c3">
																<SimplePara/>
															</entry>
															<entry colname="c4">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara>
																	<Emphasis Type="Bold">
																		<Emphasis Type="Italic">N</Emphasis>
																		<Superscript>a</Superscript>
																	</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>
																	<Emphasis Type="Bold">Recall <Superscript>b</Superscript>
																	</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>
																	<Emphasis Type="Bold">FDR <Superscript>c</Superscript>
																	</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>
																	<Emphasis Type="Bold">Recall <Superscript>b</Superscript>
																	</Emphasis>
																</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>
																	<Emphasis Type="Bold">FDR <Superscript>†</Superscript>
																	</Emphasis>
																</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>SNPs</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara/>
															</entry>
															<entry colname="c2">
																<SimplePara>2,316</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>85.2%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>0.4%</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>77.5%</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>0.4%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Deletions</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>All</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>183</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>53.6%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>2.0%</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>38.8%</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>2.7%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>1-3 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>132</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>68.2%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>2.2%</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>53.8%</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>2.7%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>≥ 4 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>51</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>15.7%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>0.0</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>0</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>n/a</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara>Insertions</SimplePara>
															</entry>
															<entry colname="c1">
																<SimplePara>All</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>167</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>53.9%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>2.2%</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>45.5%</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>1.3%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>1-3 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>128</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>66.4%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>2.3%</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>59.4%</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>1.3%</SimplePara>
															</entry>
														</row>
														<row>
															<entry colname="c0">
																<SimplePara/>
															</entry>
															<entry colname="c1">
																<SimplePara>≥ 4 bp</SimplePara>
															</entry>
															<entry colname="c2">
																<SimplePara>39</SimplePara>
															</entry>
															<entry colname="c3">
																<SimplePara>12.8%</SimplePara>
															</entry>
															<entry colname="c4">
																<SimplePara>0.0</SimplePara>
															</entry>
															<entry colname="c5">
																<SimplePara>0</SimplePara>
															</entry>
															<entry colname="c6">
																<SimplePara>n/a</SimplePara>
															</entry>
														</row>
													</tbody>
												</tgroup>
												<tfooter>
													<SimplePara>
														<Superscript>a</Superscript> Number of known variants in 600 kb of dideoxy sequence data from <CitationRef CitationID="B38">38</CitationRef> . <Superscript>b</Superscript> Ratio of confirmed to the sum of confirmed and missed predictions of the respective kind; indicates sensitivity of method. <Superscript>c</Superscript> False discovery rate, percentage of erroneous calls.</SimplePara>
												</tfooter>
											</Table>
											<Para>For a final comparison, we aligned all Est-1 reads against the three known genomes separately, with the Bur-0 and Tsu-1 genome sequences generated by introducing all known variations into the reference Col-0 genome. As expected, nearly the same set of reads could be aligned, but the graph alignments were 21.3% faster than the serial alignments. This improvement would be even greater if one took into account the additional analyses needed for merging and filtering of separate and redundant alignments.</Para>
											<Para>The results of the graph analysis of Est-1 can be downloaded from the 1001 Genomes portal <CitationRef CitationID="B33">33</CitationRef> and from TAIR <CitationRef CitationID="B40">40</CitationRef> .</Para>
										</Section2>
									</Section1>
									<Section1 ID="Sec_59483">
										<Heading>Discussion</Heading>
										<Para>The first goal for short-read mapping tools was the design of efficient alignment algorithms that were faster than the speed with which raw data were produced. Considering that intraspecific sequence differences are often more substantial than previously anticipated, a major challenge is the requirement not to disregard or misplace too many reads. With the rapidly increasing knowledge of variants, one could simply align against all known genomes for a species separately. This would not require any new methods, but it comes with the overhead of redundant alignments in conserved regions. We have shown that graph alignments are already superior with information from only two divergent genomes added to the first genome sequence produced for Arabidopsis. This advantage should become much more drastic once hundreds of genomes are incorporated into the graph structure. In addition, this should improve the workflow, as the separate handling of hundreds of separate references would become increasingly impractical.</Para>
										<Para>We demonstrated that short-read alignment against a complex graph representing multiple genomes not only is possible and produces meaningful results, but also provides access to regions that are highly divergent from the first reference. In addition, our approach reduces the number of false-positive SNP calls caused by misalignments near indels <CitationRef CitationID="B2">2</CitationRef> . To our knowledge, this constitutes the first approach that efficiently incorporates multiple references and solves resultant problems. We note in addition that the representation of multiple genomes in a complex graph structure is not restricted to short-read mapping or intraspecific analyses. Other applications are easily conceivable ( <Emphasis Type="Italic">e.g</Emphasis> ., accurate local and global alignments of longer reads (up to whole genomes) against all known genomes of a species or even against a structure representing groups of related species), enabling analysis of metagenomic samples in one step. Likewise, read alignments against splice graphs representing known isoforms with differing exon-intron junctions would be beneficial for mRNA analysis.</Para>
										<Para>Once the species-wide genome graph of Arabidopsis covers most common variants (see the <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> 1001 Genomes Project <CitationRef CitationID="B32">32</CitationRef>
											<CitationRef CitationID="B33">33</CitationRef> ), resequencing of newly collected material will become easier, as fewer inaccessible regions remain. A prerequisite for this are universal and community-wide accepted positional descriptors of insertions, for which we have advanced a proposal in this work.</Para>
										<Section2 ID="Sec_54126">
											<Heading>Ongoing development</Heading>
											<Para>The steady increase in read length will improve the likelihood that a given read spans a region of complex differences relative to the first reference. Although no theoretic limitation exists for the lengths of global alignments (GenomeMapper currently allows reads of up to 1,000 bp with unlimited numbers of mismatches or gaps), allowing more and more mismatches and gaps would strongly affect runtime. This could be addressed by further increasing the efficiency of the parallelization, which is already tuned to reduce runtime for long-read alignments with numerous gaps and mismatches.</Para>
											<Para>Another challenge that is conceptually similar to matching known SNPs relative to the reference emerges from bisulfite treatment of DNA samples for methylome analysis <CitationRef CitationID="B41">41</CitationRef> . The presence of cytosines that have been converted to thymines by bisulfite can be implemented as mismatches without penalty. This is currently being incorporated into GenomeMapper and will be supported in future versions.</Para>
										</Section2>
									</Section1>
									<Section1 ID="Sec_16362">
										<Heading>Abbreviations</Heading>
										<Para>Bp: base pair; GB: gigabytes; indels: insertions and/or deletions; k-mer: sequence signature; NIMS: nearly identical maximal substrings; POSIX: Portable Operating System Interface for Unix; RAM: random-access memory; SNP: single-nucleotide polymorphism; TAIR: The Arabidopsis Information Resource.</Para>
									</Section1>
									<Section1 ID="Sec_73915">
										<Heading>Authors' contributions</Heading>
										<Para>KS and DW designed the study. KS and JH developed and implemented GenomeMapper. SO suggested optimizations resulting in major speed improvements, extended SHORE for the analysis of genome-graph alignments, and performed the Est-1 analysis together with JH and KS. SG implemented the parallelization, as discussed with OK. NW did the plant work and generated the Illumina sequencing library. KS wrote the manuscript with help from all authors.</Para>
									</Section1>
									<Section1 ID="Sec_05533">
										<Heading>Additional data files</Heading>
										<Para>The following additional data are available with the online version of this article. Additional data file1describes supplementary methods and discussions, as well as tables listing the features of genome graph structure and the command lines used for comparison of the different alignment programs.</Para>
									</Section1>
								</Body>
								<BodyRef FileRef="http://genomebiology.com/2009/10/9/R98" TargetType="Manuscript"/>
								<ArticleBackmatter>
									<Bibliography ID="Bib1">
										<Heading>References</Heading>
										<Citation ID="CR1">
											<CitationNumber>1.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>LW</Initials>
													<FamilyName>Hillier</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>GT</Initials>
													<FamilyName>Marth</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AR</Initials>
													<FamilyName>Quinlan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Dooling</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Fewell</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Barnett</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Fox</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JI</Initials>
													<FamilyName>Glasscock</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Hickenbotham</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>W</Initials>
													<FamilyName>Huang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>VJ</Initials>
													<FamilyName>Magrini</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RJ</Initials>
													<FamilyName>Richt</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SN</Initials>
													<FamilyName>Sander</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DA</Initials>
													<FamilyName>Stewart</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Stromberg</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>EF</Initials>
													<FamilyName>Tsung</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Wylie</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Schedl</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RK</Initials>
													<FamilyName>Wilson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>ER</Initials>
													<FamilyName>Mardis</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Whole-genome sequencing and variant discovery in C. elegans.</ArticleTitle>
												<JournalTitle>Nat Methods</JournalTitle>
												<VolumeID>5</VolumeID>
												<FirstPage>183</FirstPage>
												<LastPage>188</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/nmeth.1179</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR2">
											<CitationNumber>2.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Ossowski</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Schneeberger</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RM</Initials>
													<FamilyName>Clark</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Lanz</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>N</Initials>
													<FamilyName>Warthmann</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Weigel</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Sequencing of natural strains of <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> with short reads.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>18</VolumeID>
												<FirstPage>2024</FirstPage>
												<LastPage>2033</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.080200.108</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR3">
											<CitationNumber>3.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SM</Initials>
													<FamilyName>Ahn</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TH</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Ghang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DS</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>BC</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SY</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WY</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Park</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>YS</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Reja</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Jho</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CG</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JY</Initials>
													<FamilyName>Cha</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KH</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Bhak</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">The first Korean genome sequence and analysis: full genome sequencing for a socio-ethnic group.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>19</VolumeID>
												<FirstPage>1622</FirstPage>
												<LastPage>1629</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.092197.109</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR4">
											<CitationNumber>4.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>DR</Initials>
													<FamilyName>Bentley</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Balasubramanian</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>HP</Initials>
													<FamilyName>Swerdlow</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>GP</Initials>
													<FamilyName>Smith</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Milton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CG</Initials>
													<FamilyName>Brown</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KP</Initials>
													<FamilyName>Hall</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DJ</Initials>
													<FamilyName>Evers</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CL</Initials>
													<FamilyName>Barnes</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>HR</Initials>
													<FamilyName>Bignell</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JM</Initials>
													<FamilyName>Boutell</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Bryant</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RJ</Initials>
													<FamilyName>Carter</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Keira Cheetham</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AJ</Initials>
													<FamilyName>Cox</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DJ</Initials>
													<FamilyName>Ellis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MR</Initials>
													<FamilyName>Flatbush</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>NA</Initials>
													<FamilyName>Gormley</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Humphray</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>LJ</Initials>
													<FamilyName>Irving</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MS</Initials>
													<FamilyName>Karbelashvili</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SM</Initials>
													<FamilyName>Kirk</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>X</Initials>
													<FamilyName>Liu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KS</Initials>
													<FamilyName>Maisinger</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>LJ</Initials>
													<FamilyName>Murray</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Obradovic</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Ost</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>ML</Initials>
													<FamilyName>Parkinson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MR</Initials>
													<FamilyName>Pratt</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Accurate whole human genome sequencing using reversible terminator chemistry.</ArticleTitle>
												<JournalTitle>Nature</JournalTitle>
												<VolumeID>456</VolumeID>
												<FirstPage>53</FirstPage>
												<LastPage>59</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/nature07517</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR5">
											<CitationNumber>5.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>JI</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>YS</Initials>
													<FamilyName>Ju</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Park</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JH</Initials>
													<FamilyName>Yi</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Mudge</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>NA</Initials>
													<FamilyName>Miller</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Hong</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CJ</Initials>
													<FamilyName>Bell</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>HS</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>IS</Initials>
													<FamilyName>Chung</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WC</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JS</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SH</Initials>
													<FamilyName>Seo</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JY</Initials>
													<FamilyName>Yun</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>HN</Initials>
													<FamilyName>Woo</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Suh</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>HJ</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Yavartanoo</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Kwak</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Zheng</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MK</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JY</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>O</Initials>
													<FamilyName>Gokcumen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RE</Initials>
													<FamilyName>Mills</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AW</Initials>
													<FamilyName>Zaranek</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Thakuria</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>X</Initials>
													<FamilyName>Wu</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">A highly annotated whole-genome sequence of a Korean individual.</ArticleTitle>
												<JournalTitle>Nature</JournalTitle>
												<VolumeID>460</VolumeID>
												<FirstPage>1011</FirstPage>
												<LastPage>1015</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR6">
											<CitationNumber>6.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>TJ</Initials>
													<FamilyName>Ley</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>ER</Initials>
													<FamilyName>Mardis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Ding</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Fulton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MD</Initials>
													<FamilyName>McLellan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Chen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Dooling</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>BH</Initials>
													<FamilyName>Dunford-Shore</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>McGrath</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Hickenbotham</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Cook</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Abbott</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DE</Initials>
													<FamilyName>Larson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DC</Initials>
													<FamilyName>Koboldt</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Pohl</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Smith</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Hawkins</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Abbott</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Locke</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>LW</Initials>
													<FamilyName>Hillier</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Miner</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Fulton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>V</Initials>
													<FamilyName>Magrini</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Wylie</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Glasscock</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Conyers</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>N</Initials>
													<FamilyName>Sander</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>X</Initials>
													<FamilyName>Shi</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JR</Initials>
													<FamilyName>Osborne</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Minx</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">DNA sequencing of a cytogenetically normal acute myeloid leukaemia genome.</ArticleTitle>
												<JournalTitle>Nature</JournalTitle>
												<VolumeID>456</VolumeID>
												<FirstPage>66</FirstPage>
												<LastPage>72</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/nature07485</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR7">
											<CitationNumber>7.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>ER</Initials>
													<FamilyName>Mardis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Ding</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DJ</Initials>
													<FamilyName>Dooling</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DE</Initials>
													<FamilyName>Larson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MD</Initials>
													<FamilyName>McLellan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Chen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DC</Initials>
													<FamilyName>Koboldt</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RS</Initials>
													<FamilyName>Fulton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KD</Initials>
													<FamilyName>Delehaunty</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SD</Initials>
													<FamilyName>McGrath</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>LA</Initials>
													<FamilyName>Fulton</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DP</Initials>
													<FamilyName>Locke</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>VJ</Initials>
													<FamilyName>Magrini</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RM</Initials>
													<FamilyName>Abbott</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TL</Initials>
													<FamilyName>Vickery</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JS</Initials>
													<FamilyName>Reed</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JS</Initials>
													<FamilyName>Robinson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Wylie</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SM</Initials>
													<FamilyName>Smith</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Carmichael</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JM</Initials>
													<FamilyName>Eldred</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CC</Initials>
													<FamilyName>Harris</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Walker</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JB</Initials>
													<FamilyName>Peck</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>F</Initials>
													<FamilyName>Du</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AF</Initials>
													<FamilyName>Dukes</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>GE</Initials>
													<FamilyName>Sanderson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AM</Initials>
													<FamilyName>Brummett</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Clark</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JF</Initials>
													<FamilyName>McMichael</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Recurring mutations found by sequencing an acute myeloid leukemia genome.</ArticleTitle>
												<JournalTitle>N Engl J Med</JournalTitle>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR8">
											<CitationNumber>8.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>KJ</Initials>
													<FamilyName>McKernan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>HE</Initials>
													<FamilyName>Peckham</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Costa</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>McLaughlin</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Tsung</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Fu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Clouser</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Dunkan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Ichikawa</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Lee</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Sheridan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Fu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Ranade</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Dimilanta</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Sokolsky</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Hendrickson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Kotler</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Stuart</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Malek</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Manning</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Antipova</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Perez</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Moore</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Hayashibara</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Lyons</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Beaudoin</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Coleman</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Sequence and structural variation in a human genome uncovered by short-read, massively parallel ligation sequencing using two base encoding.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>19</VolumeID>
												<FirstPage>1527</FirstPage>
												<LastPage>1541</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.091868.109</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR9">
											<CitationNumber>9.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Pushkarev</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>NF</Initials>
													<FamilyName>Neff</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SR</Initials>
													<FamilyName>Quake</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Single-molecule sequencing of an individual human genome.</ArticleTitle>
												<JournalTitle>Nat Biotechnol</JournalTitle>
												<VolumeID>27</VolumeID>
												<FirstPage>847</FirstPage>
												<LastPage>852</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/nbt.1561</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR10">
											<CitationNumber>10.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>W</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Tian</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>L</Initials>
													<FamilyName>Goodman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>W</Initials>
													<FamilyName>Fan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Guo</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Feng</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Lu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>X</Initials>
													<FamilyName>Fang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Liang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Du</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Zhao</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Hu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Yang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Zheng</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>I</Initials>
													<FamilyName>Hellmann</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Inouye</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Pool</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>X</Initials>
													<FamilyName>Yi</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Zhao</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Duan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Zhou</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Qin</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">The diploid genome sequence of an Asian individual.</ArticleTitle>
												<JournalTitle>Nature</JournalTitle>
												<VolumeID>456</VolumeID>
												<FirstPage>60</FirstPage>
												<LastPage>65</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/nature07484</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR11">
											<CitationNumber>11.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Campagna</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Albiero</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Bilardi</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Caniato</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Forcato</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Manavski</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>N</Initials>
													<FamilyName>Vitulo</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Valle</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">PASS: a program to align short sequences.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>967</FirstPage>
												<LastPage>968</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btp087</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR12">
											<CitationNumber>12.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Coarfa</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Milosavljevic</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Pash 2.0: scaleable sequence anchoring for next-generation sequencing technologies.</ArticleTitle>
												<JournalTitle>Pac Symp Biocomput</JournalTitle>
												<VolumeID>13</VolumeID>
												<FirstPage>102</FirstPage>
												<LastPage>113</LastPage>
											</BibArticle>
										</Citation>
										<Citation ID="CR13">
											<CitationNumber>13.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>HL</Initials>
													<FamilyName>Eaves</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Gao</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">MOM: maximum oligonucleotide mapping.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>969</FirstPage>
												<LastPage>970</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btp092</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR14">
											<CitationNumber>14.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>F</Initials>
													<FamilyName>Hormozdiari</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Alkan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>EE</Initials>
													<FamilyName>Eichler</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SC</Initials>
													<FamilyName>Sahinalp</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Combinatorial algorithms for structural variation detection in high-throughput sequenced genomes.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>19</VolumeID>
												<FirstPage>1270</FirstPage>
												<LastPage>1278</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.088633.108</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR15">
											<CitationNumber>15.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Jiang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>WH</Initials>
													<FamilyName>Wong</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">SeqMap: mapping massive amount of oligonucleotides to the genome.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>24</VolumeID>
												<FirstPage>2395</FirstPage>
												<LastPage>2396</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btn429</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR16">
											<CitationNumber>16.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Langmead</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Trapnell</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Pop</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SL</Initials>
													<FamilyName>Salzberg</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Ultrafast and memory-efficient alignment of short DNA sequences to the human genome.</ArticleTitle>
												<JournalTitle>Genome Biol</JournalTitle>
												<VolumeID>10</VolumeID>
												<FirstPage>R25</FirstPage>
												<LastPage/>
												<Occurrence Type="DOI">
													<Handle>10.1186/gb-2009-10-3-r25</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR17">
											<CitationNumber>17.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Durbin</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Fast and accurate short read alignment with Burrows-Wheeler transform.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>1754</FirstPage>
												<LastPage>1760</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btp324</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR18">
											<CitationNumber>18.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Ruan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Durbin</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Mapping short DNA sequencing reads and calling variants using mapping quality scores.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>18</VolumeID>
												<FirstPage>1851</FirstPage>
												<LastPage>1858</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.078212.108</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR19">
											<CitationNumber>19.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Kristiansen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">SOAP: short oligonucleotide alignment program.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>24</VolumeID>
												<FirstPage>713</FirstPage>
												<LastPage>714</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btn025</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR20">
											<CitationNumber>20.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Yu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TW</Initials>
													<FamilyName>Lam</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SM</Initials>
													<FamilyName>Yiu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Kristiansen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">SOAP2: an improved ultrafast tool for short read alignment.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>1966</FirstPage>
												<LastPage>1967</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btp336</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR21">
											<CitationNumber>21.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>N</Initials>
													<FamilyName>Malhis</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>YS</Initials>
													<FamilyName>Butterfield</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Ester</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SJ</Initials>
													<FamilyName>Jones</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Slider: maximum use of probability information for alignment of short sequence reads and SNP detection.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>6</FirstPage>
												<LastPage>13</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btn565</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR22">
											<CitationNumber>22.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Ning</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AJ</Initials>
													<FamilyName>Cox</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JC</Initials>
													<FamilyName>Mullikin</FamilyName>
												</BibAuthorName>
												<Year>2001</Year>
												<ArticleTitle Language="En">SSAHA: a fast search method for large DNA databases.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>11</VolumeID>
												<FirstPage>1725</FirstPage>
												<LastPage>1729</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.194201</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR23">
											<CitationNumber>23.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Prüfer</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>U</Initials>
													<FamilyName>Stenzel</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Dannemann</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RE</Initials>
													<FamilyName>Green</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Lachmann</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Kelso</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">PatMaN: rapid alignment of short sequences to large databases.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>24</VolumeID>
												<FirstPage>1530</FirstPage>
												<LastPage>1531</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btn223</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR24">
											<CitationNumber>24.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SM</Initials>
													<FamilyName>Rumble</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Lacroute</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>AV</Initials>
													<FamilyName>Dalca</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Fiume</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Sidow</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Brudno</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">SHRiMP: accurate mapping of short color-space reads.</ArticleTitle>
												<JournalTitle>PLoS Comput Biol</JournalTitle>
												<VolumeID>5</VolumeID>
												<FirstPage>e1000386</FirstPage>
												<LastPage/>
												<Occurrence Type="DOI">
													<Handle>10.1371/journal.pcbi.1000386</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR25">
											<CitationNumber>25.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>MC</Initials>
													<FamilyName>Schatz</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">CloudBurst: highly sensitive read mapping with MapReduce.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>25</VolumeID>
												<FirstPage>1363</FirstPage>
												<LastPage>1369</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btp236</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR26">
											<CitationNumber>26.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>AD</Initials>
													<FamilyName>Smith</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Xuan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MQ</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Using quality scores and longer reads improves accuracy of Solexa read mapping.</ArticleTitle>
												<JournalTitle>BMC Bioinformatics</JournalTitle>
												<VolumeID>9</VolumeID>
												<FirstPage>128</FirstPage>
												<LastPage/>
												<Occurrence Type="DOI">
													<Handle>10.1186/1471-2105-9-128</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR27">
											<CitationNumber>27.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Weese</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Emde</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Rausch</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Döring</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Reinert</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">RazerS: fast read mapping with sensitivity control.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>19</VolumeID>
												<FirstPage>1646</FirstPage>
												<LastPage>1654</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.088823.108</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR28">
											<CitationNumber>28.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Alkan</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JM</Initials>
													<FamilyName>Kidd</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Marques-Bonet</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Aksay</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>F</Initials>
													<FamilyName>Antonacci</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>F</Initials>
													<FamilyName>Hormozdiari</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JO</Initials>
													<FamilyName>Kitzman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Baker</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Malig</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>O</Initials>
													<FamilyName>Mutlu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>SC</Initials>
													<FamilyName>Sahinalp</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RA</Initials>
													<FamilyName>Gibbs</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>EE</Initials>
													<FamilyName>Eichler</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Personalized copy number and segmental duplication maps using next-generation sequencing.</ArticleTitle>
												<JournalTitle>Nat Genet</JournalTitle>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR29">
											<CitationNumber>29.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Lin</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Z</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>MQ</Initials>
													<FamilyName>Zhang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Ma</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">ZOOM! Zillions of oligos mapped.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>24</VolumeID>
												<FirstPage>2431</FirstPage>
												<LastPage>2437</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/btn416</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR30">
											<CitationNumber>30.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>RM</Initials>
													<FamilyName>Clark</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Schweikert</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Toomajian</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Ossowski</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Zeller</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Shinn</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>N</Initials>
													<FamilyName>Warthmann</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TT</Initials>
													<FamilyName>Hu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Fu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DA</Initials>
													<FamilyName>Hinds</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Chen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>KA</Initials>
													<FamilyName>Frazer</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DH</Initials>
													<FamilyName>Huson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Schölkopf</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Nordborg</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Rätsch</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JR</Initials>
													<FamilyName>Ecker</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Weigel</FamilyName>
												</BibAuthorName>
												<Year>2007</Year>
												<ArticleTitle Language="En">Common sequence polymorphisms shaping genetic diversity in <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> .</ArticleTitle>
												<JournalTitle>Science</JournalTitle>
												<VolumeID>317</VolumeID>
												<FirstPage>338</FirstPage>
												<LastPage>342</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1126/science.1138632</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR31">
											<CitationNumber>31.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Zeller</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RM</Initials>
													<FamilyName>Clark</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Schneeberger</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>A</Initials>
													<FamilyName>Bohlen</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Weigel</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>G</Initials>
													<FamilyName>Rätsch</FamilyName>
												</BibAuthorName>
												<Year>2008</Year>
												<ArticleTitle Language="En">Detecting polymorphic regions in <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> with resequencing microarrays.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>18</VolumeID>
												<FirstPage>918</FirstPage>
												<LastPage>929</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.070169.107</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR32">
											<CitationNumber>32.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>D</Initials>
													<FamilyName>Weigel</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Mott</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">The 1001 Genomes Project for <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> .</ArticleTitle>
												<JournalTitle>Genome Biol</JournalTitle>
												<VolumeID>10</VolumeID>
												<FirstPage>107</FirstPage>
												<LastPage/>
												<Occurrence Type="DOI">
													<Handle>10.1186/gb-2009-10-5-107</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR33">
											<CitationNumber>33.</CitationNumber>
											<BibArticle>
												<Year/>
												<ArticleTitle Language="En">Arabidopsis thaliana 1001 Genomes Project</ArticleTitle>
												<JournalTitle/>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR34">
											<CitationNumber>34.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Ma</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Tromp</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Li</FamilyName>
												</BibAuthorName>
												<Year>2002</Year>
												<ArticleTitle Language="En">PatternHunter: faster and more sensitive homology search.</ArticleTitle>
												<JournalTitle>Bioinformatics</JournalTitle>
												<VolumeID>18</VolumeID>
												<FirstPage>440</FirstPage>
												<LastPage>445</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1093/bioinformatics/18.3.440</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR35">
											<CitationNumber>35.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>SB</Initials>
													<FamilyName>Needleman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>CD</Initials>
													<FamilyName>Wunsch</FamilyName>
												</BibAuthorName>
												<Year>1970</Year>
												<ArticleTitle Language="En">A general method applicable to the search for similarities in the amino acid sequence of two proteins.</ArticleTitle>
												<JournalTitle>J Mol Biol</JournalTitle>
												<VolumeID>48</VolumeID>
												<FirstPage>443</FirstPage>
												<LastPage>453</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1016/0022-2836(70)90057-4</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR36">
											<CitationNumber>36.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Ukkonen</FamilyName>
												</BibAuthorName>
												<Year>1992</Year>
												<ArticleTitle Language="En">Approximate string-matching with <Emphasis Type="Italic">q</Emphasis> -grams and maximal matches.</ArticleTitle>
												<JournalTitle>Theoret Computer Sci</JournalTitle>
												<VolumeID>92</VolumeID>
												<FirstPage>191</FirstPage>
												<LastPage>211</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1016/0304-3975(92)90143-4</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR37">
											<CitationNumber>37.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Nusbaum</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TK</Initials>
													<FamilyName>Ohsumi</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Gomez</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Aquadro</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TC</Initials>
													<FamilyName>Victor</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>RM</Initials>
													<FamilyName>Warren</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DT</Initials>
													<FamilyName>Hung</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>BW</Initials>
													<FamilyName>Birren</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>ES</Initials>
													<FamilyName>Lander</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>DB</Initials>
													<FamilyName>Jaffe</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Sensitive, specific polymorphism discovery in bacteria using massively parallel sequencing.</ArticleTitle>
												<JournalTitle>Nature Methods</JournalTitle>
												<VolumeID>6</VolumeID>
												<FirstPage>67</FirstPage>
												<LastPage>69</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/nmeth.1286</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR38">
											<CitationNumber>38.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials/>
													<FamilyName/>
												</BibAuthorName>
												<Year>2000</Year>
												<ArticleTitle Language="En">Analysis of the genome sequence of the flowering plant <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> .</ArticleTitle>
												<JournalTitle>Nature</JournalTitle>
												<VolumeID>408</VolumeID>
												<FirstPage>796</FirstPage>
												<LastPage>815</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1038/35048692</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR39">
											<CitationNumber>39.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Nordborg</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>TT</Initials>
													<FamilyName>Hu</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Ishino</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Jhaveri</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Toomajian</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>H</Initials>
													<FamilyName>Zheng</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>E</Initials>
													<FamilyName>Bakker</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>P</Initials>
													<FamilyName>Calabrese</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Gladstone</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Goyal</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Jakobsson</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>S</Initials>
													<FamilyName>Kim</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>Y</Initials>
													<FamilyName>Morozov</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>B</Initials>
													<FamilyName>Padhukasahasram</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>V</Initials>
													<FamilyName>Plagnol</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>NA</Initials>
													<FamilyName>Rosenberg</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>C</Initials>
													<FamilyName>Shah</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JD</Initials>
													<FamilyName>Wall</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Wang</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>K</Initials>
													<FamilyName>Zhao</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>T</Initials>
													<FamilyName>Kalbfleisch</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>V</Initials>
													<FamilyName>Schulz</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>M</Initials>
													<FamilyName>Kreitman</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>J</Initials>
													<FamilyName>Bergelson</FamilyName>
												</BibAuthorName>
												<Year>2005</Year>
												<ArticleTitle Language="En">The pattern of polymorphism in <Emphasis Type="Italic">Arabidopsis thaliana</Emphasis> .</ArticleTitle>
												<JournalTitle>PLoS Biology</JournalTitle>
												<VolumeID>3</VolumeID>
												<FirstPage>e196</FirstPage>
												<LastPage/>
												<Occurrence Type="DOI">
													<Handle>10.1371/journal.pbio.0030196</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
										<Citation ID="CR40">
											<CitationNumber>40.</CitationNumber>
											<BibArticle>
												<Year/>
												<ArticleTitle Language="En">TAIR</ArticleTitle>
												<JournalTitle/>
												<VolumeID/>
												<FirstPage/>
												<LastPage/>
											</BibArticle>
										</Citation>
										<Citation ID="CR41">
											<CitationNumber>41.</CitationNumber>
											<BibArticle>
												<BibAuthorName>
													<Initials>R</Initials>
													<FamilyName>Lister</FamilyName>
												</BibAuthorName>
												<BibAuthorName>
													<Initials>JR</Initials>
													<FamilyName>Ecker</FamilyName>
												</BibAuthorName>
												<Year>2009</Year>
												<ArticleTitle Language="En">Finding the fifth base: genome-wide sequencing of cytosine methylation.</ArticleTitle>
												<JournalTitle>Genome Res</JournalTitle>
												<VolumeID>19</VolumeID>
												<FirstPage>959</FirstPage>
												<LastPage>966</LastPage>
												<Occurrence Type="DOI">
													<Handle>10.1101/gr.083451.108</Handle>
												</Occurrence>
											</BibArticle>
										</Citation>
									</Bibliography>
								</ArticleBackmatter>
							</Article>
						</Issue>
					</Volume>
				</Journal>
				<meta:Info xmlns:meta="http://www.springer.com/app/meta">
					<meta:DateLoaded>2010-05-13T19:54:20.045705+02:00</meta:DateLoaded>
					<meta:Authors>
						<meta:Author>Schneeberger, Korbinian</meta:Author>
						<meta:Author>Hagmann, Jörg</meta:Author>
						<meta:Author>Ossowski, Stephan</meta:Author>
						<meta:Author>Warthmann, Norman</meta:Author>
						<meta:Author>Gesing, Sandra</meta:Author>
						<meta:Author>Kohlbacher, Oliver</meta:Author>
						<meta:Author>Weigel, Detlef</meta:Author>
					</meta:Authors>
					<meta:Institutions>
						<meta:Institution geo="13.4113999,52.5234051,0">
							<meta:OrgName>Department of Molecular Biology</meta:OrgName>
							<meta:GeoOrg>13.4113999,52.5234051,0#Department of Molecular Biology</meta:GeoOrg>
							<meta:Country>Germany</meta:Country>
						</meta:Institution>
						<meta:Institution geo="8.680239,49.401929,0">
							<meta:OrgName>German Cancer Research Center (dkfz)</meta:OrgName>
							<meta:GeoOrg>8.680239,49.401929,0#German Cancer Research Center (dkfz)</meta:GeoOrg>
							<meta:Country>Germany</meta:Country>
						</meta:Institution>
					</meta:Institutions>
					<meta:Date>2009-09-17</meta:Date>
					<meta:Type>Article</meta:Type>
					<meta:DOI>10.1186/gb-2009-10-9-r98</meta:DOI>
					<meta:Title>Simultaneous alignment of short reads against multiple genomes</meta:Title>
					<meta:ISXN>1465-6906</meta:ISXN>
					<meta:Journal>Genome Biology</meta:Journal>
					<meta:PubName>BioMed Central</meta:PubName>
					<meta:ArticleFirstPage>R98</meta:ArticleFirstPage>
					<meta:Publication>Genome Biology</meta:Publication>
					<meta:PublicationType>Journal</meta:PublicationType>
					<meta:SubjectGroup>
						<meta:Subject Type="Primary">Life Sciences</meta:Subject>
						<meta:Subject Priority="1" Type="Secondary">Life Sciences, general</meta:Subject>
						<meta:Subject Priority="2" Type="Secondary">Biomedicine general</meta:Subject>
						<meta:Subject Priority="3" Type="Secondary">Human Genetics</meta:Subject>
						<meta:Subject Priority="4" Type="Secondary">Animal Genetics and Genomics</meta:Subject>
						<meta:Subject Priority="5" Type="Secondary">Microbial Genetics and Genomics</meta:Subject>
						<meta:Subject Priority="6" Type="Secondary">Plant Genetics &amp; Genomics</meta:Subject>
						<meta:Subject Priority="7" Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>
						<meta:Subject Priority="8" Type="Secondary">Mathematical Biology in General</meta:Subject>
					</meta:SubjectGroup>
				</meta:Info>
			</Publisher>
			<Images/>
		</result>
	</records>
	<facets>
		<facet name="subject">
			<facet-value count="3">Computational Biology/Bioinformatics</facet-value>
			<facet-value count="3">Computer Science</facet-value>
			<facet-value count="3">Life Sciences</facet-value>
			<facet-value count="2">Algorithms</facet-value>
			<facet-value count="2">Bioinformatics</facet-value>
			<facet-value count="2">Combinatorial Libraries</facet-value>
			<facet-value count="2">Computer Appl. in Life Sciences</facet-value>
			<facet-value count="2">Computer Communication Networks</facet-value>
			<facet-value count="2">Microarrays</facet-value>
			<facet-value count="2">Processor Architectures</facet-value>
			<facet-value count="1">Animal Genetics and Genomics</facet-value>
			<facet-value count="1">Biomedicine general</facet-value>
			<facet-value count="1">Business Information Systems</facet-value>
			<facet-value count="1">Computer Applications</facet-value>
			<facet-value count="1">Computer Science, general</facet-value>
			<facet-value count="1">Computer Systems Organization and Communication Networks</facet-value>
			<facet-value count="1">Data Structures, Cryptology and Information Theory</facet-value>
			<facet-value count="1">Human Genetics</facet-value>
			<facet-value count="1">Information Systems and Communication Service</facet-value>
			<facet-value count="1">Life Sciences, general</facet-value>
		</facet>
		<facet name="keyword">
			<facet-value count="2">Parallel processing</facet-value>
			<facet-value count="1">abstracts?</facet-value>
			<facet-value count="1">against</facet-value>
			<facet-value count="1">algorithm</facet-value>
			<facet-value count="1">alignment</facet-value>
			<facet-value count="1">analyzing</facet-value>
			<facet-value count="1">Bagging</facet-value>
			<facet-value count="1">collections</facet-value>
			<facet-value count="1">Computer vision</facet-value>
			<facet-value count="1">Data-intensive computing</facet-value>
			<facet-value count="1">Diamond</facet-value>
			<facet-value count="1">Distributed systems</facet-value>
			<facet-value count="1">effective</facet-value>
			<facet-value count="1">efficient</facet-value>
			<facet-value count="1">evolutionary</facet-value>
			<facet-value count="1">full</facet-value>
			<facet-value count="1">genomes</facet-value>
			<facet-value count="1">Human-in-the-loop</facet-value>
			<facet-value count="1">I/O workloads</facet-value>
			<facet-value count="1">ImageJ</facet-value>
		</facet>
		<facet name="pub">
			<facet-value count="2">BMC Bioinformatics</facet-value>
			<facet-value count="1">Genome Biology</facet-value>
			<facet-value count="1">Journal of Internet Services and Applications</facet-value>
			<facet-value count="1">Multimedia Tools and Applications</facet-value>
			<facet-value count="1">The Journal of Supercomputing</facet-value>
		</facet>
		<facet name="year">
			<facet-value count="1">2011</facet-value>
			<facet-value count="3">2010</facet-value>
			<facet-value count="2">2009</facet-value>
		</facet>
		<facet name="type">
			<facet-value count="6">Journal</facet-value>
		</facet>
		<facet name="country">
			<facet-value count="2">United States</facet-value>
			<facet-value count="1">Germany</facet-value>
			<facet-value count="1">Japan</facet-value>
			<facet-value count="1">Norway</facet-value>
		</facet>
	</facets>
</response>
