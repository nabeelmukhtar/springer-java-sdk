<?xml version="1.0" encoding="UTF-8"?>
<response>

	<query>algorithm </query>

	<apiKey>jqs3v62pb9q84z87b98mem59</apiKey>

	<result>

		<total>17145</total>

		<start>1</start>

		<pageLength>10</pageLength>

	</result>

	<records>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1471-2105</JournalID>


						<JournalPrintISSN>1471-2105</JournalPrintISSN>


						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>


						<JournalTitle>BMC Bioinformatics</JournalTitle>


						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Bioinformatics</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Microarrays</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences </JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Combinatorial Libraries</JournalSubject>


							<JournalSubject Priority="6"
Type="Secondary">Algorithms</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>7</VolumeIDStart>


							<VolumeIDEnd>7</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>1</IssueIDStart>


								<IssueIDEnd>1</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2006</Year>


										<Month>7</Month>


										<Day>13</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Hu et al; licensee BioMed Central Ltd.</CopyrightHolderName>


									<CopyrightYear>2006</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1471-2105-7-342</ArticleID>


									<ArticleDOI>10.1186/1471-2105-7-342</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Research article</ArticleCategory>


									<ArticleFirstPage>342</ArticleFirstPage>


									<ArticleLastPage>342</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year>2005</Year>


											<Month>8</Month>


											<Day>26</Day>


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year>2006</Year>


											<Month>7</Month>


											<Day>13</Day>


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Hu et al; licensee BioMed Central Ltd.</CopyrightHolderName>


										<CopyrightYear>2006</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>7</VolumeIDStart>


										<VolumeIDEnd>7</VolumeIDEnd>


										<IssueIDStart>1</IssueIDStart>


										<IssueIDEnd>1</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Jianjun</GivenName>


												<FamilyName>Hu</FamilyName>


											</AuthorName>


											<Contact>

												<Email>hu5@purdue.edu</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I2">

											<AuthorName DisplayOrder="Western">

												<GivenName>Yifeng</GivenName>


												<FamilyName>Yang</FamilyName>


											</AuthorName>


											<Contact>

												<Email>yang41@purdue.edu</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I2 I1 I3 I4">

											<AuthorName DisplayOrder="Western">

												<GivenName>Daisuke</GivenName>


												<FamilyName>Kihara</FamilyName>


											</AuthorName>


											<Contact>

												<Email>dkihara@purdue.edu</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I2">

											<OrgName>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I3">

											<OrgName>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I4">

											<OrgName>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>Understanding gene regulatory networks has become one of the central research problems in bioinformatics. More than thirty algorithms have been proposed to identify DNA regulatory sites during the past thirty years. However, the prediction accuracy of these algorithms is still quite low. Ensemble algorithms have emerged as an effective strategy in bioinformatics for improving the prediction accuracy by exploiting the synergetic prediction capability of multiple algorithms.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>We proposed a novel clustering-based ensemble algorithm named EMD for 

												<Emphasis
Type="Italic">de novo</Emphasis>
 motif discovery by combining multiple predictions from multiple runs of one or more base component algorithms. The ensemble approach is applied to the motif discovery problem for the first time. The algorithm is tested on a benchmark dataset generated from 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 RegulonDB. The EMD algorithm has achieved 22.4% improvement in terms of the nucleotide level prediction accuracy over the best stand-alone component algorithm. The advantage of the EMD algorithm is more significant for shorter input sequences, but most importantly, it always outperforms or at least stays at the same performance level of the stand-alone component algorithms even for longer sequences.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusion</Heading>


											<Para>We proposed an ensemble approach for the motif discovery problem by taking advantage of the availability of a large number of motif discovery programs. We have shown that the ensemble approach is an effective strategy for improving both sensitivity and specificity, thus the accuracy of the prediction. The advantage of the EMD algorithm is its flexibility in the sense that a new powerful algorithm can be easily added to the system.</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Keyword>ensemble</Keyword>


										<Keyword>discovering</Keyword>


										<Keyword>regulatory</Keyword>


										<Keyword>sequences</Keyword>


										<Keyword>DNA</Keyword>


										<Keyword>motifs</Keyword>


										<Keyword>algorithm</Keyword>


										<Keyword>EMD</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_79883">

										<Heading>Background</Heading>


										<Para>Identifying gene regulatory and gene expression networks has become a central problem of post-genomics biology 

											<CitationRef
CitationID="B1">1</CitationRef>


											<CitationRef
CitationID="B2">2</CitationRef>


											<CitationRef
CitationID="B3">3</CitationRef>
 . Computational prediction of DNA regulatory elements or binding sites of transcription factors is one of the essential parts of the problem. This regulatory motif discovery problem has been studied since the early years of bioinformatics, resulting more than thirty algorithms proposed, among which more than a dozen are publicly available 

											<CitationRef
CitationID="B4">4</CitationRef>


											<CitationRef
CitationID="B5">5</CitationRef>
 . However, recent comprehensive evaluations of existing motif discovery programs show that their prediction accuracy is still very low 

											<CitationRef
CitationID="B4">4</CitationRef>


											<CitationRef
CitationID="B6">6</CitationRef>
 .
										</Para>


										<Para>The technical difficulty resides in the low signal/noise ratio of this problem. A straightforward direction to improve the prediction accuracy is to use a better motif model to capture characteristics of sequence patterns of regulatory motifs ( 

											<Emphasis Type="Italic">e.g</Emphasis>
 . position-dependence information can be captured by a Hidden Markov model 

											<CitationRef
CitationID="B7">7</CitationRef>
 or by considering local pairwise nucleotide frequency 

											<CitationRef
CitationID="B8">8</CitationRef>
 , rather than a conventional position specific scoring matrix 

											<CitationRef
CitationID="B9">9</CitationRef>
 ) and a better search algorithm in the sequence space ( 

											<Emphasis Type="Italic">e.g</Emphasis>
 . Expectation Maximization 

											<CitationRef
CitationID="B10">10</CitationRef>
 or Gibbs sampling 

											<CitationRef
CitationID="B11">11</CitationRef>
 ). Another approach is to incorporate additional information, such as phylogenetic trees or homologous sequences 

											<CitationRef
CitationID="B12">12</CitationRef>


											<CitationRef
CitationID="B13">13</CitationRef>


											<CitationRef
CitationID="B14">14</CitationRef>


											<CitationRef
CitationID="B15">15</CitationRef>
 . Comparative genomics 

											<CitationRef
CitationID="B16">16</CitationRef>
 and gene expression data 

											<CitationRef
CitationID="B17">17</CitationRef>
 have also been used to improve the specificity of motif discovery.
										</Para>


										<Para>Here, we introduce another practical and powerful strategy for the motif discovery problem, that is, the ensemble approach, which is also called the meta-server approach or the jury-method. The fundamental idea of the ensemble approach is to run several different programs (multiple times) and summarize their outputs to generate the final output. Ensemble algorithms have been applied in several prediction methods in bioinformatics, such as gene prediction 

											<CitationRef
CitationID="B18">18</CitationRef>
 , protein tertiary structure prediction 

											<CitationRef
CitationID="B19">19</CitationRef>


											<CitationRef
CitationID="B20">20</CitationRef>
 , protein domain prediction 

											<CitationRef
CitationID="B21">21</CitationRef>
 and protein secondary structure prediction 

											<CitationRef
CitationID="B22">22</CitationRef>


											<CitationRef
CitationID="B23">23</CitationRef>
 . The most remarkable success of the ensemble approach would be the several meta-servers which participated in the biennial world-wide protein structure prediction contest, CASP (Critical Assessment of Techniques for Protein Structure Prediction), in 2002 and 2004 

											<CitationRef
CitationID="B24">24</CitationRef>


											<CitationRef
CitationID="B25">25</CitationRef>


											<CitationRef
CitationID="B26">26</CitationRef>


											<CitationRef
CitationID="B27">27</CitationRef>
 , which dominated the top ranks in the competition. The success of the ensemble approaches has been attributed to several factors. Albrecht et al. 

											<CitationRef
CitationID="B22">22</CitationRef>
 referred their success to the noise-filtering properties of the ensemble approach, which damp the training errors of single methods. Lundström 

											<Emphasis Type="Italic">et al</Emphasis>
 . 

											<CitationRef
CitationID="B28">28</CitationRef>
 discussed that a key for the success of an ensemble approach is to properly measure the similarity between the different models. Most ensemble algorithms use the same type of input data to make final predictions. In contrast, the ensemble algorithm by Sen 

											<Emphasis Type="Italic">et al</Emphasis>
 . 

											<CitationRef
CitationID="B29">29</CitationRef>
 combined different types of data, data mining results, threading, phylogenetic tree-based conserved residue prediction, and the structural alignment based prediction method to predict a protein-protein interaction site in a query protein tertiary structure. Despite the wide range of applications of the ensemble approach in bioinformatics, to the best of our knowledge, there is no extensive study of ensemble algorithms for the motif discovery problem.
										</Para>


										<Para>In our previous work 

											<CitationRef
CitationID="B6">6</CitationRef>
 , we showed anecdotal evidence that a simple ensemble motif discovery algorithm, called CEA, outperforms single stand-alone algorithms. At this juncture, it would be appropriate to clarify the differences between the previous work and the current work, called the EMD algorithm. In the previous work, 1) only the combination of multiple runs of an identical algorithm was considered; and 2) only one data set with a short sequence size (50 nt. long sequence added to both sides of each of known target site) was used in the benchmark. In this work: 1) We extend our ensemble approach by systematically combining predictions from five popular motif discovery algorithms, namely, AlignACE 

											<CitationRef
CitationID="B30">30</CitationRef>
 , BioProspector 

											<CitationRef
CitationID="B31">31</CitationRef>
 , MDScan 

											<CitationRef
CitationID="B32">32</CitationRef>
 , MEME 

											<CitationRef
CitationID="B33">33</CitationRef>
 , and MotifSampler 

											<CitationRef
CitationID="B34">34</CitationRef>
 . In addition, we integrated Projection 

											<CitationRef
CitationID="B35">35</CitationRef>
 to seek further improvement in terms of the scalability of EMD. All the possible combinations of one to five component algorithms are examined. 2) To be able to combine predictions of different runs from different component algorithms, a novel ensemble algorithm, EMD, is developed. 3) EMD is tested on two different types of data sets. One data set is generated from the intergenic regions of the 

											<Emphasis Type="Italic">E. coli</Emphasis>
 genome, and the other is input sequences of different lengths generated by adding margins of different sizes to each known site. The best ensemble algorithm performed 22.4% better than the best single component algorithm in terms of the nucleotide level accuracy.
										</Para>


									</Section1>


									<Section1 ID="Sec_82970">

										<Heading>Results</Heading>


										<Para>We developed a series of the EMD algorithms with all the possible combinations of two to five component algorithms. An EMD algorithm runs its component algorithms multiple times independently, and summarizes their results basically by majority (Fig. 

											<InternalRef RefID="F1">1</InternalRef>
 ). The potential of an EMD algorithm lies in the fact that it could take advantage of superb predictions of every component algorithm. The five component algorithms used are AlignACE (AL) 

											<CitationRef
CitationID="B30">30</CitationRef>
 , BioProspector (BP) 

											<CitationRef
CitationID="B31">31</CitationRef>
 , MDScan (MD) 

											<CitationRef
CitationID="B32">32</CitationRef>
 , MEME (ME) 

											<CitationRef
CitationID="B33">33</CitationRef>
 , and MotifSampler (MS) 

											<CitationRef
CitationID="B34">34</CitationRef>
 . Below in the manuscript EMD-X denotes a set of EMD algorithms with all the possible combinations of X number of component algorithms. A multi-restart algorithm and random algorithm are also included as comparison bases. We name different ensemble algorithms by concatenating the abbreviations of the component algorithms: For example, AL-BP is an EMD algorithm with AlignACE (AL) and BioProspector (BP) used as the component algorithms.
										</Para>


										<Figure Category="Standard" Float="No"
ID="F1">

											<Caption Language="En">

												<CaptionContent>

													<SimplePara>Overview of the EMD algorithm</SimplePara>


												</CaptionContent>


											</Caption>


											<MediaObject>

												<ImageObject Color="Color"
FileRef="1471-2105-7-342-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


												<TextObject>

													<Para>Overview of the EMD algorithm. After each component algorithm is run 

														<Emphasis
Type="Italic">R</Emphasis>
 times for an input sequence data set, 

														<Emphasis
Type="Italic">K</Emphasis>
 motifs are collected from each run. The right side of the figure illustrates the grouping phase of the algorithm for the sequence number 1 and the final prediction of sites for the site group number 1 of the input sequence No. 1. See the text for the details.
													</Para>


												</TextObject>


											</MediaObject>


										</Figure>


										<Section2 ID="Sec_18385">

											<Heading>Results of EMD on ECRDB62A intergenic sequence data set</Heading>


											<Para>In Table 

												<InternalRef RefID="T1">1</InternalRef>
 , the performance of the best ensemble algorithm in EMD-2 to EMD-5 is compared with that of the five stand-alone component algorithms, five multi-restart algorithms (RS-XX) and a random algorithm on the intergenic sequence data set. The performance is evaluated by the nucleotide level and site level accuracy in terms of the performance coefficient (n/sPC), sensitivity (n/sSn) and specificity (n/sSp). First, the nucleotide level performance coefficient (nPC) is very low for single component algorithms. Even for the best algorithm, BioProspector, nPC does not exceed 0.18. It is remarkable that all of the four ensemble algorithms outperform the best single component algorithm in terms of nPC. EMD-AL-BP-MD is the best ensemble algorithm, achieving an nPC score of 0.213. This is a 22.4% improvement over the best single algorithm, BioProspector, whose nPC is 0.174. Note here that the improvement of EMD over its component algorithms comes from an increase of both the sensitivity and specificity. Surprisingly, the performance of the multi-restart algorithms is worse than that of the corresponding single component algorithm in each case ( 

												<Emphasis Type="Italic">e.g</Emphasis>
 . RS-AL is worse than AlignACE). nPC, nSn and nSp all dropped. This is because the score given to predicted motifs by the algorithms does not always reflect the accuracy well 

												<CitationRef
CitationID="B6">6</CitationRef>
 and also collecting the highest-scoring motifs from multiple runs can result in only picking up similar or identical motif predictions and neglecting the sub-optimal motifs. This observation highlights the necessity of reporting multiple top-scored motif predictions rather than reporting a single top predicted motif. The comparison of the EMD algorithms with the multiple-restart algorithms illustrates that the improved performance of the EMD algorithms is not simply due to the increased number of runs of component algorithms but a synergetic effect of the multiple runs.
											</Para>


											<Table Float="No" ID="T1">

												<Caption Language="En">

													<CaptionNumber>Table 1</CaptionNumber>


													<CaptionContent>

														<SimplePara>The prediction accuracy tested on ECRDB62A set.</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="7">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara>Algorithm</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>nPC</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>nSn</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>nSp</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>sPC</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>sSn</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>sSp</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>BP-MD 

																	<Superscript>a)</Superscript>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.183</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.215</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.280</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.303</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.428</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.407</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">AL-BP-MD</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.213</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.262</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.296</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.324</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.456</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.437</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-BP-MD-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.209</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.255</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.293</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.321</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.423</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.446</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-BP-MD-ME-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.197</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.238</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.286</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.316</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.438</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.437</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AlignACE</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.141</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.218</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.171</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.264</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.351</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.396</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">BioProspector</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.174</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.205</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.268</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.287</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.415</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.369</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>MDScan</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.146</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.174</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.223</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.244</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.345</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.349</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">MEME</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.160</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.260</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.190</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.300</Emphasis>


																	<Superscript>d)</Superscript>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.440</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.430</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">MotifSampler</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.150</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.180</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.230</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.300</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.320</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.490</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>RS-AL 

																	<Superscript>b)</Superscript>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.139</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.204</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.166</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.229</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.329</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.341</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">RS-BP</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.150</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.178</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.231</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.262</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.390</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.350</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>RS-MD</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.107</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.125</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.169</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.170</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.254</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.271</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>RS-ME</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.133</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.162</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.203</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.213</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.418</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.282</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>RS-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.127</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.148</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.187</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.235</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.260</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.384</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Random 

																	<Superscript>c)</Superscript>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.050</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.061</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.083</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.100</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.161</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.146</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>a) The best algorithm among EMD-X (X = 2~5) are compared with component algorithms, b) the multi-restart algorithms, and c) the random algorithms. The best performances in terms of nPC or sPC among algorithms of a same category are highlighted in bold. d) Both MEME and MotifSampler are highlighted because they have the same performance in terms of sPC.</SimplePara>


												</tfooter>


											</Table>


											<Para>At the site level, the EMD-AL-BP-MD again achieves the highest prediction performance in sPC, sSn, and sSp. All of the EMD algorithms exceed 0.30 in terms of sPC. The same arguments above for the nucleotide level accuracy also hold for the site level performance.</Para>


										</Section2>


										<Section2 ID="Sec_06444">

											<Heading>Comparison of different combinations of component algorithms</Heading>


											<Para>With five component algorithms, there are a total of 31 unique combinations to compose an ensemble algorithm. Table 

												<InternalRef RefID="T2">2</InternalRef>
 shows the nPC scores for EMD algorithms with all possible combinations of component algorithms tested on ECRDB61B-200 data set (200 nt placed on both sides of known sites). Note that the four EMD algorithms in Table 

												<InternalRef RefID="T1">1</InternalRef>
 are the best EMD algorithms amongst those with the same number of employed component algorithms benchmarked on ECRDB62A dataset. Please also note that the algorithms shown in Table 

												<InternalRef RefID="T2">2</InternalRef>
 are all EMD algorithms even if some EMD algorithms only employ a single component algorithm; e.g. EMD-AL in Table 

												<InternalRef RefID="T2">2</InternalRef>
 , results of 20 runs of AlignACE are combined.
											</Para>


											<Table Float="No" ID="T2">

												<Caption Language="En">

													<CaptionNumber>Table 2</CaptionNumber>


													<CaptionContent>

														<SimplePara>Comparison of nucleotide level prediction accuracy (nPC) of EMD algorithms with different combinations of component algorithms 

															<Superscript>a)</Superscript>
 .
														</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="6">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara>Algorithm</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>nPC</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>Algorithm</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>nPC</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>Algorithm</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>nPC</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.178</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>AL-BP</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.213</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>AL-BP-MD</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.239</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>BP</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.201</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>AL-ME</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.203</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>AL-BP-ME</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.225</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">MD</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.228</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>AL-MS</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.208</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>AL-BP-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.213</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>ME</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.199</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>BP-MD</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.241</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">AL-MD-ME</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.250</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.206</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>BP-ME</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.214</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>AL-MD-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.241</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Average</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.202</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>ME-MS</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.220</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>AL-ME-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.220</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">AL-BP-MD-ME</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.254</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>BP-MS</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.207</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>BP-MD-ME</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.243</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-BP-MD-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.244</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>MD-ME</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.235</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>BP-MD-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.249</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-BP-ME-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.232</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>AL-MD</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.240</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>BP-ME-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.223</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-MD-ME-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.252</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">MD-MS</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.250</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>MD-ME-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.241</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>BP-MD-ME-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.248</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>Average</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.223</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>Average</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.234</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Average</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.246</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara>AL-BP-MD-ME-MS</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.254</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>a) Tested on ECRDB61B-200 data set. b) The best EMD algorithms among those of a given number of component algorithms are highlighted in bold.</SimplePara>


												</tfooter>


											</Table>


											<Para>Firstly, on average the accuracy improves as the number of component algorithms increases (Table 

												<InternalRef RefID="T2">2</InternalRef>
 ). The average accuracy of EMD algorithms with a single component algorithm is 0.202, and the average accuracy monotonically increases up to 0.254 by the EMD algorithm with five component algorithms. Secondly, the standard deviation of the accuracy among the EMD algorithms with the same number of component algorithms is small, which ranges from 0.008 (EMD-4) to 0.015 (EMD-1). These results prove the positive effect of the ensemble approach in the motif discovery problem. The standard deviation of EMD-X algorithms decreases as the number of component algorithms increases. It is observed that MD, which is the best algorithm in constructing EMD algorithms with a single component algorithm, is always involved in the best EMD algorithm among those with a higher number of component algorithms (EMD-2 to EMD-4). Interestingly, AL, which performed the worst in the single component EMD algorithm with nPC of 0.178, can contribute to improved accuracy. Indeed AL-MD-ME performed the best among the EMD-3. These results vividly show the synergetic effect of the EMD algorithm. Note here that the standard error of each EMD algorithm is very small. Indeed it is less than 0.003 for all of the cases, measured from twenty independent runs. This is consistent with our previous observation that the standard deviation of results (nPC) of the single component algorithms is very small 

												<CitationRef
CitationID="B6">6</CitationRef>
 .
											</Para>


										</Section2>


										<Section2 ID="Sec_07821">

											<Heading>Number of runs of component algorithms on EMD performance</Heading>


											<Para>The number of runs of each component algorithms is an important parameter in an EMD algorithm. It can affect the prediction performance and it also determines the required computational time. We examined the effect of the number of different runs of component algorithms in terms of the nucleotide level prediction accuracy (nPC) (Table 

												<InternalRef RefID="T3">3</InternalRef>
 ). In Table 

												<InternalRef RefID="T3">3</InternalRef>
 , the number of runs of the EMD algorithms composed of two component algorithms is changed from 5 to 20 with an interval of 5 runs. More generally, any combinations of different number of runs can be assigned to each component algorithm. However, the exhaustive combinations of different number of runs have not been tried here because the current study would be enough to observe the behavior of EMD algorithms and also because it is too computationally expensive.
											</Para>


											<Table Float="No" ID="T3">

												<Caption Language="En">

													<CaptionNumber>Table 3</CaptionNumber>


													<CaptionContent>

														<SimplePara>The performance (nPC) of EMD-2 algorithms with respect to the number of runs of its component algorithms 

															<Superscript>a)</Superscript>
 .
														</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="5">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara>Algorithm\No. of Runs</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>5</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>15</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>20</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-BP</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.219</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.216</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.219</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.213</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-ME</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.231</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.231</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.241</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.240</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.182</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.208</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.203</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.203</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>BP-MD</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.190</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.206</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.197</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.208</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>BP-ME</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.240</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.242</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.238</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.241</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>ME-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.210</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.216</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.214</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.213</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>BP-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.208</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.207</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.207</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.207</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>MD-ME</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.222</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.233</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.227</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.235</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>AL-MD</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.236</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.236</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.236</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.250</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>MD-MS</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.195</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.225</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.220</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.220</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Average</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.214</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.222</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.220</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">0.223</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>a) ECRDB61B-200 (margin size of 200) data set is used.b) The nPC value is in bold if it is the best performance among tested for that EMD algorithm.</SimplePara>


												</tfooter>


											</Table>


											<Para>From Table 

												<InternalRef RefID="T3">3</InternalRef>
 , it can be seen that on average, increasing the number of runs contributes to the improvement of the performance (see the average nPC value at the bottom row). This general trend does not necessarily apply to a particular EMD algorithm. The optimal number of runs for a particular EMD algorithm may be reflecting characteristics of component algorithms, and not very straightforward to determine. However, the general trend exists that the more number of runs increases the accuracy, and more importantly, the accuracy does not show a dramatic degeneration when the number of runs increases. Therefore, practically for the EMD algorithm, it is not inappropriate to set the number of runs to 20 times or probably any number between 10 and 20.
											</Para>


										</Section2>


										<Section2 ID="Sec_39235">

											<Heading>Scalability</Heading>


											<Para>We examined the scalability of the EMD algorithms in terms of the length of input sequences (Fig. 

												<InternalRef RefID="F2">2</InternalRef>
 ). One algorithm from each EMD-X (X ranges from 1 to 5) is examined. Input sequences of different length are prepared as ECRDB61B data set with different margin sizes ranging from 20 to 800 (these correspond to the sequence lengths ranging from approximately 60 to 1620 nt, because the sequence length is the total of two margins and a site width). For comparison, the results of the best individual component algorithm, MDScan (MD), and the best multi-restart algorithm of BioProspector (RS-BP) are also shown.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F2">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Scalability of the EMD algorithms</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-342-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Scalability of the EMD algorithms. The nucleotide level prediction performance was compared with the best base algorithm MDscan (MD) and the best multi-restart algorithm (RS-BP). This evaluation is done on ECRDB61B-200 data set. The y-axis shows the nucleotide level accuracy (nPC). The error bars are not shown because the standard error is very small (less than 0.003).</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>Figure 

												<InternalRef RefID="F2">2</InternalRef>
 shows that all of the EMD algorithms outperform MD and RS-BP in terms of nPC when the margin size is up to 200. Note that error bars are not drawn because the standard error of all EMD algorithms is less than 0.003 when computed over twenty runs. The performance of the RS-BP algorithm drops sharply as the margin size increases. This is consistent with the observation from Table 

												<InternalRef RefID="T1">1</InternalRef>
 that multi-restart algorithms perform the worst. For the margin size of 300 or longer, the performance of all the algorithms start to converge, and the performance of the EMD algorithms does not show a large improvement over single MD algorithm.
											</Para>


											<Para>One possible reason for the ineffectiveness of EMD algorithms for the data sets with longer sequences may be that only 20 runs of the component algorithms are not sufficient. To check whether increasing the number of runs can improve the results, we run the AL-BP-MD-MS algorithm with different numbers of runs ranging from 10 to 50 with a step size of 10. The corresponding nPC scores were 0.183, 0.187, 0.181, 0.185, and 0.187, respectively. Therefore, there is no significant performance improvement observed when up to fifty runs are conducted for each of the five component algorithms. To ameliorate the loss of advantage for longer margins, we have also added another algorithm, Projection 

												<CitationRef
CitationID="B35">35</CitationRef>
 , which has a good performance at longer padding sequence length, to EMD-BP-MD-MS algorithm (thus EMD-BP-MD-MS-PR algorithm is composed). The EMD-BP-MD-MS-PR did perform the best for a sequence of the margin size of 100, 200 and 300, but did not show a superior performance for a longer margin size.
											</Para>


											<Para>At this point, the scalability is still an issue for the future work, but note that the majority of the intergenic regions in 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 are short. Indeed the average length of the intergenic regions of 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 is 300 nt, and 95.2% of them are shorter than 420 nt (which corresponds to the sequences of a margin size of 200), where the EMD algorithms showed their superiority over the component algorithm (Fig 

												<InternalRef RefID="F2">2</InternalRef>
 ).
											</Para>


										</Section2>


										<Section2 ID="Sec_44123">

											<Heading>Performance on the dataset with shuffled margin sequences</Heading>


											<Para>We have carried out additional testing on a dataset with artificially shuffled margin sequences, named ECRDB61C (Tab. 

												<InternalRef RefID="T4">4</InternalRef>
 ). A drawback of the ECRDB62A and ECRDB61B datasets used above is that some input sequences contain multiple sites, so that the computed accuracy on the datasets may not precisely reflect the actual performance of the algorithms. In contrast, sequences in the ECRDB61C set has a target site in the middle of the sequence with artificially shuffled flanking sequences on both sides, thereby it is guaranteed that only one target site exists in a sequence. We tested AL-BP-MD-ME-MS, because this combination achieved the best nPC on ECRDB61B-200 data set (Tab. 

												<InternalRef RefID="T2">2</InternalRef>
 ). The results show that the algorithm performed better on the shuffled margin sequences (ECRDB61C) when the input sequence is short (the margin size of 100), but the performance difference vanishes as the input sequence length increases. The better specificity (nSp) largely contributed in the improvement in the nPC for ECRDB61C-100.
											</Para>


											<Table Float="No" ID="T4">

												<Caption Language="En">

													<CaptionNumber>Table 4</CaptionNumber>


													<CaptionContent>

														<SimplePara>The performance of AL-BP-MD-ME-MS on a dataset of shuffled margin sequences 

															<Superscript>a)</Superscript>
 .
														</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="10">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<colspec colname="c7" colnum="7" />


													<colspec colname="c8" colnum="8" />


													<colspec colname="c9" colnum="9" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara>nPC</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>nSn</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>nSp</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Margin size (nt)</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>200</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>200</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>200</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>400</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Original set 

																	<Superscript>b)</Superscript>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.288</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.254</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.197</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.328</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.292</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.234</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>0.416</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>0.360</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>0.270</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Shuffled set 

																	<Superscript>c)</Superscript>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.317</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.255</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.187</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.340</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.275</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.201</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>0.481</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>0.375</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>0.266</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>a) To the both sides of known sites in the ECRDB61B data set, artificially shuffled sequences with the size of 100, 200, and 400 nt are attached. The statistics of the di-mer nucleotide frequency used to generate the shuffled margin sequences are taken from intergenic regions of the 

														<Emphasis
Type="Italic">E. coli</Emphasis>
 genome.b) The performance on the original ECRDB61B-100, 200, 400 set.c) The performance on the data set with shuffled margin sequences.
													</SimplePara>


												</tfooter>


											</Table>


										</Section2>


									</Section1>


									<Section1 ID="Sec_95333">

										<Heading>Discussion</Heading>


										<Para>We have developed the EMD algorithm, a framework of an ensemble algorithm for regulatory site motif discovery. The ensemble approach has been successfully applied in several prediction methods in bioinformatics 

											<CitationRef
CitationID="B19">19</CitationRef>


											<CitationRef
CitationID="B22">22</CitationRef>


											<CitationRef
CitationID="B28">28</CitationRef>


											<CitationRef
CitationID="B36">36</CitationRef>
 . The importance of comparing results of different programs is also recognized in the field of motif discovery. Melina is a web-based tool which help visualize and compare outputs of various DNA motif finding programs 

											<CitationRef
CitationID="B37">37</CitationRef>
 . However, to the best of our knowledge, this is the first extensive report of an ensemble approach for DNA motif discovery, which combines component algorithms to improved regulatory motif predictions. Using the framework we developed, we have tested all the possible combinations of five component algorithms on a benchmark dataset of experimentally verified regulatory motifs in 

											<Emphasis Type="Italic">E. coli</Emphasis>
 . In terms of the nucleotide level accuracy (nPC) on intergenic region data set, the best EMD algorithm, AL-BP-MD achieved about 4 points (or 22.4%) better accuracy than that of the best component algorithm, BioProspector (Table 

											<InternalRef RefID="T1">1</InternalRef>
 ). Considering the low prediction accuracy of current single component algorithms, this improvement is significant. The advantage of the EMD algorithms over the single algorithms decreases for a long input sequence set. However, importantly, the performance (nPC) of the EMD algorithms was never worse than single algorithms, therefore, users of the EMD algorithms will never lose accuracy by using them.
										</Para>


										<Para>The largest advantage of the EMD algorithm is its flexibility of incorporating new component algorithms. That is, if a novel superior motif discovery algorithm is made available, it can be readily incorporated to the EMD algorithm system. In this study, the five component algorithms we employed were all sequence-based algorithms, because they are easily available on the internet and the basis of more recent algorithms. But there is no difficulty in incorporating advanced motif discovery algorithms which use additional information such as a phylogenetic tree, because the essence of the EMD algorithm is to combine individual independent predictions. Another advantage of the EMD algorithm is that it is particularly suitable for running on a distributed computer system, such as a grid computing or a Linux cluster, which will be certainly one of the main computational powers in the next generation.</Para>


										<Para>Below we discuss major considerations in designing a good ensemble algorithm: 1) Consensus function: how to combine predictions from different algorithms with different confidence levels; 2) Diversity of predictions: how to select and run component algorithms to obtain diverse predictions to ensure successful combination; and 3) Strength of component predictions: ways to weight the predictions from different algorithms.</Para>


										<Section2 ID="Sec_24911">

											<Heading>Ensemble grouping and weighting</Heading>


											<Para>The key of an ensemble approach is how to assemble individual independent predictions from different algorithms. In the current implementation, predicted sites from each algorithm are sorted and grouped first by their score, then further grouped across the results from different algorithms. The intention of this implementation is to increase the specificity of the final prediction by considering the significance of predicted sites. Thus, commonly predicted sites with a high score by different algorithms will be picked up well. In this way, the weight to predicted sites defined by the score is implicitly counted.</Para>


											<Para>For further improvement of the ensemble algorithm, we discuss different strategies for clustering predicted sites. A drawback of sorting predicted sites by their score is that sites from different motifs in a sequence can be mixed and clustered together, although it has an advantage of increasing the specificity of the final prediction. Alternatively, all predicted sites for a sequence can be placed on the sequence and votes can be cast to sequence positions occupied by predicted sites. Votes from a predicted site can reflect its score assigned by the component algorithm. Another idea is after grouping predicted sites by the score for each algorithm as it is done in the current implementation, each group can be corresponded to other overlapping groups (in terms of their location in sequence) from a different algorithm, but not by the score rank.</Para>


											<Para>Weighting predicted sites is another important issue in an EMD algorithm. In the current implementation, all predicted sites have an equal weight of one, although as it is mentioned above, the significance scores assigned to them by each algorithm are implicitly counted in the clustering phase. Generally, a weight to a predicted site will originate from two sources, from the reliability of individual prediction indicated by the score and from the reliability of the algorithm itself, which can be measured by the overall prediction accuracy on a certain benchmark dataset. The weighting of sites can be considered in the voting phase in our implementation. Counting the two sources of weights, a vote for a predicted site may be revised to w 

												<Subscript>1b</Subscript>
 *w 

												<Subscript>2a</Subscript>
 instead of one, where w 

												<Subscript>1</Subscript>
 will be proportional to the assigned score to a particular site, b, and w 

												<Subscript>2</Subscript>
 will be proportional to the overall accuracy of the algorithm, a. One way of doing this is to develop reasonable formulae for w 

												<Subscript>1</Subscript>
 and w 

												<Subscript>2</Subscript>
 and see if the weights improve the overall results or not. The difficulty lies especially in determining weights for algorithms, w 

												<Subscript>2a</Subscript>
 , because they depend on the combination of component algorithms in an EMD algorithm. As shown in Table 

												<InternalRef RefID="T2">2</InternalRef>
 , sometimes a combination of a relatively more accurate algorithm and a relatively less accurate one performs better than a combination of two average ones, so the resulting performance of an EMD algorithm depends on the compatibility of component algorithms, not necessarily to the accuracy of individual algorithms. Another way to change the strength of contribution of an algorithm is to change the number of runs.
											</Para>


											<Para>An alternative way to find appropriate weights will be to use an optimization algorithm such as neural network or genetic algorithm. The weights assigned to component algorithms in an EMD algorithm, w 

												<Subscript>2a</Subscript>
 , could be optimized. As for the weight assigned to individual site prediction, first a formula for w 

												<Subscript>1b</Subscript>
 should be developed using several adjustable parameters, and those parameters will be optimized. An advantage of using an optimization technique is that the other parameters such as the number of runs and/or built-in parameters for each component algorithm may be able to be optimized at the same time. Again since the appropriate weights and the other parameters may totally depend on the combination of component algorithms, they should be optimized every time a different combination is tried.
											</Para>


										</Section2>


										<Section2 ID="Sec_13698">

											<Heading>Diversity of predictions from component algorithms</Heading>


											<Para>To ensure that the predictions from component algorithms cover most target motifs, it is desirable that diverse predictions are generated from component algorithms. Diversified predictions would contribute in increasing the sensitivity of the final prediction. For stochastic algorithms, multiple runs usually naturally generate multiple different predictions. For deterministic algorithms like MEME and MDScan, we changed some parameters (named diversity parameters) to force them to generate different predictions. This idea of the diversity parameter is reasonable since the optimal parameter for a given input data set cannot be estimated in advance. However, a possible downside is that if the parameter is excessively changed, the quality of the predictions can be significantly deteriorated, resulting in a poor consensus building. Another possible way to diversify predictions is to feed different runs or algorithms with different subset data sampled from the original input sequences, which is yet to be explored.</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_98479">

										<Heading>Conclusion</Heading>


										<Para>We have introduced an ensemble approach to the motif discovery problem. To the best of our knowledge, this is the first time that an ensemble approach is used in the motif discovery problem. By combining multiple predictions from multiple runs of one or more component algorithms, our ensemble algorithm showed good improvement in the sensitivity and specificity and thus, the overall accuracy over stand-alone component algorithms. The EMD algorithm is scalable in the sense that the EMD algorithms performed better, or at worst, equally compared to individual component algorithms. The improvement in the accuracy over the component algorithm is more significant for shorter input sequences. Considering the importance of the regulatory motif discovery in gene expression analysis and the poor performance of the current individual motif discovery algorithms, our EMD algorithms can be a very useful tool in the era of systems biology.</Para>


									</Section1>


									<Section1 ID="Sec_11061">

										<Heading>Methods</Heading>


										<Section2 ID="Sec_49361">

											<Heading>Benchmark data sets</Heading>


											<Para>The benchmark data set is generated from RegulonDB 

												<CitationRef
CitationID="B38">38</CitationRef>
 , which stores motif information of 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 K12. Since this benchmark set is basically the same as the one used in our previous study 

												<CitationRef
CitationID="B6">6</CitationRef>
 , we only briefly describe it here.
											</Para>


											<Para>Three types of data sets (Type A, B, and C) are prepared. Type A data sets are generated from the intergenic regions of 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome. The intergenic region sequences are taken from the KEGG database 

												<CitationRef
CitationID="B39">39</CitationRef>
 . This set A contains 62 motif groups and is called ECRDB62A. It has the following characteristics: the average number of sequences per motif group is 12; the average number of sites per sequence is 1.85; the average sequence length is 300 nt; the average site width is 22.83.
											</Para>


											<Para>Types B and C data sets include sequences with symmetric margins on both sides of known sites. The difference between the sets B and C is that actual flanking sequences in the 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome are used for margins for the set B, whereas artificial sequences are used for the type C.
											</Para>


											<Para>Set B is termed ECRDB61B-X, where X denotes the margin size of the sequences. The length of the margins is changed from 20 to 800, so that the scalability of algorithms in terms of the input sequence length can be examined. Because margins are added on both sides of a site, the actual length of an input sequence is the total of the site width and two times the margin size. The number 61 denotes the number of motif groups available in this benchmark set. When a large margin size is used, it sometimes happened that an extended sequence includes another site of the same motif group, thus the two input sequences contain the same set of two sites of the same type. In such a case, one of the input sequences is removed, and if this procedure reduces the number of input sequence to one, the entire motif group is removed from the benchmark dataset. It also happened that an input sequence of a certain motif group contains another site of a different type when a large margin size is used. But since this case happens in a real situation, primarily we kept these sequences in the dataset. In such a case, because sites of a target motif are still abundant, we expect an algorithm is able to pick the target sites as one of the top K scoring motifs. We also observe that when the margin sizes are larger ( 

												<Emphasis Type="Italic">e.g</Emphasis>
 . &gt;500 nt), some part of the sequences are located in the coding regions. However, as shown in the previous study 

												<CitationRef
CitationID="B6">6</CitationRef>
 , no significant influence has been observed of these variations on the prediction accuracy. In ECRDB62A and ECRDB61B-X dataset, there are input sequences which have multiple identical motifs (i.e. two AraC motifs on a sequence) on it. In those cases, positions of both motifs are considered to be correct. And the dataset is cleaned so that a certain motif position only occurs once in a motif group. Also a motif group does not contain different motifs multiple times, so that the motif discovery algorithms do not confuse by them.
											</Para>


											<Para>In addition to the type A and B sets, we have prepared the type C set, where artificially shuffled sequences are used for margins on both sides of known sites. This dataset is termed ECRDB61C-X. X represents the length of the margin sequence. The same set of the known sites are used as in ECRDB61B data set. Three different margin sizes, 100, 200, and 400 nt. are used. The margin sequences are artificially shuffled, while preserving the di-mer nucleotide frequency of intergenic regions of the 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome. The motivation of using the type C set is to evaluate the performance of the algorithms on sequences which surely contain only one target regulatory site in a sequence. The benchmark data sets are available at our web site 

												<CitationRef
CitationID="B40">40</CitationRef>
 .
											</Para>


										</Section2>


										<Section2 ID="Sec_41551">

											<Heading>Ensemble motif discovery algorithm</Heading>


											<Para>The Ensemble Motif Discovery (EMD) algorithm represents a family of motif discovery algorithms that combine multiple results from individual component algorithms. In this study, we primarily used five component algorithms which are described in the next section. A certain ensemble algorithm can be uniquely identified by the set of component algorithms used, the number of times each component algorithm is run (R 

												<Subscript>i</Subscript>
 , i denotes for a certain component algorithm), the parameter set used to run each component algorithm (P 

												<Subscript>i</Subscript>
 ), and the number of top scoring motifs considered from each run (T 

												<Subscript>i</Subscript>
 ). Thus, an EMD algorithm can be specified as EMD(-CA 

												<Subscript>i</Subscript>
 -R 

												<Subscript>i</Subscript>
 -P 

												<Subscript>i</Subscript>
 -T 

												<Subscript>i</Subscript>
 ) 

												<Subscript>M</Subscript>
 , where CA 

												<Subscript>i</Subscript>
 denotes a certain component algorithm with the conditions, R 

												<Subscript>i</Subscript>
 , P 

												<Subscript>i</Subscript>
 , T 

												<Subscript>i</Subscript>
 (i goes from 1 to 

												<Emphasis Type="Italic">M</Emphasis>
 , 

												<Emphasis Type="Italic">M</Emphasis>
 is the number of different component algorithms combined).
											</Para>


											<Para>More generally, P 

												<Subscript>i</Subscript>
 and T 

												<Subscript>i</Subscript>
 can be changed for each different run of the algorithm i, which can introduce further variations to the EMD algorithms. However, since it is not possible to explore every possible condition, some of the parameters are set to be the same. The number of motifs to be reported from individual runs is set to five for all runs of all the component algorithms (T 

												<Subscript>i</Subscript>
 = 5). The number of runs (R 

												<Subscript>i</Subscript>
 ) is set to 20 if not specified otherwise for all component algorithms. For a single component algorithm, the same parameter set is used for all the runs. For deterministic algorithms, however, the parameter set is changed for each run because otherwise they produce identical results. For a deterministic algorithm, ten different values are prepared for a tuning parameter (we call it a diversity parameter), one of which is selected randomly for each run. Since we don't change R 

												<Subscript>i</Subscript>
 and T 

												<Subscript>i</Subscript>
 for each different algorithm i, and the parameter set P 

												<Subscript>i</Subscript>
 is set to be same in all the runs of the algorithm i, the notation of an EMD algorithm can be simplified to be EMD(-CA 

												<Subscript>i</Subscript>
 ) 

												<Subscript>M</Subscript>
 . Specifically, we denote EMD-X (X = 1~5) as the set of all the possible combinations of X component algorithms.
											</Para>


										</Section2>


										<Section2 ID="Sec_57074">

											<Heading>Algorithm of the EMD</Heading>


											<Para>Suppose an EMD algorithm combines 

												<Emphasis Type="Italic">M</Emphasis>
 component algorithms, A 

												<Subscript>i</Subscript>
 , (i = 1..M). The EMD algorithm predicts motifs in a set of 

												<Emphasis Type="Italic">N</Emphasis>
 sequences, S 

												<Subscript>i</Subscript>
 , (i = 1..N). Here a 

												<Emphasis Type="Italic">motif</Emphasis>
 is defined as a set of local regions (= 

												<Emphasis Type="Italic">sites</Emphasis>
 ) in input sequences which are detected to be similar to each other. In the other words, one or sometimes more 

												<Emphasis Type="Italic">sites</Emphasis>
 are predicted in each input sequence, and all of the sites in an input sequence set form a 

												<Emphasis Type="Italic">motif</Emphasis>
 . The EMD algorithm has five steps to generate its final prediction: collecting, grouping, voting, smoothing, and extracting. The overview of the algorithm is provided in Figure 

												<InternalRef RefID="F1">1</InternalRef>
 .
											</Para>


											<Section3 ID="Sec_10063">

												<Heading>(1) Collecting</Heading>


												<Para>Each algorithm A 

													<Subscript>i</Subscript>
 runs 

													<Emphasis Type="Italic">R</Emphasis>
 times against an input dataset of 

													<Emphasis Type="Italic">N</Emphasis>
 sequences and reports top 

													<Emphasis Type="Italic">K</Emphasis>
 scoring motifs for each run. For a motif, a component algorithm A 

													<Subscript>i</Subscript>
 usually detects one site per input sequence by a single run, resulting the total of 

													<Emphasis
Type="Italic">R*K</Emphasis>
 predicted sites in a sequence. If an algorithm A 

													<Subscript>i</Subscript>
 reports more than 

													<Emphasis Type="Italic">K</Emphasis>
 motifs, only the top 

													<Emphasis Type="Italic">K</Emphasis>
 scoring motifs are considered. Oppositely, if less than 

													<Emphasis Type="Italic">K</Emphasis>
 motifs are predicted by an algorithm, all of the motifs are considered. Also since sometimes an algorithm picks more than one site or no sites in a sequence, the total number of predicted sites in a sequences can be more or less than 

													<Emphasis
Type="Italic">R*K</Emphasis>
 . Now for each combination of an input sequence and a component algorithm, all the predicted sites are collected. Figure 

													<InternalRef
RefID="F1">1</InternalRef>
 illustrates the collecting phase of sites in the input sequence number 1 from all the algorithms, A 

													<Subscript>1</Subscript>
 to A 

													<Subscript>M</Subscript>
 .
												</Para>


											</Section3>


											<Section3 ID="Sec_13889">

												<Heading>(2) Grouping</Heading>


												<Para>From the collecting phase above, for an input sequence S 

													<Subscript>i</Subscript>
 we have about 

													<Emphasis
Type="Italic">R*K</Emphasis>
 predicted sites from each of the algorithm A 

													<Subscript>i</Subscript>
 . In the grouping phase, first, all the predicted sites in an input sequence S 

													<Subscript>i</Subscript>
 by a certain algorithm A 

													<Subscript>i</Subscript>
 are sorted by the algorithm's major statistical score. Then the predicted sites are divided into 

													<Emphasis Type="Italic">K</Emphasis>
 groups by the sorted score, with each of the groups having an equal number of predicted sites. Because usually an input sequence has 

													<Emphasis
Type="Italic">R*K</Emphasis>
 sites, most of the groups have 

													<Emphasis Type="Italic">R</Emphasis>
 sites.
												</Para>


												<Para>Then, the groups of the same score rank across the results by 

													<Emphasis Type="Italic">M</Emphasis>
 different algorithms are joined together. This results in 

													<Emphasis Type="Italic">K</Emphasis>
 groups of predicted sites from all predictions made by all algorithms.
												</Para>


												<Para>The reason for employing this sorting step is to take the score assigned to predicted sites by the algorithm into account. We have observed from prediction results of single component algorithms that the correct site position is frequently predicted within the top couple of score ranks most of the time. Therefore, a local region predicted as the target site consistently in every run by the algorithm within the top scores can be considered to be more reliable.</Para>


												<Para>Basically, each of the 

													<Emphasis Type="Italic">K</Emphasis>
 groups will finally produce a predicted site. Thus in total EMD outputs 

													<Emphasis Type="Italic">K</Emphasis>
 predicted sites for an input sequence. But if the average number of predicted sites, 

													<Emphasis Type="Italic">B</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , for a given input sequence S 

													<Subscript>i</Subscript>
 , is more than one, there is an option to output 

													<Emphasis Type="Italic">B</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 sites for the sequence S 

													<Subscript>i</Subscript>
 from each group. The subsequent steps of voting, smoothing, and extracting steps explain how EMD construct final predictions from the collected sites in each group.
												</Para>


											</Section3>


											<Section3 ID="Sec_10508">

												<Heading>(3) Voting</Heading>


												<Para>For each of the 

													<Emphasis Type="Italic">K</Emphasis>
 predicted site groups in a sequence S 

													<Subscript>i</Subscript>
 , all the predicted sites in a group are placed on the sequence. Then for each position p in a sequence, the number of times the position p is included, or votes for the position p, 

													<Emphasis Type="Italic">V</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 , in the predicted sites is counted. Figure 

													<InternalRef
RefID="F1">1</InternalRef>
 shows the number of votes 

													<Emphasis Type="Italic">V</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 along the sequence position p for the site group 1.
												</Para>


											</Section3>


											<Section3 ID="Sec_02788">

												<Heading>(4) Smoothing</Heading>


												<Para>The vote 

													<Emphasis Type="Italic">V</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 along an input sequence is smoothed using a sliding window of a width of 

													<Emphasis Type="Italic">W</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">s</Emphasis>

													</Subscript>
 , which is half of the specified motif width 

													<Emphasis Type="Italic">W</Emphasis>
 ( 

													<Emphasis Type="Italic">W</Emphasis>
 is a user specified parameter, the default value is 15 and W 

													<Subscript>s</Subscript>
 is 8). The sliding window starts from the left most position of the sequence, and the sum of the votes in the window,VpsMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGwbGvdaqhaaWcbaGaemiCaahabaGaem4Camhaaaaa@30E6@, is placed in the center position 

													<Emphasis Type="Italic">p</Emphasis>
 of the window. In the case that 

													<Emphasis Type="Italic">W</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">s</Emphasis>

													</Subscript>
 is even, the smoothed score,VpsMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGwbGvdaqhaaWcbaGaemiCaahabaGaem4Camhaaaaa@30E6@, is placed at the 

													<Emphasis Type="Italic">q</Emphasis>
 -th position in the window, where 

													<Emphasis Type="Italic">q</Emphasis>
 = ( 

													<Emphasis Type="Italic">W</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">s</Emphasis>

													</Subscript>
 /2)+1.
												</Para>


											</Section3>


											<Section3 ID="Sec_08426">

												<Heading>(5) Extracting final prediction of sites</Heading>


												<Para>The final stage is to pick up the top local peak (or the top 

													<Emphasis Type="Italic">B</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 local peaks) in the smoothed voting curve,VpsMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGwbGvdaqhaaWcbaGaemiCaahabaGaem4Camhaaaaa@30E6@. Then a window of the length 

													<Emphasis Type="Italic">W</Emphasis>
 is placed as the final prediction of the site, with the center of the window positioned at the peak.
												</Para>


												<Para>The smoothing and the extracting phases are aimed to decide the final site prediction by majority votes. Although it may be possible that minority votes for different motifs are not selected as one of the final predictions, but this procedure will be superior in picking up sites which are consistently predicted with a high score rank. Alternative ways to combine different predictions are discussed in Discussion.</Para>


											</Section3>


										</Section2>


										<Section2 ID="Sec_63954">

											<Heading>Component algorithms</Heading>


											<Para>There are several factors that need to be considered to develop an ensemble algorithm. First, we have to identify whether a component algorithm is a stochastic or deterministic algorithm. In an ensemble algorithm, the component algorithms are usually run multiple times and all the results are combined together. We need a way to control the proportion of predictions from different algorithms to avoid any bias. For deterministic component algorithms, such as MDScan 

												<CitationRef
CitationID="B32">32</CitationRef>
 and MEME 

												<CitationRef
CitationID="B33">33</CitationRef>
 , multiple runs generate identical predictions, which will strongly bias the final combined result. To address this problem, we introduced the diversity parameter(s), which is defined as one or more parameters of an algorithm that one can tune to generate different predictions. In this study, only one diversity parameter is chosen for the deterministic algorithms, namely, MEME and MDScan.
											</Para>


											<Para>Five motif discovery programs, namely, AlignACE 

												<CitationRef
CitationID="B30">30</CitationRef>
 , MEME 

												<CitationRef
CitationID="B33">33</CitationRef>
 , BioProspector 

												<CitationRef
CitationID="B31">31</CitationRef>
 , MDScan 

												<CitationRef
CitationID="B32">32</CitationRef>
 , and MotifSampler 

												<CitationRef
CitationID="B34">34</CitationRef>
 are selected as the component algorithms for composing the ensemble algorithms. These algorithms only use DNA sequence information as input to identify the regulatory motifs. These algorithms are selected because of their wide use and being ready for download from the internet, allowing us to do large scale local runs. Below we describe the parameter setting of each algorithm. A random algorithm is also introduced to evaluate the statistical significance of the prediction accuracy of the ensemble algorithm.
											</Para>


											<Para>One difficulty in testing the performance of an algorithm is to set optimal parameters. Here most of the parameters for the component algorithms are set as default values except those which can be easily estimated from general biology knowledge, as we did in the previous study 

												<CitationRef
CitationID="B6">6</CitationRef>
 . For example, we have chosen 15 as the expected motif width for the component algorithms (except for MEME, which can adjust motif width by itself), because 15 is the approximate average between the default value of the algorithms and the average motif width in ECRDB62A, which is 21. The reason why we used default parameters (except for the few parameters mentioned below) for the component algorithms is that it is infeasible to try all the possible combinations of parameters of multiple component algorithms, and the default setting for an algorithm should work reasonably well in most of the cases, because it is set up by the authors of the algorithm. The parameter set used for each component algorithms can be found at our web site 

												<CitationRef
CitationID="B40">40</CitationRef>
 . This is the same parameter set used in the previous study 

												<CitationRef
CitationID="B6">6</CitationRef>
 .
											</Para>


										</Section2>


										<Section2 ID="Sec_51082">

											<Heading>AlignACE</Heading>


											<Para>AlignACE 

												<CitationRef
CitationID="B30">30</CitationRef>
 is a stochastic motif discovery program based on the widely adopted Gibbs Sampling method 

												<CitationRef
CitationID="B11">11</CitationRef>
 . Running parameters for AlignACE were set as the default except for the background fractional GC content gcback set to 0.5, which is calculated from the whole 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome. The expected motif width was set to 15. The major statistical score in AlignACE is the MAP score, being the larger is better.
											</Para>


										</Section2>


										<Section2 ID="Sec_85071">

											<Heading>BioProspector</Heading>


											<Para>BioProspector 

												<CitationRef
CitationID="B31">31</CitationRef>
 is another variant of the Gibbs Sampling algorithm, which has fifteen parameters to fine-tune its prediction behavior. We used the default values for most of these parameters except for: the motif width, which was set to 15; the number of top motifs to report, which was set to 5. The background frequency model was generated using the whole 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome and the third order Markov model was used. The order of the background model and subsequent ones for MDScan, MotifSampler and MEME was determined based on our previous benchmark study of these component algorithms 

												<CitationRef
CitationID="B6">6</CitationRef>
 . BioProspector uses a maximum a posterior (MAP) score to evaluate candidate motifs.
											</Para>


										</Section2>


										<Section2 ID="Sec_32047">

											<Heading>MDScan</Heading>


											<Para>MDScan 

												<CitationRef
CitationID="B32">32</CitationRef>
 is an enumerative deterministic greedy algorithm. Among its ten parameters, we only specified the following parameters. The motif width was set to 15. The background frequency model was generated using the whole 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome and the third order Markov model was used. MDScan uses a maximum a posterior (MAP) score to evaluate candidate motifs. We chose the -s parameter &lt; the number of top motifs to scan and refine &gt; as the diversity parameter to generate different predictions for multiple runs. It ranged from 10 to 100 with a step size of 10.
											</Para>


										</Section2>


										<Section2 ID="Sec_55438">

											<Heading>MEME</Heading>


											<Para>MEME (Multiple Expectation Maximization Estimation) 

												<CitationRef
CitationID="B33">33</CitationRef>
 is a deterministic algorithm based on the expectation maximization (EM) technique. It is the only algorithm in this evaluation that does not require a motif width parameter, because MEME can estimate by itself. MEME has 28 parameters. We set the maximum dataset size in characters to one million, the maximum running time to 3600 CPU seconds, the maximal number of motifs to find to five, and the minimum number of sites for each motif to one. The third order Markov model was used for the background frequency model. Default values were used for all the other parameters. We chose the -maxw &lt;maximum motif width&gt; as the diversity parameter, ranging from 10 to 19, with the step size of 1.
											</Para>


										</Section2>


										<Section2 ID="Sec_75863">

											<Heading>MotifSampler</Heading>


											<Para>MotifSampler 

												<CitationRef
CitationID="B34">34</CitationRef>
 is another motif discovery program based on Gibbs sampling. MotifSampler has seven major parameters. We made the following adjustments to the default parameter values. We searched five different motifs of a width of fifteen. The number of repeating runs was set to five. The background frequency model was generated using the intergenic region sequences of all 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 genome and the third order Markov model was used. We used the consensus score as the statistical measure for the quality of the predicted motifs.
											</Para>


										</Section2>


										<Section2 ID="Sec_96156">

											<Heading>The multi-restart algorithm</Heading>


											<Para>One interesting question for ensemble algorithms is whether the performance improvement of the ensemble algorithm is due to more number of runs or to synergetic effect of multiple runs of multiple algorithms. We developed a multi-restart algorithm (RS) and compared its performance against that of ensemble algorithms. The basic idea of RS algorithm is to run a given algorithm multiple times and use the highest scored predictions as the final results. The multi-restart algorithm works as follows:</Para>


											<Para>1) Run the component algorithm for 

												<Emphasis Type="Italic">R</Emphasis>
 times, with each run reporting top 

												<Emphasis Type="Italic">K</Emphasis>
 motifs.
											</Para>


											<Para>2) Collect all the predicted motifs and sort them by the major statistical score of the algorithm.</Para>


											<Para>3) Report the top 

												<Emphasis Type="Italic">K</Emphasis>
 motifs among all the sorted motifs as the final prediction.
											</Para>


										</Section2>


										<Section2 ID="Sec_84222">

											<Heading>The random algorithm</Heading>


											<Para>In a random motif algorithm, a certain number of sites are randomly picked up as predictions of sites. The number of sites picked up is decided for each input sequence as follows: First, conducted 10 runs of AlignACE, BioProspector, MotifSampler and one run of MEME to get the minimum (nSiteMin) and the maximum number (nSiteMax) of predicted sites. Then, the number of sites to be predicted is randomly chosen between nSiteMin and nSiteMax. The random algorithm is run 1000 times, and the average performance is reported.</Para>


										</Section2>


										<Section2 ID="Sec_18139">

											<Heading>Measure of prediction accuracy</Heading>


											<Para>We use two levels of performance criteria: nucleotide and site levels to measure the prediction accuracy. A more detailed description is given in the previous study 

												<CitationRef
CitationID="B6">6</CitationRef>
 . The nucleotide level accuracy measures include the performance coefficient (nPC), the sensitivity (nSn) and the specificity (nSp). The site level accuracy measures include the site level performance coefficient (sPC), the sensitivity (sSn) and the specificity (sSp). As described above, an EMD algorithm reports K (or sometimes less) motifs for a given input dataset. In this study, we evaluated the accuracy of the best prediction out of the K motifs 

												<CitationRef
CitationID="B6">6</CitationRef>
 .
											</Para>


										</Section2>


										<Section2 ID="Sec_93553">

											<Heading>Nucleotide level accuracy</Heading>


											<Para>First, for each target site with overlapping predicted sites in an input sequence, we define the following values to calculate the accuracy metrics at the nucleotide level: nTP (true positive), the number of target site positions predicted as site positions; nTN (true negative), the number of non-site positions predicted as non-site positions; nFP (false positive), the number of non-site positions predicted as site positions; nFN (false negative), the number of target site positions predicted as non-site positions.</Para>


											<Para>The nucleotide level performance coefficient (nPC), sensitivity (nSn) and specificity (nSp) for a pair of target/predicted sites is defined as:</Para>


											<Para>In addition to the accuracy score for target sites with overlapping predictions, we need to address the cases of targeted sites which do not overlap with any predicted sites or predicted sites without overlapping any targeted sites. We define the number of non-overlapping target and predicted site pairs as the larger number among MT and MP, where MT denotes for the number of missing targeted sites and MP denotes for the number of wrong predictions. The accuracy scores of these non-overlapping pairs are set to zero. This definition will penalize algorithms that report either too many or too few site predictions. Based on the scores defined for the site pairs, the accuracy scores of a motif discovery program are calculated as:</Para>


											<Para>Thus, the score is first averaged over all site pairs in a sequence, followed by averaging over all sequences in a motif group, and finally averaging over all the motif groups. Note that we allow multiple sites on a sequence as targeted sites.</Para>


										</Section2>


										<Section2 ID="Sec_19567">

											<Heading>Site level accuracy</Heading>


											<Para>The site level accuracy indicates if predicted sites overlap with true sites by one or more nucleotide position. We define, sTP, sFP, and sFN as follows: sTP, the number of predicted sites which overlaps with the true sites by at least one nt; sFP, the number of predicted sites which have no overlaps with the true sites; sFN, the number of true sites that have no overlaps with any predicted sites.</Para>


											<Para>For each input sequence, we define the site level performance coefficient (sPC), sensitivity (sSn), and specificity (sSp) in the following way:</Para>


											<Para>The site level accuracy score for an input sequence set (or a motif group) is the average of the score over all the sequences. The site level accuracy score of the entire benchmark data set is the average of the scores for all input sequence sets.</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_66463">

										<Heading>Authors' contributions</Heading>


										<Para>JH designed and implemented the algorithm, carried out the initial benchmark study and drafted the manuscript. YDY carried out additional benchmark study required to revise the manuscript. DK conceived of the study, participated in its coordination, and wrote the manuscript. All authors read and approved the final manuscript.</Para>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/1471-2105/7/342"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Brazma</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>I</Initials>


													<FamilyName>Jonassen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Vilo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Ukkonen</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Predicting gene regulatory elements in silico on a genomic scale</ArticleTitle>


												<JournalTitle>Genome Res</JournalTitle>


												<VolumeID>8</VolumeID>


												<FirstPage>1202</FirstPage>


												<LastPage>1215</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>FC</Initials>


													<FamilyName>Holstege</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>EG</Initials>


													<FamilyName>Jennings</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JJ</Initials>


													<FamilyName>Wyrick</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TI</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CJ</Initials>


													<FamilyName>Hengartner</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MR</Initials>


													<FamilyName>Green</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TR</Initials>


													<FamilyName>Golub</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>ES</Initials>


													<FamilyName>Lander</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RA</Initials>


													<FamilyName>Young</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Dissecting the regulatory circuitry of a eukaryotic genome</ArticleTitle>


												<JournalTitle>Cell</JournalTitle>


												<VolumeID>95</VolumeID>


												<FirstPage>717</FirstPage>


												<LastPage>728</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1016/S0092-8674(00)81641-4</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JJ</Initials>


													<FamilyName>Wyrick</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RA</Initials>


													<FamilyName>Young</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Deciphering gene expression regulatory networks</ArticleTitle>


												<JournalTitle>Curr Opin Genet Dev</JournalTitle>


												<VolumeID>12</VolumeID>


												<FirstPage>130</FirstPage>


												<LastPage>136</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1016/S0959-437X(02)00277-0</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tompa</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TL</Initials>


													<FamilyName>Bailey</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>GM</Initials>


													<FamilyName>Church</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MB</Initials>


													<FamilyName>De</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Eskin</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>AV</Initials>


													<FamilyName>Favorov</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MC</Initials>


													<FamilyName>Frith</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Fu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>WJ</Initials>


													<FamilyName>Kent</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>VJ</Initials>


													<FamilyName>Makeev</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>AA</Initials>


													<FamilyName>Mironov</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>WS</Initials>


													<FamilyName>Noble</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Pavesi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Pesole</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Regnier</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Simonis</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Sinha</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Thijs</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>HJ</Initials>


													<FamilyName>van</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Vandenbogaert</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Weng</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Workman</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Ye</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Zhu</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Assessing computational tools for the discovery of transcription factor binding sites</ArticleTitle>


												<JournalTitle>Nat Biotechnol</JournalTitle>


												<VolumeID>23</VolumeID>


												<FirstPage>137</FirstPage>


												<LastPage>144</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1038/nbt1053</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>WW</Initials>


													<FamilyName>Wasserman</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Sandelin</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Applied bioinformatics for the identification of regulatory elements</ArticleTitle>


												<JournalTitle>Nat Rev Genet</JournalTitle>


												<VolumeID>5</VolumeID>


												<FirstPage>276</FirstPage>


												<LastPage>287</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1038/nrg1315</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Hu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Kihara</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Limitations and potentials of current motif discovery algorithms</ArticleTitle>


												<JournalTitle>Nucleic Acid Res</JournalTitle>


												<VolumeID>33</VolumeID>


												<FirstPage>4899</FirstPage>


												<LastPage>4913</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/gki791</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ellrott</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>FM</Initials>


													<FamilyName>Sladek</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Jiang</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Identifying transcription factor binding sites through Markov chain optimization</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18 Suppl 2</VolumeID>


												<FirstPage>S100</FirstPage>


												<LastPage>S109</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Osada</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Zaslavsky</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Singh</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Comparative analysis of methods for representing and searching for transcription factor binding sites</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>20</VolumeID>


												<FirstPage>3516</FirstPage>


												<LastPage>3525</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bth438</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Gribskov</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>AD</Initials>


													<FamilyName>McLachlan</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Eisenberg</FamilyName>


												</BibAuthorName>


												<Year>1987</Year>


												<ArticleTitle
Language="En">Profile analysis: detection of distantly related proteins</ArticleTitle>


												<JournalTitle>Proc Natl Acad Sci U S A</JournalTitle>


												<VolumeID>84</VolumeID>


												<FirstPage>4355</FirstPage>


												<LastPage>4358</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1073/pnas.84.13.4355</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TL</Initials>


													<FamilyName>Bailey</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Elkan</FamilyName>


												</BibAuthorName>


												<Year>1995</Year>


												<ArticleTitle
Language="En">Unsupervised Learning of Multiple Motifs in Biopolymers Using Expectation Maximization</ArticleTitle>


												<JournalTitle>Machine Learning</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>51</FirstPage>


												<LastPage>80</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>CE</Initials>


													<FamilyName>Lawrence</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SF</Initials>


													<FamilyName>Altschul</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MS</Initials>


													<FamilyName>Boguski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JS</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>AF</Initials>


													<FamilyName>Neuwald</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JC</Initials>


													<FamilyName>Wootton</FamilyName>


												</BibAuthorName>


												<Year>1993</Year>


												<ArticleTitle
Language="En">Detecting subtle sequence signals: a Gibbs sampling strategy for multiple alignment</ArticleTitle>


												<JournalTitle>Science</JournalTitle>


												<VolumeID>262</VolumeID>


												<FirstPage>208</FirstPage>


												<LastPage>14</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Blanchette</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Schwikowski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tompa</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Algorithms for phylogenetic footprinting</ArticleTitle>


												<JournalTitle>J Comput Biol</JournalTitle>


												<VolumeID>9</VolumeID>


												<FirstPage>211</FirstPage>


												<LastPage>223</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1089/10665270252935421</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Blanchette</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tompa</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Discovery of regulatory elements by a computational method for phylogenetic footprinting</ArticleTitle>


												<JournalTitle>Genome Res</JournalTitle>


												<VolumeID>12</VolumeID>


												<FirstPage>739</FirstPage>


												<LastPage>748</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1101/gr.6902</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Wang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>GD</Initials>


													<FamilyName>Stormo</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Combining phylogenetic data with co-regulated genes to identify regulatory motifs</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>19</VolumeID>


												<FirstPage>2369</FirstPage>


												<LastPage>2380</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/btg329</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR15">

											<CitationNumber>15.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Sinha</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Blanchette</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tompa</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">PhyME: a probabilistic algorithm for finding motifs in sets of orthologous sequences</ArticleTitle>


												<JournalTitle>BMC Bioinformatics</JournalTitle>


												<VolumeID>5</VolumeID>


												<FirstPage>170</FirstPage>


												<LastPage />


												<Occurrence Type="DOI">

													<Handle>10.1186/1471-2105-5-170</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR16">

											<CitationNumber>16.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Kellis</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Patterson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Birren</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Berger</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>ES</Initials>


													<FamilyName>Lander</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Methods in comparative genomics: genome correspondence, gene identification and regulatory motif discovery</ArticleTitle>


												<JournalTitle>J Comput Biol</JournalTitle>


												<VolumeID>11</VolumeID>


												<FirstPage>319</FirstPage>


												<LastPage>355</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1089/1066527041410319</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR17">

											<CitationNumber>17.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>EM</Initials>


													<FamilyName>Conlon</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>XS</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JD</Initials>


													<FamilyName>Lieb</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JS</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Integrating regulatory motif discovery and genome-wide expression analysis</ArticleTitle>


												<JournalTitle>Proc Natl Acad Sci U S A</JournalTitle>


												<VolumeID>100</VolumeID>


												<FirstPage>3339</FirstPage>


												<LastPage>3344</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1073/pnas.0630591100</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR18">

											<CitationNumber>18.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Aggarwal</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>EA</Initials>


													<FamilyName>Worthey</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>PD</Initials>


													<FamilyName>McDonagh</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>PJ</Initials>


													<FamilyName>Myler</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Importing statistical measures into Artemis enhances gene identification in the Leishmania genome project</ArticleTitle>


												<JournalTitle>BMC Bioinformatics</JournalTitle>


												<VolumeID>4</VolumeID>


												<FirstPage>23</FirstPage>


												<LastPage />


												<Occurrence Type="DOI">

													<Handle>10.1186/1471-2105-4-23</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR19">

											<CitationNumber>19.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ginalski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Elofsson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Fischer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Rychlewski</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">3D-Jury: a simple approach to improve protein structure predictions</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>19</VolumeID>


												<FirstPage>1015</FirstPage>


												<LastPage>1018</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/btg124</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR20">

											<CitationNumber>20.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Fischer</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">3D-SHOTGUN: A novel, cooperative, fold-recognition meta-predictor</ArticleTitle>


												<JournalTitle>Proteins</JournalTitle>


												<VolumeID>51</VolumeID>


												<FirstPage>434</FirstPage>


												<LastPage>41</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1002/prot.10357</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR21">

											<CitationNumber>21.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HK</Initials>


													<FamilyName>Saini</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Fischer</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Meta-DP: domain prediction meta-server</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>2917</FirstPage>


												<LastPage>2920</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bti445</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR22">

											<CitationNumber>22.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Albrecht</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SC</Initials>


													<FamilyName>Tosatto</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Lengauer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Valle</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Simple consensus procedures are effective and sufficient in secondary structure prediction</ArticleTitle>


												<JournalTitle>Protein Eng</JournalTitle>


												<VolumeID>16</VolumeID>


												<FirstPage>459</FirstPage>


												<LastPage>462</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/protein/gzg063</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR23">

											<CitationNumber>23.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Nishikawa</FamilyName>


												</BibAuthorName>


												<Year>1990</Year>


												<ArticleTitle
Language="En">[Prediction of protein secondary structure by a new joint method]</ArticleTitle>


												<JournalTitle>Seikagaku</JournalTitle>


												<VolumeID>62</VolumeID>


												<FirstPage>1490</FirstPage>


												<LastPage>1496</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR24">

											<CitationNumber>24.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ginalski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Rychlewski</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Detection of reliable and unexpected protein fold predictions using 3D-Jury</ArticleTitle>


												<JournalTitle>Nucleic Acids Res</JournalTitle>


												<VolumeID>31</VolumeID>


												<FirstPage>3291</FirstPage>


												<LastPage>3292</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/gkg503</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR25">

											<CitationNumber>25.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>LN</Initials>


													<FamilyName>Kinch</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JO</Initials>


													<FamilyName>Wrabl</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SS</Initials>


													<FamilyName>Krishna</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>I</Initials>


													<FamilyName>Majumdar</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RI</Initials>


													<FamilyName>Sadreyev</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Qi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Pei</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Cheng</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>NV</Initials>


													<FamilyName>Grishin</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">CASP5 assessment of fold recognition target predictions</ArticleTitle>


												<JournalTitle>Proteins</JournalTitle>


												<VolumeID>53 Suppl 6</VolumeID>


												<FirstPage>395</FirstPage>


												<LastPage>409</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1002/prot.10557</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR26">

											<CitationNumber>26.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Tramontano</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Morea</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Assessment of homology-based predictions in CASP5</ArticleTitle>


												<JournalTitle>Proteins</JournalTitle>


												<VolumeID>53 Suppl 6</VolumeID>


												<FirstPage>352</FirstPage>


												<LastPage>368</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1002/prot.10543</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR27">

											<CitationNumber>27.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Venclovas</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Zemla</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Fidelis</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Moult</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Assessment of progress over the CASP experiments</ArticleTitle>


												<JournalTitle>Proteins</JournalTitle>


												<VolumeID>53 Suppl 6</VolumeID>


												<FirstPage>585</FirstPage>


												<LastPage>595</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1002/prot.10530</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR28">

											<CitationNumber>28.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Lundstrom</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Rychlewski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Bujnicki</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Elofsson</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">Pcons: a neural-network-based consensus predictor that improves fold recognition</ArticleTitle>


												<JournalTitle>Protein Sci</JournalTitle>


												<VolumeID>10</VolumeID>


												<FirstPage>2354</FirstPage>


												<LastPage>2362</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1110/ps.08501</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR29">

											<CitationNumber>29.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TZ</Initials>


													<FamilyName>Sen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Kloczkowski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RL</Initials>


													<FamilyName>Jernigan</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Yan</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Honavar</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>KM</Initials>


													<FamilyName>Ho</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CZ</Initials>


													<FamilyName>Wang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Ihm</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Cao</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>X</Initials>


													<FamilyName>Gu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Dobbs</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Predicting binding sites of hydrolase-inhibitor complexes by combining several methods</ArticleTitle>


												<JournalTitle>BMC Bioinformatics</JournalTitle>


												<VolumeID>5</VolumeID>


												<FirstPage>205</FirstPage>


												<LastPage />


												<Occurrence Type="DOI">

													<Handle>10.1186/1471-2105-5-205</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR30">

											<CitationNumber>30.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>FP</Initials>


													<FamilyName>Roth</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JD</Initials>


													<FamilyName>Hughes</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>PW</Initials>


													<FamilyName>Estep</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>GM</Initials>


													<FamilyName>Church</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Finding DNA regulatory motifs within unaligned noncoding sequences clustered by whole-genome mRNA quantitation</ArticleTitle>


												<JournalTitle>Nat Biotechnol</JournalTitle>


												<VolumeID>16</VolumeID>


												<FirstPage>939</FirstPage>


												<LastPage>945</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1038/nbt1098-939</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR31">

											<CitationNumber>31.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>X</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DL</Initials>


													<FamilyName>Brutlag</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JS</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">BioProspector: discovering conserved DNA motifs in upstream regulatory regions of co-expressed genes</ArticleTitle>


												<JournalTitle>Pac Symp Biocomput</JournalTitle>


												<VolumeID />


												<FirstPage>127</FirstPage>


												<LastPage>138</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR32">

											<CitationNumber>32.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>XS</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DL</Initials>


													<FamilyName>Brutlag</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JS</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">An algorithm for finding protein-DNA binding sites with applications to chromatin-immunoprecipitation microarray experiments</ArticleTitle>


												<JournalTitle>Nat Biotechnol</JournalTitle>


												<VolumeID>20</VolumeID>


												<FirstPage>835</FirstPage>


												<LastPage>839</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR33">

											<CitationNumber>33.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TL</Initials>


													<FamilyName>Bailey</FamilyName>


												</BibAuthorName>


												<Year>1995</Year>


												<ArticleTitle
Language="En">Unsupervised learning of multiple motifs in biopolymers using expectation maximization</ArticleTitle>


												<JournalTitle>Machine Learning</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>51</FirstPage>


												<LastPage>80</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR34">

											<CitationNumber>34.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Thijs</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Marchal</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Lescot</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Rombauts</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MB</Initials>


													<FamilyName>De</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Rouze</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Moreau</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">A Gibbs sampling method to detect overrepresented motifs in the upstream regions of coexpressed genes</ArticleTitle>


												<JournalTitle>J Comput Biol</JournalTitle>


												<VolumeID>9</VolumeID>


												<FirstPage>447</FirstPage>


												<LastPage>464</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1089/10665270252935566</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR35">

											<CitationNumber>35.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Buhler</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tompa</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Finding motifs using random projections</ArticleTitle>


												<JournalTitle>J Comput Biol</JournalTitle>


												<VolumeID>9</VolumeID>


												<FirstPage>225</FirstPage>


												<LastPage>242</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1089/10665270252935430</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR36">

											<CitationNumber>36.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Nilsson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Persson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>HG</Initials>


													<FamilyName>von</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Prediction of partial membrane protein topologies using a consensus approach</ArticleTitle>


												<JournalTitle>Protein Sci</JournalTitle>


												<VolumeID>11</VolumeID>


												<FirstPage>2974</FirstPage>


												<LastPage>2980</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1110/ps.0226702</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR37">

											<CitationNumber>37.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Poluliakh</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Takagi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Nakai</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Melina: motif extraction from promoter regions of potentially co-regulated genes</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>19</VolumeID>


												<FirstPage>423</FirstPage>


												<LastPage>424</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/btf872</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR38">

											<CitationNumber>38.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Salgado</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Gama-Castro</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Martinez-Antonio</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>az-Peredo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>F</Initials>


													<FamilyName>Sanchez-Solano</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Peralta-Gil</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Garcia-Alonso</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Jimenez-Jacinto</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Santos-Zavaleta</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Bonavides-Martinez</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Collado-Vides</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">RegulonDB (version 4.0): transcriptional regulation, operon organization and growth conditions in Escherichia coli K-12</ArticleTitle>


												<JournalTitle>Nucleic Acids Res</JournalTitle>


												<VolumeID>32</VolumeID>


												<FirstPage>D303</FirstPage>


												<LastPage>D306</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/gkh140</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR39">

											<CitationNumber>39.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Kanehisa</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Goto</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Kawashima</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Okuno</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Hattori</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">The KEGG resource for deciphering the genome</ArticleTitle>


												<JournalTitle>Nucleic Acids Res</JournalTitle>


												<VolumeID>32 Database issue</VolumeID>


												<FirstPage>D277</FirstPage>


												<LastPage>D280</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/gkh063</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR40">

											<CitationNumber>40.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Hu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>YD</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Kihara</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Supplementary material for the paper</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:Authors>

						<meta:Author>Hu, Jianjun</meta:Author>

						<meta:Author>Yang, Yifeng</meta:Author>

						<meta:Author>Kihara, Daisuke</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution geo="-86.9160800,40.4220971,0">

							<meta:OrgName>Purdue University</meta:OrgName>

							<meta:GeoOrg>-86.9160800,40.4220971,0#Purdue University</meta:GeoOrg>

							<meta:Country>United States</meta:Country>

						</meta:Institution>

						<meta:Institution geo="-86.9160800,40.4220971,0">

							<meta:OrgName>Purdue University</meta:OrgName>

							<meta:GeoOrg>-86.9160800,40.4220971,0#Purdue University</meta:GeoOrg>

							<meta:Country>United States</meta:Country>

						</meta:Institution>

						<meta:Institution geo="-86.9160800,40.4220971,0">

							<meta:OrgName>Purdue University</meta:OrgName>

							<meta:GeoOrg>-86.9160800,40.4220971,0#Purdue University</meta:GeoOrg>

							<meta:Country>United States</meta:Country>

						</meta:Institution>

						<meta:Institution geo="-86.9160800,40.4220971,0">

							<meta:OrgName>Purdue University</meta:OrgName>

							<meta:GeoOrg>-86.9160800,40.4220971,0#Purdue University</meta:GeoOrg>

							<meta:Country>United States</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2006-07-13</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1471-2105-7-342</meta:DOI>

					<meta:Title>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</meta:Title>

					<meta:ISXN>1471-2105</meta:ISXN>

					<meta:Journal>BMC Bioinformatics</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>342</meta:ArticleFirstPage>

					<meta:Publication>BMC Bioinformatics</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Bioinformatics</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Microarrays</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Combinatorial Libraries</meta:Subject>

						<meta:Subject Priority="6"
Type="Secondary">Algorithms</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images>

				<Image Id="5-10.1186_1471-2105-7-342-0" xml:lang="en"
language="en">

					<Caption>

						<p>Overview of the EMD algorithm</p>


					</Caption>

					<FullText>

						<p>
The overview of the algorithm is provided in Figure 1 ..
						</p>

						<p>
Figure 1 illustrates the collecting phase of sites in the input
sequence number 1 from all the algorithms, A 

							<sub>1</sub>
 to A

							<sub>M</sub>
 ..

						</p>

						<p>
Figure 1 shows the number of votes V 

							<sub>p</sub>
 along the
sequence position p for the site group 1..

						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-7-342-1.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Hu, Jianjun</Author>

						<Author>Yang, Yifeng</Author>

						<Author>Kihara, Daisuke</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</Institution>

					</Institutions>

					<ArticleTitle>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>

					<DOI>10.1186/1471-2105-7-342</DOI>

					<PubDate>2006-07-13</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>7</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Hu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>ensemble</Keyword>

						<Keyword>discovering</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>sequences</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>motifs</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>EMD</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-7-342.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T11:13:33.108Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-7-342-1" xml:lang="en"
language="en">

					<Caption>

						<p>Scalability of the EMD algorithms</p>


					</Caption>

					<FullText>

						<p>

							<p>Figure 2 shows that all of the EMD algorithms outperform MD and
RS-BP in terms of nPC when the margin size is up to 200.</p>


						</p>

						<p>
coli is 300 nt, and 95.2% of them are shorter than 420 nt (which
corresponds to the sequences of a margin size of 200), where the
EMD algorithms showed their superiority over the component
algorithm (Fig 2 )..
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-7-342-2.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Hu, Jianjun</Author>

						<Author>Yang, Yifeng</Author>

						<Author>Kihara, Daisuke</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</Institution>

					</Institutions>

					<ArticleTitle>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>

					<DOI>10.1186/1471-2105-7-342</DOI>

					<PubDate>2006-07-13</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>7</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Hu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>ensemble</Keyword>

						<Keyword>discovering</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>sequences</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>motifs</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>EMD</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-7-342.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T11:13:33.108Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-7-342-4" xml:lang="en"
language="en">

					<Caption>

						<p>The performance (nPC) of EMD-2 algorithms with respect to the number of runs of its component algorithms 

							<sup>a)</sup>
 .
						</p>


					</Caption>

					<FullText>

						<p>
We examined the effect of the number of different runs of component
algorithms in terms of the nucleotide level prediction accuracy
(nPC) (Table 3 ).
						</p>

						<p>
In Table 3 , the number of runs of the EMD algorithms composed of
two component algorithms is changed from 5 to 20 with an interval
of 5 runs.
						</p>

						<p>

							<p>From Table 3 , it can be seen that on average, increasing the
number of runs contributes to the improvement of the performance
(see the average nPC value at the bottom row).</p>


						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T3">

							<Caption Language="En">

								<CaptionNumber>Table 3</CaptionNumber>


								<CaptionContent>

									<SimplePara>The performance (nPC) of EMD-2 algorithms with respect to the number of runs of its component algorithms 

										<Superscript>a)</Superscript>
 .
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="5">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara>Algorithm\No. of Runs</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>5</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>15</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>20</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-BP</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.219</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.216</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">0.219</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.213</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-ME</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.231</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.231</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">0.241</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.240</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.182</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.208</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.203</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.203</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>BP-MD</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.190</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.206</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.197</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.208</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>BP-ME</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.240</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.242</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.238</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.241</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>ME-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.210</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.216</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.214</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.213</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>BP-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.208</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.207</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.207</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.207</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>MD-ME</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.222</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.233</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.227</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.235</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-MD</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.236</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.236</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.236</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.250</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>MD-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.195</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.225</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.220</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.220</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Average</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.214</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.222</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.220</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.223</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>a) ECRDB61B-200 (margin size of 200) data set is used.b) The nPC value is in bold if it is the best performance among tested for that EMD algorithm.</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Hu, Jianjun</Author>

						<Author>Yang, Yifeng</Author>

						<Author>Kihara, Daisuke</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</Institution>

					</Institutions>

					<ArticleTitle>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>

					<DOI>10.1186/1471-2105-7-342</DOI>

					<PubDate>2006-07-13</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>7</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>ensemble</Keyword>

						<Keyword>discovering</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>sequences</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>motifs</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>EMD</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Hu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-7-342.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2009-11-26T11:13:33.108Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-7-342-5" xml:lang="en"
language="en">

					<Caption>

						<p>The performance of AL-BP-MD-ME-MS on a dataset of shuffled margin sequences 

							<sup>a)</sup>
 .
						</p>


					</Caption>

					<FullText />

					<Table>

						<Table Float="No" ID="T4">

							<Caption Language="En">

								<CaptionNumber>Table 4</CaptionNumber>


								<CaptionContent>

									<SimplePara>The performance of AL-BP-MD-ME-MS on a dataset of shuffled margin sequences 

										<Superscript>a)</Superscript>
 .
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="10">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<colspec colname="c7" colnum="7" />


								<colspec colname="c8" colnum="8" />


								<colspec colname="c9" colnum="9" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


										<entry colname="c1">

											<SimplePara>nPC</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>nSn</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>nSp</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Margin size (nt)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>100</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>200</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>400</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>100</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>200</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>400</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>100</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>200</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>400</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Original set 

												<Superscript>b)</Superscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.288</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.254</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.197</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.328</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.292</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.234</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>0.416</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>0.360</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>0.270</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Shuffled set 

												<Superscript>c)</Superscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.317</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.255</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.187</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.340</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.275</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.201</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>0.481</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>0.375</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>0.266</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>a) To the both sides of known sites in the ECRDB61B data set, artificially shuffled sequences with the size of 100, 200, and 400 nt are attached. The statistics of the di-mer nucleotide frequency used to generate the shuffled margin sequences are taken from intergenic regions of the 

									<Emphasis Type="Italic">E. coli</Emphasis>
 genome.b) The performance on the original ECRDB61B-100, 200, 400 set.c) The performance on the data set with shuffled margin sequences.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Hu, Jianjun</Author>

						<Author>Yang, Yifeng</Author>

						<Author>Kihara, Daisuke</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</Institution>

					</Institutions>

					<ArticleTitle>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>

					<DOI>10.1186/1471-2105-7-342</DOI>

					<PubDate>2006-07-13</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>7</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>ensemble</Keyword>

						<Keyword>discovering</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>sequences</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>motifs</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>EMD</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Hu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-7-342.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2009-11-26T11:13:33.108Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-7-342-2" xml:lang="en"
language="en">

					<Caption>

						<p>The prediction accuracy tested on ECRDB62A set.</p>


					</Caption>

					<FullText>

						<p>

							<p>In Table 1 , the performance of the best ensemble algorithm in
EMD-2 to EMD-5 is compared with that of the five stand-alone
component algorithms, five multi-restart algorithms (RS-XX) and a
random algorithm on the intergenic sequence data set.</p>


						</p>

						<p>
Note that the four EMD algorithms in Table 1 are the best EMD
algorithms amongst those with the same number of employed component
algorithms benchmarked on ECRDB62A dataset.
						</p>

						<p>
This is consistent with the observation from Table 1 that
multi-restart algorithms perform the worst.
						</p>

						<p>
In terms of the nucleotide level accuracy (nPC) on intergenic
region data set, the best EMD algorithm, AL-BP-MD achieved about 4
points (or 22.4%) better accuracy than that of the best component
algorithm, BioProspector (Table 1 ).
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T1">

							<Caption Language="En">

								<CaptionNumber>Table 1</CaptionNumber>


								<CaptionContent>

									<SimplePara>The prediction accuracy tested on ECRDB62A set.</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="7">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara>Algorithm</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>nPC</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>nSn</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>nSp</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>sPC</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>sSn</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>sSp</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>BP-MD 

												<Superscript>a)</Superscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.183</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.215</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.280</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.303</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.428</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.407</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">AL-BP-MD</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.213</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.262</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">0.296</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.324</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">0.456</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">0.437</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-BP-MD-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.209</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.255</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.293</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.321</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.423</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.446</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-BP-MD-ME-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.197</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.238</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.286</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.316</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.438</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.437</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AlignACE</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.141</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.218</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.171</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.264</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.351</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.396</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">BioProspector</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.174</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.205</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">0.268</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.287</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.415</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.369</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>MDScan</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.146</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.174</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.223</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.244</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.345</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.349</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">MEME</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.160</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.260</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.190</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.300</Emphasis>


												<Superscript>d)</Superscript>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">0.440</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">0.430</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">MotifSampler</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.150</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.180</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.230</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.300</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">0.320</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">0.490</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>RS-AL 

												<Superscript>b)</Superscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.139</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.204</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.166</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.229</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.329</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.341</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">RS-BP</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.150</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">0.178</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">0.231</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">0.262</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">0.390</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">0.350</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>RS-MD</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.107</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.125</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.169</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.170</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.254</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.271</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>RS-ME</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.133</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.162</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.203</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.213</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.418</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.282</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>RS-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.127</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.148</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.187</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.235</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.260</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.384</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Random 

												<Superscript>c)</Superscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.050</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.061</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.083</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>0.100</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.161</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>0.146</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>a) The best algorithm among EMD-X (X = 2~5) are compared with component algorithms, b) the multi-restart algorithms, and c) the random algorithms. The best performances in terms of nPC or sPC among algorithms of a same category are highlighted in bold. d) Both MEME and MotifSampler are highlighted because they have the same performance in terms of sPC.</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Hu, Jianjun</Author>

						<Author>Yang, Yifeng</Author>

						<Author>Kihara, Daisuke</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</Institution>

					</Institutions>

					<ArticleTitle>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>

					<DOI>10.1186/1471-2105-7-342</DOI>

					<PubDate>2006-07-13</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>7</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>ensemble</Keyword>

						<Keyword>discovering</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>sequences</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>motifs</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>EMD</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Hu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-7-342.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2009-11-26T11:13:33.108Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-7-342-3" xml:lang="en"
language="en">

					<Caption>

						<p>Comparison of nucleotide level prediction accuracy (nPC) of EMD algorithms with different combinations of component algorithms 

							<sup>a)</sup>
 .
						</p>


					</Caption>

					<FullText>

						<p>
Table 2 shows the nPC scores for EMD algorithms with all possible
combinations of component algorithms tested on ECRDB61B-200 data
set (200 nt placed on both sides of known sites).
						</p>

						<p>
Please also note that the algorithms shown in Table 2 are all EMD
algorithms even if some EMD algorithms only employ a single
component algorithm; e.g.
						</p>

						<p>
EMD-AL in Table 2 , results of 20 runs of AlignACE are combined..
						</p>

						<p>

							<p>Firstly, on average the accuracy improves as the number of
component algorithms increases (Table 2 ).</p>


						</p>

						<p>
As shown in Table 2 , sometimes a combination of a relatively more
accurate algorithm and a relatively less accurate one performs
better than a combination of two average ones, so the resulting
performance of an EMD algorithm depends on the compatibility of
component algorithms, not necessarily to the accuracy of individual
algorithms.
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T2">

							<Caption Language="En">

								<CaptionNumber>Table 2</CaptionNumber>


								<CaptionContent>

									<SimplePara>Comparison of nucleotide level prediction accuracy (nPC) of EMD algorithms with different combinations of component algorithms 

										<Superscript>a)</Superscript>
 .
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="6">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara>Algorithm</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>nPC</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>Algorithm</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>nPC</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>Algorithm</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>nPC</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.178</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>AL-BP</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.213</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>AL-BP-MD</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.239</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>BP</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.201</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>AL-ME</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.203</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>AL-BP-ME</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.225</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">MD</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.228</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>AL-MS</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.208</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>AL-BP-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.213</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>ME</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.199</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>BP-MD</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.241</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis
Type="Bold">AL-MD-ME</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">0.250</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.206</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>BP-ME</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.214</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>AL-MD-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.241</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Average</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.202</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>ME-MS</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.220</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>AL-ME-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.220</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


										<entry colname="c1">

											<SimplePara />


										</entry>


										<entry colname="c2">

											<SimplePara />


										</entry>


										<entry colname="c3">

											<SimplePara />


										</entry>


										<entry colname="c4">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">AL-BP-MD-ME</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">0.254</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>BP-MS</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.207</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>BP-MD-ME</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.243</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-BP-MD-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.244</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>MD-ME</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.235</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>BP-MD-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.249</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-BP-ME-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.232</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>AL-MD</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.240</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>BP-ME-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.223</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AL-MD-ME-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.252</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">MD-MS</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">0.250</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>MD-ME-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.241</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>BP-MD-ME-MS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.248</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>Average</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.223</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>Average</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>0.234</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


										<entry colname="c1">

											<SimplePara />


										</entry>


										<entry colname="c2">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Average</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.246</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara />


										</entry>


										<entry colname="c3">

											<SimplePara />


										</entry>


										<entry colname="c4">

											<SimplePara>AL-BP-MD-ME-MS</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">0.254</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>a) Tested on ECRDB61B-200 data set. b) The best EMD algorithms among those of a given number of component algorithms are highlighted in bold.</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Hu, Jianjun</Author>

						<Author>Yang, Yifeng</Author>

						<Author>Kihara, Daisuke</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Computer Science, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Department of Biological Sciences, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>Markey Center for Structural Biology, Purdue University, West Lafayette, IN, 47907, USA</Institution>

						<Institution>The Bindley Bioscience Center, Discovery Park, Purdue University, West Lafayette, IN, 47907, USA</Institution>

					</Institutions>

					<ArticleTitle>EMD: an ensemble algorithm for discovering regulatory motifs in DNA sequences</ArticleTitle>

					<DOI>10.1186/1471-2105-7-342</DOI>

					<PubDate>2006-07-13</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>7</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>ensemble</Keyword>

						<Keyword>discovering</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>sequences</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>motifs</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>EMD</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Hu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-7-342.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2009-11-26T11:13:33.108Z</DateLoaded>

				</Image>

			</Images>

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1471-2105</JournalID>


						<JournalPrintISSN>1471-2105</JournalPrintISSN>


						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>


						<JournalTitle>BMC Bioinformatics</JournalTitle>


						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Bioinformatics</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Microarrays</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Combinatorial Libraries</JournalSubject>


							<JournalSubject Priority="6"
Type="Secondary">Algorithms</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>11</VolumeIDStart>


							<VolumeIDEnd>11</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>1</IssueIDStart>


								<IssueIDEnd>1</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2010</Year>


										<Month>3</Month>


										<Day>16</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Lee et al; licensee BioMed Central Ltd.</CopyrightHolderName>


									<CopyrightYear>2010</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1471-2105-11-132</ArticleID>


									<ArticleDOI>10.1186/1471-2105-11-132</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Methodology article</ArticleCategory>


									<ArticleFirstPage>132</ArticleFirstPage>


									<ArticleLastPage>132</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year>2009</Year>


											<Month>11</Month>


											<Day>18</Day>


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year>2010</Year>


											<Month>3</Month>


											<Day>16</Day>


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Lee et al; licensee BioMed Central Ltd.</CopyrightHolderName>


										<CopyrightYear>2010</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>11</VolumeIDStart>


										<VolumeIDEnd>11</VolumeIDEnd>


										<IssueIDStart>1</IssueIDStart>


										<IssueIDEnd>1</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1 I2 I3">

											<AuthorName DisplayOrder="Western">

												<GivenName>Hsiao</GivenName>


												<FamilyName>Lee</FamilyName>


											</AuthorName>


											<Contact>

												<Email>ping@csmu.edu.tw</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I4">

											<AuthorName DisplayOrder="Western">

												<GivenName>Tzu-Fang</GivenName>


												<FamilyName>Sheu</FamilyName>


											</AuthorName>


											<Contact>

												<Email>fang@pu.edu.tw</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I3">

											<AuthorName DisplayOrder="Western">

												<GivenName>Chuan</GivenName>


												<FamilyName>Tang</FamilyName>


											</AuthorName>


											<Contact>

												<Email>cytang@cs.nthu.edu.tw</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I2">

											<OrgName>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I3">

											<OrgName>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I4">

											<OrgName>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>DNA signatures are distinct short nucleotide sequences that provide valuable information that is used for various purposes, such as the design of Polymerase Chain Reaction primers and microarray experiments. Biologists usually use a discovery algorithm to find unique signatures from DNA databases, and then apply the signatures to microarray experiments. Such discovery algorithms require to set some input factors, such as signature length 

												<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

												<Emphasis Type="Italic">d</Emphasis>
 , which affect the discovery results. However, suggestions about how to select proper factor values are rare, especially when an unfamiliar DNA database is used. In most cases, biologists typically select factor values based on experience, or even by guessing. If the discovered result is unsatisfactory, biologists change the input factors of the algorithm to obtain a new result. This process is repeated until a proper result is obtained. Implicit signatures under the discovery condition ( 

												<Emphasis Type="Italic">l</Emphasis>
 , 

												<Emphasis Type="Italic">d</Emphasis>
 ) are defined as the signatures of length ≤ 

												<Emphasis Type="Italic">l</Emphasis>
 with mismatch tolerance ≥ 

												<Emphasis Type="Italic">d</Emphasis>
 . A discovery algorithm that could discover all implicit signatures, such that those that meet the requirements concerning the results, would be more helpful than one that depends on trial and error. However, existing discovery algorithms do not address the need to discover all implicit signatures.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>This work proposes two discovery algorithms - the consecutive multiple discovery (CMD) algorithm and the parallel and incremental signature discovery (PISD) algorithm. The PISD algorithm is designed for efficiently discovering signatures under a certain discovery condition. The algorithm finds new results by using previously discovered results as candidates, rather than by using the whole database. The PISD algorithm further increases discovery efficiency by applying parallel computing. The CMD algorithm is designed to discover implicit signatures efficiently. It uses the PISD algorithm as a kernel routine to discover implicit signatures efficiently under every feasible discovery condition.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusions</Heading>


											<Para>The proposed algorithms discover implicit signatures efficiently. The presented CMD algorithm has up to 97% less execution time than typical sequential discovery algorithms in the discovery of implicit signatures in experiments, when eight processing cores are used.</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Heading>Keywords</Heading>


										<Keyword>parallel</Keyword>


										<Keyword>unique</Keyword>


										<Keyword>algorithm</Keyword>


										<Keyword>databases</Keyword>


										<Keyword>DNA</Keyword>


										<Keyword>incremental</Keyword>


										<Keyword>discovery</Keyword>


										<Keyword>efficient</Keyword>


										<Keyword>signature</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_37645">

										<Heading>Background</Heading>


										<Para>Mutations introduce variations and divergence into DNA sequences within and among species. Differences among DNA sequences are extensively used to identify species 

											<CitationRef
CitationID="B1">1</CitationRef>


											<CitationRef
CitationID="B2">2</CitationRef>


											<CitationRef
CitationID="B3">3</CitationRef>


											<CitationRef
CitationID="B4">4</CitationRef>
 . For example, specific oligonucleotides have already been used in the Polymerase Chain Reaction (PCR) method to identify 14 human pathogenic yeast species 

											<CitationRef
CitationID="B5">5</CitationRef>
 . A unique DNA signature is a sequence that occurs in a DNA database only once, and has some minimum mutation distance from all other sequences in the database. Unique signature discovery 

											<CitationRef
CitationID="B6">6</CitationRef>
 is the finding of unique signatures in a set of DNA sequences. They are accelerating various areas of research, including the map-based cloning of genes that control traits, comparative genome analysis, protein identification, and the development of various methods that depend on gene-specific oligonucleotides, such as the DNA microarray technology.
										</Para>


										<Para>The methods of signature discovery have been widely studied, and many related tools and applications have been developed 

											<CitationRef
CitationID="B1">1</CitationRef>


											<CitationRef
CitationID="B6">6</CitationRef>


											<CitationRef
CitationID="B7">7</CitationRef>


											<CitationRef
CitationID="B8">8</CitationRef>


											<CitationRef
CitationID="B9">9</CitationRef>


											<CitationRef
CitationID="B10">10</CitationRef>


											<CitationRef
CitationID="B11">11</CitationRef>


											<CitationRef
CitationID="B12">12</CitationRef>


											<CitationRef
CitationID="B13">13</CitationRef>


											<CitationRef
CitationID="B14">14</CitationRef>


											<CitationRef
CitationID="B15">15</CitationRef>


											<CitationRef
CitationID="B16">16</CitationRef>
 . For example, 

											<CitationRef
CitationID="B14">14</CitationRef>
 integrates multiple bioinformatics algorithms to determine horizontally transferred, pathotype-specific signature genes as targets for specific, high-throughput molecular diagnostic applications and reverse vaccinology screens; insignia 

											<CitationRef
CitationID="B15">15</CitationRef>
 is a web application for rapidly identifying unique DNA signatures, and hybseek 

											<CitationRef
CitationID="B16">16</CitationRef>
 is a web service for efficiently designing both pathogen-specific and compatible primer pairs for DNA-based diagnostic multi-analyte assays.
										</Para>


										<Para>The algorithm of Zheng et al. 

											<CitationRef
CitationID="B17">17</CitationRef>
 and IMUS 

											<CitationRef
CitationID="B18">18</CitationRef>
 are two hamming-distance-based unique signature discovery algorithms. These two algorithms deal with DNA databases. Let 

											<Emphasis Type="Italic">l</Emphasis>
 and 

											<Emphasis Type="Italic">d</Emphasis>
 be two positive integers, where 

											<Emphasis Type="Italic">d</Emphasis>
 ≤ 

											<Emphasis Type="Italic">l</Emphasis>
 . An 

											<Emphasis Type="Italic">l</Emphasis>
 -pattern is a string of 

											<Emphasis Type="Italic">l</Emphasis>
 characters in the alphabet set{A, C, G, T}. A pattern 

											<Emphasis Type="Italic">P</Emphasis>
 is ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 )-mismatched to a pattern 

											<Emphasis Type="Italic">Q</Emphasis>
 if the length of 

											<Emphasis Type="Italic">P</Emphasis>
 and 

											<Emphasis Type="Italic">Q</Emphasis>
 is 

											<Emphasis Type="Italic">l</Emphasis>
 and the hamming distance, which is the number of mismatches, between 

											<Emphasis Type="Italic">P</Emphasis>
 and 

											<Emphasis Type="Italic">Q</Emphasis>
 does not exceed 

											<Emphasis Type="Italic">d</Emphasis>
 . An 

											<Emphasis Type="Italic">l</Emphasis>
 -pattern 

											<Emphasis Type="Italic">P</Emphasis>
 is referred to as a unique signature with mismatch tolerance 

											<Emphasis Type="Italic">d</Emphasis>
 if and only if no other pattern 

											<Emphasis Type="Italic">Q</Emphasis>
 exists in the given DNA database such that 

											<Emphasis Type="Italic">P</Emphasis>
 and 

											<Emphasis Type="Italic">Q</Emphasis>
 are ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 )-mismatched. Zheng's algorithm and the IMUS algorithm are designed for efficiently discovering the unique signatures under the discovery conditions of signature length 

											<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

											<Emphasis Type="Italic">d</Emphasis>
 .
										</Para>


										<Para>Zheng's algorithm, called the UO algorithm hereafter, is based on the observation that if two patterns, 

											<Emphasis Type="Italic">P</Emphasis>
 and 

											<Emphasis Type="Italic">Q</Emphasis>
 , are ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 )-mismatched, then at least one of the partitions of 

											<Emphasis Type="Italic">λ P</Emphasis>
 is ( 

											<Emphasis Type="Italic">l</Emphasis>
 / 

											<Emphasis Type="Italic">λ</Emphasis>
 , 1)-mismatched to the corresponding part in 

											<Emphasis Type="Italic">Q</Emphasis>
 , where 

											<Emphasis Type="Italic">λ</Emphasis>
 = ⌊ 

											<Emphasis Type="Italic">d</Emphasis>
 /2⌋ + 1 and all partitions have equal length. The UO algorithm is a two-phase algorithm. In the first phase, the algorithm divides DNA sequences into patterns of length 

											<Emphasis Type="Italic">l</Emphasis>
 / 

											<Emphasis Type="Italic">λ</Emphasis>
 . An index system is built based on the 

											<Emphasis Type="Italic">l</Emphasis>
 / 

											<Emphasis Type="Italic">λ</Emphasis>
 -patterns as index keys, in which 

											<Emphasis Type="Italic">l</Emphasis>
 -patterns that contain the same index key are gathered in a single index entry. Assume that 

											<Emphasis Type="Italic">K</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">P</Emphasis>

											</Subscript>
 is an index key, and 

											<Emphasis Type="Italic">K</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">Q</Emphasis>

											</Subscript>
 is one of the keys that are ( 

											<Emphasis Type="Italic">l</Emphasis>
 / 

											<Emphasis Type="Italic">λ</Emphasis>
 , 1)-mismatched to 

											<Emphasis Type="Italic">K</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">P</Emphasis>

											</Subscript>
 . In the second phase, the UO algorithm performs complete string comparisons on the 

											<Emphasis Type="Italic">l</Emphasis>
 -patterns in the entries 

											<Emphasis Type="Italic">K</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">Q</Emphasis>

											</Subscript>
 and 

											<Emphasis Type="Italic">K</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">P</Emphasis>

											</Subscript>
 to check whether they are ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 )-mismatched. The unique signatures emerge after all of the duplicated patterns have been pruned.
										</Para>


										<Para>The IMUS algorithm improves upon the UO algorithm. The IMUS algorithm is based on the observation that if two patterns 

											<Emphasis Type="Italic">P</Emphasis>
 and 

											<Emphasis Type="Italic">Q</Emphasis>
 are ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 )-mismatched, then at least one of the two halves of 

											<Emphasis Type="Italic">P</Emphasis>
 is ( 

											<Emphasis Type="Italic">l</Emphasis>
 /2, ⌊ 

											<Emphasis Type="Italic">d</Emphasis>
 /2⌋)-mismatched to the corresponding part of 

											<Emphasis Type="Italic">Q</Emphasis>
 . In the processing-kernel level, the UO and IMUS algorithms are similar. The main difference between them is the number of partitions in an 

											<Emphasis Type="Italic">l</Emphasis>
 -pattern. The IMUS algorithm divides an 

											<Emphasis Type="Italic">l</Emphasis>
 -pattern into two partitions, whereas the UO algorithm divides a pattern into ⌊ 

											<Emphasis Type="Italic">d</Emphasis>
 /2⌋ + 1 partitions. Since the mismatch tolerance 

											<Emphasis Type="Italic">d</Emphasis>
 is small (usually 

											<Emphasis Type="Italic">d &lt;</Emphasis>
 6) in most discoveries of short signatures (of length 

											<Emphasis Type="Italic">l</Emphasis>
 ≤ 40), the IMUS algorithm reduces the number of partitions in an 

											<Emphasis Type="Italic">l</Emphasis>
 -pattern to decrease the number of required string comparisons, and thus increases the discovery efficiency. A consequence is that more memory is required to store the index that is used in the IMUS algorithm. An additional frequency filter, which represents an enhanced usage of the frequency distance, defined in 

											<CitationRef
CitationID="B19">19</CitationRef>
 , is used in the IMUS algorithm as a pre-filter to prevent unnecessary comparisons between dissimilar patterns. However, most signature discovery algorithms have the problem that we do not know how to select proper factor values, such as the proper ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 ) values in the UO or IMUS algorithm, because the proper discovery result is defined on a case-by-case basis. In most cases, factor values are selected based on domain knowledge or experience or even by guessing. The factor settings are then used in the discovery algorithm to discover signatures. If the result is unacceptable, then the factor values are changed to get other results. The process is repeated until satisfactory results are found. This situation often arises when an unfamiliar DNA database is being used. A method that can efficiently find all of the signatures that satisfy feasible discovery conditions, instead of repeated trial and error, enabling users to select the proper signatures, is needed. In other words, when the discovery condition is given in terms of signature length 

											<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

											<Emphasis Type="Italic">d</Emphasis>
 , a discovery algorithm can be use to discover not only the signatures with exact ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 ) but also all signatures that meet stricter discovery conditions - with a length smaller than 

											<Emphasis Type="Italic">l</Emphasis>
 or a mismatch tolerance larger than 

											<Emphasis Type="Italic">d</Emphasis>
 . Then, the signatures that meet our requirements can be selected directly from the results. The signatures of length ≤ 

											<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance ≥ 

											<Emphasis Type="Italic">d</Emphasis>
 are called the implicit signatures under the discovery condition ( 

											<Emphasis Type="Italic">l</Emphasis>
 , 

											<Emphasis Type="Italic">d</Emphasis>
 ). Providing researchers with all implicit signatures without manually changing the factor values would be helpful. One challenge is how to discover efficiently all implicit signatures from DNA databases under a certain discovery condition. An intuitive solution is to use the UO or IMUS algorithm iteratively to perform a complete discovery under all feasible discovery conditions. However, this solution is not sufficiently efficient. The UO and IMUS algorithms are specifically designed for discovering signatures that meet a certain discovery condition, but they cannot discover all of the implicit signatures. Accordingly, an efficient algorithm for discovering all implicit signatures under a certain discovery condition is needed.
										</Para>


										<Para>The idea of the 'incremental' has been used in many research areas, such as data mining and knowledge discovery 

											<CitationRef
CitationID="B20">20</CitationRef>


											<CitationRef
CitationID="B21">21</CitationRef>
 , communications 

											<CitationRef
CitationID="B22">22</CitationRef>


											<CitationRef
CitationID="B23">23</CitationRef>


											<CitationRef
CitationID="B24">24</CitationRef>


											<CitationRef
CitationID="B25">25</CitationRef>
 and computer graphics and visualization 

											<CitationRef
CitationID="B26">26</CitationRef>


											<CitationRef
CitationID="B27">27</CitationRef>
 . The definitions of the term 'incremental' vary slightly among fields. Here, 'incremental' is used to refer to the fact that a new result is obtained by processing the previously discovered signatures, rather than by performing a complete discovery on the whole database. Additionally, since an increasing number of computers have multi-core processors, parallel computing is applied to accelerate the signature discovery processes. This work proposes an algorithm that is called the Consecutive Multiple Discovery (CMD) algorithm, which is designed specifically for discovering all implicit signatures under a certain discovery condition from DNA databases. The CMD algorithm is an iterative algorithm. It includes an algorithm called Parallel and Incremental Signature Discovery (PISD) algorithm as a kernel routine. The PISD algorithm enhances the hamming-distance-based unique signature discovery algorithms, the UO and IMUS algorithms, by using the incremental and parallel computing techniques. The PISD algorithm is based on observations of hamming-distance-based signatures, and discovers new results by reusing previously discovered signatures but with looser discovery conditions. For example, the algorithm can find signatures of length 

											<Emphasis Type="Italic">l</Emphasis>
 = 28 and mismatch tolerance 

											<Emphasis Type="Italic">d</Emphasis>
 = 4 by processing the signatures of 

											<Emphasis Type="Italic">l</Emphasis>
 = 30 and 

											<Emphasis Type="Italic">d</Emphasis>
 = 2. The scope of the search is far smaller than the size of the input database. The PISD algorithm runs faster than the typical UO and IMUS algorithms because it reuses the discovered signatures as candidates, rather than all of the patterns in the database. Based on the results from the experiments on human chromosome 13 EST databases, the proposed CMD algorithm discovers all implicit signatures and performs 33.74 times faster than the typical algorithm when eight processing cores are used.
										</Para>


									</Section1>


									<Section1 ID="Sec_82004">

										<Heading>Results and Discussion</Heading>


										<Section2 ID="Sec_39711">

											<Heading>Algorithm</Heading>


											<Para>The proposed Consecutive Multiple Discovery (CMD) algorithm efficiently discovers all of the implicit signatures of length ≤ 

												<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance ≥ 

												<Emphasis Type="Italic">d</Emphasis>
 under the discovery condition ( 

												<Emphasis Type="Italic">l</Emphasis>
 , 

												<Emphasis Type="Italic">d</Emphasis>
 ). The CMD algorithm uses the parallel and incremental signature discovery (PISD) algorithm as a kernel routine. Given a discovery condition ( 

												<Emphasis Type="Italic">l</Emphasis>
 , 

												<Emphasis Type="Italic">d</Emphasis>
 ), the PISD algorithm is designed for efficiently discovering signatures of length 

												<Emphasis Type="Italic">l'</Emphasis>
 and tolerance 

												<Emphasis Type="Italic">d'</Emphasis>
 , and then the CMD algorithm uses the PISD to find all of the implicit signatures of length 

												<Emphasis Type="Italic">l'</Emphasis>
 ≤ 

												<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

												<Emphasis Type="Italic">d'</Emphasis>
 ≥ 

												<Emphasis Type="Italic">d</Emphasis>
 . The PISD algorithm is based on observations of the hamming-distance-based signatures, and uses parallel computing to increase discovery efficiency. The PISD algorithm applies a scheduling heuristic, which is called the parallel entry list (PEL) heuristic, to generate a reordered entry list when parallel computing is used. This entry list improves the performance of the proposed PISD algorithm.
											</Para>


											<Section3 ID="Sec_06313">

												<Heading>The parallel and incremental signature discovery (PISD) algorithm</Heading>


												<Para>Let Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 denote the set of the unique signatures discovered by the UO or IMUS algorithm under the discovery condition ( 

													<Emphasis Type="Italic">l</Emphasis>
 , 

													<Emphasis Type="Italic">d</Emphasis>
 ). We have the observations as follows:
												</Para>


												<Para> 

													<Emphasis
Type="Bold">Observation 1.</Emphasis>
 ∀ 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 -1, 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">P</Emphasis>
 must be a substring of a pattern 

													<Emphasis Type="Italic">Q</Emphasis>
 in Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 .
												</Para>


												<Para>Assume 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 -1, 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">P'</Emphasis>
 is a pattern of length 

													<Emphasis Type="Italic">l</Emphasis>
 - 1. Since 

													<Emphasis Type="Italic">P</Emphasis>
 is a signature of condition ( 

													<Emphasis Type="Italic">l</Emphasis>
 - 1, 

													<Emphasis Type="Italic">d</Emphasis>
 ), HD( 

													<Emphasis Type="Italic">P</Emphasis>
 , 

													<Emphasis Type="Italic">P'</Emphasis>
 ) &gt; 

													<Emphasis Type="Italic">d</Emphasis>
 , where HD( 

													<Emphasis Type="Italic">P</Emphasis>
 , 

													<Emphasis Type="Italic">P'</Emphasis>
 ) is the hamming distance between 

													<Emphasis Type="Italic">P</Emphasis>
 and 

													<Emphasis Type="Italic">P'</Emphasis>
 .
												</Para>


												<Para>Let 

													<Emphasis Type="Italic">x</Emphasis>
 be a character in{A, C, G, T}. Assume 

													<Emphasis Type="Italic">Q</Emphasis>
 = 

													<Emphasis Type="Italic">x</Emphasis>
 + 

													<Emphasis Type="Italic">P</Emphasis>
 and 

													<Emphasis Type="Italic">Q'</Emphasis>
 is a pattern of length 

													<Emphasis Type="Italic">l</Emphasis>
 , where + means string concatenation. HD( 

													<Emphasis Type="Italic">Q</Emphasis>
 , 

													<Emphasis Type="Italic">Q'</Emphasis>
 ) = HD( 

													<Emphasis Type="Italic">x</Emphasis>
 + 

													<Emphasis Type="Italic">P</Emphasis>
 ,+) ≥ HD( 

													<Emphasis Type="Italic">P</Emphasis>
 ,) 

													<Emphasis
Type="Italic">&gt; d</Emphasis>
 , whereis the 

													<Emphasis Type="Italic">i</Emphasis>
 -th character of 

													<Emphasis Type="Italic">Q'</Emphasis>
 anddenotes the substring starting from the 

													<Emphasis Type="Italic">i</Emphasis>
 -th to the 

													<Emphasis Type="Italic">j</Emphasis>
 -th characters in 

													<Emphasis Type="Italic">Q'</Emphasis>
 . Hence, 

													<Emphasis Type="Italic">P</Emphasis>
 is a substring of 

													<Emphasis Type="Italic">Q</Emphasis>
 and 

													<Emphasis Type="Italic">Q</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 in this case.
												</Para>


												<Para>The proof of the case with 

													<Emphasis Type="Italic">Q</Emphasis>
 = 

													<Emphasis Type="Italic">P</Emphasis>
 + 

													<Emphasis Type="Italic">x</Emphasis>
 can be done in the same way, yielding the result that 

													<Emphasis Type="Italic">P</Emphasis>
 is a substring of 

													<Emphasis Type="Italic">Q</Emphasis>
 and 

													<Emphasis Type="Italic">Q</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 .
												</Para>


												<Para>Therefore, the observation holds.</Para>


												<Para> 

													<Emphasis
Type="Bold">Observation 2.</Emphasis>
 ∀ 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>
 +1
													</Subscript>
 , 

													<Emphasis Type="Italic">P</Emphasis>
 must be in Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 .
												</Para>


												<Para>Assume 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>
 +1
													</Subscript>
 and 

													<Emphasis Type="Italic">P'</Emphasis>
 is a pattern of length 

													<Emphasis Type="Italic">l</Emphasis>
 . Since 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>
 +1
													</Subscript>
 , HD( 

													<Emphasis Type="Italic">P</Emphasis>
 , 

													<Emphasis Type="Italic">P'</Emphasis>
 ) &gt; 

													<Emphasis Type="Italic">d</Emphasis>
 + 1 &gt; 

													<Emphasis Type="Italic">d</Emphasis>
 , where HD( 

													<Emphasis Type="Italic">P</Emphasis>
 , 

													<Emphasis Type="Italic">P'</Emphasis>
 ) is the hamming distance between 

													<Emphasis Type="Italic">P</Emphasis>
 and 

													<Emphasis Type="Italic">P'</Emphasis>
 . Thus, 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 . The observation holds.
												</Para>


												<Para> 

													<Emphasis
Type="Bold">Observation 3.</Emphasis>
 ∀ 

													<Emphasis Type="Italic">P</Emphasis>
 ∈ Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 - 

														<Emphasis
Type="Italic">a</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>
 + 

														<Emphasis
Type="Italic">b</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">P</Emphasis>
 must be a substring of a pattern 

													<Emphasis Type="Italic">Q</Emphasis>
 in Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 , where 

													<Emphasis Type="Italic">a</Emphasis>
 and 

													<Emphasis Type="Italic">b</Emphasis>
 are positive integers, and 

													<Emphasis Type="Italic">a</Emphasis>
 &lt; 

													<Emphasis Type="Italic">l</Emphasis>
 .
												</Para>


												<Para>The observations can be used to improve the hamming-distance-based signature discovery algorithms, including the UO and IMUS algorithms. Based on these observations, the unique signatures of factors ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 ) must be discoverable from the unique signatures that satisfy the discovery condition ( 

													<Emphasis Type="Italic">l</Emphasis>
 , 

													<Emphasis Type="Italic">d</Emphasis>
 ), where 

													<Emphasis Type="Italic">l'</Emphasis>
 ≤ 

													<Emphasis Type="Italic">l</Emphasis>
 and 

													<Emphasis Type="Italic">d'</Emphasis>
 ≥ 

													<Emphasis Type="Italic">d</Emphasis>
 . Accordingly, the discovery is incremental, reducing the scope of the search in the discovery process. Hereafter, this heuristic is called 'incremental discovery'.
												</Para>


												<Para>For example, Table 

													<InternalRef
RefID="T1">1(A)</InternalRef>
 presents a DNA database of three sequences. Table 

													<InternalRef
RefID="T1">1(B)</InternalRef>
 lists the five patterns in the database. Table 

													<InternalRef
RefID="T1">1(C)</InternalRef>
 presents Ω 

													<Subscript>5,1</Subscript>
 , Ω 

													<Subscript>5,2</Subscript>
 , Ω 

													<Subscript>4,1</Subscript>
 and Ω 

													<Subscript>4,2</Subscript>
 . Each pattern in Ω 

													<Subscript>5,2</Subscript>
 is in Ω 

													<Subscript>5,1</Subscript>
 , and all of the patterns in Ω 

													<Subscript>4,1</Subscript>
 and Ω 

													<Subscript>4,2</Subscript>
 are implicit in Ω 

													<Subscript>5,1</Subscript>
 . Restated, to discover Ω 

													<Subscript>5,2</Subscript>
 , Ω 

													<Subscript>4,1</Subscript>
 or Ω 

													<Subscript>4,2</Subscript>
 , the patterns in Ω 

													<Subscript>5,1</Subscript>
 can be used as candidates, instead of all of the patterns in the database. Since the number of patterns in Ω 

													<Subscript>5,1</Subscript>
 , 5, is less than the number of patterns in the database, 12, the discovery process is accelerated.
												</Para>


												<Table Float="No" ID="T1">

													<Caption Language="En">

														<CaptionNumber>Table 1</CaptionNumber>


														<CaptionContent>

															<SimplePara>An example of implicit signatures.</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="2">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) A DNA database.</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CCCTAATG</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>TTAATAAT</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>ATAATGCG</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) All 5-patterns in the database.</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CCCTA, CCTAA, CTAAT, TAATG, TTAAT, TAATA</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>AATAA, ATAAT, ATAAT, TAATG, AATGC, ATGCG</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(C) Some unique signatures in the database.</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Ω 

																		<Subscript>5,1</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>CCCTA, CCTAA, AATAA, AATGC, ATGCG</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Ω 

																		<Subscript>5,2</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>ATGCG</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Ω 

																		<Subscript>4,1</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>CCCT, CCTA, ATGC, TGCG</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Ω 

																		<Subscript>4,2</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>TGCG</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>Let Ω 

															<Subscript> 

																<Emphasis
Type="Italic">l</Emphasis>
 , 

																<Emphasis
Type="Italic">d</Emphasis>

															</Subscript>
 denote the set of the unique signatures of length 

															<Emphasis
Type="Italic">l</Emphasis>
 and mismatch tolerance 

															<Emphasis
Type="Italic">d</Emphasis>
 . The result shows that all of the patterns in Ω 

															<Subscript>5,2</Subscript>
 , Ω 

															<Subscript>4,1</Subscript>
 and Ω 

															<Subscript>4,2</Subscript>
 are implicit in Ω 

															<Subscript>5,1</Subscript>
 .
														</SimplePara>


													</tfooter>


												</Table>


												<Para>Additional file1presents the PISD algorithm. Let 

													<Emphasis Type="Italic">l'</Emphasis>
 be the desired signature length and 

													<Emphasis Type="Italic">d'</Emphasis>
 be the mismatch tolerance. Divide all of the DNA sequences in the input database into 

													<Emphasis Type="Italic">α</Emphasis>
 -patterns, where the value of 

													<Emphasis Type="Italic">α</Emphasis>
 is related to the selected hamming-distance-based signature discovery algorithm. For example, 

													<Emphasis Type="Italic">α</Emphasis>
 = 

													<Emphasis Type="Italic">l'</Emphasis>
 /2 for the IMUS algorithm, and 

													<Emphasis Type="Italic">α</Emphasis>
 = 

													<Emphasis Type="Italic">l'</Emphasis>
 /(⌊ 

													<Emphasis Type="Italic">d'</Emphasis>
 /2⌋ + 1) for the UO algorithm. A 

													<Emphasis Type="Italic">l'</Emphasis>
 -pattern comprises 

													<Emphasis Type="Italic">l'</Emphasis>
 / 

													<Emphasis Type="Italic">α</Emphasis>
 consecutive 

													<Emphasis Type="Italic">α</Emphasis>
 -patterns. An index of 4 

													<Superscript> 

														<Emphasis
Type="Italic">α</Emphasis>

													</Superscript>
 entries is built with the 

													<Emphasis Type="Italic">α</Emphasis>
 -patterns as index keys. A multi-level index can be adopted if the index is too large to be fit in the main memory. The 

													<Emphasis Type="Italic">l'</Emphasis>
 -patterns that contain a certain 

													<Emphasis Type="Italic">α</Emphasis>
 -pattern are collected in an entry. Each entry maintains a list of the locations of the pattern in the database, which is called a pattern list. The patterns in the input database are called data patterns, and the patterns that are discovered by a hamming-distance-based signature discovery algorithm are referred to as candidate patterns. Based on the observations of hamming-distance-based discovery and incremental discovery, the new result obtained under stricter discovery conditions can be discovered from the candidate patterns obtained under looser conditions. To accelerate access, the candidate patterns are arranged in the pattern list in an entry prior to the non-candidate patterns. A pointer indicates the end of the candidate patterns in the pattern list. A processing order list of all of the entries in the index is constructed. If a multiple-processor system is used, then the processing order list is generated by the PEL heuristic (described in the following section); otherwise, the order list includes the entries in an arbitrary order.
												</Para>


												<Para> 

													<Emphasis
Type="Bold">Observation 4. (UO observation)</Emphasis>
 if two patterns, 

													<Emphasis Type="Italic">P</Emphasis>
 and 

													<Emphasis Type="Italic">Q</Emphasis>
 , are ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )-mismatched, then at least one of the (⌊ 

													<Emphasis Type="Italic">d'</Emphasis>
 /2⌋ + 1) partitions of 

													<Emphasis Type="Italic">P</Emphasis>
 is ( 

													<Emphasis Type="Italic">α</Emphasis>
 , 1)-mismatched to the corresponding part in 

													<Emphasis Type="Italic">Q</Emphasis>
 , where 

													<Emphasis Type="Italic">α</Emphasis>
 = 

													<Emphasis Type="Italic">l'</Emphasis>
 /(⌊ 

													<Emphasis Type="Italic">d'</Emphasis>
 /2⌋ + 1) and all partitions have equal length.
												</Para>


												<Para> 

													<Emphasis
Type="Bold">Observation 5. (IMUS observation)</Emphasis>
 if two patterns 

													<Emphasis Type="Italic">P</Emphasis>
 and 

													<Emphasis Type="Italic">Q</Emphasis>
 are ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )-mismatched, then at least one of the two halves of 

													<Emphasis Type="Italic">P</Emphasis>
 is ( 

													<Emphasis Type="Italic">α</Emphasis>
 , ⌊ 

													<Emphasis Type="Italic">d'</Emphasis>
 /2⌋)-mismatched to the corresponding part of 

													<Emphasis Type="Italic">Q</Emphasis>
 , where 

													<Emphasis Type="Italic">α</Emphasis>
 = 

													<Emphasis Type="Italic">l'</Emphasis>
 /2.
												</Para>


												<Para>Two index entries are called similar entries if the number of mismatches between the keys of the entries is less than or equal to a certain value 

													<Emphasis Type="Italic">β</Emphasis>
 . This value is also related to the employed discovery algorithm, for example, 

													<Emphasis Type="Italic">β</Emphasis>
 is 1 in the UO algorithm, and 

													<Emphasis Type="Italic">β</Emphasis>
 = ⌊ 

													<Emphasis Type="Italic">d'</Emphasis>
 /2⌋ in the IMUS algorithm. Assume 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">Q</Emphasis>

													</Subscript>
 are index keys, and 

													<Emphasis Type="Italic">P</Emphasis>
 and 

													<Emphasis Type="Italic">Q</Emphasis>
 are the 

													<Emphasis Type="Italic">l'</Emphasis>
 -patterns listed in the entries of keys 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">Q</Emphasis>

													</Subscript>
 , respectively. Based on Observations 4 and 5, if 

													<Emphasis Type="Italic">Q</Emphasis>
 is ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )-mismatched to 

													<Emphasis Type="Italic">P</Emphasis>
 , then 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">Q</Emphasis>

													</Subscript>
 must be ( 

													<Emphasis Type="Italic">α</Emphasis>
 , 

													<Emphasis Type="Italic">β</Emphasis>
 )-mismatched to 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 , such that the entries of keys 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">Q</Emphasis>

													</Subscript>
 are similar. Since all the patterns that are ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )-mismatched to a pattern 

													<Emphasis Type="Italic">P</Emphasis>
 must be in the entries that are similar to the entry whose key is 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">P</Emphasis>
 is compared to all of the patterns in the similar entries, to determine whether 

													<Emphasis Type="Italic">P</Emphasis>
 is unique. The pattern 

													<Emphasis Type="Italic">P</Emphasis>
 is a unique signature if no pattern is ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )-mismatched to it. Since the new result can be discovered from the candidate patterns, the PISD processes only the candidate patterns. An available processor is assigned to handle the next untreated entry (based on the assumption that the key of the entry is 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 ) in the processing order list 

													<Superscript>.</Superscript>
 Assume that 

													<Emphasis Type="Italic">P</Emphasis>
 is one of the candidate patterns in the entry. 

													<Emphasis Type="Italic">P</Emphasis>
 is compared to all of the patterns in the similar entries, which are those whose keys are ( 

													<Emphasis Type="Italic">α</Emphasis>
 , 

													<Emphasis Type="Italic">β</Emphasis>
 )-mismatched to 

													<Emphasis Type="Italic">K</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">P</Emphasis>

													</Subscript>
 . Each of the comparisons is a complete string comparison of 

													<Emphasis Type="Italic">l'</Emphasis>
 characters. The candidate 

													<Emphasis Type="Italic">l'</Emphasis>
 -patterns that are ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )-mismatched to any of the 

													<Emphasis Type="Italic">l'</Emphasis>
 -patterns in the similar entries are discarded, and the remaining candidate patterns are new unique signatures.
												</Para>


											</Section3>


											<Section3 ID="Sec_01641">

												<Heading>The scheduling heuristic for parallelism</Heading>


												<Para>One of the ways to accelerate signature discovery is to apply parallel computing. Assume that a computer of 

													<Emphasis Type="Italic">n</Emphasis>
 processors is employed in signature discovery, and that processor 

													<Emphasis Type="Italic">i</Emphasis>
 takes 

													<Emphasis Type="Italic">t</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 time units to complete its tasks. The overall processing time 

													<Emphasis Type="Italic">T</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">n</Emphasis>

													</Subscript>
 required by the computer to complete the discovery is, which means that the processor that takes longest dominates the overall processing time.
												</Para>


												<Para>The optimal processing time when 

													<Emphasis Type="Italic">n</Emphasis>
 processors are used is 

													<Emphasis Type="Italic">T</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">n</Emphasis>

													</Subscript>
 = 

													<Emphasis Type="Italic">T</Emphasis>


													<Subscript>1</Subscript>
 / 

													<Emphasis Type="Italic">n</Emphasis>
 , which equals 1/ 

													<Emphasis Type="Italic">n</Emphasis>
 of the processing time of a single-processor computer.
												</Para>


												<Para>The simplest way to apply parallel computing to the proposed PISD algorithm is to assign randomly an available processor to process the patterns in the index in an arbitrary order. The treatment of an entry is referred to as a task. For example, a computer with four processors is used to handle 

													<Emphasis Type="Italic">N</Emphasis>
 tasks. Processor 1 can be assigned to task 1, ..., and processor 4 can be assigned to task 4. Assume that processor 3 is the first to complete its task; the processor is immediately assigned to the next task, task 5. The next available processor is similarly assigned to the next task until all of the 

													<Emphasis Type="Italic">N</Emphasis>
 tasks are completed. If four tasks are processed simultaneously, then ideally, the overall processing time is reduced to one quarter of that which would be required using a single-processor computer.
												</Para>


												<Para>However, two potential problems must be considered when parallel computing is applied to the proposed PISD algorithm. First, if one of the last few tasks requires much processing time, then the overall processing time may be longer than the optimal processing time. For example, Figure 

													<InternalRef
RefID="F1">1</InternalRef>
 shows a list of six tasks. All of the tasks can be completed in 22 time units by a single-processor computer. The optimal processing time is therefore 22/2 = 11 units for a two-processor computer. However, in this case, processor 1 is assigned to {A, D, F}, and processor 2 is assigned to {B, C, E}. The processing times are 15 and 7 units respectively, and the overall processing time is 15 units, which exceeds the optimal processing time. This situation can be avoided by arranging long tasks before the others in the processing order list. Here, the long tasks are moved forward in the processing order list, yielding the result in Figure 

													<InternalRef
RefID="F2">2</InternalRef>
 . In the new list, processor 1 performs tasks {F, D} and processor 2 performs tasks {A, C, E, B}. The overall processing time is 11 units, which equals the optimal processing time.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F1">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>An example of the first potential problem of parallel signature discovery</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-11-132-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">An example of the first potential problem of parallel signature discovery</Emphasis>
 . The tasks can be completed in 22 time units by a single-processor computer. The overall processing time is 15 units for a two-processor computer, which exceeds the optimal processing time, 11 units.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Figure Category="Standard"
Float="No" ID="F2">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>The result of moving long tasks forward in the processing order list</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-11-132-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">The result of moving long tasks forward in the processing order list</Emphasis>
 . The long tasks are moved forward in the processing order list in Figure 2, yielding the new processing order list. The overall processing time for a two-processor computer is 11 units, which equals the optimal processing time.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>The second potential problem is that the time required to process a task may exceed the optimal processing time, 

													<Emphasis Type="Italic">T</Emphasis>


													<Subscript>1</Subscript>
 / 

													<Emphasis Type="Italic">n</Emphasis>
 . For example, Figure 

													<InternalRef
RefID="F3">3</InternalRef>
 shows a list of six tasks. All of the tasks can be completed in 24 units by a single-processor computer. When a two-processor computer is used to handle the tasks, processors 1 and 2 are assigned to tasks {A, C, E} and {B, D, F}, and taking 5 and 19 units, respectively. The overall processing time is 19 units. Long tasks are moved forward, yielding the new processing order list that is shown in Figure 

													<InternalRef
RefID="F4">4</InternalRef>
 . In this situation, processor 1 is assigned to task F only, and processor 2 is assigned to the other tasks. The overall processing time is then 16 units, which still exceeds the optimal processing time, because task F takes 16 units, which exceeds the sum of the times required to complete all of the other tasks. Hence if less time were to be spent on task F, then the overall processing time would be reduced. Generally, when an entry has more patterns than the other entries, a task that handles this entry takes more time to complete. Therefore, some of the longest entries are divided into 

													<Emphasis Type="Italic">n</Emphasis>
 equal partitions, which are then treated as typical entries, where 

													<Emphasis Type="Italic">n</Emphasis>
 is the number of available processors. For example, task F in Figure 

													<InternalRef
RefID="F4">4</InternalRef>
 can be divided into two tasks with identical processing times, yielding the new task list in Figure 

													<InternalRef
RefID="F5">5</InternalRef>
 . After the division, processor 1 is assigned to tasks {F 

													<Subscript>1</Subscript>
 , B, D, A}, and processor 2 is assigned to tasks {F 

													<Subscript>2</Subscript>
 , C, E}. The overall processing time is 12 units, which equals the optimal processing time.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F3">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>An example of the second potential problem of parallel signature discovery</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-11-132-3" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">An example of the second potential problem of parallel signature discovery</Emphasis>
 . The tasks can be completed in 24 units by a single-processor computer. The overall processing time is 19 units for a two-processor computer, which exceeds the optimal processing time, 12 units.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Figure Category="Standard"
Float="No" ID="F4">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>The result of moving long tasks forward in the processing order list</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-11-132-4" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">The result of moving long tasks forward in the processing order list</Emphasis>
 . The long tasks are moved forward in the processing order list in Figure 4, yielding the new processing order list. The overall processing time for a two-processor computer is 16 units, which still exceeds the optimal processing time.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Figure Category="Standard"
Float="No" ID="F5">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>The result of dividing long tasks into short tasks in the processing order list</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-11-132-5" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">The result of dividing long tasks into short tasks in the processing order list</Emphasis>
 . Task F in Figure 5 is divided into two tasks with identical processing times, yielding the new task list. The overall processing time for a two-processor computer is 12 units, which equals the optimal processing time.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>Based on the above discussion, the order of tasks in the processing order list influences the overall processing time for parallel discovery. Since the proposed discovery algorithm PISD focuses on processing candidate patterns, the processing time of a task is proportional to the number of candidate patterns in the entry. The index entries can be sorted in descending order of the number of candidate patterns therein, and the sorted list can be used as the processing order list. Entries that contain more candidate patterns are expected to be at the top of the list. However, the sorting process takes 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis Type="Italic">N</Emphasis>
 log 

													<Emphasis Type="Italic">N</Emphasis>
 ) time for 

													<Emphasis Type="Italic">N</Emphasis>
 entries, which is significant.
												</Para>


												<Para>A simple and efficient scheduling heuristic, called the parallel entry list (PEL), is provided. It yields a processing order list for tasks in which the tasks that involve more candidate patterns are before those that involve fewer. Additional file2displays the PEL heuristic. The PEL heuristic is similar to a partial quicksort. Unlike quicksort, the PEL heuristic is iterative, and only operates on the left part of a list in each iteration. Firstly, the PEL heuristic generates a processing order list 

													<Emphasis Type="Italic">L</Emphasis>
 that consists of all of the index entries in arbitrary order, and 

													<Emphasis Type="Italic">w</Emphasis>
 is defined as the number of index entries in 

													<Emphasis Type="Italic">L</Emphasis>
 . The average number of candidate patterns ( 

													<Emphasis Type="Italic">g</Emphasis>
 ) in each entry is computed, where 

													<Emphasis Type="Italic">g</Emphasis>
 equals (total number of candidate patterns)/ 

													<Emphasis Type="Italic">w</Emphasis>
 . Let 

													<Emphasis Type="Italic">L</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 represent the 

													<Emphasis Type="Italic">i</Emphasis>
 -th entry in 

													<Emphasis Type="Italic">L</Emphasis>
 , and || be the number of candidate patterns in 

													<Emphasis Type="Italic">Li</Emphasis>
 . Then, the PEL heuristic searches for the maximal value 

													<Emphasis Type="Italic">r</Emphasis>
 such that || &gt; 

													<Emphasis Type="Italic">g</Emphasis>
 and the minimal value 

													<Emphasis Type="Italic">k</Emphasis>
 such that || ≤ 

													<Emphasis Type="Italic">g</Emphasis>
 , and then exchanges 

													<Emphasis Type="Italic">L</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">L</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 . The searches and exchanges continue until 

													<Emphasis Type="Italic">r</Emphasis>
 &lt; 

													<Emphasis Type="Italic">k</Emphasis>
 . The process scans the entries from 

													<Emphasis Type="Italic">L</Emphasis>


													<Subscript>1</Subscript>
 to 

													<Emphasis Type="Italic">L</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">w</Emphasis>

													</Subscript>
 in 

													<Emphasis Type="Italic">L</Emphasis>
 and 

													<Emphasis Type="Italic">w</Emphasis>
 is updated to the current value of 

													<Emphasis Type="Italic">r</Emphasis>
 . Then, the entries in 

													<Emphasis Type="Italic">L</Emphasis>
 are divided into two parts: if 

													<Emphasis Type="Italic">i</Emphasis>
 ≤ 

													<Emphasis Type="Italic">w</Emphasis>
 , then || &gt; 

													<Emphasis Type="Italic">g</Emphasis>
 ; otherwise, || ≤ 

													<Emphasis Type="Italic">g</Emphasis>
 . Assume 

													<Emphasis Type="Italic">w'</Emphasis>
 is the most recent value of the variable 

													<Emphasis Type="Italic">w</Emphasis>
 . Since || &gt; 

													<Emphasis Type="Italic">g</Emphasis>
 , ∀ 

													<Emphasis Type="Italic">i</Emphasis>
 ≤ 

													<Emphasis Type="Italic">w</Emphasis>
 , 

													<Emphasis Type="Italic">w</Emphasis>
 &lt; 

													<Emphasis Type="Italic">w'</Emphasis>
 /2. Then, the PEL heuristic focuses on the first part of 

													<Emphasis Type="Italic">L</Emphasis>
 , and moves the long entries forward until, where 

													<Emphasis Type="Italic">n</Emphasis>
 is the number of available processors and 

													<Emphasis Type="Italic">N</Emphasis>
 is the number of index entries in 

													<Emphasis Type="Italic">L</Emphasis>
 . Now, the first 

													<Emphasis Type="Italic">w</Emphasis>
 entries in 

													<Emphasis Type="Italic">L</Emphasis>
 are the top 

													<Emphasis Type="Italic">w</Emphasis>
 entries, which contain the most candidate patterns. The first 

													<Emphasis Type="Italic">w</Emphasis>
 entries are removed from 

													<Emphasis Type="Italic">L</Emphasis>
 , and the candidate patterns in each entry are divided into 

													<Emphasis Type="Italic">n</Emphasis>
 partitions of equal number of patterns. The 

													<Emphasis Type="Italic">nw</Emphasis>
 partitions are then put into 

													<Emphasis Type="Italic">L</Emphasis>
 , and treated as typical entries in successive processes. The total number of scans performed on is, where 

													<Emphasis Type="Italic">m</Emphasis>
 is the number of iterations of the main outer loop (line 10 to 27 of the PEL heuristic in Additional file2), which moves the long entries forward. The time complexity of the PEL heuristic is 

													<Emphasis Type="Italic">O</Emphasis>
 (3 

													<Emphasis Type="Italic">N</Emphasis>
 ) = 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis Type="Italic">N</Emphasis>
 ).
												</Para>


												<Para>As an example of the above, consider an entry list 

													<Emphasis Type="Italic">L</Emphasis>
 , shown in Table 

													<InternalRef
RefID="T2">2(A)</InternalRef>
 . The average number of candidate patterns in each entry ( 

													<Emphasis Type="Italic">g</Emphasis>
 ) is 41. The leftmost entry in 

													<Emphasis Type="Italic">L</Emphasis>
 that contains fewer than 

													<Emphasis Type="Italic">g</Emphasis>
 candidate patterns, and the rightmost entry that contains more than 

													<Emphasis Type="Italic">g</Emphasis>
 candidate patterns are sought. The respective results are entries A and J. These two entries are exchanged in 

													<Emphasis Type="Italic">L</Emphasis>
 . Entries B and G as well as D and E are similarly exchanged. Table 

													<InternalRef
RefID="T2">2(B)</InternalRef>
 shows the new processing order list. Now, 

													<Emphasis Type="Italic">w</Emphasis>
 is four, and the number of candidate patterns in each of the first 

													<Emphasis Type="Italic">w</Emphasis>
 = 4 entries exceeds 

													<Emphasis Type="Italic">g</Emphasis>
 = 41, while that in the other entries is less than 41. Then, only the region of the first four entries is considered in the next step. The average number of the candidate patterns in each entry within this region is computed, yielding 

													<Emphasis Type="Italic">g</Emphasis>
 = 79. In this region, the leftmost and rightmost entries that contain fewer than and more than 79 candidate patterns are J and E, respectively. J and E are exchanged in the list, yielding Table 

													<InternalRef
RefID="T2">2(C)</InternalRef>
 . Assume a two-processor computer is used. Entry E is divided into two partitions E 

													<Subscript>1</Subscript>
 and E 

													<Subscript>2</Subscript>
 , and E 

													<Subscript>1</Subscript>
 and E 

													<Subscript>2</Subscript>
 are added to the list. The new list is as shown in Table 

													<InternalRef
RefID="T2">2(D)</InternalRef>
 . Processor 1 will handle entries E 

													<Subscript>1</Subscript>
 , G, D, F, B, I and A, and processor 2 will handle entries E 

													<Subscript>2</Subscript>
 , C, J, H and K. The total number of candidate patterns to be treated by each processor is 227.
												</Para>


												<Table Float="No" ID="T2">

													<Caption Language="En">

														<CaptionNumber>Table 2</CaptionNumber>


														<CaptionContent>

															<SimplePara>An example of using the PEL heuristic to build an entry list.</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="13">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<colspec colname="c5"
colnum="5" />


														<colspec colname="c6"
colnum="6" />


														<colspec colname="c7"
colnum="7" />


														<colspec colname="c8"
colnum="8" />


														<colspec colname="c9"
colnum="9" />


														<colspec colname="c10"
colnum="10" />


														<colspec colname="c11"
colnum="11" />


														<colspec colname="c12"
colnum="12" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The original entry list.</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">ID</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">A</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">B</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">C</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">D</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">E</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">F</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">G</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">H</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara> 

																		<Emphasis
Type="Bold">I</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara> 

																		<Emphasis
Type="Bold">J</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara> 

																		<Emphasis
Type="Bold">K</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>|*|</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>33</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>26</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>49</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>5</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>143</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>9</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>72</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>29</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara>11</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara>55</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara>22</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) After first iteration, 

																			<Emphasis
Type="Italic">w</Emphasis>
 = 4.
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">ID</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">J</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">G</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">C</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">E</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">D</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">F</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">B</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">H</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara> 

																		<Emphasis
Type="Bold">I</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara> 

																		<Emphasis
Type="Bold">A</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara> 

																		<Emphasis
Type="Bold">K</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>|*|</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>55</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>72</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>49</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>143</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>5</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>9</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>26</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>29</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara>11</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara>33</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara>22</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(C) After second iteration, 

																			<Emphasis
Type="Italic">w</Emphasis>
 = 1.
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">ID</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">E</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">G</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">C</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">J</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">D</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">F</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">B</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">H</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara> 

																		<Emphasis
Type="Bold">I</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara> 

																		<Emphasis
Type="Bold">A</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara> 

																		<Emphasis
Type="Bold">K</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>|*|</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>143</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>72</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>49</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>55</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>5</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>9</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>26</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>29</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara>11</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara>33</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara>22</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(D) The final entry list.</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">ID</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">E 

																			<Subscript>1</Subscript>

																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">E 

																			<Subscript>2</Subscript>

																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">G</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">C</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">J</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">D</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">F</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">B</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara> 

																		<Emphasis
Type="Bold">H</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara> 

																		<Emphasis
Type="Bold">I</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara> 

																		<Emphasis
Type="Bold">A</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara> 

																		<Emphasis
Type="Bold">K</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>|*|</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>71</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>72</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>72</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>49</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>55</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>5</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>9</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>26</SimplePara>


																</entry>


																<entry colname="c9">

																	<SimplePara>29</SimplePara>


																</entry>


																<entry colname="c10">

																	<SimplePara>11</SimplePara>


																</entry>


																<entry colname="c11">

																	<SimplePara>33</SimplePara>


																</entry>


																<entry colname="c12">

																	<SimplePara>22</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>Let |*| denote the number of candidate patterns in an entry. Part (A) presents the original entry list. Entries A and J, B and G as well as D and E are exchanged. Part (B) presents the new entry list. Entries J and E are exchanged in the next iteration, yielding Part (C). Assume a two-processor computer is used. Entry E is divided into two partitions E 

															<Subscript>1</Subscript>
 and E 

															<Subscript>2</Subscript>
 , and E 

															<Subscript>1</Subscript>
 and E 

															<Subscript>2</Subscript>
 are added to the list. The new list is as shown in Part (D).
														</SimplePara>


													</tfooter>


												</Table>


											</Section3>


											<Section3 ID="Sec_56678">

												<Heading>The consecutive multiple discovery (CMD) algorithm</Heading>


												<Para>Additional file3displays the consecutive multiple discovery (CMD) algorithm. Let 

													<Emphasis Type="Italic">l</Emphasis>
 and 

													<Emphasis Type="Italic">d</Emphasis>
 be two integers. The CMD algorithm is an iterative algorithm, which uses the PISD algorithm as a kernel routine, to discover all implicit signatures under the discovery condition of length 

													<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

													<Emphasis Type="Italic">d</Emphasis>
 . Firstly, the UO or IMUS algorithm is used to discover the unique signatures that satisfy the discovery condition ( 

													<Emphasis Type="Italic">l</Emphasis>
 , 

													<Emphasis Type="Italic">d</Emphasis>
 ). The signatures discovered by UO or IMUS are applied as candidates in successive discoveries. The feasible discovery conditions are all combinations of the possible 

													<Emphasis Type="Italic">l'</Emphasis>
 and 

													<Emphasis Type="Italic">d'</Emphasis>
 , which means {( 

													<Emphasis Type="Italic">l'</Emphasis>
 ≤ 

													<Emphasis Type="Italic">l</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 ≥ 

													<Emphasis Type="Italic">d</Emphasis>
 )}. In each discovery, the PISD algorithm is used to discover new signatures from the candidates under a feasible discovery condition. The discovery process continues until all of the implicit signatures are discovered.
												</Para>


											</Section3>


										</Section2>


										<Section2 ID="Sec_27251">

											<Heading>Testing</Heading>


											<Para>This section evaluates the performance of the proposed algorithms. Since the incremental discovery and parallel computing mentioned in the previous sections can be applied to the UO and IMUS algorithms, briefly, the CMD (or PISD) with the UO and IMUS kernels are denoted as CMD 

												<Subscript>UO</Subscript>
 and CMD 

												<Subscript>IMUS</Subscript>
 (or PISD 

												<Subscript>UO</Subscript>
 and PISD 

												<Subscript>IMUS</Subscript>
 ), respectively. The algorithms are analyzed based on a uniformly distributed database. The first part of this section presents these analyses. To evaluate the performance of the UO, IMUS, CMD 

												<Subscript>UO</Subscript>
 and CMD 

												<Subscript>IMUS</Subscript>
 algorithms, they are applied to human chromosome 13 and 21 EST databases for signature discovery. The second part of this section presents the experimental results.
											</Para>


											<Section3 ID="Sec_99618">

												<Heading>Mathematical analyses</Heading>


												<Para>The CMD algorithm is an iterative algorithm. It includes the PISD algorithm as a kernel routine. Accordingly, the time complexity of the PISD algorithm dominates that of the CMD algorithm. First, the time complexity of the PISD 

													<Subscript>UO</Subscript>
 algorithm is analyzed under a certain discovery condition, and then, the results are integrated, yielding the time complexity of the CMD 

													<Subscript>UO</Subscript>
 algorithm. The analyses of the PISD 

													<Subscript>IMUS</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms can be done in a similar way.
												</Para>


												<Para>Let 

													<Emphasis Type="Italic">l'</Emphasis>
 be the signature length and 

													<Emphasis Type="Italic">d'</Emphasis>
 be the mismatch tolerance. 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 denotes the index system built under the condition of signature length 

													<Emphasis Type="Italic">l'</Emphasis>
 in the PISD 

													<Subscript>UO</Subscript>
 algorithm. 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 consists of 4 

													<Superscript> 

														<Emphasis
Type="Italic">α</Emphasis>

													</Superscript>
 pattern entries, where 

													<Emphasis Type="Italic">α</Emphasis>
 = 

													<Emphasis Type="Italic">l'</Emphasis>
 /(⌊ 

													<Emphasis Type="Italic">d'</Emphasis>
 /2⌋ + 1) is the length of the entry keys. Let 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 be the 

													<Emphasis Type="Italic">i</Emphasis>
 -th entry in 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 ·| 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 | denotes the number of all patterns in 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 anddenotes the number of candidate patterns in 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 . HD( 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ) denotes the hamming distance between 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 , which is defined as the hamming distance between the entry keys of 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 . Because only the candidate patterns have to be considered,| 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 | string comparisons are performed on the patterns in 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 . Additionally, ∑ 

													<Subscript> 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 | 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 | string comparisons are required to check possible mutants, where 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ∈ 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 such that HD( 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ) = 1. All characters in an 

													<Emphasis Type="Italic">l'</Emphasis>
 -pattern excluding the entry key region are compared in each of the string comparisons, yielding 

													<Emphasis Type="Italic">l'</Emphasis>
 - 

													<Emphasis Type="Italic">α</Emphasis>
 character comparisons.
												</Para>


												<Para>The total amount of character comparisons used in the PISD 

													<Subscript>UO</Subscript>
 algorithm, denoted as, is:
												</Para>


												<Para>where 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ∈ 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 such that HD( 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ) = 1.
												</Para>


												<Para>Let 

													<Emphasis Type="Italic">l</Emphasis>
 be the desired signature length and 

													<Emphasis Type="Italic">d</Emphasis>
 be the mismatch tolerance of pattern uniqueness. the CMD 

													<Subscript>UO</Subscript>
 algorithm uses the PISD 

													<Subscript>UO</Subscript>
 algorithm to find all of the implicit signatures of length 

													<Emphasis Type="Italic">l'</Emphasis>
 ≤ 

													<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

													<Emphasis Type="Italic">d'</Emphasis>
 ≥ 

													<Emphasis Type="Italic">d</Emphasis>
 . The time complexity of the CMDUO algorithm, denoted as, is:
												</Para>


												<Para>where 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ∈ 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 such that HD( 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ) = 1.
												</Para>


												<Para>Assume the input DNA database 

													<Emphasis Type="Italic">D</Emphasis>
 and the set of the discovered signatures Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 ≤ 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d'</Emphasis>
 ≥ 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 are uniformly distributed. Let∈ {Ω 

													<Subscript> 

														<Emphasis
Type="Italic">x</Emphasis>
 , 

														<Emphasis
Type="Italic">y</Emphasis>

													</Subscript>
 } 

													<Emphasis Type="Italic">l'</Emphasis>
 ≤ 

													<Emphasis Type="Italic">x</Emphasis>
 ≥ 

													<Emphasis Type="Italic">l</Emphasis>
 and 

													<Emphasis Type="Italic">d</Emphasis>
 ≤ 

													<Emphasis Type="Italic">y</Emphasis>
 ≥ 

													<Emphasis Type="Italic">d'</Emphasis>
 }, where ( 

													<Emphasis Type="Italic">x</Emphasis>
 , 

													<Emphasis Type="Italic">y</Emphasis>
 ) ≠ ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 ), be the set of signatures discovered in the strictest iteration prior to the iteration of ( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 ). Assume the sizes of 

													<Emphasis Type="Italic">D</Emphasis>
 andare denoted as | 

													<Emphasis Type="Italic">D</Emphasis>
 | and ||. The index system built in this uniformly distributed case is denoted as. Each entry inshould contain || ≈ | 

													<Emphasis Type="Italic">D</Emphasis>
 |/4 

													<Superscript> 

														<Emphasis
Type="Italic">α</Emphasis>

													</Superscript>
 patterns, andof them are candidate patterns. In this case, the amount of character comparisons used in the PISD 

													<Subscript>UO</Subscript>
 algorithm, denoted as, is:
												</Para>


												<Para>where 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ∈ 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>

													</Subscript>
 such that HD(σ 

													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">σ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">l'</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ) = 1, and 

													<Emphasis Type="Italic">κ</Emphasis>
 = 3 

													<Emphasis Type="Italic">α</Emphasis>
 is the number of all possible 1 base permutations of a string of length 

													<Emphasis Type="Italic">α</Emphasis>
 . Note thatbecause of the uniform assumption.
												</Para>


												<Para>In the uniformly distributed case, the number of character comparisons used in the CMD 

													<Subscript>UO</Subscript>
 algorithm, denoted as, is:
												</Para>


												<Para>where 

													<Emphasis Type="Italic">κ</Emphasis>
 = 3 

													<Emphasis Type="Italic">α</Emphasis>
 .
												</Para>


												<Para>The main difference between the PISD 

													<Subscript>UO</Subscript>
 and the typical UO algorithms is that the two algorithms use different candidate sets. The PISD 

													<Subscript>UO</Subscript>
 algorithm uses the previously discovered signatures as candidates, but the UO algorithm uses all of the patterns in the database as candidates. The amount of character comparisons used in the UO algorithm for discovering signatures from the uniformly distributed database 

													<Emphasis Type="Italic">D</Emphasis>
 can be obtained by replacingwith 

													<Emphasis Type="Italic">D</Emphasis>
 . The formula is:
												</Para>


												<Para>where 

													<Emphasis Type="Italic">κ</Emphasis>
 = 3 

													<Emphasis Type="Italic">α</Emphasis>
 .
												</Para>


												<Para>The UO algorithm is executed repeatedly to discover all implicit signatures. The time complexity of using the UO algorithm, denoted as, is:</Para>


												<Para>where 

													<Emphasis Type="Italic">κ</Emphasis>
 = 3 

													<Emphasis Type="Italic">α</Emphasis>
 .
												</Para>


												<Para>The gain delivered by the CMD 

													<Subscript>UO</Subscript>
 algorithm is:
												</Para>


												<Para>where 

													<Emphasis Type="Italic">κ</Emphasis>
 = 3 

													<Emphasis Type="Italic">α</Emphasis>
 .
												</Para>


												<Para>It means that the CMD 

													<Subscript>UO</Subscript>
 algorithm performs 

													<Emphasis Type="Italic">G</Emphasis>
 ≥ | 

													<Emphasis Type="Italic">D</Emphasis>
 |/|Ω 

													<Subscript> 

														<Emphasis
Type="Italic">l</Emphasis>
 , 

														<Emphasis
Type="Italic">d</Emphasis>

													</Subscript>
 | times faster than the typical UO algorithm, when discovering implicit signatures from a uniformly distributed database.
												</Para>


											</Section3>


											<Section3 ID="Sec_64439">

												<Heading>Performance evaluation</Heading>


												<Para>The platform that was adopted in this experiment was a Dell PowerEdge R900 server with two Intel Xeon E7430 2.13 GHz quad-core CPUs, 12 GB RAM and 900 GB disk space. The operating system was Red Hat Enterprise Linux 5. The algorithms were implemented in JAVA language, and the programs were compiled by JDK 1.6. The DNA data that were used in the experiments were from the human chromosome 13 and 21 EST databases. Before the experiments, the remarks in the databases were removed; all of the universal characters, such as 'don't care', were replaced with 'A', and DNA sequences that were shorter than 36 bases were discarded. The experimental data are denoted as 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 (human chromosome 13 EST database) and 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 (human chromosome 21 EST database), and their corresponding sizes were approximately 36.44 M and 22.21 M bases.
												</Para>


												<Para>The pooled oligo probes, that are used to screen an EST library, such as the BAC library, generally have lengths from 24 to 40 bases 

													<CitationRef
CitationID="B28">28</CitationRef>
 . Our experimental results on unique signature discoveries, with the criteria of exact matches, also shows that most of the human EST sequences can be distinctly labeled by signatures of length greater than 18 bases. Accordingly, the experiments in this section focused on discovering signatures of length between 24 and 30 with mismatch tolerances of two and four.
												</Para>


												<Para>For reasons of performance and memory consumption, a two-level index was used in the implementation of the IMUS and CMD 

													<Subscript>IMUS</Subscript>
 algorithms. The first level of the index comprised 4 

													<Superscript>10</Superscript>
 direct-accessible entries, and a binary search was used to locate a specified entry in the second level. The index systems that were used in the implementation of the UO and CMD 

													<Subscript>UO</Subscript>
 algorithms were one-level, and all of the entries in their index systems were directly accessible. Since the purpose of our experiments was to evaluate the improvements provided by incremental discovery and parallel computing, additional filters, such as the frequency filter that was used in the IMUS algorithm, was excluded from the kernels of the algorithms.
												</Para>


												<Para>Since ⌊ 

													<Emphasis Type="Italic">d</Emphasis>
 /2⌋ + 1 = 2 when 

													<Emphasis Type="Italic">d</Emphasis>
 = 2, the kernels of the UO and IMUS algorithms are very similar under this condition. Only the performance of the IMUS and CMD 

													<Subscript>IMUS</Subscript>
 algorithms was examined when mismatch tolerance was two. Table 

													<InternalRef
RefID="T3">3</InternalRef>
 presents the discovery conditions that were used in our experiments. In the experiments on the UO and IMUS algorithms, the UO and IMUS algorithms were executed repeatedly to discover all of the signatures under all feasible discovery conditions. The experiments were performed on a one-processor computer. Before the performance of the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms was evaluated, the IMUS algorithm was used to discover signatures under the discovery condition of length 

													<Emphasis Type="Italic">l</Emphasis>
 = 30 and mismatch tolerance 

													<Emphasis Type="Italic">d</Emphasis>
 = 2. The discovery on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 took approximately 19.6 minutes and 20.88% of the patterns from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 were discovered as signatures. The discovery on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 took about 5.3 minutes and 22.07% of the patterns from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 were discovered as signatures. In each successive experiment, the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms used the discovered signatures of 

													<Emphasis Type="Italic">l</Emphasis>
 = 30 and 

													<Emphasis Type="Italic">d</Emphasis>
 = 2 as candidates to produce new results.
												</Para>


												<Table Float="No" ID="T3">

													<Caption Language="En">

														<CaptionNumber>Table 3</CaptionNumber>


														<CaptionContent>

															<SimplePara>The discovery conditions used in our experiments.</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="9">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<colspec colname="c5"
colnum="5" />


														<colspec colname="c6"
colnum="6" />


														<colspec colname="c7"
colnum="7" />


														<colspec colname="c8"
colnum="8" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">The used discovery conditions.</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara />


																</entry>


																<entry colname="c2">

																	<SimplePara />


																</entry>


																<entry colname="c3">

																	<SimplePara />


																</entry>


																<entry colname="c4">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara />


																</entry>


																<entry colname="c6">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara />


																</entry>


																<entry colname="c8">

																	<SimplePara>•</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>•</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>•</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>• indicates that the discovery condition was used in the experiments by the specified algorithm.</SimplePara>


													</tfooter>


												</Table>


												<Para>The percentage time saved is used to evaluate the improvements in the processing time of an algorithm. The time saving is defined as (1-(processing time of the CMD 

													<Subscript>UO</Subscript>
 (or CMD 

													<Subscript>IMUS</Subscript>
 ) algorithm)/(processing time of the UO (or IMUS) algorithm))*100%. A larger 'saving' means a greater improvement by the CMD 

													<Subscript>UO</Subscript>
 or CMD 

													<Subscript>IMUS</Subscript>
 algorithm. The term 'overall' refers to the total processing time required for the UO, IMUS, CMD 

													<Subscript>UO</Subscript>
 or CMD 

													<Subscript>IMUS</Subscript>
 algorithm to discover all of the signatures that satisfy the discovery conditions.
												</Para>


												<Para>First, improvements in the time of discovery associated with incremental discovery are examined. For a single processing core, the performance of the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms was evaluated by using the algorithms to discover signatures from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 and 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 . Tables 

													<InternalRef
RefID="T4">4</InternalRef>
 and 

													<InternalRef
RefID="T5">5</InternalRef>
 present the processing time that for the UO and IMUS algorithms, and the time savings delivered by the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms. The tables also present the processing time required to discover signatures under every discovery condition. In the experiments, the proposed CMD 

													<Subscript>UO</Subscript>
 algorithm took 76.2% less processing time than the UO algorithm to discover all of the implicit signatures from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 , and about 74% less processing time to discover those from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 . With respect to the performance of the CMD 

													<Subscript>IMUS</Subscript>
 algorithm, it took about 67% and 52% less processing time than the IMUS algorithm to discover all of the signatures from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 and 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 . Greater overheads in accessing indices caused the percentage processing time saved by the CMD 

													<Subscript>IMUS</Subscript>
 algorithm to be less than that saved by the CMD 

													<Subscript>UO</Subscript>
 algorithm.
												</Para>


												<Table Float="No" ID="T4">

													<Caption Language="En">

														<CaptionNumber>Table 4</CaptionNumber>


														<CaptionContent>

															<SimplePara>The performance of the CMD 

																<Subscript>UO</Subscript>
 algorithm when using a single processing core.
															</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="5">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>13</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>UO</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>03:49:54</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>09:56:59</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>35:56:27</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>49:43:20</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:50:52</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>02:22:32</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>08:37:03</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>11:50:27</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>77.87</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>76.12</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>76.02</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>76.19</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>21</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>UO</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>01:10:18</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>03:09:37</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>11:12:18</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>15:32:13</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:17:10</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:49:34</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>02:55:27</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>04:02:11</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>75.58</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>73.86</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>73.90</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>74.02</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>The table presents the processing time that for the UO algorithm, and the time savings delivered by the CMD 

															<Subscript>UO</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
														</SimplePara>


													</tfooter>


												</Table>


												<Table Float="No" ID="T5">

													<Caption Language="En">

														<CaptionNumber>Table 5</CaptionNumber>


														<CaptionContent>

															<SimplePara>The performance of the CMD 

																<Subscript>IMUS</Subscript>
 algorithm when using a single processing core.
															</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="9">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<colspec colname="c5"
colnum="5" />


														<colspec colname="c6"
colnum="6" />


														<colspec colname="c7"
colnum="7" />


														<colspec colname="c8"
colnum="8" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>13</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>IMUS</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:23:56</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:30:24</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:43:35</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:50:57</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>01:10:14</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>01:57:30</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>03:57:25</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>09:34:01</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:03:06</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:04:54</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:08:33</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:20:22</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>00:27:09</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>00:44:03</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>01:21:22</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>03:09:29</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>87.03</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>83.84</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>80.35</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>60.01</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>61.33</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>62.50</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>65.73</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>66.99</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>21</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>IMUS</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:06:25</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:08:22</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:11:48</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:19:17</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>00:25:08</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>00:40:13</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>01:16:54</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>03:08:07</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:01:27</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:02:13</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:03:48</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:11:08</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>00:13:38</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>00:20:17</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>00:37:28</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>01:29:59</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>77.43</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>73.61</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>67.81</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>42.24</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>45.80</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>49.56</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>51.28</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>52.17</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>The table presents the processing time that for the IMUS algorithm, and the time savings delivered by the CMD 

															<Subscript>IMUS</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
														</SimplePara>


													</tfooter>


												</Table>


												<Para>To elucidate the benefits of parallel computing for signature discovery, various number of processing cores were used and the PISD 

													<Subscript>UO</Subscript>
 and PISD 

													<Subscript>IMUS</Subscript>
 algorithms were used to discover the signatures of ( 

													<Emphasis Type="Italic">l'</Emphasis>
 = 24, 

													<Emphasis Type="Italic">d'</Emphasis>
 = 4) from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 . Table 

													<InternalRef
RefID="T6">6</InternalRef>
 shows the experimental results: the acceleration is the processing time normalized to the processing time when one processor is used. When the PISD 

													<Subscript>UO</Subscript>
 algorithm is used, the acceleration of the discovery processes is almost proportional to the number of processing cores used. The acceleration values of the PISD 

													<Subscript>IMUS</Subscript>
 algorithm increase with the number of processing cores such that the discovery process using eight processing cores is approximately 4.6 times faster than that using a single core.
												</Para>


												<Table Float="No" ID="T6">

													<Caption Language="En">

														<CaptionNumber>Table 6</CaptionNumber>


														<CaptionContent>

															<SimplePara>The benefits of parallel computing for signature discovery.</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="5">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) PISD 

																			<Subscript>UO</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">CPUs</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">1</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">2</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">4</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">8</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Time</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>08:37:03</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>04:22:04</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>02:07:22</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>01:04:23</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Acceleration</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>1.00</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>1.97</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>4.06</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>8.03</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) PISD 

																			<Subscript>IMUS</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">CPUs</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">1</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">2</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">4</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">8</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Time</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>01:21:22</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:52:24</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:28:33</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:17:36</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Acceleration</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>1.00</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>1.55</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>2.85</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>4.62</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>With various number of processing cores, the PISD 

															<Subscript>UO</Subscript>
 and PISD 

															<Subscript>IMUS</Subscript>
 algorithms were used to discover the signatures of ( 

															<Emphasis
Type="Italic">l'</Emphasis>
 = 24, 

															<Emphasis
Type="Italic">d'</Emphasis>
 = 4) from 

															<Emphasis
Type="Italic">D</Emphasis>


															<Subscript>13</Subscript>
 . The table shows the experimental results. The acceleration is the processing time normalized to the processing time when one processor is used. The time format used in the table is HH:MM:SS.
														</SimplePara>


													</tfooter>


												</Table>


												<Para>Finally, the improvements in the discovery performance delivered by a combination of incremental discovery and parallel computing are examined. In this case, the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms discovered signatures from the databases using eight processing cores. Tables 

													<InternalRef
RefID="T7">7</InternalRef>
 and 

													<InternalRef
RefID="T8">8</InternalRef>
 present the time savings made by the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms. Tables 

													<InternalRef
RefID="T9">9</InternalRef>
 and 

													<InternalRef
RefID="T10">10</InternalRef>
 show the number of discovered signatures under each discovery condition. In the experiments, the proposed CMD 

													<Subscript>UO</Subscript>
 algorithm took 97% less processing time than the UO algorithm to discover all of the implicit signatures from 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 , and about 96.7% less processing time to complete discovery on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 . The CMD 

													<Subscript>IMUS</Subscript>
 algorithm took about 92.6% and 88.8% less processing time than the IMUS algorithm, to discover all of the signatures from the experimental data 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 and 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 , respectively.
												</Para>


												<Table Float="No" ID="T7">

													<Caption Language="En">

														<CaptionNumber>Table 7</CaptionNumber>


														<CaptionContent>

															<SimplePara>The performance of the CMD 

																<Subscript>UO</Subscript>
 algorithm when using eight processing cores.
															</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="5">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>13</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>UO</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>03:49:54</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>09:56:59</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>35:56:27</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>49:43:20</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:06:27</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:17:36</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>01:04:23</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>01:28:26</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>97.19</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>97.05</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>97.01</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>97.04</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>21</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>UO</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>01:10:18</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>03:09:37</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>11:12:18</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>15:32:13</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:02:13</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:06:03</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:22:20</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:30:36</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>96.85</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>96.81</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>96.68</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>96.72</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>The CMD 

															<Subscript>UO</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the time savings made by the CMD 

															<Subscript>UO</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
														</SimplePara>


													</tfooter>


												</Table>


												<Table Float="No" ID="T8">

													<Caption Language="En">

														<CaptionNumber>Table 8</CaptionNumber>


														<CaptionContent>

															<SimplePara>The performance of the CMD 

																<Subscript>IMUS</Subscript>
 algorithm when using eight processing cores.
															</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="9">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<colspec colname="c5"
colnum="5" />


														<colspec colname="c6"
colnum="6" />


														<colspec colname="c7"
colnum="7" />


														<colspec colname="c8"
colnum="8" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>13</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>IMUS</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:23:56</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:30:24</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:43:35</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:50:57</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>01:10:14</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>01:57:30</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>03:57:25</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>09:34:01</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:00:40</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:00:59</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:01:33</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:04:58</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>00:06:31</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>00:10:18</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>00:17:36</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>00:42:35</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>97.21</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>96.77</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>96.44</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>90.25</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>90.72</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>91.23</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>92.59</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>92.58</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) The results on 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>21</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara> 

																		<Emphasis
Type="Bold">overall</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>IMUS</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:06:25</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:08:22</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:11:48</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:19:17</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>00:25:08</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>00:40:13</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>01:16:54</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>03:08:07</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>00:00:20</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>00:00:30</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>00:00:43</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>00:02:44</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>00:03:22</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>00:04:57</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>00:08:29</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>00:21:05</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>saving(%)</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>94.81</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>94.02</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>93.93</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>85.83</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>86.60</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>87.69</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>88.97</SimplePara>


																</entry>


																<entry colname="c8">

																	<SimplePara>88.79</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>The CMD 

															<Subscript>IMUS</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the time savings made by the CMD 

															<Subscript>IMUS</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
														</SimplePara>


													</tfooter>


												</Table>


												<Table Float="No" ID="T9">

													<Caption Language="En">

														<CaptionNumber>Table 9</CaptionNumber>


														<CaptionContent>

															<SimplePara>The number of signatures discovered by the CMD 

																<Subscript>UO</Subscript>
 algorithm when using eight processing cores.
															</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="4">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The number of discovered signatures in 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>13</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>UO</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>4018054</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>3918976</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>3401911</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>4018054</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>3918976</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>3401911</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) The number of discovered signatures in 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>21</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(27,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>UO</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>2581787</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>2525108</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>2277644</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>UO</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>2581787</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>2525108</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>2277644</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>The CMD 

															<Subscript>UO</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the number of discovered signatures under each discovery condition.
														</SimplePara>


													</tfooter>


												</Table>


												<Table Float="No" ID="T10">

													<Caption Language="En">

														<CaptionNumber>Table 10</CaptionNumber>


														<CaptionContent>

															<SimplePara>The number of signatures discovered by the CMD 

																<Subscript>IMUS</Subscript>
 algorithm when using eight processing cores.
															</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="8">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<colspec colname="c3"
colnum="3" />


														<colspec colname="c4"
colnum="4" />


														<colspec colname="c5"
colnum="5" />


														<colspec colname="c6"
colnum="6" />


														<colspec colname="c7"
colnum="7" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(A) The number of discovered signatures in 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>13</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>IMUS</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>4676722</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>4661305</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>4612508</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>4018054</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>3967920</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>3836732</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>3401911</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>4676722</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>4661305</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>4612508</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>4018054</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>3967920</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>3836732</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>3401911</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(B) The number of discovered signatures in 

																			<Emphasis
Type="Italic">D</Emphasis>


																			<Subscript>21</Subscript>
 .
																		</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">( 

																			<Emphasis
Type="Italic">l'</Emphasis>
 , 

																			<Emphasis
Type="Italic">d'</Emphasis>
 )
																		</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,2)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(30,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(28,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(26,4)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara> 

																		<Emphasis
Type="Bold">(24,4)</Emphasis>

																	</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>IMUS</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>3017278</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>3010587</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>2982960</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>2581787</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>2552522</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>2482618</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>2277644</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>CMD 

																		<Subscript>IMUS</Subscript>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>3017278</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>3010587</SimplePara>


																</entry>


																<entry colname="c3">

																	<SimplePara>2982960</SimplePara>


																</entry>


																<entry colname="c4">

																	<SimplePara>2581787</SimplePara>


																</entry>


																<entry colname="c5">

																	<SimplePara>2552522</SimplePara>


																</entry>


																<entry colname="c6">

																	<SimplePara>2482618</SimplePara>


																</entry>


																<entry colname="c7">

																	<SimplePara>2277644</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>The CMD 

															<Subscript>IMUS</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the number of discovered signatures under each discovery condition.
														</SimplePara>


													</tfooter>


												</Table>


												<Para>The experimental results reveal that the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms with one processing core require up to 76% and 67% less processing time to find all implicit signatures than the typical UO and IMUS algorithms, respectively. Moreover, up to 97% and 93% of the processing time is saved when the CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms are executed using eight processing cores. Restated, the proposed CMD 

													<Subscript>UO</Subscript>
 and CMD 

													<Subscript>IMUS</Subscript>
 algorithms perform 4.2 and 3.03 times faster than the typical UO and IMUS algorithms when one processing core is used, and 33.74 and 13.48 times faster when eight processing cores are used.
												</Para>


											</Section3>


										</Section2>


									</Section1>


									<Section1 ID="Sec_05543">

										<Heading>Conclusions</Heading>


										<Para>This work proposes two unique signature discovery algorithms - the consecutive multiple discovery (CMD) algorithm and the parallel and incremental signature discovery (PISD) algorithm. The CMD algorithm is designed to discover all implicit signatures from DNA databases, providing all implicit signatures to users, especially when they are using an unfamiliar DNA database. The PISD algorithm is a parallel and incremental enhancement of existing signature discovery algorithms. It is based on incremental discovery, and efficiently discovers signatures under a certain discovery condition. This incremental strategy can be adapted to all hamming-distance-based unique signature discovery algorithms. The PISD algorithm has a significantly shorter processing time for signature discovery than typical discovery algorithms. The PISD algorithm is the kernel of the CMD algorithm. Consequently, the CMD algorithm provides an efficient means of implicit signature discovery.</Para>


									</Section1>


									<Section1 ID="Sec_64718">

										<Heading>Authors' contributions</Heading>


										<Para>HPL carried out the unique signature studies, participated in the design of the study, programmed the algorithms, evaluated the experimental results and drafted the manuscript. TFS participated in its design and coordination, performed the mathematical analysis and drafted the manuscript. CYT convinced of the study and helped to gather data. All authors read and approved the final manuscript.</Para>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/1471-2105/11/132"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Kaderali</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Schliep</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Selecting signature oligonucleotides to identify organisms using DNA arrays</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>1340</FirstPage>


												<LastPage>1349</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/18.10.1340</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Francois</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Charbonnier</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Jacquet</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Utinger</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Bento</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Lew</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>GM</Initials>


													<FamilyName>Kresbach</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Ehrat</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Schlegel</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Schrenzel</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Rapid bacterial identification using evanescent-waveguide oligonucleotide microarray classification</ArticleTitle>


												<JournalTitle>Journal of Microbiological Methods</JournalTitle>


												<VolumeID>65</VolumeID>


												<FirstPage>390</FirstPage>


												<LastPage>403</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1016/j.mimet.2005.08.012</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Mateo-Marti</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Briones</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CM</Initials>


													<FamilyName>Pradier</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JA</Initials>


													<FamilyName>Martin-Gago</FamilyName>


												</BibAuthorName>


												<Year>2007</Year>


												<ArticleTitle
Language="En">A DNA biosensor based on peptide nucleic acids on gold surfaces</ArticleTitle>


												<JournalTitle>Biosensors and Bioelectronics</JournalTitle>


												<VolumeID>22</VolumeID>


												<FirstPage>1926</FirstPage>


												<LastPage>1932</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1016/j.bios.2006.08.012</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HS</Initials>


													<FamilyName>Eom</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>BH</Initials>


													<FamilyName>Hwang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DH</Initials>


													<FamilyName>Kim</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>IB</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>YH</Initials>


													<FamilyName>Kim</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>HJ</Initials>


													<FamilyName>Cha</FamilyName>


												</BibAuthorName>


												<Year>2007</Year>


												<ArticleTitle
Language="En">Multiple detection of food-borne pathogenic bacteria using a novel 16S rDNA-based oligonucleotide signature chip</ArticleTitle>


												<JournalTitle>Biosensors and Bioelectronics</JournalTitle>


												<VolumeID>22</VolumeID>


												<FirstPage>845</FirstPage>


												<LastPage>853</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1016/j.bios.2006.03.005</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>BM</Initials>


													<FamilyName>Kiryu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CP</Initials>


													<FamilyName>Kiryu</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Rapid identification of Candida albicans and other human pathogenic yeasts by using oligonucleotides in a PCR</ArticleTitle>


												<JournalTitle>J Clin Microbiol</JournalTitle>


												<VolumeID>73</VolumeID>


												<FirstPage>1634</FirstPage>


												<LastPage>1641</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>F</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>GD</Initials>


													<FamilyName>Stormo</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">Selection of optimal DNA oligos for gene expression arrays</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>17</VolumeID>


												<FirstPage>1067</FirstPage>


												<LastPage>1076</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/17.11.1067</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JM</Initials>


													<FamilyName>Rouillard</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CJ</Initials>


													<FamilyName>Herbert</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Zuker</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Oligoarray: Genome-scale oligonucleotide design for microarrays</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>486</FirstPage>


												<LastPage>487</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/18.3.486</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>SS</Initials>


													<FamilyName>Kim</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CH</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>KM</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SD</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">A New Scheme for Nucleotide Sequence Signature Extraction</ArticleTitle>


												<JournalTitle>Proceedings of the 5th International Conference on Machine Learning and Applications</JournalTitle>


												<VolumeID />


												<FirstPage>162</FirstPage>


												<LastPage>167</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>AM</Initials>


													<FamilyName>Phillippy</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JA</Initials>


													<FamilyName>Mason</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ayanbule</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DD</Initials>


													<FamilyName>Sommer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Taviani</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Huq</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RR</Initials>


													<FamilyName>Colwell</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>IT</Initials>


													<FamilyName>Knight</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SL</Initials>


													<FamilyName>Salzberg</FamilyName>


												</BibAuthorName>


												<Year>2007</Year>


												<ArticleTitle
Language="En">Comprehensive DNA Signature Discovery and Validation</ArticleTitle>


												<JournalTitle>PLoS Computational Biology</JournalTitle>


												<VolumeID>3</VolumeID>


												<FirstPage />


												<LastPage />


												<Occurrence Type="DOI">

													<Handle>10.1371/journal.pcbi.0030098</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>SA</Initials>


													<FamilyName>van Hijum</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>de Jong</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Buist</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Kok</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>OP</Initials>


													<FamilyName>Kuipers</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">UniFrag and GenomePrimer: selection of primers for genome-wide production of unique amplicons</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>19</VolumeID>


												<FirstPage>1580</FirstPage>


												<LastPage>1582</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/btg203</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HH</Initials>


													<FamilyName>Chou</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>AP</Initials>


													<FamilyName>Hsia</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DL</Initials>


													<FamilyName>Mooney</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>PS</Initials>


													<FamilyName>Schnable</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">PICKY: oligo microarray design for large genomes</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>20</VolumeID>


												<FirstPage>2893</FirstPage>


												<LastPage>2902</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bth347</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>EK</Initials>


													<FamilyName>Nordberg</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">YODA: selecting signature oligonucleotides</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>1365</FirstPage>


												<LastPage>1370</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bti182</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Rahmann</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Rapid large-scale oligonucleotide selection for microarrays</ArticleTitle>


												<JournalTitle>Proc of the First IEEE Computer Society Bioinformatics Conference (CSB'02)</JournalTitle>


												<VolumeID />


												<FirstPage>54</FirstPage>


												<LastPage>63</LastPage>


												<Occurrence Type="DOI">

													<Handle>full_text</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HB</Initials>


													<FamilyName>Amin</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A-G</Initials>


													<FamilyName>Hashem</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RK</Initials>


													<FamilyName>Aziz</FamilyName>


												</BibAuthorName>


												<Year>2009</Year>


												<ArticleTitle
Language="En">Bioinformatics determination of ETEC signature genes as potential targets for molecular diagnosis and reverse vaccinology</ArticleTitle>


												<JournalTitle>BMC Bioinformatics</JournalTitle>


												<VolumeID>10</VolumeID>


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR15">

											<CitationNumber>15.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>AM</Initials>


													<FamilyName>Phillippy</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ayanbule</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>NJ</Initials>


													<FamilyName>Edwards</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SL</Initials>


													<FamilyName>Salzberg</FamilyName>


												</BibAuthorName>


												<Year>2009</Year>


												<ArticleTitle
Language="En">Insignia: a DNA signature search web server for diagnostic assay development</ArticleTitle>


												<JournalTitle>Nucleic Acids Research</JournalTitle>


												<VolumeID>37</VolumeID>


												<FirstPage>229</FirstPage>


												<LastPage>234</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/gkp286</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR16">

											<CitationNumber>16.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Frech</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Breuer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Ronacher</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Kern</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Sohn</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Gebauer</FamilyName>


												</BibAuthorName>


												<Year>2009</Year>


												<ArticleTitle
Language="En">hybseek: Pathogen primer design tool for diagnostic multi-analyte assays</ArticleTitle>


												<JournalTitle>Computer Methods and Programs in Biomedicine</JournalTitle>


												<VolumeID>94</VolumeID>


												<FirstPage>152</FirstPage>


												<LastPage>160</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1016/j.cmpb.2008.12.007</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR17">

											<CitationNumber>17.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Zheng</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TJ</Initials>


													<FamilyName>Close</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Jiang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Lonardi</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Efficient Selection of Unique and Popular Oligos for Large EST Databases</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>20</VolumeID>


												<FirstPage>2101</FirstPage>


												<LastPage>2112</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bth210</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR18">

											<CitationNumber>18.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HP</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TF</Initials>


													<FamilyName>Sheu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>YT</Initials>


													<FamilyName>Tsei</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CH</Initials>


													<FamilyName>Shih</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CY</Initials>


													<FamilyName>Tang</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Efficient Discovery of Unique Signatures on Whole-genome EST Databases</ArticleTitle>


												<JournalTitle>Proceeding of the 20th annual ACM Symposium on Applied Computing (SAC2005)</JournalTitle>


												<VolumeID />


												<FirstPage>100</FirstPage>


												<LastPage>104</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR19">

											<CitationNumber>19.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HP</Initials>


													<FamilyName>Lee</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>YT</Initials>


													<FamilyName>Tsai</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CH</Initials>


													<FamilyName>Shih</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TF</Initials>


													<FamilyName>Sheu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CY</Initials>


													<FamilyName>Tang</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">A Novel Approach for Efficient Query of Single Nucleotide Variation in DNA Databases</ArticleTitle>


												<JournalTitle>Proceeding of the Eighth Annual International Conference on Research in Computational Molecular Biology (RECOMB 2004)</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR20">

											<CitationNumber>20.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Zhang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Kao</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CL</Initials>


													<FamilyName>Yip</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">A Comparison Study on Algorithms for Incremental Update of Frequent Sequences</ArticleTitle>


												<JournalTitle>Proceeding of the Second IEEE Conference on Data Mining (ICDM2002)</JournalTitle>


												<VolumeID />


												<FirstPage>554</FirstPage>


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR21">

											<CitationNumber>21.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Xu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Ma</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">An Incremental Algorithm for Mining Classification Rules in Incomplete Information Systems</ArticleTitle>


												<JournalTitle>Proceeding of the International Conference of the North American Fuzzy Information Processing Society (NAFIPS 2004)</JournalTitle>


												<VolumeID />


												<FirstPage>446</FirstPage>


												<LastPage>449</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR22">

											<CitationNumber>22.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Varma</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Chalasani</FamilyName>


												</BibAuthorName>


												<Year>1992</Year>


												<ArticleTitle
Language="En">An Incremental Algorithm for TDM Switching Assignments in Satellite and Terrestrial Networks</ArticleTitle>


												<JournalTitle>IEEE Journal on Selected Areas in Communications (JSAC)</JournalTitle>


												<VolumeID>10</VolumeID>


												<FirstPage>364</FirstPage>


												<LastPage>377</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1109/49.126988</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR23">

											<CitationNumber>23.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DY</Initials>


													<FamilyName>Liu</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Incremental Algorithm for Detecting Community Structure in Dynamic Networks</ArticleTitle>


												<JournalTitle>Proceedings of 2005 International Conference on Machine Learning and Cybernetics</JournalTitle>


												<VolumeID />


												<FirstPage>2284</FirstPage>


												<LastPage>2290</LastPage>


												<Occurrence Type="DOI">

													<Handle>full_text</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR24">

											<CitationNumber>24.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>MG</Initials>


													<FamilyName>Rabbat</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RD</Initials>


													<FamilyName>Nowak</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Quantized Incremental Algorithms for Distributed Optimization</ArticleTitle>


												<JournalTitle>IEEE Journal on Selected Areas in Communications (JSAC)</JournalTitle>


												<VolumeID>23</VolumeID>


												<FirstPage>798</FirstPage>


												<LastPage>808</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1109/JSAC.2005.843546</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR25">

											<CitationNumber>25.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>FX</Initials>


													<FamilyName>Sun</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Errors Estimating of Incompletion and Updating Strategy in IDS</ArticleTitle>


												<JournalTitle>Proceeding of 2006 International Conference on Machine Learning and Cybernetics</JournalTitle>


												<VolumeID />


												<FirstPage>2948</FirstPage>


												<LastPage>2953</LastPage>


												<Occurrence Type="DOI">

													<Handle>full_text</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR26">

											<CitationNumber>26.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>MK</Initials>


													<FamilyName>Ponamgi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Manocha</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MC</Initials>


													<FamilyName>Lin</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">Incremental Algorithms for Collision Detection between Polygonal Models</ArticleTitle>


												<JournalTitle>IEEE Transactions on Visualization and Computer Graphics</JournalTitle>


												<VolumeID>3</VolumeID>


												<FirstPage>51</FirstPage>


												<LastPage>64</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1109/2945.582346</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR27">

											<CitationNumber>27.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Archambault</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Munzner</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Auber</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Smashing Peacocks Further: Drawing Quasi-Trees from Biconnected Components</ArticleTitle>


												<JournalTitle>IEEE Transactions on Visualization and Computer Graphics</JournalTitle>


												<VolumeID>12</VolumeID>


												<FirstPage>813</FirstPage>


												<LastPage>820</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1109/TVCG.2006.177</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR28">

											<CitationNumber>28.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Han</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Sutherland</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Jewett</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Campbell</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Meincke</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Tesmer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Iundt</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Fawcett</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Kim</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Deaven</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Doggett</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">Construction of a BAC contig map of chromosome 16q by two-dimensional overgo hybridization</ArticleTitle>


												<JournalTitle>Genome Research</JournalTitle>


												<VolumeID>10</VolumeID>


												<FirstPage>714</FirstPage>


												<LastPage>721</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1101/gr.10.5.714</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:DateLoaded>2010-05-13T19:48:09.188988+02:00</meta:DateLoaded>

					<meta:Authors>

						<meta:Author>Lee, Hsiao</meta:Author>

						<meta:Author>Sheu, Tzu-Fang</meta:Author>

						<meta:Author>Tang, Chuan</meta:Author>

					</meta:Authors>

					<meta:Institutions />

					<meta:Date>2010-03-16</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1471-2105-11-132</meta:DOI>

					<meta:Title>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</meta:Title>

					<meta:ISXN>1471-2105</meta:ISXN>

					<meta:Journal>BMC Bioinformatics</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>132</meta:ArticleFirstPage>

					<meta:Publication>BMC Bioinformatics</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Bioinformatics</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Microarrays</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Combinatorial Libraries</meta:Subject>

						<meta:Subject Priority="6"
Type="Secondary">Algorithms</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images>

				<Image Id="5-10.1186_1471-2105-11-132-0" xml:lang="en"
language="en">

					<Caption>

						<p>An example of the first potential problem of parallel signature discovery</p>


					</Caption>

					<FullText>

						<p>
For example, Figure 1 shows a list of six tasks.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-11-132-1.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-1" xml:lang="en"
language="en">

					<Caption>

						<p>The result of moving long tasks forward in the processing order list</p>


					</Caption>

					<FullText>

						<p>
Here, the long tasks are moved forward in the processing order
list, yielding the result in Figure 2 .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-11-132-2.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-2" xml:lang="en"
language="en">

					<Caption>

						<p>An example of the second potential problem of parallel signature discovery</p>


					</Caption>

					<FullText>

						<p>
For example, Figure 3 shows a list of six tasks.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-11-132-3.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-3" xml:lang="en"
language="en">

					<Caption>

						<p>The result of moving long tasks forward in the processing order list</p>


					</Caption>

					<FullText>

						<p>
Long tasks are moved forward, yielding the new processing order
list that is shown in Figure 4 .
						</p>

						<p>
For example, task F in Figure 4 can be divided into two tasks with
identical processing times, yielding the new task list in Figure 5
.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-11-132-4.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-4" xml:lang="en"
language="en">

					<Caption>

						<p>The result of dividing long tasks into short tasks in the processing order list</p>


					</Caption>

					<FullText>

						<p>
For example, task F in Figure 4 can be divided into two tasks with
identical processing times, yielding the new task list in Figure 5
.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-11-132-5.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-5" xml:lang="en"
language="en">

					<Caption>

						<p>An example of implicit signatures.</p>


					</Caption>

					<FullText>

						<p>

							<p>For example, Table 1(A) presents a DNA database of three
sequences.</p>


						</p>

						<p>
Table 1(B) lists the five patterns in the database.
						</p>

						<p>
Table 1(C) presents Ω 

							<sub>5,1</sub>
 , Ω 

							<sub>5,2</sub>
 , Ω

							<sub>4,1</sub>
 and Ω 

							<sub>4,2</sub>
 .

						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T1">

							<Caption Language="En">

								<CaptionNumber>Table 1</CaptionNumber>


								<CaptionContent>

									<SimplePara>An example of implicit signatures.</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="2">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) A DNA database.</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CCCTAATG</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>TTAATAAT</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>ATAATGCG</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) All 5-patterns in the database.</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CCCTA, CCTAA, CTAAT, TAATG, TTAAT, TAATA</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>AATAA, ATAAT, ATAAT, TAATG, AATGC, ATGCG</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(C) Some unique signatures in the database.</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Ω 

												<Subscript>5,1</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>CCCTA, CCTAA, AATAA, AATGC, ATGCG</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Ω 

												<Subscript>5,2</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>ATGCG</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Ω 

												<Subscript>4,1</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>CCCT, CCTA, ATGC, TGCG</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Ω 

												<Subscript>4,2</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>TGCG</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>Let Ω 

									<Subscript> 

										<Emphasis Type="Italic">l</Emphasis>
 , 

										<Emphasis Type="Italic">d</Emphasis>

									</Subscript>
 denote the set of the unique signatures of length 

									<Emphasis Type="Italic">l</Emphasis>
 and mismatch tolerance 

									<Emphasis Type="Italic">d</Emphasis>
 . The result shows that all of the patterns in Ω 

									<Subscript>5,2</Subscript>
 , Ω 

									<Subscript>4,1</Subscript>
 and Ω 

									<Subscript>4,2</Subscript>
 are implicit in Ω 

									<Subscript>5,1</Subscript>
 .
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-6" xml:lang="en"
language="en">

					<Caption>

						<p>An example of using the PEL heuristic to build an entry list.</p>


					</Caption>

					<FullText>

						<p>

							<p>As an example of the above, consider an entry list L , shown in
Table 2(A) .</p>


						</p>

						<p>
Table 2(B) shows the new processing order list.
						</p>

						<p>
J and E are exchanged in the list, yielding Table 2(C) .
						</p>

						<p>
The new list is as shown in Table 2(D) .
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T2">

							<Caption Language="En">

								<CaptionNumber>Table 2</CaptionNumber>


								<CaptionContent>

									<SimplePara>An example of using the PEL heuristic to build an entry list.</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="13">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<colspec colname="c7" colnum="7" />


								<colspec colname="c8" colnum="8" />


								<colspec colname="c9" colnum="9" />


								<colspec colname="c10" colnum="10" />


								<colspec colname="c11" colnum="11" />


								<colspec colname="c12" colnum="12" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The original entry list.</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">ID</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">A</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">B</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">C</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">D</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">E</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">F</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">G</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">H</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara> 

												<Emphasis Type="Bold">I</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara> 

												<Emphasis Type="Bold">J</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara> 

												<Emphasis Type="Bold">K</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>|*|</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>33</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>26</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>49</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>5</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>143</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>9</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>72</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>29</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>11</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara>55</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara>22</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) After first iteration, 

													<Emphasis Type="Italic">w</Emphasis>
 = 4.
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">ID</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">J</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">G</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">C</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">E</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">D</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">F</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">B</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">H</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara> 

												<Emphasis Type="Bold">I</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara> 

												<Emphasis Type="Bold">A</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara> 

												<Emphasis Type="Bold">K</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>|*|</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>55</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>72</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>49</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>143</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>9</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>26</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>29</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>11</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara>33</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara>22</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(C) After second iteration, 

													<Emphasis Type="Italic">w</Emphasis>
 = 1.
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">ID</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">E</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">G</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">C</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">J</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">D</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">F</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">B</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">H</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara> 

												<Emphasis Type="Bold">I</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara> 

												<Emphasis Type="Bold">A</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara> 

												<Emphasis Type="Bold">K</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>|*|</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>143</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>72</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>49</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>55</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>9</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>26</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>29</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>11</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara>33</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara>22</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(D) The final entry list.</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">ID</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">E 

													<Subscript>1</Subscript>

												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">E 

													<Subscript>2</Subscript>

												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">G</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">C</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">J</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">D</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">F</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">B</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara> 

												<Emphasis Type="Bold">H</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara> 

												<Emphasis Type="Bold">I</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara> 

												<Emphasis Type="Bold">A</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara> 

												<Emphasis Type="Bold">K</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>|*|</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>71</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>72</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>72</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>49</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>55</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>5</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>9</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>26</SimplePara>


										</entry>


										<entry colname="c9">

											<SimplePara>29</SimplePara>


										</entry>


										<entry colname="c10">

											<SimplePara>11</SimplePara>


										</entry>


										<entry colname="c11">

											<SimplePara>33</SimplePara>


										</entry>


										<entry colname="c12">

											<SimplePara>22</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>Let |*| denote the number of candidate patterns in an entry. Part (A) presents the original entry list. Entries A and J, B and G as well as D and E are exchanged. Part (B) presents the new entry list. Entries J and E are exchanged in the next iteration, yielding Part (C). Assume a two-processor computer is used. Entry E is divided into two partitions E 

									<Subscript>1</Subscript>
 and E 

									<Subscript>2</Subscript>
 , and E 

									<Subscript>1</Subscript>
 and E 

									<Subscript>2</Subscript>
 are added to the list. The new list is as shown in Part (D).
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-7" xml:lang="en"
language="en">

					<Caption>

						<p>The discovery conditions used in our experiments.</p>


					</Caption>

					<FullText>

						<p>
Table 3 presents the discovery conditions that were used in our
experiments.
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T3">

							<Caption Language="En">

								<CaptionNumber>Table 3</CaptionNumber>


								<CaptionContent>

									<SimplePara>The discovery conditions used in our experiments.</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="9">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<colspec colname="c7" colnum="7" />


								<colspec colname="c8" colnum="8" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">The used discovery conditions.</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara />


										</entry>


										<entry colname="c2">

											<SimplePara />


										</entry>


										<entry colname="c3">

											<SimplePara />


										</entry>


										<entry colname="c4">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara />


										</entry>


										<entry colname="c6">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara />


										</entry>


										<entry colname="c8">

											<SimplePara>•</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>•</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>•</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>• indicates that the discovery condition was used in the experiments by the specified algorithm.</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-8" xml:lang="en"
language="en">

					<Caption>

						<p>The performance of the CMD 

							<sub>UO</sub>
 algorithm when using a single processing core.
						</p>


					</Caption>

					<FullText>

						<p>
Tables 4 and 5 present the processing time that for the UO and IMUS
algorithms, and the time savings delivered by the CMD 

							<sub>UO</sub>

and CMD 

							<sub>IMUS</sub>
 algorithms.

						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T4">

							<Caption Language="En">

								<CaptionNumber>Table 4</CaptionNumber>


								<CaptionContent>

									<SimplePara>The performance of the CMD 

										<Subscript>UO</Subscript>
 algorithm when using a single processing core.
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="5">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>UO</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>03:49:54</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>09:56:59</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>35:56:27</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>49:43:20</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:50:52</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>02:22:32</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>08:37:03</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>11:50:27</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>77.87</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>76.12</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>76.02</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>76.19</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>UO</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>01:10:18</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>03:09:37</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>11:12:18</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>15:32:13</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:17:10</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:49:34</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>02:55:27</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>04:02:11</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>75.58</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>73.86</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>73.90</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>74.02</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>The table presents the processing time that for the UO algorithm, and the time savings delivered by the CMD 

									<Subscript>UO</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-9" xml:lang="en"
language="en">

					<Caption>

						<p>The performance of the CMD 

							<sub>IMUS</sub>
 algorithm when using a single processing core.
						</p>


					</Caption>

					<FullText>

						<p>
Tables 4 and 5 present the processing time that for the UO and IMUS
algorithms, and the time savings delivered by the CMD 

							<sub>UO</sub>

and CMD 

							<sub>IMUS</sub>
 algorithms.

						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T5">

							<Caption Language="En">

								<CaptionNumber>Table 5</CaptionNumber>


								<CaptionContent>

									<SimplePara>The performance of the CMD 

										<Subscript>IMUS</Subscript>
 algorithm when using a single processing core.
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="9">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<colspec colname="c7" colnum="7" />


								<colspec colname="c8" colnum="8" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>IMUS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:23:56</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:30:24</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:43:35</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:50:57</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>01:10:14</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>01:57:30</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>03:57:25</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>09:34:01</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:03:06</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:04:54</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:08:33</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:20:22</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>00:27:09</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>00:44:03</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>01:21:22</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>03:09:29</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>87.03</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>83.84</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>80.35</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>60.01</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>61.33</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>62.50</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>65.73</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>66.99</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>IMUS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:06:25</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:08:22</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:11:48</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:19:17</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>00:25:08</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>00:40:13</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>01:16:54</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>03:08:07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:01:27</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:02:13</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:03:48</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:11:08</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>00:13:38</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>00:20:17</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>00:37:28</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>01:29:59</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>77.43</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>73.61</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>67.81</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>42.24</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>45.80</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>49.56</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>51.28</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>52.17</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>The table presents the processing time that for the IMUS algorithm, and the time savings delivered by the CMD 

									<Subscript>IMUS</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-10" xml:lang="en"
language="en">

					<Caption>

						<p>The benefits of parallel computing for signature discovery.</p>


					</Caption>

					<FullText>

						<p>
Table 6 shows the experimental results: the acceleration is the
processing time normalized to the processing time when one
processor is used.
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T6">

							<Caption Language="En">

								<CaptionNumber>Table 6</CaptionNumber>


								<CaptionContent>

									<SimplePara>The benefits of parallel computing for signature discovery.</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="5">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">(A) PISD 

													<Subscript>UO</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">CPUs</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">1</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">2</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">4</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">8</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Time</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>08:37:03</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>04:22:04</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>02:07:22</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>01:04:23</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Acceleration</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1.00</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>1.97</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>4.06</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>8.03</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">(B) PISD 

													<Subscript>IMUS</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">CPUs</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">1</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">2</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">4</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">8</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Time</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>01:21:22</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:52:24</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:28:33</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:17:36</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Acceleration</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1.00</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>1.55</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2.85</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4.62</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>With various number of processing cores, the PISD 

									<Subscript>UO</Subscript>
 and PISD 

									<Subscript>IMUS</Subscript>
 algorithms were used to discover the signatures of ( 

									<Emphasis Type="Italic">l'</Emphasis>
 = 24, 

									<Emphasis Type="Italic">d'</Emphasis>
 = 4) from 

									<Emphasis Type="Italic">D</Emphasis>


									<Subscript>13</Subscript>
 . The table shows the experimental results. The acceleration is the processing time normalized to the processing time when one processor is used. The time format used in the table is HH:MM:SS.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-11" xml:lang="en"
language="en">

					<Caption>

						<p>The performance of the CMD 

							<sub>UO</sub>
 algorithm when using eight processing cores.
						</p>


					</Caption>

					<FullText>

						<p>
Tables 7 and 8 present the time savings made by the CMD

							<sub>UO</sub>
 and CMD 

							<sub>IMUS</sub>
 algorithms.

						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T7">

							<Caption Language="En">

								<CaptionNumber>Table 7</CaptionNumber>


								<CaptionContent>

									<SimplePara>The performance of the CMD 

										<Subscript>UO</Subscript>
 algorithm when using eight processing cores.
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="5">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>UO</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>03:49:54</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>09:56:59</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>35:56:27</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>49:43:20</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:06:27</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:17:36</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>01:04:23</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>01:28:26</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>97.19</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>97.05</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>97.01</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>97.04</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>UO</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>01:10:18</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>03:09:37</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>11:12:18</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>15:32:13</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:02:13</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:06:03</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:22:20</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:30:36</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>96.85</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>96.81</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>96.68</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>96.72</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>The CMD 

									<Subscript>UO</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the time savings made by the CMD 

									<Subscript>UO</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-12" xml:lang="en"
language="en">

					<Caption>

						<p>The performance of the CMD 

							<sub>IMUS</sub>
 algorithm when using eight processing cores.
						</p>


					</Caption>

					<FullText>

						<p>
Tables 7 and 8 present the time savings made by the CMD

							<sub>UO</sub>
 and CMD 

							<sub>IMUS</sub>
 algorithms.

						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T8">

							<Caption Language="En">

								<CaptionNumber>Table 8</CaptionNumber>


								<CaptionContent>

									<SimplePara>The performance of the CMD 

										<Subscript>IMUS</Subscript>
 algorithm when using eight processing cores.
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="9">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<colspec colname="c7" colnum="7" />


								<colspec colname="c8" colnum="8" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>IMUS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:23:56</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:30:24</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:43:35</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:50:57</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>01:10:14</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>01:57:30</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>03:57:25</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>09:34:01</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:00:40</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:00:59</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:01:33</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:04:58</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>00:06:31</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>00:10:18</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>00:17:36</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>00:42:35</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>97.21</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>96.77</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>96.44</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>90.25</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>90.72</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>91.23</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>92.59</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>92.58</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) The results on 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara> 

												<Emphasis Type="Bold">overall</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>IMUS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:06:25</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:08:22</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:11:48</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:19:17</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>00:25:08</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>00:40:13</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>01:16:54</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>03:08:07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>00:00:20</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>00:00:30</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>00:00:43</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>00:02:44</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>00:03:22</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>00:04:57</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>00:08:29</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>00:21:05</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>saving(%)</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>94.81</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>94.02</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>93.93</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>85.83</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>86.60</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>87.69</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>88.97</SimplePara>


										</entry>


										<entry colname="c8">

											<SimplePara>88.79</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>The CMD 

									<Subscript>IMUS</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the time savings made by the CMD 

									<Subscript>IMUS</Subscript>
 algorithm. The time format used in the table is HH:MM:SS.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-13" xml:lang="en"
language="en">

					<Caption>

						<p>The number of signatures discovered by the CMD 

							<sub>UO</sub>
 algorithm when using eight processing cores.
						</p>


					</Caption>

					<FullText>

						<p>
Tables 9 and 10 show the number of discovered signatures under each
discovery condition.
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T9">

							<Caption Language="En">

								<CaptionNumber>Table 9</CaptionNumber>


								<CaptionContent>

									<SimplePara>The number of signatures discovered by the CMD 

										<Subscript>UO</Subscript>
 algorithm when using eight processing cores.
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="4">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The number of discovered signatures in 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>UO</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>4018054</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3918976</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>3401911</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>4018054</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3918976</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>3401911</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) The number of discovered signatures in 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(27,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>UO</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>2581787</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>2525108</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2277644</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>UO</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>2581787</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>2525108</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2277644</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>The CMD 

									<Subscript>UO</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the number of discovered signatures under each discovery condition.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-11-132-14" xml:lang="en"
language="en">

					<Caption>

						<p>The number of signatures discovered by the CMD 

							<sub>IMUS</sub>
 algorithm when using eight processing cores.
						</p>


					</Caption>

					<FullText>

						<p>
Tables 9 and 10 show the number of discovered signatures under each
discovery condition.
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T10">

							<Caption Language="En">

								<CaptionNumber>Table 10</CaptionNumber>


								<CaptionContent>

									<SimplePara>The number of signatures discovered by the CMD 

										<Subscript>IMUS</Subscript>
 algorithm when using eight processing cores.
									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="8">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<colspec colname="c6" colnum="6" />


								<colspec colname="c7" colnum="7" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(A) The number of discovered signatures in 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>13</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>IMUS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>4676722</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>4661305</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>4612508</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4018054</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>3967920</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>3836732</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>3401911</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>4676722</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>4661305</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>4612508</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4018054</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>3967920</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>3836732</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>3401911</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">(B) The number of discovered signatures in 

													<Emphasis Type="Italic">D</Emphasis>


													<Subscript>21</Subscript>
 .
												</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis Type="Bold">( 

													<Emphasis Type="Italic">l'</Emphasis>
 , 

													<Emphasis Type="Italic">d'</Emphasis>
 )
												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">(28,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Bold">(26,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Bold">(24,2)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis Type="Bold">(30,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis Type="Bold">(28,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara> 

												<Emphasis Type="Bold">(26,4)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara> 

												<Emphasis Type="Bold">(24,4)</Emphasis>

											</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>IMUS</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>3017278</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3010587</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2982960</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2581787</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>2552522</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>2482618</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>2277644</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>CMD 

												<Subscript>IMUS</Subscript>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>3017278</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3010587</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2982960</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2581787</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>2552522</SimplePara>


										</entry>


										<entry colname="c6">

											<SimplePara>2482618</SimplePara>


										</entry>


										<entry colname="c7">

											<SimplePara>2277644</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>The CMD 

									<Subscript>IMUS</Subscript>
 algorithm discovered signatures from the databases using eight processing cores. The table presents the number of discovered signatures under each discovery condition.
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Lee, Hsiao</Author>

						<Author>Sheu, Tzu-Fang</Author>

						<Author>Tang, Chuan</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Applied Information Sciences, Chung Shan Medical University, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Medical Research, Chung Shan Medical University Hospital, Taichung, 40201 Taiwan, ROC</Institution>

						<Institution>Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan, ROC</Institution>

						<Institution>Department of Computer Science and Communication Engineering, Providence University, Taichung, 43301 Taiwan, ROC</Institution>

					</Institutions>

					<ArticleTitle>A parallel and incremental algorithm for efficient unique signature discovery on DNA databases</ArticleTitle>

					<DOI>10.1186/1471-2105-11-132</DOI>

					<PubDate>2010-03-16</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>11</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>parallel</Keyword>

						<Keyword>unique</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>databases</Keyword>

						<Keyword>DNA</Keyword>

						<Keyword>incremental</Keyword>

						<Keyword>discovery</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>signature</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Lee et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-11-132.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2010-08-20T19:12:10.710601+02:00</DateLoaded>

				</Image>

			</Images>

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1471-2105</JournalID>


						<JournalPrintISSN>1471-2105</JournalPrintISSN>


						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>


						<JournalTitle>BMC Bioinformatics</JournalTitle>


						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Bioinformatics</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Microarrays</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences </JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Combinatorial Libraries</JournalSubject>


							<JournalSubject Priority="6"
Type="Secondary">Algorithms</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>7</VolumeIDStart>


							<VolumeIDEnd>7</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>Suppl 4</IssueIDStart>


								<IssueIDEnd>Suppl 4</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2006</Year>


										<Month>12</Month>


										<Day>12</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Ning and Leong; licensee BioMed Central Ltd</CopyrightHolderName>


									<CopyrightYear>2006</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1471-2105-7-S4-S12</ArticleID>


									<ArticleDOI>10.1186/1471-2105-7-S4-S12</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">Towards a better solution to the shortest common supersequence problem: the deposition and reduction algorithm</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Research</ArticleCategory>


									<ArticleFirstPage>S12</ArticleFirstPage>


									<ArticleLastPage>S12</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year />


											<Month />


											<Day />


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year />


											<Month />


											<Day />


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Ning and Leong; licensee BioMed Central Ltd</CopyrightHolderName>


										<CopyrightYear>2006</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>7</VolumeIDStart>


										<VolumeIDEnd>7</VolumeIDEnd>


										<IssueIDStart>Suppl 4</IssueIDStart>


										<IssueIDEnd>Suppl 4</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Kang</GivenName>


												<FamilyName>Ning</FamilyName>


											</AuthorName>


											<Contact>

												<Email>ningkang@comp.nus.edu.sg</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Hon</GivenName>


												<FamilyName>Leong</FamilyName>


											</AuthorName>


											<Contact>

												<Email>leonghw@comp.nus.edu.sg</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>Department of Computer Science, National University of Singapore, Science Drive, Singapore 117543, Singapore</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>The problem of finding a Shortest Common Supersequence (SCS) of a set of sequences is an important problem with applications in many areas. It is a key problem in biological sequences analysis. The SCS problem is well-known to be NP-complete. Many heuristic algorithms have been proposed. Some heuristics work well on a 

												<Emphasis Type="Italic">few</Emphasis>
 long sequences (as in sequence comparison applications); others work well on many 

												<Emphasis Type="Italic">short</Emphasis>
 sequences (as in oligo-array synthesis). Unfortunately, most do not work well on 

												<Emphasis
Type="Italic">large SCS instances</Emphasis>
 where there are 

												<Emphasis Type="Italic">many</Emphasis>
 , 

												<Emphasis Type="Italic">long</Emphasis>
 sequences.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>In this paper, we present a 

												<Emphasis
Type="Italic">Deposition and Reduction (DR) algorithm</Emphasis>
 for solving large SCS instances of biological sequences. There are two processes in our DR algorithm: 

												<Emphasis
Type="Italic">deposition</Emphasis>
 process, and 

												<Emphasis
Type="Italic">reduction</Emphasis>
 process. The deposition process is responsible for generating a small set of common supersequences; and the reduction process shortens these common supersequences by removing some characters while preserving the common supersequence property. Our evaluation on simulated data and real DNA and protein sequences show that our algorithm consistently produces the best results compared to many well-known heuristic algorithms, and especially on large instances.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusion</Heading>


											<Para>Our DR algorithm provides a partial answer to the open problem of designing efficient heuristic algorithm for SCS problem on many long sequences. Our algorithm has a bounded approximation ratio. The algorithm is efficient, both in running time and space complexity and our evaluation shows that it is practical even for SCS problems on many long sequences.</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Keyword>problem</Keyword>


										<Keyword>shortest</Keyword>


										<Keyword>common</Keyword>


										<Keyword>solution</Keyword>


										<Keyword>algorithm</Keyword>


										<Keyword>Towards</Keyword>


										<Keyword>deposition</Keyword>


										<Keyword>better</Keyword>


										<Keyword>reduction</Keyword>


										<Keyword>supersequence</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_27916">

										<Heading>Background</Heading>


										<Para>The problem of finding a 

											<Emphasis
Type="Italic">Shortest Common Supersequence</Emphasis>
 (SCS) of a given set of sequences is a very important problem in computer science, especially in computational molecular biology. The SCS of a set of sequences can be stated as follows: Given two sequences 

											<Emphasis Type="Italic">S</Emphasis>
 = 

											<Emphasis Type="Italic">s</Emphasis>


											<Subscript>1</Subscript>


											<Emphasis Type="Italic">s</Emphasis>


											<Subscript>2</Subscript>
 ... 

											<Emphasis Type="Italic">s</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">m</Emphasis>

											</Subscript>
 and 

											<Emphasis Type="Italic">T</Emphasis>
 = 

											<Emphasis Type="Italic">t</Emphasis>


											<Subscript>1</Subscript>


											<Emphasis Type="Italic">t</Emphasis>


											<Subscript>2</Subscript>
 ... 

											<Emphasis Type="Italic">t</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">n</Emphasis>

											</Subscript>
 , over an alphabet set Σ = {σ 

											<Subscript>1</Subscript>
 , σ 

											<Subscript>2</Subscript>
 ,...,σ 

											<Subscript> 

												<Emphasis Type="Italic">q</Emphasis>

											</Subscript>
 }, we say that 

											<Emphasis Type="Italic">S</Emphasis>
 is the 

											<Emphasis
Type="Italic">subsequence</Emphasis>
 of 

											<Emphasis Type="Italic">T</Emphasis>
 (and equivalently, 

											<Emphasis Type="Italic">T</Emphasis>
 is the 

											<Emphasis
Type="Italic">supersequence</Emphasis>
 of 

											<Emphasis Type="Italic">S</Emphasis>
 ) if for every 

											<Emphasis Type="Italic">s</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">j</Emphasis>

											</Subscript>
 , there isfor some 1 ≤ 

											<Emphasis Type="Italic">i</Emphasis>


											<Subscript>1</Subscript>
 &lt; 

											<Emphasis Type="Italic">i</Emphasis>


											<Subscript>2</Subscript>
 &lt; ... &lt; 

											<Emphasis Type="Italic">i</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">m</Emphasis>

											</Subscript>
 ≤ 

											<Emphasis Type="Italic">n</Emphasis>
 . Given a finite set of sequences 

											<Emphasis Type="Italic">S</Emphasis>
 = { 

											<Emphasis Type="Italic">S</Emphasis>


											<Subscript>1</Subscript>
 , 

											<Emphasis Type="Italic">S</Emphasis>


											<Subscript>2</Subscript>
 ,..., 

											<Emphasis Type="Italic">S</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">k</Emphasis>

											</Subscript>
 }, a 

											<Emphasis
Type="Italic">common supersequence of S</Emphasis>
 is a sequence 

											<Emphasis Type="Italic">T</Emphasis>
 such that 

											<Emphasis Type="Italic">T</Emphasis>
 is a supersequence of 

											<Emphasis Type="Italic">every</Emphasis>
 sequence 

											<Emphasis Type="Italic">S</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">j</Emphasis>

											</Subscript>
 (1 ≤ 

											<Emphasis Type="Italic">j</Emphasis>
 ≤ 

											<Emphasis Type="Italic">k</Emphasis>
 ) in 

											<Emphasis Type="Italic">S</Emphasis>
 . Then, a 

											<Emphasis
Type="Italic">shortest common supersequence</Emphasis>
 ( 

											<Emphasis Type="Italic">SCS</Emphasis>
 ) 

											<Emphasis Type="Italic">of S</Emphasis>
 is a supersequence of 

											<Emphasis Type="Italic">S</Emphasis>
 that has 

											<Emphasis Type="Italic">minimum</Emphasis>
 length. In this paper, we shall assume that 

											<Emphasis Type="Italic">k</Emphasis>
 is the number of sequences in 

											<Emphasis Type="Italic">S</Emphasis>
 , 

											<Emphasis Type="Italic">n</Emphasis>
 is the length of each sequence, and 

											<Emphasis Type="Italic">q</Emphasis>
 = |Σ| is the size of the alphabet.
										</Para>


										<Para>The SCS problem has applications in many diverse areas, including data compression 

											<CitationRef
CitationID="B1">1</CitationRef>
 , scheduling 

											<CitationRef
CitationID="B2">2</CitationRef>
 , query optimization 

											<CitationRef
CitationID="B3">3</CitationRef>
 , text comparison and analysis, and biological sequence comparisons and analysis 

											<CitationRef
CitationID="B4">4</CitationRef>


											<CitationRef
CitationID="B5">5</CitationRef>
 . As a result, the SCS problem has been very intensively researched 

											<CitationRef
CitationID="B6">6</CitationRef>


											<CitationRef
CitationID="B7">7</CitationRef>
 . One basic result is that the SCS of 

											<Emphasis Type="Italic">two</Emphasis>
 sequences of length 

											<Emphasis Type="Italic">n</Emphasis>
 can be computed using dynamic programming in 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">n</Emphasis>


											<Superscript>2</Superscript>
 ) time and 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">n</Emphasis>


											<Superscript>2</Superscript>
 ) space (see, for example, 

											<CitationRef
CitationID="B8">8</CitationRef>
 ). There are also several papers that reported improvements on the running time and space required for dynamic programming algorithms 

											<CitationRef
CitationID="B7">7</CitationRef>
 . For a fixed 

											<Emphasis Type="Italic">k</Emphasis>
 , the dynamic programming algorithm can be extended to solve the SCS problem for 

											<Emphasis Type="Italic">k</Emphasis>
 sequences of length 

											<Emphasis Type="Italic">n</Emphasis>
 in 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">n</Emphasis>


											<Superscript> 

												<Emphasis Type="Italic">k</Emphasis>

											</Superscript>
 ) time and space. Clearly, this algorithm is not practical for large 

											<Emphasis Type="Italic">k</Emphasis>
 . The general SCS problem on arbitrary 

											<Emphasis Type="Italic">k</Emphasis>
 sequences of length 

											<Emphasis Type="Italic">n</Emphasis>
 is well-known to be NP-hard. In fact, Jiang and Li 

											<CitationRef
CitationID="B8">8</CitationRef>
 showed that even the problem of finding a constant-ratio approximation solution is also NP-hard.
										</Para>


										<Para>A trivial algorithm, called Alphabet 

											<CitationRef
CitationID="B6">6</CitationRef>
 gives an approximation ratio of 

											<Emphasis Type="Italic">q</Emphasis>
 = |Σ|. In practice, it is well known that heuristic algorithms produce results that are better than the Alphabet algorithm. Many heuristic algorithms have been proposed for the general SCS problem, including Alphabet 

											<CitationRef
CitationID="B6">6</CitationRef>
 , Majority Merge 

											<CitationRef
CitationID="B8">8</CitationRef>
 , Tournament 

											<CitationRef
CitationID="B9">9</CitationRef>
 , Greedy 

											<CitationRef
CitationID="B9">9</CitationRef>
 , and Reduce-Expand 

											<CitationRef
CitationID="B6">6</CitationRef>
 . Several heuristic algorithms were also proposed specifically for computing the SCS of DNA sequences (with alphabet size of 4). These include Min-Height 

											<CitationRef
CitationID="B10">10</CitationRef>
 , Sum-Height 

											<CitationRef
CitationID="B10">10</CitationRef>
 heuristics. (Interestingly, the Majority Merge 

											<CitationRef
CitationID="B8">8</CitationRef>
 and Sum-Height 

											<CitationRef
CitationID="B10">10</CitationRef>
 heuristic are the same algorithm.) Recently, we 

											<CitationRef
CitationID="B11">11</CitationRef>
 proposed 

											<Emphasis
Type="Italic">look-ahead extensions</Emphasis>
 of these heuristics, as well as a post-processing reduction procedure and studied the performances of these algorithms on DNA sequences to be used for the synthesis of oligo-array.
										</Para>


										<Para>This paper focuses on algorithms for solving 

											<Emphasis
Type="Italic">large SCS instances</Emphasis>
 . By 

											<Emphasis
Type="Italic">large SCS instances</Emphasis>
 , we mean SCS instances 

											<Emphasis Type="Italic">S</Emphasis>
 in which
										</Para>


										<Para>(a) the sequences in 

											<Emphasis Type="Italic">S</Emphasis>
 are 

											<Emphasis Type="Italic">long</Emphasis>
 ( 

											<Emphasis Type="Italic">n</Emphasis>
 is 100 to 1000),
										</Para>


										<Para>(b) there are 

											<Emphasis Type="Italic">many</Emphasis>
 sequences ( 

											<Emphasis Type="Italic">k</Emphasis>
 is 100 or more), and
										</Para>


										<Para>(c) the alphabet set may be 

											<Emphasis Type="Italic">big</Emphasis>
 ( 

											<Emphasis Type="Italic">q</Emphasis>
 is 20 for protein sequences).
										</Para>


										<Para>Large SCS instances arise more frequently in the post-genome era in biological applications dealing with DNA and protein sequences.</Para>


										<Para>In this paper, we propose to solve large SCS instances with our 

											<Emphasis
Type="Italic">Deposition and Reduction</Emphasis>
 algorithm (DR). The DR algorithm is based on the post processing algorithm that we have proposed on the SCS problem for DNA oligos 

											<CitationRef
CitationID="B11">11</CitationRef>
 . The DR algorithm is suitable for solving large SCS instances – for example, SCS instances with up to 5,000 DNA and protein sequences each of length 1000. We present experimental evaluation using simulated data and real DNA and protein sequences to show that our DR algorithm outperforms the other heuristic algorithms for the SCS problem on these large SCS instances.
										</Para>


										<Section2 ID="Sec_43268">

											<Heading>Previous research of the SCS problem</Heading>


											<Para>We now present a brief survey of several heuristic algorithms for the SCS problem. Due to space limitation, we will focus only on algorithms that we have included in our comparative study.</Para>


											<Para>Let 

												<Emphasis Type="Italic">S</Emphasis>
 be any instance of the SCS problem and let 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ) be the supersequence of 

												<Emphasis Type="Italic">S</Emphasis>
 computed by a heuristic algorithm 

												<Emphasis Type="Italic">A</Emphasis>
 . Let 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ) denote an optimal solution for the instance 

												<Emphasis Type="Italic">S</Emphasis>
 . Then, we say that 

												<Emphasis Type="Italic">A</Emphasis>
 has an approximation ratio of λ if | 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|/| 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )| ≤ λ for all instances 

												<Emphasis Type="Italic">S</Emphasis>
 .
											</Para>


											<Section3 ID="Sec_11752">

												<Heading>Alphabet algorithm 6</Heading>


												<Para>The Alphabet algorithm is a very simple algorithm. Let 

													<Emphasis Type="Italic">S</Emphasis>
 be a set of sequences of maximum length 

													<Emphasis Type="Italic">n</Emphasis>
 over the alphabet Σ = {σ 

													<Subscript>1</Subscript>
 , σ 

													<Subscript>2</Subscript>
 ,...,σ 

													<Subscript> 

														<Emphasis
Type="Italic">q</Emphasis>

													</Subscript>
 }, then the Alphabet algorithm outputs a common supersequence of (σ 

													<Subscript>1</Subscript>
 σ 

													<Subscript>2</Subscript>
 ...σ 

													<Subscript> 

														<Emphasis
Type="Italic">q</Emphasis>

													</Subscript>
 ) 

													<Superscript> 

														<Emphasis
Type="Italic">n</Emphasis>

													</Superscript>
 . The Alphabet algorithm has an approximation ratio of 

													<Emphasis Type="Italic">q</Emphasis>
 = |Σ|. The time complexity of the Alphabet algorithm is O( 

													<Emphasis Type="Italic">qn</Emphasis>
 ). There have also been modifications of the Alphabet algorithm that uses information from 

													<Emphasis Type="Italic">S</Emphasis>
 to "remove" redundant characters in (σ 

													<Subscript>1</Subscript>
 σ 

													<Subscript>2</Subscript>
 ...σ 

													<Subscript> 

														<Emphasis
Type="Italic">q</Emphasis>

													</Subscript>
 ) 

													<Superscript> 

														<Emphasis
Type="Italic">n</Emphasis>

													</Superscript>
 . These methods improve the performance in practice, but not in the worst case approximation ratio of 

													<Emphasis Type="Italic">q</Emphasis>
 .
												</Para>


											</Section3>


											<Section3 ID="Sec_98540">

												<Heading>Majority-Merge algorithm 8</Heading>


												<Para>The Majority-Merge algorithm (MM) is a simple, greedy heuristic algorithm. Suppose we analyze every sequence from left to right, the 

													<Emphasis
Type="Italic">frontier</Emphasis>
 is defined as the rightmost characters to be analyzed. Initially, the supersequence 

													<Emphasis Type="Italic">CS</Emphasis>
 is empty. At each step, let 

													<Emphasis Type="Italic">s</Emphasis>
 be the majority among the " 

													<Emphasis
Type="Italic">frontier</Emphasis>
 " characters of the remaining portions of the sequences in 

													<Emphasis Type="Italic">S</Emphasis>
 . Set 

													<Emphasis Type="Italic">CS</Emphasis>
 = 

													<Emphasis Type="Italic">CS</Emphasis>
 || 

													<Emphasis Type="Italic">s</Emphasis>
 (where "||" represent concatenation) and delete the " 

													<Emphasis
Type="Italic">frontier</Emphasis>
 " 

													<Emphasis Type="Italic">s</Emphasis>
 characters from sequences in 

													<Emphasis Type="Italic">S</Emphasis>
 . Repeat until no sequences are left. This algorithm is the same as the 

													<Emphasis
Type="Bold">Sum Height algorithm (SH)</Emphasis>
 proposed in 

													<CitationRef
CitationID="B10">10</CitationRef>
 . This algorithm does 

													<Emphasis
Type="Italic">not</Emphasis>
 have any worst-case approximation ratio, but performs very well in practice. The time complexity of the Majority-Merge algorithm is O( 

													<Emphasis
Type="Italic">qkn</Emphasis>
 ).
												</Para>


											</Section3>


											<Section3 ID="Sec_52309">

												<Heading>Greedy and Tournament algorithms 12</Heading>


												<Para>The Greedy algorithm (GRDY) and Tournament algorithm (TOUR) studied in 

													<CitationRef
CitationID="B12">12</CitationRef>
 are two variations of an iterative scheme based on combining "best" sequence pairs. Given any pair of sequences, 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 and 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 , an optimal supersequence of the pair, denoted by SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ), can be computed in 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis Type="Italic">n</Emphasis>


													<Superscript>2</Superscript>
 ) using dynamic programming. The Greedy algorithm first chooses the "best" sequence pair – the pair that gives the shortest SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">i</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">j</Emphasis>

													</Subscript>
 ). Without loss of generality, we assume that these two sequences are 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 and 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 . The algorithm then replaces the two sequences 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 and 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 by their supersequence, SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 ). The algorithm proceeds recursively. Thus, we can express it as follows:
												</Para>


												<Para>Greedy( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 ,..., 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Subscript>
 ) = Greedy(SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 ), 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>3</Subscript>
 ,..., 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Subscript>
 ).
												</Para>


												<Para>The Tournament algorithm is similar to the Greedy algorithm. It builds a "tournament" based on finding 

													<Emphasis
Type="Italic">multiple</Emphasis>
 best pairs at each round and can be expressed schematically as follows:
												</Para>


												<Para>Tournament( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 ,..., 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Subscript>
 ) = Tournament(SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>1</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>2</Subscript>
 ), SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>3</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript>4</Subscript>
 ),...,SCS( 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">k</Emphasis>
 -1
													</Subscript>
 , 

													<Emphasis Type="Italic">S</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Subscript>
 )).
												</Para>


												<Para>Both Greedy and Tournament algorithms have 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis Type="Italic">k</Emphasis>


													<Superscript>2</Superscript>


													<Emphasis Type="Italic">n</Emphasis>


													<Superscript>2</Superscript>
 ) time complexity and 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis
Type="Italic">kn + n</Emphasis>


													<Superscript>2</Superscript>
 ) space complexity. Unfortunately, it was shown in 

													<CitationRef
CitationID="B9">9</CitationRef>
 that both Greedy and Tournament do not have approximation ratios.
												</Para>


											</Section3>


											<Section3 ID="Sec_35387">

												<Heading>Reduce-Expand algorithm 6</Heading>


												<Para>The Reduce-Expand algorithm (RE) is based on reducing sequences to 

													<Emphasis
Type="Italic">basic sequences</Emphasis>
 , which are sequences that have no adjacent characters of the same alphabet. For example, sequence "AACGG" can be reduced to basic sequence of "AG". The expand process tries to add characters into the common subsequence of the sequences, while preserving the common subsequence property. Using a process of reduce, auxiliary set and expand, this algorithm can produce short SCS on binary sequences as well as datasets with few sequences and more alphabets. The RE algorithm has an approximation ratio of 

													<Emphasis Type="Italic">q</Emphasis>
 = |Σ| and has time complexity 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis Type="Italic">q</Emphasis>


													<Superscript>2+ 

														<Emphasis
Type="Italic">α</Emphasis>

													</Superscript>


													<Emphasis Type="Italic">kn</Emphasis>


													<Superscript>2+ 

														<Emphasis
Type="Italic">α</Emphasis>

													</Superscript>
 log 

													<Emphasis Type="Italic">n</Emphasis>
 ) and space complexity 

													<Emphasis Type="Italic">O</Emphasis>
 ( 

													<Emphasis Type="Italic">nk</Emphasis>
 + 

													<Emphasis Type="Italic">n</Emphasis>


													<Superscript>2</Superscript>
 ), where 

													<Emphasis Type="Italic">α</Emphasis>
 ≥ 0 is a constant integer. It was shown in 

													<CitationRef
CitationID="B6">6</CitationRef>
 that RE performs well on longer sequences (up to 300) with larger alphabets. However, the experimental studies were confined to datasets with relatively few sequences ( 

													<Emphasis Type="Italic">k</Emphasis>
 is small, up to about 20). For large SCS instances, the space and time requirements of RE may be a limiting factor.
												</Para>


												<Para>We next survey heuristic algorithms 

													<CitationRef
CitationID="B10">10</CitationRef>


													<CitationRef
CitationID="B11">11</CitationRef>


													<CitationRef
CitationID="B13">13</CitationRef>
 that were designed specifically to target the SCS of DNA sequences – to be used for synthesis of DNA microarrays and oligo-arrays. To describe these methods, we adopt the notations used in 

													<CitationRef
CitationID="B10">10</CitationRef>
 , and used examples in DNA sequences. An example of sequence deposition is shown in Figure 

													<InternalRef
RefID="F1">1</InternalRef>
 . After 

													<Emphasis Type="Italic">t</Emphasis>
 cycles, a partial sequence has been synthesized. The height of each partially constructed sequence is defined as the number of bases in it. Indication of how much work that has been accomplished after 

													<Emphasis Type="Italic">t</Emphasis>
 cycles is measured in terms of (i) the 

													<Emphasis
Type="Italic">Min-Height</Emphasis>
 ( 

													<Emphasis Type="Bold"> 

														<Emphasis
Type="Italic">MH</Emphasis>

													</Emphasis>
 ) – the height of the 

													<Emphasis
Type="Italic">shortest</Emphasis>
 partially constructed sequences after 

													<Emphasis Type="Italic">t</Emphasis>
 cycles, or (ii) the 

													<Emphasis
Type="Italic">Sum-Height</Emphasis>
 ( 

													<Emphasis Type="Bold"> 

														<Emphasis
Type="Italic">SH</Emphasis>

													</Emphasis>
 ) – the 

													<Emphasis
Type="Italic">sum of the heights</Emphasis>
 of the partially constructed sequences.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F1">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>A step-by-step illustration of the process of finding a common supersequence via deposition</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S12-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">A step-by-step illustration of the process of finding a common supersequence via deposition</Emphasis>
 . The characters above bar represents the sequences have yet to be deposited. The underlined blue characters are the characters that are deposited in the current step. The final result is CS = "ACGCT", which, for this example, is also the optimal result.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


											</Section3>


											<Section3 ID="Sec_87558">

												<Heading>Min-Height 10</Heading>


												<Para>The Min-Height (MH) greedy method, denoted here by 

													<Emphasis Type="Bold">MH</Emphasis>
 , selects a character that will 

													<Emphasis
Type="Italic">extend the shortest</Emphasis>
 partially constructed oligo by one base (thus, potentially increasing the minimum height). When there is a tie, we randomly pick one such character.
												</Para>


											</Section3>


											<Section3 ID="Sec_22982">

												<Heading>Sum-Height 1011</Heading>


												<Para>The Sum-Height (SH) greedy method, denoted by 

													<Emphasis Type="Bold">SH</Emphasis>
 , selects a character that will result in the 

													<Emphasis
Type="Italic">largest increase</Emphasis>
 in the sum-height. This method is the same as the Majority Merge (MM) algorithm. Again, ties are broken randomly.
												</Para>


												<Para>Both MH and SH are very fast algorithms, with time complexity of O( 

													<Emphasis
Type="Italic">qkn</Emphasis>
 ) time. In general, they work well for DNA sequences.
												</Para>


											</Section3>


											<Section3 ID="Sec_19656">

												<Heading>Look-ahead extensions of SH and MH</Heading>


												<Para>A natural way to improve the fast greedy algorithms MH and SH is to apply a " 

													<Emphasis
Type="Italic">look-ahead</Emphasis>
 " strategy to it 

													<CitationRef
CitationID="B11">11</CitationRef>
 . This strategy looks at a number of steps ahead before deciding which character(s) is the 

													<Emphasis
Type="Italic">best</Emphasis>
 to be added. More specifically, choose two integers 

													<Emphasis Type="Italic">m</Emphasis>
 and 

													<Emphasis Type="Italic">l</Emphasis>
 such that 

													<Emphasis Type="Italic">l</Emphasis>
 ≤ 

													<Emphasis Type="Italic">m</Emphasis>
 . Then, the 

													<Emphasis
Type="Italic">look-ahead extension of the SH method</Emphasis>
 works as follows: (i) Examine all possible partial sequences that can be generated in 

													<Emphasis Type="Italic">m</Emphasis>
 cycles; (ii) for each such generated partial sequences, compute the resulting sum height; and (iii) select the partial sequence that will result in the 

													<Emphasis
Type="Italic">largest increase of sum-height in m cycles</Emphasis>
 . Then, we extend the chosen sequence by 

													<Emphasis Type="Italic">l</Emphasis>
 (≤ 

													<Emphasis Type="Italic">m</Emphasis>
 ) characters. (It can be shown that extending by 

													<Emphasis Type="Italic">l</Emphasis>
 characters (instead of 

													<Emphasis Type="Italic">m</Emphasis>
 ) gives the potential of obtaining even better increase in the sum-height after 

													<Emphasis Type="Italic">m</Emphasis>
 cycles.) Here we break the tie arbitrarily. This look ahead algorithm is called the 

													<Emphasis Type="Bold">( 

														<Emphasis
Type="Italic">m</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 )-look-ahead SH
													</Emphasis>
 , and is abbreviated to 

													<Emphasis Type="Bold">( 

														<Emphasis
Type="Italic">m</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) 

														<Emphasis
Type="Italic">-LA-SH</Emphasis>

													</Emphasis>
 .
												</Para>


												<Para>The look-ahead extension of MH is defined in a similar fashion, and is denoted by 

													<Emphasis Type="Bold">( 

														<Emphasis
Type="Italic">m</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) 

														<Emphasis
Type="Italic">-LA-MH</Emphasis>

													</Emphasis>
 .
												</Para>


												<Para>It is easy to see that an increase in 

													<Emphasis Type="Italic">m</Emphasis>
 naturally leads to a shorter SCS, but at a cost of a substantial increase in computing time. Experimentation done in 

													<CitationRef
CitationID="B11">11</CitationRef>
 indicated that LA-SH gives the best performance and that (3,1) 

													<Emphasis
Type="Italic">-LA-SH</Emphasis>
 gives the best trade-off between running time and quality of the SCS solution.
												</Para>


											</Section3>


											<Section3 ID="Sec_93614">

												<Heading>Look-Ahead Post Processing algorithm 11</Heading>


												<Para>Finally, we recently proposed a Look-Ahead Post Processing (LAP) algorithm to solve the SCS problem on many DNA sequences that produce very good results for the case when the number of sequences 

													<Emphasis Type="Italic">k</Emphasis>
 is large ( 

													<Emphasis Type="Italic">k</Emphasis>
 reaching 100,000) and sequence length 

													<Emphasis Type="Italic">n</Emphasis>
 is relatively small ( 

													<Emphasis Type="Italic">n</Emphasis>
 up to 60). The LAP algorithm outperforms Alphabet, SH, and MH heuristics on these SCS instances.
												</Para>


											</Section3>


											<Section3 ID="Sec_84684">

												<Heading>Hubbell-Morris-Winkler algorithm 13</Heading>


												<Para>The Hubbell-Morris-Winkler algorithm is another heuristic algorithm specifically designed for SCS of DNA oligos (short DNA sequences). The method consists of two key steps. First, it finds a shortest periodical strategy (one out of 4 

													<Superscript>4</Superscript>
 periodical strategies) corresponding to the following string: ( 

													<Emphasis
Type="Italic">XYZ</Emphasis>
 ) ( 

													<Emphasis
Type="Italic">XYZ</Emphasis>
 ) ... ( 

													<Emphasis
Type="Italic">XYZU</Emphasis>
 )( 

													<Emphasis
Type="Italic">XYZU</Emphasis>
 ), where 

													<Emphasis Type="Italic">X</Emphasis>
 , 

													<Emphasis Type="Italic">Y</Emphasis>
 , 

													<Emphasis Type="Italic">Z</Emphasis>
 , 

													<Emphasis Type="Italic">U</Emphasis>
 denote the four different characters in some order. Second, examine each character in a cycle from the first to the last and remove the character if either (a) it is not needed by any oligo, or (b) it can be added in a later cycle. Hubbell-Morris-Winkler algorithm has comparable performance compared to our LAP algorithm on DNA sequences. 

													<Emphasis
Type="Italic">However, the algorithm is only suitable for sequences small alphabet size q such as DNA sequences</Emphasis>
 . For protein sequences (where 

													<Emphasis Type="Italic">q</Emphasis>
 = 20), there are 20 

													<Superscript>20</Superscript>
 periodical strategies and so the Hubbell-Morris-Winkler algorithm is not practical any more.
												</Para>


											</Section3>


											<Section3 ID="Sec_25168">

												<Heading>Other algorithms</Heading>


												<Para>Other SCS algorithms have also been proposed 

													<CitationRef
CitationID="B14">14</CitationRef>


													<CitationRef
CitationID="B15">15</CitationRef>


													<CitationRef
CitationID="B16">16</CitationRef>
 , but were not included in this study because they involve meta-heuristic search (genetic algorithms, ant colony optimization) and have very long running times. Thus, they are not practical for the large SCS instances considered in this paper.
												</Para>


											</Section3>


										</Section2>


										<Section2 ID="Sec_68999">

											<Heading>Performance ratio</Heading>


											<Para>To compare the performances of different algorithms across different instances of different sizes, we define the notion of 

												<Emphasis
Type="Italic">performance ratio</Emphasis>
 . For any instance 

												<Emphasis Type="Italic">S</Emphasis>
 , the 

												<Emphasis
Type="Italic">performance ratio</Emphasis>
 , 

												<Emphasis Type="Italic">R</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ), 

												<Emphasis
Type="Italic">of algorithm A on instance S</Emphasis>
 is defined by 

												<Emphasis Type="Italic">R</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ) = | 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|/| 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|, where 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ) is an optimal SCS solution to the instance 

												<Emphasis Type="Italic">S</Emphasis>
 .
											</Para>


											<Para>However, | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )| is unknown – it is not feasible to compute | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )| since the SCS problem is NP-hard. Luckily, it is possible to compute a lower bound for | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|. We choose a small sample set, 

												<Emphasis Type="Italic">SS</Emphasis>
 , of representative sequences in 

												<Emphasis Type="Italic">S</Emphasis>
 and use an exact dynamic programming algorithm to compute 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">SS</Emphasis>
 ), the shortest common supersequence of the sample set 

												<Emphasis Type="Italic">SS</Emphasis>
 . Clearly, | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">SS</Emphasis>
 )| is a lower bound on the optimum length, namely, | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">SS</Emphasis>
 )| ≤ | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|. Then the 

												<Emphasis
Type="Italic">performance ratio R</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ) can also be upper bounded by the estimate 

												<Emphasis Type="Italic">R'</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 ) = | 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|/| 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">SS</Emphasis>
 )|. We shall use the estimated performance ratio bounds as a "metric" for comparing different algorithms later in this paper.
											</Para>


											<Para>The gap between | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )| and the computed lower bound, | 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">SS</Emphasis>
 )| depends on how the sample set 

												<Emphasis Type="Italic">SS</Emphasis>
 is chosen and also on its size. However, a large 

												<Emphasis Type="Italic">SS</Emphasis>
 will render the computation infeasible. For DNA sequences, we use the sample set 

												<Emphasis Type="Italic">SS</Emphasis>
 = { 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>A</Subscript>
 , 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>C</Subscript>
 , 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>G</Subscript>
 , 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>T</Subscript>
 } of 4 representative sequences from 

												<Emphasis Type="Italic">S</Emphasis>
 , where each representative sequence 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>A</Subscript>
 (and similarly, 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>C</Subscript>
 , 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>G</Subscript>
 , 

												<Emphasis Type="Italic">SS</Emphasis>


												<Subscript>T</Subscript>
 ) is a sequence in 

												<Emphasis Type="Italic">S</Emphasis>
 that has the largest number of A's (and C, G, T, respectively). Then dynamic programming is applied on { 

												<Emphasis Type="Italic">S</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">S</Emphasis>


												<Subscript>2</Subscript>
 ,... 

												<Emphasis Type="Italic">S</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">q</Emphasis>

												</Subscript>
 } to obtain a lower bound of the SCS length. For protein sequences with 

												<Emphasis Type="Italic">q</Emphasis>
 = 20, we choose only top few (usually 4) such representative sequences in the sample set 

												<Emphasis Type="Italic">SS</Emphasis>
 .
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_65746">

										<Heading>Method</Heading>


										<Para>Since biological datasets usually contain many long sequences (namely 

											<Emphasis
Type="Italic">large SCS instances</Emphasis>
 ), it is important to devise an effective algorithm that works well on large SCS instances (for both DNA and protein sequences). Among the existing heuristic algorithms, several of them (Greedy, Tournament, Reduce-Expand and to a less extend, Majority Merge) perform well on SCS instances where there are few long sequences (small 

											<Emphasis Type="Italic">k</Emphasis>
 and large 

											<Emphasis Type="Italic">n</Emphasis>
 ). Several heuristic algorithms (Majority Merge, Min Height and their Look-Ahead variants, and LAP) perform well on SCS instances with many short sequences (large 

											<Emphasis Type="Italic">k</Emphasis>
 and small 

											<Emphasis Type="Italic">n</Emphasis>
 ). However, relatively few research studies have been carried out to see how they perform on large SCS instances dealing with many long sequences (where both 

											<Emphasis Type="Italic">k</Emphasis>
 and 

											<Emphasis Type="Italic">n</Emphasis>
 are large). In fact, Barone 

											<Emphasis Type="Italic">et al</Emphasis>
 . had in 

											<CitationRef
CitationID="B6">6</CitationRef>
 , raised the question on "how to design efficient (both in terms of time and space) heuristic algorithm on many long sequences".
										</Para>


										<Para>In our recent work 

											<CitationRef
CitationID="B11">11</CitationRef>
 , we have compared the performances of several algorithms (MH, SH, (3,1)-LA-SH, and LAP) on SCS instances where 

											<Emphasis Type="Italic">k</Emphasis>
 is large, but 

											<Emphasis Type="Italic">n</Emphasis>
 is relatively small. The good performance of our LAP algorithm 

											<CitationRef
CitationID="B11">11</CitationRef>
 , indicates that the post processing approach 

											<Emphasis
Type="Italic">may also be effective on large SCS instances</Emphasis>
 (such as large sets of long DNA and protein sequences). This paper discusses how we modify the LAP algorithm to solve large SCS instances.
										</Para>


										<Section2 ID="Sec_38965">

											<Heading>Our DR algorithm for large SCS instances</Heading>


											<Para>In the 

												<Emphasis
Type="Italic">Deposition and Reduction</Emphasis>
 algorithm that we proposed, there are two processes: in the 

												<Emphasis
Type="Italic">deposition process</Emphasis>
 , we first generate a 

												<Emphasis Type="Bold"> 

													<Emphasis
Type="Italic">template pool</Emphasis>

												</Emphasis>
 – a small set of 

												<Emphasis
Type="Italic">SCS templates</Emphasis>
 (or 

												<Emphasis
Type="Italic">templates</Emphasis>
 , in short). Each template is a common supersequence of the SCS instance 

												<Emphasis Type="Italic">S</Emphasis>
 . The 

												<Emphasis
Type="Italic">reduction process</Emphasis>
 shortens these templates by attempting to remove some characters while preserving the common supersequence property. This uses the post processing procedure introduced in 

												<CitationRef
CitationID="B11">11</CitationRef>
 .
											</Para>


										</Section2>


										<Section2 ID="Sec_15338">

											<Heading>Deposition process (template generation)</Heading>


											<Para>For the deposition process, we use two algorithms to produce candidate templates that will be included in the 

												<Emphasis
Type="Bold">template pool</Emphasis>
 . Clearly, the performance of the algorithm is dependent on obtaining good SCS templates. Based on results from our previous study 

												<CitationRef
CitationID="B11">11</CitationRef>
 , we have selected (3,1)-LA-SH as one of the algorithms used to generate one of the templates. Another template used was that generated by Alphabet (largely so that the algorithm will have a worst-case performance guarantee).
											</Para>


										</Section2>


										<Section2 ID="Sec_67259">

											<Heading>Reduction process</Heading>


											<Para>In the reduction process, we apply a reduction procedure to each SCS template in the template pool to obtain a shorter SCS. Finally, the shortest result obtained after this reduction process is selected as the final output of the algorithm. The reduction procedure we use is just a simple extension of the post-processing procedure in 

												<CitationRef
CitationID="B11">11</CitationRef>
 . We give a brief description here. We are given a SCS template 

												<Emphasis Type="Italic">S</Emphasis>
 = 

												<Emphasis Type="Italic">S</Emphasis>
 [1] 

												<Emphasis Type="Italic">S</Emphasis>
 [2]... 

												<Emphasis Type="Italic">S</Emphasis>
 [ 

												<Emphasis Type="Italic">m</Emphasis>
 ], with 

												<Emphasis Type="Italic">m</Emphasis>
 characters ( 

												<Emphasis Type="Italic">S</Emphasis>
 is, of course, a common supersequence). The following approach, using 

												<Emphasis Type="Italic">S</Emphasis>
 as a template and a method 

												<Emphasis Type="Italic">A</Emphasis>
 , seeks to reduce the number of characters in 

												<Emphasis Type="Italic">S</Emphasis>
 . The detailed algorithm is given in Figure 

												<InternalRef RefID="F2">2</InternalRef>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F2">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>The procedure of the reduction process</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S12-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>The procedure of the reduction process.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>In the current implementation of our Deposition and Reduction algorithm (DR), we have included the template generated by the Alphabet algorithm. Thus, our DR algorithm also has an approximation ratio of 

												<Emphasis Type="Italic">q</Emphasis>
 = |Σ|. However, in practice, our experiment show the results from the (3,1)-LA-SH template is much better than those obtained from Alphabet and the performance ratios of the DR algorithm is much lower than 

												<Emphasis Type="Italic">q</Emphasis>
 .
											</Para>


											<Para>The time complexity of the deposition process is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">q</Emphasis>


												<Superscript>3</Superscript>


												<Emphasis Type="Italic">kn</Emphasis>
 ), the time complexity of the reduction process is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">q</Emphasis>


												<Superscript>3</Superscript>


												<Emphasis Type="Italic">kn</Emphasis>


												<Superscript>2</Superscript>
 ), so the total time complexity of the Deposition and Reduction algorithm is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">q</Emphasis>


												<Superscript>3</Superscript>


												<Emphasis Type="Italic">kn</Emphasis>


												<Superscript>2</Superscript>
 ). This is larger than that of Alphabet algorithm, but smaller than Greedy algorithm and Tournament algorithm where 

												<Emphasis Type="Italic">q</Emphasis>
 is not very large and 

												<Emphasis Type="Italic">k</Emphasis>
 is large. It is also smaller than that of the Reduce-Expand algorithm, especially when 

												<Emphasis Type="Italic">n</Emphasis>
 is large. The space complexity of the Deposition and Reduction algorithm is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">q</Emphasis>


												<Superscript>3</Superscript>


												<Emphasis Type="Italic">kn</Emphasis>
 ).
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_25930">

										<Heading>Results</Heading>


										<Para>Our post process algorithm is written in Java and Perl. The experiments are performed on a PC with 3.0 GHz CPU and 1.0 GB memory, running on a Linux operating system. We have selected Alphabet (ALPHA) 

											<CitationRef
CitationID="B6">6</CitationRef>
 , Reduce-Expand (RE) 

											<CitationRef
CitationID="B6">6</CitationRef>
 , Tournament (TOUR), Greedy (GRDY) 

											<CitationRef
CitationID="B12">12</CitationRef>
 and Majority Merge (MM) 

											<CitationRef
CitationID="B8">8</CitationRef>
 algorithms, as well as Lower Bound (LB) for comparison with our Deposition and Reduction (DR) algorithm on large SCS instances. We also used abbreviations defined above for the different heuristic algorithms. We note here that the RE algorithm was not included in the comparative study of large SCS instances as the RE algorithm took too long to run on these large SCS instances. Instead, we compare the RE algorithm with our DR algorithm directly on smaller SCS instances in the subsection on "Comparison with Reduce-Expand Algorithm".
										</Para>


										<Para>For the large SCS instances, we use both simulated sequences as well as real (DNA and protein) sequences. It is easy to see that results on datasets with sequences of different lengths are similar to those results on datasets with sequences of same lengths. Therefore, in this study, we have only used simulated sequences of same length in the dataset, and also truncated real sequence to same lengths in each datasets. In the tables of experimental results below, 

											<Emphasis Type="Italic">k</Emphasis>
 denotes the number of sequences, and 

											<Emphasis Type="Italic">n</Emphasis>
 denotes the length of the sequence. For DNA sequences, the size of alphabet 

											<Emphasis Type="Italic">q</Emphasis>
 = 4, while for protein sequences 

											<Emphasis Type="Italic">q</Emphasis>
 = 20.
										</Para>


										<Section2 ID="Sec_49286">

											<Heading>Results on simulated sequences</Heading>


											<Para>We first carry out comparative study on simulated DNA sequences. For each value of 

												<Emphasis Type="Italic">k</Emphasis>
 = 100, 500, 1000, 5000, and 

												<Emphasis Type="Italic">n</Emphasis>
 = 100, 1000, we generated 10 random datasets of DNA sequences. These datasets are shown in Tables 

												<InternalRef RefID="T1">1</InternalRef>
 and 

												<InternalRef RefID="T2">2</InternalRef>
 where each row represents the composite result for the 10 instances. For each row, we also compute the average lower bound (denoted by LB in Tables 

												<InternalRef RefID="T1">1</InternalRef>
 and 

												<InternalRef RefID="T2">2</InternalRef>
 ) over the 10 instances. Then, each algorithm, 

												<Emphasis Type="Italic">A</Emphasis>
 , is run on these 10 instances to get an average (and standard deviation) of | 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">A</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|, the length of the SCS. The estimated performance ratios of the different algorithms (denoted by Ratio in Tables 

												<InternalRef RefID="T1">1</InternalRef>
 and 

												<InternalRef RefID="T2">2</InternalRef>
 ) are obtained by dividing the average SCS lengths by the lower bound LB. As mentioned earlier, these are 

												<Emphasis
Type="Italic">upper bounds</Emphasis>
 on the true performance ratios of the algorithms.
											</Para>


											<Table Float="No" ID="T1">

												<Caption Language="En">

													<CaptionNumber>Table 1</CaptionNumber>


													<CaptionContent>

														<SimplePara>The results of the Deposition process and the Reduction process.</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="7">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Sequences</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Length of CS (averaged over 10 instances)</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">k</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">n</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">LB</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis Type="Bold">| 

																		<Emphasis
Type="Italic">CS</Emphasis>


																		<Subscript> 

																			<Emphasis
Type="Italic">D</Emphasis>

																		</Subscript>
 |
																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">R'</Emphasis>


																		<Subscript> 

																			<Emphasis
Type="Italic">D</Emphasis>

																		</Subscript>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis Type="Bold">| 

																		<Emphasis
Type="Italic">CS</Emphasis>


																		<Subscript> 

																			<Emphasis
Type="Italic">R</Emphasis>

																		</Subscript>
 |
																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">R'</Emphasis>

																	</Emphasis>


																	<Subscript> 

																		<Emphasis
Type="Italic"> 

																			<Emphasis
Type="Bold">R</Emphasis>

																		</Emphasis>

																	</Subscript>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>158.0</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>267.8 (7.1)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.69</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">264.5 (6.8)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.67</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>160.6</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>274.9 (7.3)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.71</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">273.0 (7.1)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.70</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>161.3</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>277.1 (6.0)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">275.6 (6.2)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.71</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>5000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>162.9</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>281.6 (4.0)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">279.9 (4.2)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1441.8</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2495.4 (50.6)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">2480.2 (55.5)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1457.6</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2543.9 (48.6)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.75</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">2527.2 (52.1)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1472.3</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2557.5 (45.1)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.74</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">2540.0 (47.4)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>5000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1481.6</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2566.6 (35.1)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">2548.3 (39.7)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>Each row of the table represents 10 randomly generated instances. For each row, we list the average value for LB (the lower bound), | 

														<Emphasis
Type="Italic">CS</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">D</Emphasis>

														</Subscript>
 |, and | 

														<Emphasis
Type="Italic">CS</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">R</Emphasis>

														</Subscript>
 | (the length of the common supersequences after Depostion and Reduction), their standard deviations (in parenthesis), and the estimated performance ratios 

														<Emphasis
Type="Italic">R'</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">D</Emphasis>

														</Subscript>
 and 

														<Emphasis
Type="Italic">R'</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">R</Emphasis>

														</Subscript>
 (| 

														<Emphasis
Type="Italic">CS</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">D</Emphasis>

														</Subscript>
 |/LB and | 

														<Emphasis
Type="Italic">CS</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">R</Emphasis>

														</Subscript>
 |/LB)
													</SimplePara>


												</tfooter>


											</Table>


											<Table Float="No" ID="T2">

												<Caption Language="En">

													<CaptionNumber>Table 2</CaptionNumber>


													<CaptionContent>

														<SimplePara>A comparison of the lengths of the SCS results obtained by different algorithms on simulated DNA sequences.</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="13">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<colspec colname="c7" colnum="7" />


													<colspec colname="c8" colnum="8" />


													<colspec colname="c9" colnum="9" />


													<colspec colname="c10" colnum="10" />


													<colspec colname="c11" colnum="11" />


													<colspec colname="c12" colnum="12" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Sequences</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Length of CS (averaged over 10 instances)</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">k</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">n</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">LB</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">ALPHA</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">TOUR</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">GRDY</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">MM</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">DR</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>158.0</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.53</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>304.6 (15.3)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>1.93</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>303.9 (11.9)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>1.92</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>276.8 (5.5)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.75</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">264.5 (6.8)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.67</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>160.6</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.49</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>329.0 (18.2)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.05</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>321.6 (9.1)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.00</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>286.4 (7.7)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.78</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">273.0 (7.1)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.70</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>161.3</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.48</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>335.4 (19.9)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.08</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>329.0 (17.8)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.04</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>289.1 (8.2)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.79</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">275.6 (6.2)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.71</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>5000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>162.9</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.46</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>339.8 (21.2)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.09</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>336.2 (21.2)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.06</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>294.8 (10.6)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.81</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">279.9 (4.2)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1441.8</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>4000</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.77</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>2936.7 (146.6)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.04</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>2921.5 (143.4)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.03</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>2547.1 (24.7)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.77</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">2480.2 (55.5)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1457.6</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>4000</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.74</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>3049.6 (150.0)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.09</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>3043.6 (145.2)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.09</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>2578.0 (23.3)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.77</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">2527.2 (52.1)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1472.3</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>4000</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.72</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>3142.3 (176.9)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.13</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>3115.5 (173.6)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.12</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>2590.9 (26.5)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.76</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">2540.0 (47.4)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.73</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>5000</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1481.6</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>4000</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.70</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>3194.5 (221.9)</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2.16</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>3172.8 (199.0)</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara>2.14</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara>2602.1 (25.6)</SimplePara>


															</entry>


															<entry colname="c10">

																<SimplePara>1.76</SimplePara>


															</entry>


															<entry colname="c11">

																<SimplePara> 

																	<Emphasis
Type="Bold">2548.3 (39.7)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c12">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.72</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>The average and standard deviation (in parenthesis) over 10 randomly generated instances are given. The estimated performance ratios are also given for each algorithm.</SimplePara>


												</tfooter>


											</Table>


											<Para>In Table 

												<InternalRef RefID="T1">1</InternalRef>
 , we first study the effect of Deposition process and Reduction process in our DR algorithm. To this end, we define 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">D</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">R</Emphasis>

												</Subscript>
 to be the common supersequences obtained after the Deposition and Reduction process, respectively. From Table 

												<InternalRef RefID="T1">1</InternalRef>
 , we observed that both the Deposition and the Reduction processes are effective. The Deposition process always output templates with 

												<Emphasis
Type="Italic">|template</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">D</Emphasis>

												</Subscript>


												<Emphasis Type="Italic">|</Emphasis>
 / 

												<Emphasis
Type="Italic">|opt(S)|</Emphasis>
 less than 1.8 for both long ( 

												<Emphasis Type="Italic">n</Emphasis>
 = 1000) and short ( 

												<Emphasis Type="Italic">n</Emphasis>
 = 100) sequences. In the following tables, we will see that this is much better than results of Alphabet, indicating the superiority of (3,1)-LA-SH template. The Reduction process can further reduce the lengths of the results by between 2 to 6 characters for 

												<Emphasis Type="Italic">n</Emphasis>
 = 100, and by about 20 characters for 

												<Emphasis Type="Italic">n</Emphasis>
 = 1000, and the performance ratios are correspondingly reduced. The standard deviations for the results of both Deposition and Reduction processes are not big, indication that both the Deposition process and the Reduction process give stable results.
											</Para>


											<Para>In Table 

												<InternalRef RefID="T2">2</InternalRef>
 , we then compared the various heuristic algorithms – Alphabet, Tournament, Greedy, Majority Merge (or Sum Height) with our DR algorithm on these 

												<Emphasis
Type="Italic">simulated</Emphasis>
 DNA datasets. To assist the comparison, we have also included the lower bound (LB), and the average estimated performance ratios.
											</Para>


											<Para>The results in Table 

												<InternalRef RefID="T2">2</InternalRef>
 show 

												<Emphasis
Type="Italic">very clearly</Emphasis>
 that for the datasets with large 

												<Emphasis Type="Italic">k</Emphasis>
 and 

												<Emphasis Type="Italic">n</Emphasis>
 , the DR algorithm consistently gives the best results, followed quite closely by MM, while algorithms TOUR and GRDY are quite a bit worse. Generally, we observe that the length of the SCS obtained increases with the number of sequences, which is to be expected.
											</Para>


											<Para>The performances of the algorithms on medium length sequences ( 

												<Emphasis Type="Italic">n</Emphasis>
 = 100) also differ slightly from those for long sequences ( 

												<Emphasis Type="Italic">n</Emphasis>
 = 1000). For 

												<Emphasis Type="Italic">n</Emphasis>
 = 100, our DR algorithm produces results that are, on average, shorter than those of MM by 13.5 characters, by 49.4 characters than those of GRDY, and by 54 characters than those of TOUR. For long sequences ( 

												<Emphasis Type="Italic">n</Emphasis>
 = 1000), the difference are more pronounced, by 55.6 characters for MM, by 539 characters for GRDY, and by 557 characters for TOUR.
											</Para>


											<Para>Similar to the results observed in 

												<CitationRef
CitationID="B1">1</CitationRef>
 , the performance ratios obtained by all the algorithms are quite a bit below the worst-case ratio of 4 for the trivial Alphabet algorithm. The standard deviation results in Table 

												<InternalRef RefID="T2">2</InternalRef>
 also show that DR algorithm is relatively stable – more stable than GRDY and TOUR, but not as stable the MM algorithm.
											</Para>


											<Para>We have not done a similar comparison on simulated protein sequences, but we expect that the relative performances of these algorithms on random protein sequences would be similar to those for random DNA sequences. In the next section, we show results on real DNA and protein sequences.</Para>


										</Section2>


										<Section2 ID="Sec_78824">

											<Heading>Results on real biological sequences</Heading>


											<Para>In this section, we compared the algorithms Alphabet and MM with our DR on datasets obtained from real DNA and protein sequences. For this study, we have to exclude the GRDY and TOUR algorithms since their performances are much worse than MM or DR, and they are also very time-consuming on these datasets.</Para>


											<Para>For this experimental comparison, we have randomly selected DNA sequences from the NCBI viral genomes 

												<CitationRef
CitationID="B17">17</CitationRef>
 . These DNA sequences are truncated so that they have lengths ( 

												<Emphasis Type="Italic">n</Emphasis>
 ) of 500 and 1000 and combined to obtained many datasets that are grouped into four cases (DNA-1 to DNA-4) with 

												<Emphasis Type="Italic">k</Emphasis>
 = 100 and 500, and 10 randomly selected datasets for each setting, as shown in Table 

												<InternalRef RefID="T3">3</InternalRef>
 . The protein sequences (with 

												<Emphasis Type="Italic">q</Emphasis>
 = |Σ| = 20) are from SwissProt 

												<CitationRef
CitationID="B18">18</CitationRef>
 and these have been truncated to length 500 and we have used 

												<Emphasis Type="Italic">k</Emphasis>
 = 100, 500, and 1000 to truncate datasets, and 10 randomly selected datasets for each setting. These datasets are grouped into three cases (PROT-1 to PROT-3) as shown in Table 

												<InternalRef RefID="T3">3</InternalRef>
 . Both real DNA and protein sequences datasets are available as additional supplemental materials.
											</Para>


											<Table Float="No" ID="T3">

												<Caption Language="En">

													<CaptionNumber>Table 3</CaptionNumber>


													<CaptionContent>

														<SimplePara>The comparison of the lengths of the SCS results obtained by different algorithms on selected DNA and protein sequences.</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="10">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<colspec colname="c7" colnum="7" />


													<colspec colname="c8" colnum="8" />


													<colspec colname="c9" colnum="9" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Sequences</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Length of CS (averaged over 10 instances)</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">k</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">n</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">LB</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">ALPHA</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">MM</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">DR</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Ratio</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">DNA sequences</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>DNA-1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>686.6</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>2.91</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>1359.7 (18.7)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>1.98</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">1346.4 (19.6)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.96</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>DNA-2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>689.5</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>2.90</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>1430.5 (22.1)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>2.07</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">1420.7 (18.1)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">2.06</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>DNA-3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>1361.0</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>4,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>2.94</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2698.0 (39.9)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>1.98</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">2675.7 (37.2)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">1.97</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>DNA-4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>1364.4</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>4,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>2.93</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>2822.3 (43.4)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>2.07</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">2769.1 (32.7)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">2.03</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Protein sequences</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>PROT-1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>800.6</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>10,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>12.49</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>5312.7 (81.9)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>6.64</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">4846.3 (73.9)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">6.05</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>PROT-2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>803.5</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>10,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>12.45</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>5935.6 (61.8)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>7.39</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">5548.6 (54.3)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">6.91</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>PROT-3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1000</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>809.8</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>10,000</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>12.35</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>6082.4 (44.9)</SimplePara>


															</entry>


															<entry colname="c7">

																<SimplePara>7.51</SimplePara>


															</entry>


															<entry colname="c8">

																<SimplePara> 

																	<Emphasis
Type="Bold">5734.0 (43.9)</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c9">

																<SimplePara> 

																	<Emphasis
Type="Bold">7.08</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>The average and standard deviation (in parenthesis) are given. The estimated performance ratios are also given for each algorithm.</SimplePara>


												</tfooter>


											</Table>


											<Para>The comparison results for MM and DR algorithms on these selected DNA and protein sequences are shown in Table 

												<InternalRef RefID="T3">3</InternalRef>
 . The results for real DNA sequences are similar to those for simulated DNA sequences. Again, the results clearly show that DR consistently outperform the MM algorithm for both type of sequences. The performance ratio | 

												<Emphasis Type="Italic">CS</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">DR</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )|/| 

												<Emphasis Type="Italic">opt</Emphasis>
 ( 

												<Emphasis Type="Italic">S</Emphasis>
 )| of DR algorithm is about 2.1 for DNA sequences with length 500 and 1000, and 6.0~7.51 for protein sequences with length 500. These are much less than the approximation ratios. We also observe that the length of the SCS obtained increases with the number of sequences.
											</Para>


											<Para>Comparing Table 

												<InternalRef RefID="T3">3</InternalRef>
 with Table 

												<InternalRef RefID="T2">2</InternalRef>
 , we also observed that the performance ratios for MM and DR are generally a little bigger for real DNA sequences as compared to simulated DNA sequences for similar values of 

												<Emphasis Type="Italic">k</Emphasis>
 and 

												<Emphasis Type="Italic">n</Emphasis>
 . This may be attributed to the fact that the real DNA sequences are selected from different viral genomes and thus, there is high variance in the GC contents and this may have resulted in longer SCS. Whereas, in the simulated DNA sequences, the GC content is predefined for each of the simulated DNA sequences datasets and this may have resulted in the shorter SCS obtained by the algorithms. This observation is also consistent with results in 

												<CitationRef
CitationID="B11">11</CitationRef>
 .
											</Para>


											<Para>The results for protein sequences show a similar trend as those for DNA sequences. Note that the approximation ratio for the Alphabet algorithm is 20 for protein sequences. Our results show that for DR on protein sequences with 

												<Emphasis Type="Italic">n</Emphasis>
 = 500, the performance ratio is smaller than those for MM. The difference in the performance ratios of DR and MM is larger for protein sequences that have a larger alphabet. In terms of the length of the SCS result, DR obtains SCS results that are between 300 to 450 characters shorter compared to those obtained by MM.
											</Para>


											<Para>The standard deviations observed in Table 

												<InternalRef RefID="T3">3</InternalRef>
 for DNA and protein sequences are relatively small, again indicating that both MM and DR algorithms give relatively stable performance.
											</Para>


										</Section2>


										<Section2 ID="Sec_86641">

											<Heading>Comparison with Reduce-Expand algorithm</Heading>


											<Para>The Reduce-Expand (RE) algorithm 

												<CitationRef
CitationID="B1">1</CitationRef>
 is currently one of the best algorithms for the general SCS problem for small SCS instances. Because of the high running time of the RE algorithm, we have only compared RE with our DR algorithm on simulated datasets with relatively few, short DNA sequences ( 

												<Emphasis Type="Italic">q</Emphasis>
 = 4, 

												<Emphasis Type="Italic">k</Emphasis>
 = 5, 10, 50, 100, 

												<Emphasis Type="Italic">n</Emphasis>
 = 10, 100, and with 10 randomly generated datasets for each setting), as well as small datasets of real DNA and protein sequences. The results on simulated datasets are shown in Table 

												<InternalRef RefID="T4">4</InternalRef>
 .
											</Para>


											<Table Float="No" ID="T4">

												<Caption Language="En">

													<CaptionNumber>Table 4</CaptionNumber>


													<CaptionContent>

														<SimplePara>The comparison of the lengths of the SCS results between RE and DR on simulated DNA sequences.</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="5">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Sequences</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Length of SCS (averaged)</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">k</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">n</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">ALPHA</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">RE</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">DR</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>5</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>40</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>20.45 (0.53)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">20.05 (1.36)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>40</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>26.04 (0.93)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">25.00 (1.76)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>50</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>40</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>35.00 (1.00)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">29.90 (1.55)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>40</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>34.69 (0.97)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">30.10 (1.30)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>5</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>188.08 (5.55)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">195.60 (6.27)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>10</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>229.28 (7.61)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">224.20 (6.28)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>50</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>286.12 (13.34)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">257.85 (6.80)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>281.95 (10.86)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis
Type="Bold">264.45 (6.86)</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>In each cell, the average and standard deviation (in parenthesis) over 10 randomly generated instances are given.</SimplePara>


												</tfooter>


											</Table>


											<Para>The results in Table 

												<InternalRef RefID="T4">4</InternalRef>
 indicate that DR is only comparable to (and for 

												<Emphasis Type="Italic">n</Emphasis>
 = 100, even longer than) RE in the lengths of the results when there are only 5 sequences, but outperform RE in the lengths of the results when the number of sequences is over 10. The more sequences, and the longer the sequences, the larger differences between the length of the results of DR and RE. Especially for longer (length of 100) sequences, the results of DR can be about 5 to 20 characters shorter than the results of RE.
											</Para>


											<Para>We have also compared the algorithms Alphabet and RE with our DR on datasets obtained from real DNA and protein sequences. The sequences in datasets (DNA-5, DNA-6, PROT-4 and PROT-5, and 10 randomly selected datasets for each setting) are shorter from those in Table 

												<InternalRef RefID="T3">3</InternalRef>
 , and the results are shown in Table 

												<InternalRef RefID="T5">5</InternalRef>
 .
											</Para>


											<Table Float="No" ID="T5">

												<Caption Language="En">

													<CaptionNumber>Table 5</CaptionNumber>


													<CaptionContent>

														<SimplePara>The comparison the lengths of the SCS results between RE and DR on selected DNA and protein sequences.</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="6">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">k</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">n</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">ALPHA</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">RE</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">DR</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">DNA</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>DNA-5</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>301.04 (8.85)</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">284.40 (4.54)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>DNA-6</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>400</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>308.87 (7.61)</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">296.80 (3.66)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis Type="Bold"> 

																		<Emphasis
Type="Italic">Protein</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>PROT-4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2,000</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>1028.90 (35.54)</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">1008.70 (14.29)</Emphasis>

																</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>PROT-5</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>500</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2,000</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>1232.10 (31.03)</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara> 

																	<Emphasis
Type="Bold">1199.80 (13.80)</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


												<tfooter>

													<SimplePara>In each cell, the average and standard deviation (in parenthesis) over 10 randomly generated instances are given.</SimplePara>


												</tfooter>


											</Table>


											<Para>The results on real DNA and protein sequences are consistent with the results on simulated sequences. The DR algorithm can outperform the RE algorithm by about 10 characters for DNA sequences of length 100. The SCS results of the DR algorithm are also shorter than the results of the RE algorithm by more than 20 characters on protein sequences. The more sequences, and the longer the sequences, the larger differences between the length of the results of the DR and RE algorithm.</Para>


										</Section2>


										<Section2 ID="Sec_57555">

											<Heading>Computational efficiency</Heading>


											<Para>Our Deposition and Reduction (DR) algorithm has time complexity of O( 

												<Emphasis Type="Italic">q</Emphasis>


												<Superscript>3</Superscript>


												<Emphasis Type="Italic">kn</Emphasis>


												<Superscript>2</Superscript>
 ) and space complexity of O( 

												<Emphasis Type="Italic">q</Emphasis>


												<Superscript>3</Superscript>


												<Emphasis Type="Italic">kn</Emphasis>
 ). In our experiments, the DR algorithm is very fast, even for large SCS datasets – for example, for 1000 sequences of length 1000 each, it take an average of 5–10 minutes per instance. In comparison to existing algorithms, it is slower than Majority Merge, Min Height (less than 1 minute), but it is much faster than Greedy and Tournament algorithms (both of which need more than 60 minutes). The DR algorithm is also much faster than the Reduce-Expand algorithm (our Perl implementation). For a simulated DNA dataset with 100 sequences each of length 100, the DR algorithm can get result in less than 10 seconds, while the Reduce-Expand algorithm needs more than 10 minutes. The actual computer memory used by the DR algorithm is about 50 M for some of our very large datasets; Majority Merge needs about 10 M for these same datasets. The software is available upon request, and the web services portal will be available soon.
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_13000">

										<Heading>Conclusion</Heading>


										<Para>In this paper, we have proposed a Deposition and Reduction (DR) algorithm for solving large SCS instances. The DR algorithm is composed of the Deposition process to generate good templates, and the Reduction process to reduce templates from template pool to get short result. These processes are shown to be powerful for the SCS problem.</Para>


										<Para>We have compared the performance of our DR algorithm with some of the best heuristic algorithms on different sequences datasets, especially on many long sequences. The DR algorithm has superior performance than Alphabet, Tournament, Greedy and Majority Merge algorithms in practice, especially on many long sequences. It also outperforms the Reduce-Expand algorithm for sequences of length 50~100. The DR algorithm is also very efficient in time and space, which partially answer the question that Barone et al. raised 

											<CitationRef
CitationID="B6">6</CitationRef>
 about how to design efficient (both in terms of time and space) heuristic algorithm on many long sequences.
										</Para>


										<Para>The Deposition and Reduction algorithm is an extension of our previous study for SCS problem on DNA oligos 

											<CitationRef
CitationID="B11">11</CitationRef>
 . To our best knowledge, our Deposition and Reduction is one of the best heuristic algorithms (both in terms of performance ratios and in terms of time and space needed) for the SCS problem on biological sequences such as the DNA and protein sequences, especially for datasets with many long sequences. We believe that the use of the Deposition and Reduction algorithm can facilitate the biological sequencing process for bioinformatics researches.
										</Para>


										<Para>There are many other post process strategies for the SCS problem, such as better look ahead strategies, which may lead to better performance. We are currently trying to further analyze these strategies for SCS problems by comparing the results of different strategies.</Para>


										<Para>As a general computational framework, this Deposition and Reduction algorithm can also be applied on more general applications such as text comparison and compression, query optimization and scheduling. We will also work on these more general problems in the future.</Para>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/content/pdf/1471-2105-7-S4-info.pdf"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JA</Initials>


													<FamilyName>Storer</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">Data compression: methods and theory</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>DE</Initials>


													<FamilyName>Foulser</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Q</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<Year>1992</Year>


												<ArticleTitle
Language="En">Theory and algorithms for plan merging</ArticleTitle>


												<JournalTitle>Artificial Intelligence</JournalTitle>


												<VolumeID>57</VolumeID>


												<FirstPage>143</FirstPage>


												<LastPage>181</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TK</Initials>


													<FamilyName>Sellis</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">Multiple-query optimization</ArticleTitle>


												<JournalTitle>ACM Transactions on Database Systems (TODS)</JournalTitle>


												<VolumeID>13</VolumeID>


												<FirstPage>23</FirstPage>


												<LastPage>52</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TH</Initials>


													<FamilyName>Cormen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CE</Initials>


													<FamilyName>Leiserson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RL</Initials>


													<FamilyName>Rivest</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Stein</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">Introduction to Algorithms</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Sankoff</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Kruskal</FamilyName>


												</BibAuthorName>


												<Year>1983</Year>


												<ArticleTitle
Language="En">Time Warps, String Edits and Macromolecules: the Theory and Practice of Sequence Comparisons</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Barone</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Bonizzoni</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>GD</Initials>


													<FamilyName>Vedova</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Mauri</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">An approximation algorithm for the shortest common supersequence problem: an experimental analysis</ArticleTitle>


												<JournalTitle>Symposium on Applied Computing, Proceedings of the 2001 ACM symposium on Applied computing: 2001</JournalTitle>


												<VolumeID />


												<FirstPage>56</FirstPage>


												<LastPage>60</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Gusfield</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">Algorithms on strings, trees, and sequences: computer science and computational biology</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Jiang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<Year>1995</Year>


												<ArticleTitle
Language="En">On the approximation of shortest common supersequences and longest common subsequences</ArticleTitle>


												<JournalTitle>SIAM Journal of Computing</JournalTitle>


												<VolumeID>24</VolumeID>


												<FirstPage>1122</FirstPage>


												<LastPage>1139</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>VG</Initials>


													<FamilyName>Timkovsky</FamilyName>


												</BibAuthorName>


												<Year>1993</Year>


												<ArticleTitle
Language="En">On the approximation of shortest common non-subsequences and supersequences</ArticleTitle>


												<JournalTitle>Technical report</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Kasif</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Weng</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Derti</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Beigel</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>DeLisi</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">A computational framework for optimal masking in the synthesis of oligonucleotide microarrays</ArticleTitle>


												<JournalTitle>Nucleic Acids Research</JournalTitle>


												<VolumeID>30</VolumeID>


												<FirstPage>e106</FirstPage>


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ning</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>KP</Initials>


													<FamilyName>Choi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>HW</Initials>


													<FamilyName>Leong</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Zhang</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">A Post Processing Method for Optimizing Synthesis Strategy for Oligonucleotide Microarrays</ArticleTitle>


												<JournalTitle>Nucleic Acids Research</JournalTitle>


												<VolumeID>33</VolumeID>


												<FirstPage>e144</FirstPage>


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>RW</Initials>


													<FamilyName>Irving</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Fraser</FamilyName>


												</BibAuthorName>


												<Year>1993</Year>


												<ArticleTitle
Language="En">On the Worst-Case Behaviour of Some Approximation Algorithms for the Shortest Common Supersequence of k Strings</ArticleTitle>


												<JournalTitle>Proceedings of the 4th Annual Symposium on Combinatorial Pattern Matching</JournalTitle>


												<VolumeID />


												<FirstPage>63</FirstPage>


												<LastPage>73</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>EA</Initials>


													<FamilyName>Hubbell</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MS</Initials>


													<FamilyName>Morris</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JL</Initials>


													<FamilyName>Winkler</FamilyName>


												</BibAuthorName>


												<Year>1996</Year>


												<ArticleTitle
Language="En">Computer-aided engineering system for design of sequence arrays and lithographic masks</ArticleTitle>


												<JournalTitle>US Patent no 5571639</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Branke</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Middendorf</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>F</Initials>


													<FamilyName>Schneider</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Improved heuristics and a genetic algorithm for finding short supersequences</ArticleTitle>


												<JournalTitle>OR Spectrum</JournalTitle>


												<VolumeID>20</VolumeID>


												<FirstPage>39</FirstPage>


												<LastPage>45</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR15">

											<CitationNumber>15.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Nicosia</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Oriolo</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">An approximate A* algorithm and its application to the SCS problem</ArticleTitle>


												<JournalTitle>Theoretical Computer Science</JournalTitle>


												<VolumeID>290</VolumeID>


												<FirstPage>2021</FirstPage>


												<LastPage>2029</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR16">

											<CitationNumber>16.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Michels</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Middendorf</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">An Island Model based Ant System with Lookahead for the Shortest Common Supersequence Problem</ArticleTitle>


												<JournalTitle>Fifth International Comference On Parallel Problem Solving From Nature (PPSN'98): 1998</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR17">

											<CitationNumber>17.</CitationNumber>


											<BibArticle>

												<Year />


												<ArticleTitle
Language="En">NCBI Viral Genomes</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR18">

											<CitationNumber>18.</CitationNumber>


											<BibArticle>

												<Year />


												<ArticleTitle
Language="En">Swiss-Prot Release 45.5 of 04-Jan-2005</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:Authors>

						<meta:Author>Ning, Kang</meta:Author>

						<meta:Author>Leong, Hon</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution geo="103.774730,1.296322,0">

							<meta:OrgName>National University of Singapore</meta:OrgName>

							<meta:GeoOrg>103.774730,1.296322,0#National University of Singapore</meta:GeoOrg>

							<meta:Country>Singapore</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2006-12-12</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1471-2105-7-S4-S12</meta:DOI>

					<meta:Title>Towards a better solution to the shortest common supersequence problem: the deposition and reduction algorithm</meta:Title>

					<meta:ISXN>1471-2105</meta:ISXN>

					<meta:Journal>BMC Bioinformatics</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>S12</meta:ArticleFirstPage>

					<meta:Publication>BMC Bioinformatics</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Bioinformatics</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Microarrays</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Combinatorial Libraries</meta:Subject>

						<meta:Subject Priority="6"
Type="Secondary">Algorithms</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images />

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1471-2105</JournalID>


						<JournalPrintISSN>1471-2105</JournalPrintISSN>


						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>


						<JournalTitle>BMC Bioinformatics</JournalTitle>


						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Bioinformatics</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Microarrays</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences </JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Combinatorial Libraries</JournalSubject>


							<JournalSubject Priority="6"
Type="Secondary">Algorithms</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>8</VolumeIDStart>


							<VolumeIDEnd>8</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>1</IssueIDStart>


								<IssueIDEnd>1</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2007</Year>


										<Month>2</Month>


										<Day>23</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolderName>


									<CopyrightYear>2007</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1471-2105-8-61</ArticleID>


									<ArticleDOI>10.1186/1471-2105-8-61</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Research article</ArticleCategory>


									<ArticleFirstPage>61</ArticleFirstPage>


									<ArticleLastPage>61</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year>2006</Year>


											<Month>6</Month>


											<Day>30</Day>


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year>2007</Year>


											<Month>2</Month>


											<Day>23</Day>


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolderName>


										<CopyrightYear>2007</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>8</VolumeIDStart>


										<VolumeIDEnd>8</VolumeIDEnd>


										<IssueIDStart>1</IssueIDStart>


										<IssueIDEnd>1</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Iosifina</GivenName>


												<FamilyName>Pournara</FamilyName>


											</AuthorName>


											<Contact>

												<Email>i.pournara@cryst.bbk.ac.uk</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Lorenz</GivenName>


												<FamilyName>Wernisch</FamilyName>


											</AuthorName>


											<Contact>

												<Email>l.wernisch@cryst.bbk.ac.uk</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>School of Crystallography, Birkbeck College, University of London, London, UK</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>Most existing algorithms for the inference of the structure of gene regulatory networks from gene expression data assume that the activity levels of transcription factors (TFs) are proportional to their mRNA levels. This assumption is invalid for most biological systems. However, one might be able to reconstruct unobserved activity profiles of TFs from the expression profiles of target genes. A simple model is a two-layer network with unobserved TF variables in the first layer and observed gene expression variables in the second layer. TFs are connected to regulated genes by weighted edges. The weights, known as 

												<Emphasis
Type="Italic">factor loadings</Emphasis>
 , indicate the strength and direction of regulation. Of particular interest are methods that produce sparse networks, networks with few edges, since it is known that most genes are regulated by only a small number of TFs, and most TFs regulate only a small number of genes.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>In this paper, we explore the performance of five factor analysis algorithms, Bayesian as well as classical, on problems with biological context using both simulated and real data. Factor analysis (FA) models are used in order to describe a larger number of observed variables by a smaller number of unobserved variables, the 

												<Emphasis
Type="Italic">factors</Emphasis>
 , whereby all correlation between observed variables is explained by common factors. Bayesian FA methods allow one to infer sparse networks by enforcing sparsity through priors. In contrast, in the classical FA, matrix rotation methods are used to enforce sparsity and thus to increase the interpretability of the inferred factor loadings matrix. However, we also show that Bayesian FA models that do not impose sparsity through the priors can still be used for the reconstruction of a gene regulatory network if applied in conjunction with matrix rotation methods. Finally, we show the added advantage of merging the information derived from all algorithms in order to obtain a combined result.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusion</Heading>


											<Para>Most of the algorithms tested are successful in reconstructing the connectivity structure as well as the TF profiles. Moreover, we demonstrate that if the underlying network is sparse it is still possible to reconstruct hidden activity profiles of TFs to some degree without prior connectivity information.</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Keyword>networks</Keyword>


										<Keyword>gene</Keyword>


										<Keyword>regulatory</Keyword>


										<Keyword>Factor</Keyword>


										<Keyword>profiles</Keyword>


										<Keyword>factor</Keyword>


										<Keyword>analysis</Keyword>


										<Keyword>activity</Keyword>


										<Keyword>transcription</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_20711">

										<Heading>Background</Heading>


										<Para>Factor analysis (FA) as well as principal component analysis (PCA) is used to describe a number of observed variables by a smaller number of unobserved variables. Unlike PCA, FA also includes independent additive measurement errors on the observed variables. FA assumes that the observed variables become uncorrelated given a set of hidden variables called 

											<Emphasis Type="Italic">factors</Emphasis>
 . It can also be seen as a clustering method where the variables described by the same factors are highly correlated, thus belonging to the same cluster, while the variables depending on different factors are uncorrelated and placed in different clusters.
										</Para>


										<Para>FA has been successfully used in a number of areas such as computer vision, pattern recognition, economics and more recently in bioinformatics 

											<CitationRef
CitationID="B1">1</CitationRef>


											<CitationRef
CitationID="B2">2</CitationRef>


											<CitationRef
CitationID="B3">3</CitationRef>


											<CitationRef
CitationID="B4">4</CitationRef>
 . The suitability of FA for gene expression analysis is also the motivation of this work. Genes are transcribed into mRNAs which in turn are translated into proteins. Some of these proteins activate or inhibit, as transcription factors (TFs), the transcription of a number of other genes creating a complex 

											<Emphasis
Type="Italic">gene regulatory network</Emphasis>
 . The number of transcription factors is much smaller than the number of transcribed genes and most genes are regulated only by a small number of transcription factors. Hence, the matrix that describes the connections between the transcription factors and the regulated genes is sparse. Using microarrays, mRNA levels of thousands of genes can be measured simultaneously, but no direct information is obtained about TF activity. Our aim is two-fold: to identify the genes regulated by a common TF, that is, to reconstruct the connectivity structure and weights in a two-layer network, and to reconstruct the activity profile of each TF.
										</Para>


										<Para>Liao et al. 

											<CitationRef
CitationID="B5">5</CitationRef>
 have suggested the use of a network component analysis (NCA) algorithm for reconstructing the profiles of the TFs (see also 

											<CitationRef
CitationID="B6">6</CitationRef>
 and 

											<CitationRef
CitationID="B7">7</CitationRef>
 ), while Boulesteix and Strimmer 

											<CitationRef
CitationID="B8">8</CitationRef>
 have used an approach based on partial least squares regression. They have both shown that such methods can faithfully reconstruct the expression profiles of the TFs. However, both methods rely heavily on the availability of connectivity information. Nonzero positions in the 

											<Emphasis
Type="Italic">factor loadings matrix</Emphasis>
 , which describes the connections between the factors and the genes, need to be specified in advance. The algorithms then estimate the values at these positions (which might turn out to be zero). This is a strong limitation since often only little information about genes regulated by specific TFs is available. FA models are faced with a much harder task where both the structure of the factor loadings matrix and the activity profiles of the factors have to be reconstructed. Independent component analysis (ICA) has also been widely used in bioinformatics (see for example 

											<CitationRef
CitationID="B9">9</CitationRef>


											<CitationRef
CitationID="B10">10</CitationRef>
 and 

											<CitationRef
CitationID="B11">11</CitationRef>
 ). This approach assumes that the transcription factors are statistically independent. A comparison of NCA and ICA can be found in Liao et al. 

											<CitationRef
CitationID="B5">5</CitationRef>
 , and thus ICA will not be considered further here. A further advantage of the Bayesian FA models is that any information about the underlying structure can be easily incorporated through priors. This improves performance, but is not required for the algorithms to be applicable in the first place, as in the case of NCA and its generalisations.
										</Para>


										<Para>Hinton et al. 

											<CitationRef
CitationID="B12">12</CitationRef>
 first introduced an EM algorithm for factor analysis in order to model the manifolds of digitised images of handwritten digits. Later Ghahramani and Hinton 

											<CitationRef
CitationID="B13">13</CitationRef>
 presented an exact EM algorithm for both factor analyzers and mixtures of factor analyzers. More recently, Utsugi and Kumagai 

											<CitationRef
CitationID="B14">14</CitationRef>
 used a Gibbs sampler instead of the EM algorithm suggested by Ghahramani and Hinton 

											<CitationRef
CitationID="B13">13</CitationRef>
 for mixtures of factor analyzers. West 

											<CitationRef
CitationID="B3">3</CitationRef>
 was the first to introduce Bayesian factor analysis in the bioinformatics field. To accommodate the required sparsity regarding the connections between the factors and the genes, he suggested the use of a mixture prior on the factor loadings matrix. As is shown in the results section, the predicted factor loadings matrix has the desired sparsity, at the expense of increasing computing time as the number of hidden variables increases. Recently, Sabatti and James 

											<CitationRef
CitationID="B4">4</CitationRef>
 have used the framework by West 

											<CitationRef
CitationID="B3">3</CitationRef>
 for the reconstruction of transcription factor profiles. In order to avoid the computational burden of estimating the factor loadings matrix at each step of the Gibbs sampler and to facilitate the reconstruction process, they set a large number of entries to zero based on information obtained from the Vocabulon algorithm 

											<CitationRef
CitationID="B15">15</CitationRef>
 . This algorithm scans DNA sequences for multiple motifs and associates with each transcription factor a probability of binding to a specific site. This approach resembles the approach of Liao et al. 

											<CitationRef
CitationID="B5">5</CitationRef>
 , and Boulesteix and Strimmer 

											<CitationRef
CitationID="B8">8</CitationRef>
 where the structure of the factor loadings matrix is given in advance.
										</Para>


										<Para>Note that the algorithms of Ghahramani and Hinton 

											<CitationRef
CitationID="B13">13</CitationRef>
 , and Utsugi and Kumagai 

											<CitationRef
CitationID="B14">14</CitationRef>
 have not previously been applied to biological data, and that the algorithm of Sabatti and James 

											<CitationRef
CitationID="B4">4</CitationRef>
 is an adaptation of the algorithm of West 

											<CitationRef
CitationID="B3">3</CitationRef>
 with the difference that an informative prior is used for the factor loadings matrix. Also, Sabatti and James 

											<CitationRef
CitationID="B4">4</CitationRef>
 applied the FA model to yeast and 

											<Emphasis Type="Italic">E. coli</Emphasis>
 data, while West 

											<CitationRef
CitationID="B3">3</CitationRef>
 applied his algorithm to cancer data.
										</Para>


										<Para>In this paper, we suggest the use of Fokoue's algorithm 

											<CitationRef
CitationID="B16">16</CitationRef>
 as an alternative to West's algorithm 

											<CitationRef
CitationID="B3">3</CitationRef>
 . This algorithm utilises a Gamma prior distribution on the variance of the factor loadings matrix that imposes the required sparsity but, at the same time, avoids the computational burden introduced by the use of a mixture prior 

											<CitationRef
CitationID="B3">3</CitationRef>
 . Since this algorithm avoids the combinatorial problem of West's algorithm, a prior knowledge on the underlying model is not required. At the same time, we give a thorough review of all FA algorithms mentioned above and examine the applicability of those algorithms to biological data. To the best of our knowledge such a comparison of FA algorithms in the scope of analyzing microarray data has not been presented before. Moreover, we extend these algorithms by suggesting a further factor rotation analysis which produces additional sparsity of the factor loadings matrix. This additional sparsity not only facilitates the interpretation of the results, but it is also useful in a biological context where a very sparse matrix is required. Finally, we show that merging the information provided by each algorithm to obtain a combined result leads to better performance. The algorithms are compared based on their ability to reconstruct the underlying factor loadings matrix and the profiles of the transcription factors.
										</Para>


										<Para>The comparison is done on both simulated data where the true answer is known and on experimental data. We evaluate the performance of the algorithms on the Hemoglobin data obtained by Liao et al. 

											<CitationRef
CitationID="B5">5</CitationRef>
 and on the Escherichia coli ( 

											<Emphasis Type="Italic">E. coli</Emphasis>
 ) data in Kao et al. 

											<CitationRef
CitationID="B6">6</CitationRef>
 . Although time series data show correlation that is ignored in a factor analysis, which in fact assumes independence across data points, we used these data sets for comparison of our results with that in Liao et al. 

											<CitationRef
CitationID="B5">5</CitationRef>
 , Kao et al. 

											<CitationRef
CitationID="B6">6</CitationRef>
 , and Boulesteix and Strimmer 

											<CitationRef
CitationID="B8">8</CitationRef>
 .
										</Para>


										<Section2 ID="Sec_50348">

											<Heading>Factor analysis model</Heading>


											<Para>Let us assume that we have a random observed vector variable 

												<Emphasis Type="Italic">x</Emphasis>
 of 

												<Emphasis Type="Italic">P</Emphasis>
 dimensions, 

												<Emphasis Type="Italic">x</Emphasis>
 = ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>1</Subscript>
 ,..., 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">P</Emphasis>

												</Subscript>
 )'. We denote an instance of this vector with a superscript 

												<Emphasis Type="Italic">n</Emphasis>
 and we assume that we have 

												<Emphasis Type="Italic">N</Emphasis>
 such instances, 

												<Emphasis Type="Italic">x</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 where 

												<Emphasis Type="Italic">n</Emphasis>
 = 1,..., 

												<Emphasis Type="Italic">N</Emphasis>
 . Similarly, 

												<Emphasis Type="Italic">f</Emphasis>
 = ( 

												<Emphasis Type="Italic">f</Emphasis>


												<Subscript>1</Subscript>
 ,..., 

												<Emphasis Type="Italic">f</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">K</Emphasis>

												</Subscript>
 )' is a vector of 

												<Emphasis Type="Italic">K</Emphasis>
 hidden variables, known as 

												<Emphasis
Type="Italic">factors</Emphasis>
 . Note that the number 

												<Emphasis Type="Italic">K</Emphasis>
 of factors is always smaller than or equal to the number 

												<Emphasis Type="Italic">P</Emphasis>
 of observed variables. The factor analysis model states that the observed variables are a linear combination of the factors plus a mean and an error term. For case 

												<Emphasis Type="Italic">n</Emphasis>

											</Para>


											<Para>where 

												<Emphasis Type="Italic">μ</Emphasis>
 = ( 

												<Emphasis Type="Italic">μ</Emphasis>


												<Subscript>1</Subscript>
 ,..., 

												<Emphasis Type="Italic">μ</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">P</Emphasis>

												</Subscript>
 )' and 

												<Emphasis Type="Italic">ε</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 = (ε1nMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF1oqzdaqhaaWcbaGaeGymaedabaGaemOBa4gaaaaa@30DC@,...,εPnMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF1oqzdaqhaaWcbaGaemiuaafabaGaemOBa4gaaaaa@3115@)' are column vectors of dimension 

												<Emphasis Type="Italic">P</Emphasis>
 with elements corresponding to the mean and the error of the 

												<Emphasis Type="Italic">P</Emphasis>
 observed variables. The vector 

												<Emphasis Type="Italic">μ</Emphasis>
 is the same for all cases. Λ is the unobserved 

												<Emphasis
Type="Italic">transition matrix</Emphasis>
 also referred to as the 

												<Emphasis
Type="Italic">factor loadings matrix</Emphasis>
 . The factor loadings matrix has 

												<Emphasis Type="Italic">P</Emphasis>
 × 

												<Emphasis Type="Italic">K</Emphasis>
 dimensions. That is, each column corresponds to a factor and each row corresponds to an observed variable. The entries of the factor loadings matrix indicate the strength of the dependence of each observed variable on each factor. For example, if 

												<Emphasis Type="Italic">λ</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">pk</Emphasis>

												</Subscript>
 is zero, then variable 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">p</Emphasis>

												</Subscript>
 is independent of factor 

												<Emphasis Type="Italic">f</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 . In matrix form equation 1 is
											</Para>


											<Para>where 

												<Emphasis Type="Italic">X</Emphasis>
 = ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Superscript>1</Superscript>
 ,..., 

												<Emphasis Type="Italic">x</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">N</Emphasis>

												</Superscript>
 ), 

												<Emphasis Type="Italic">F</Emphasis>
 = ( 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript>1</Superscript>
 ,..., 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">N</Emphasis>

												</Superscript>
 ), 

												<Emphasis Type="Italic">E</Emphasis>
 = ( 

												<Emphasis Type="Italic">ε</Emphasis>


												<Superscript>1</Superscript>
 ,..., 

												<Emphasis Type="Italic">ε</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">N</Emphasis>

												</Superscript>
 ), 

												<Emphasis Type="Italic">M</Emphasis>
 = 

												<Emphasis Type="Italic">μe</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">N</Emphasis>

												</Subscript>
 with 

												<Emphasis Type="Italic">e</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">N</Emphasis>

												</Subscript>
 an 

												<Emphasis Type="Italic">N</Emphasis>
 dimensional row vector of ones. FA models assume that the error terms 

												<Emphasis Type="Italic">ε</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 are independent, and multivariate normally distributed with mean zero and covariance matrix Ψ, 

												<Emphasis Type="Italic">ε</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 ~NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@(0, Ψ), where Ψ = diag(ψ12MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFipqEdaqhaaWcbaGaeGymaedabaGaeGOmaidaaaaa@3090@,...,ψP2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFipqEdaqhaaWcbaGaemiuaafabaGaeGOmaidaaaaa@30C9@). Thus the probability distribution of 

												<Emphasis Type="Italic">x</Emphasis>
 for each observed case 

												<Emphasis Type="Italic">n</Emphasis>
 has a multivariate normal density given by
											</Para>


											<Para>or in matrix notation</Para>


											<Para>where tr is the trace, the sum of the diagonal elements. In the methods section, we discuss in detail the prior and posterior probabilities of the parameters 

												<Emphasis Type="Italic">F</Emphasis>
 , 

												<Emphasis Type="Italic">μ</Emphasis>
 , Λ and Ψ, as well as algorithms for their estimation.
											</Para>


										</Section2>


										<Section2 ID="Sec_14062">

											<Heading>Identifiability problems</Heading>


											<Para>As shown in equation 5 in the methods section, the complete density of the data, when factors are integrated out, is given by a normal distribution with covariance matrix Λ 

												<Emphasis Type="Italic">Σ</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 Λ' + Ψ. There is a scale identifiability problem associated with Λ and Σ 

												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 . In order to avoid this problem, we could either restrict the columns of Λ to unit vectors or set Σ 

												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 to the identity matrix. The second approach is often preferred in factor analysis.
											</Para>


											<Para>There is also an identifiability problem associated with equation 2. Let us assume that we have an orthogonal matrix 

												<Emphasis Type="Italic">Q</Emphasis>
 of dimensions 

												<Emphasis Type="Italic">K</Emphasis>
 × 

												<Emphasis Type="Italic">K</Emphasis>
 with 

												<Emphasis Type="Italic">QQ</Emphasis>
 ' = 

												<Emphasis Type="Italic">Q'Q</Emphasis>
 = 

												<Emphasis Type="Italic">I</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">K</Emphasis>

												</Subscript>
 . Then we can have
											</Para>


											<Para>Λ 

												<Emphasis Type="Italic">F</Emphasis>
 = Λ 

												<Emphasis Type="Italic">QQ</Emphasis>
 ' 

												<Emphasis Type="Italic">F</Emphasis>
 = Λ* 

												<Emphasis Type="Italic">F</Emphasis>
 *
											</Para>


											<Para>with cov( 

												<Emphasis Type="Italic">F</Emphasis>
 *) = cov( 

												<Emphasis Type="Italic">F</Emphasis>
 ). That is, it is not possible to distinguish between Λ and all its possible orthogonal transformations Λ* based on knowledge of the product Λ 

												<Emphasis Type="Italic">F</Emphasis>
 only. However, as we show in the results section, if the loadings matrix underlying the data generating process is sparse enough, it can often be reconstructed. This can be done either by using sparsity priors on the entries of the loadings matrix in a Bayesian setting or by orthogonal rotations enforcing sparsity (see methods section).
											</Para>


											<Para>Note that orthogonal transformations also include permutations of the factors. Factors could be ordered by the amount of variance explained. Or, as in the case of regulatory networks, we would have to map known TFs to the inferred factors. In Sabatti and James 

												<CitationRef
CitationID="B4">4</CitationRef>
 , the factors are constrained by assigning a priori zero values to the factor loadings matrix. Here, we map the TFs to the inferred factors based on previous knowledge about their activity profiles, as for example reported in Kao et al. 

												<CitationRef
CitationID="B6">6</CitationRef>
 .
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_72000">

										<Heading>Results and Discussion</Heading>


										<Para>We compare the algorithms by Ghahramani and Hinton 

											<CitationRef
CitationID="B13">13</CitationRef>
 (Z), Utsugi and Kumagai 

											<CitationRef
CitationID="B14">14</CitationRef>
 (U), Fokoue 

											<CitationRef
CitationID="B16">16</CitationRef>
 (F), and West 

											<CitationRef
CitationID="B3">3</CitationRef>
 (W) on simulated and real biological data. Algorithm W is based on updating hidden indicator variables representing network connections. For a full exploration of the posterior probability, all possible combinations of hidden values need to be evaluated, thus an exponential number of combinations of these variables. We therefore suggest and test a version (Ws) of the algorithm with independent updates of hidden variables. We also compare these Bayesian FA algorithms with classical FA (as implemented in the Matlab function 

											<Emphasis Type="Italic">factoran</Emphasis>
 (M)).
										</Para>


										<Para>In order to evaluate the strengths and weakness of such algorithms we simulate comparatively 'easy' data (that is from linear models) to be able to focus on the question how far sparsity in the connectivity allows identification of the loadings and the factor matrix. Moreover, as shown in the PhD thesis by Pournara 

											<CitationRef
CitationID="B17">17</CitationRef>
 the assumption of linearity is not a severe one given the small amount of data and the significant amounts of noise present in microarray data, especially after taking logarithms of mRNA abundance levels or ratios (see also Kao et al. 

											<CitationRef
CitationID="B6">6</CitationRef>
 ). In a second step, instead of resorting to simulated nonlinear data, which would have invited questions about the choice of particular nonlinear functional forms, we apply the algorithms to real microarray data and evaluate their performance there directly.
										</Para>


										<Section2 ID="Sec_80263">

											<Heading>Simulated networks</Heading>


											<Para>We test the algorithms on simulated networks. For the generation of random networks we start with a description of network characteristics such as the indegree distribution of genes and outdegree distributions of TFs, which we take from known regulatory networks of 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 . For each TF, we then select random genes subject to these constraints. The activity levels of the factors 

												<Emphasis Type="Italic">F</Emphasis>
 are drawn from a Gaussian distribution with zero mean and covariance matrix 

												<Emphasis Type="Italic">I</Emphasis>
 . The vector 

												<Emphasis Type="Italic">μ</Emphasis>
 of means is set to zero. All non-zero loadings are set to 1. A noise term 

												<Emphasis Type="Italic">E</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">p</Emphasis>

												</Subscript>
 is added in each dimension 

												<Emphasis Type="Italic">p</Emphasis>
 with zero mean and varianceψp2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFipqEdaqhaaWcbaGaemiCaahabaGaeGOmaidaaaaa@3109@as
											</Para>


											<Para>whereσp2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemiCaahabaGaeGOmaidaaaaa@30FE@is the variance of the data in dimension 

												<Emphasis Type="Italic">p</Emphasis>
 , and 

												<Emphasis Type="Italic">snr</Emphasis>
 is a signal to noise ratio. We evaluate the performance of the algorithms by calculating the mean of squared error (MSE) for the predicted factor loadings matrix Λ and the factor matrix 

												<Emphasis Type="Italic">F</Emphasis>
 . We identify the labels of the factors by choosing the column permutation of 

												<Emphasis Type="Italic">F</Emphasis>
 that gives the smallest MSE.
											</Para>


											<Para>As discussed above, the loadings and factor matrices are only identifiable up to a rotation. Sparsity of the true loadings matrix helps to overcome this lack in identifiability. In algorithms F and W the parameters are estimated by imposing sparsity on the loadings matrix directly. Others, not imposing any prior sparsity, cannot be expected to find the correct solution without further processing, for example, by orthogonal transformations to a sparse form. Results can be improved by normalising the column vectors of the loadings matrix before the transformation, that is, by dividing each vector by its Euclidean length. The inverse of the orthogonal transformation of the loadings matrix is used to transform the factor matrix correspondingly. Finally, in order to assess how successful a factor analysis is independently of the identifiability problem for orthogonal transformations, we apply a procrustes orthogonal transformation (that is, one minimising squared vector distances, see methods section) of the column vectors of the reconstructed loadings matrix onto the column vectors of the true loadings matrix. Such rotation is possible since in the case of simulated data the true loadings matrix is known.</Para>


											<Section3 ID="Sec_46456">

												<Heading>Simulated E. coli networks</Heading>


												<Para>We assume that there are only a few TFs in 

													<Emphasis
Type="Italic">E. coli</Emphasis>
 that control the expression profiles of most genes. This assumption is also supported by the connectivity matrix as inferred from RegulonDB 

													<CitationRef
CitationID="B18">18</CitationRef>
 and the current literature in Kao et al. 

													<CitationRef
CitationID="B6">6</CitationRef>
 . The matrix is reproduced in Figure 

													<InternalRef
RefID="F1">1(a)</InternalRef>
 . It is very sparse with most genes regulated by 1 to 3 TFs, and with only a few TFs regulating a larger number of genes as shown in Figures 

													<InternalRef
RefID="F1">1(b)</InternalRef>
 and 

													<InternalRef
RefID="F1">1(c)</InternalRef>
 .
												</Para>


												<Figure Category="Standard"
Float="No" ID="F1">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Factor loadings matrix of the E. coli network</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Factor loadings matrix of the E. coli network</Emphasis>
 . (a) connectivity matrix of 

																<Emphasis
Type="Italic">E. coli</Emphasis>
 as suggested by Kao et al. [6] (a black entry corresponds to a non interaction while a white entry corresponds to an interaction), (b) distribution of the number of genes regulated by each TF, and (c) distribution of the number of TFs regulating each gene in the 

																<Emphasis
Type="Italic">E. coli</Emphasis>
 network of (a).
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>We generated random networks consisting of 50 genes and 8 TFs. Since the performance of the algorithms depends on the number of nonzero entries in the loadings matrix Λ, we generated networks with densities ranging from 15 to 40 percent of nonzero entries. Figure 

													<InternalRef
RefID="F2">2</InternalRef>
 shows the distributions of the genes and TFs for three networks with densities of 15, 25 and 40 percent. Networks with density less than 25 have distributions similar to that in the 

													<Emphasis
Type="Italic">E. coli</Emphasis>
 network of Figure 

													<InternalRef
RefID="F1">1</InternalRef>
 .
												</Para>


												<Figure Category="Standard"
Float="No" ID="F2">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Distributions of genes and TFs for the simulated networks</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Distributions of genes and TFs for the simulated networks</Emphasis>
 . The plots on the left hand side show the distribution of the number of genes regulated by each TF for three networks with densities 15, 25 and 40, respectively. The right hand side plots show the distribution of the number of TFs regulating each gene for the same networks.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>Figure 

													<InternalRef
RefID="F3">3(a)</InternalRef>
 shows the MSE for the Λ matrix for all the FA algorithms and for different network densities. Shown are the mean value for three random networks for each density. From each network 100 data points were generated, and the 

													<Emphasis
Type="Italic">snr</Emphasis>
 was set to 10. For sparse networks algorithms W and F give a smaller MSE than the other algorithms. However, both algorithms perform worse than algorithms Z and U on dense networks. The sparsity priors in W and F obviously hamper reconstruction of dense networks. Classical FA shows an average performance for sparse networks but decreasing performance for dense ones. Algorithm W performs better than algorithm F only for extremely sparse networks. The version Ws of algorithm W with independent updates of entries in Λ gives results similar to that of W which uses a block update, but with a much faster Gibbs sampling step.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F3">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Evaluation of the FA algorithms on E. coli simulated networks</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-3" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Evaluation of the FA algorithms on E. coli simulated networks</Emphasis>
 . Mean squared errors (MSEs) for Λ, the varimax rotated Λ 

																<Subscript> 

																	<Emphasis
Type="Italic">vari</Emphasis>

																</Subscript>
 , and the procrustes rotated Λ 

																<Subscript> 

																	<Emphasis
Type="Italic">procr</Emphasis>

																</Subscript>
 are shown. The first column (a) shows the MSEs of Λ versus the network density, the second column (b) shows the MSEs of Λ versus the dataset size, and the third column (c) shows the MSEs of Λ for different values of the 

																<Emphasis
Type="Italic">snr</Emphasis>
 . These tests are for networks consisting of 50 genes and 8 TFs. Shown are the mean for 3 different networks. For the definition of the symbols M, Z, U, F, W and Ws see page 6.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>Figure 

													<InternalRef
RefID="F3">3(a)</InternalRef>
 also shows the MSE for the varimax and procrustes rotated matrix Λ 

													<Subscript> 

														<Emphasis
Type="Italic">rot</Emphasis>

													</Subscript>
 . Varimax and quartimax rotation give similar results. The equamax rotation gives a slightly higher MSE for sparse matrices and lower MSE for dense matrices (results not shown). Once a varimax rotation is applied to matrices obtained by the FA algorithms, the difference between them regarding the MSE is significantly reduced. It appears that the performance of algorithm F for sparse matrices is better without a varimax rotation; actually so much so that algorithm F is still better than all the other algorithms even after application of varimax. The procrustes rotation indicates the ability of all the FA algorithms to reconstruct a factor loadings matrix that has a very small MSE. However, it also shows that finding the best possible rotation is difficult.
												</Para>


												<Para>Two more tests were performed to investigate the behavior of the FA algorithms on datasets of different size (ranging from 25 to 100 cases) and data generated with different values of 

													<Emphasis
Type="Italic">snr</Emphasis>
 (ranging from 0.5 to 100). Note that the classical FA algorithm uses the covariance matrix of the data and thus the number of cases must be greater than the number of variables. That is, the 

													<Emphasis
Type="Italic">factoran</Emphasis>
 script was not run for datasets of 25 cases. These two tests were applied to networks with density 15. Figure 

													<InternalRef
RefID="F3">3(b)</InternalRef>
 shows again that algorithms Z and U perform similarly regardless of the number of cases in the dataset. Moreover, algorithms F and W also perform similarly and have a much smaller MSE than the other algorithms. Once the varimax rotation is applied, all the algorithms give a similar performance with a smaller MSE achieved as the number of cases increases. For sparse networks with small densities even a very small dataset is enough to reconstruct the factor loadings matrix. The procrustes rotation indicates that algorithm W produces a factor loadings matrix which, if properly rotated, is very close to the true matrix for very sparse networks.
												</Para>


												<Para>Figure 

													<InternalRef
RefID="F3">3(c)</InternalRef>
 shows the results for different values of 

													<Emphasis
Type="Italic">snr</Emphasis>
 . As the amount of noise increases, the performance of most algorithms decreases. Algorithm F has the best performance overall. Varimax rotation improves the performances of the other algorithms and makes them comparable to the results of F and W. Note that algorithm W seems to perform worse when the data are free of noise (snr 100) than when there is at least some small amount of noise (snr 10). However, when we apply varimax rotation to this algorithm we see that the performance decreases indeed with increasing amounts of noise.
												</Para>


												<Para>Figure 

													<InternalRef
RefID="F4">4(a)</InternalRef>
 shows the change in the log likelihood for a chosen representative run over 3000 cycles of the Gibbs sampling for algorithms U, F and W. It suggests that all algorithms converge, but algorithm F converges faster than the others. Finally, Figure 

													<InternalRef
RefID="F4">4(b)</InternalRef>
 shows the average time consumed by each algorithm. The number of burn-in and sample collection steps (3000) is the same for all the FA algorithms. As mentioned above, for very sparse networks algorithm W produces a better result than algorithms Z, U and the classical FA, but it requires considerably longer time for convergence when the number of factors and genes is large. The results of our version of algorithm W with single updates (Ws) and algorithm W are similar, while Ws is approximately 10 times faster than W. Note that the EM algorithm Z and classical FA are the fastest FA algorithms by reaching convergence within a few seconds. Algorithm Z was downloaded from 

													<CitationRef
CitationID="B19">19</CitationRef>
 . All the other FA algorithms were also implemented in MATLAB and run on a 3.06 Ghz Xeon cluster.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F4">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Convergence test and processing time</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-4" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Convergence test and processing time</Emphasis>
 . (a) convergence test for the Gibbs sampling algorithms, and (b) the average time consumed by each algorithm.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>Summarising, algorithms F and W perform better on sparse matrices than algorithms Z, U and M because they implicitly capture the required sparsity on the factor loadings matrix. However, if an appropriate orthogonal rotation of the matrices Λ and 

													<Emphasis Type="Italic">F</Emphasis>
 is applied, the performances of all the FA algorithms are enhanced and become comparable.
												</Para>


											</Section3>


										</Section2>


										<Section2 ID="Sec_49038">

											<Heading>Biological data</Heading>


											<Para>We further compare the FA algorithms to two biological datasets; the Hemoglobin dataset from Liao et al. 

												<CitationRef
CitationID="B5">5</CitationRef>
 , where the connectivity matrix and the profiles of the factors are known to some degree, and the 

												<Emphasis
Type="Italic">E. coli</Emphasis>
 dataset, where the TF profiles and some interactions have been suggested by Kao et al. 

												<CitationRef
CitationID="B6">6</CitationRef>
 .
											</Para>


											<Section3 ID="Sec_38362">

												<Heading>Hemoglobin dataset</Heading>


												<Para>The absorbance spectra of seven hemoglobin solutions ( 

													<Emphasis Type="Italic">M</Emphasis>
 1,..., 

													<Emphasis Type="Italic">M</Emphasis>
 7) were measured in Liao et al. 

													<CitationRef
CitationID="B5">5</CitationRef>
 . Each spectrum is the outcome of a linear combination of the concentrations of three components: oxyhemoglobin (OxyHb), methemoglobin (MetHb) and cyano-methemoglobin (CyanoHb). This dataset consists of 321 measurements for each of the seven hemoglobin solutions.
												</Para>


												<Para>We first compared the algorithms by Fokoue 

													<CitationRef
CitationID="B16">16</CitationRef>
 , West 

													<CitationRef
CitationID="B3">3</CitationRef>
 , and by Tran et al. 

													<CitationRef
CitationID="B7">7</CitationRef>
 (GNCA) fixing the positions of zeros in the loadings matrix. Note that the algorithm by Tran et al. 

													<CitationRef
CitationID="B7">7</CitationRef>
 requires this connectivity matrix as an input and is unlikely to work properly without this information. Tran et al. 

													<CitationRef
CitationID="B7">7</CitationRef>
 have presented an extension of the NCA algorithm 

													<CitationRef
CitationID="B5">5</CitationRef>
 , the GNCA (generalised network component analysis) algorithm. For details regarding the different versions of the GNCA algorithm see 

													<CitationRef
CitationID="B7">7</CitationRef>
 . We present the results for versions GNCA and GNCA 

													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 . Each algorithm was run 20 times. For algorithms GNCA and GNCA 

													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 , we consider the run with the least MSE, while for the FA algorithms we consider the average of these runs.
												</Para>


												<Para>As shown in Figure 

													<InternalRef
RefID="F5">5(a)</InternalRef>
 , the MSE in the estimation of Λ is approximately equal for all algorithms except GNCA 

													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 , and it is very similar before and after procrustes rotation. This figure indicates that fixing the zero loadings simplifies the task of identifying the underlying factor loadings matrix considerably. Figure 

													<InternalRef
RefID="F5">5(b)</InternalRef>
 shows the MSE in the estimation of the factor profiles, and these profiles are plotted in Figure 

													<InternalRef
RefID="F6">6</InternalRef>
 . The MSE for the reconstruction of the factor profiles is close to zero for all the algorithms except the algorithm GNCA 

													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 . We used the inverse of the rotation matrix returned for Λ by the procrustes method to rotate the factors. The rotation increases the MSE of the factors since the best rotation for Λ is not necessarily the best rotation for 

													<Emphasis Type="Italic">F</Emphasis>
 . However, it is still considerably small.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F5">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Reconstruction of the factor loadings matrix for the Hemoglobin data</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-5" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Reconstruction of the factor loadings matrix for the Hemoglobin data</Emphasis>
 . Mean square errors (MSEs) for (a) the factor loadings matrix Λ and (b) the factors matrix 

																<Emphasis
Type="Italic">F</Emphasis>
 . The positions of the zero entries in the loadings matrix are given a priori. FA stands for the output of a given FA algorithm. The procrustes (P) factor rotation method is applied to this output to indicate the performance of the algorithms when the best possible rotation is achieved.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Figure Category="Standard"
Float="No" ID="F6">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Reconstruction of the factors matrix for the Hemoglobin data</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-6" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Reconstruction of the factors matrix for the Hemoglobin data</Emphasis>
 . Shown are (a) the true profiles of OxyHb, MetHb and CyanoHb, (b) the reconstructed profiles given by algorithm F, (c) the reconstructed profiles given by algorithm W, (d) the reconstructed profiles given by algorithm GNCA, and (e) the reconstructed profiles given by algorithm GNCA 

																<Subscript> 

																	<Emphasis
Type="Italic">r</Emphasis>

																</Subscript>
 . The positions of the zero entries in the loadings matrix are given a priori. The light gray curves are the profiles given by the 20 different Gibbs sampling runs, and the black curves are the average profiles. In these figures, the average profile of each factor coincides with its profile given by each single run.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>We also evaluated the algorithms without providing prior information about the underlying structure of the factor loadings matrix. This can, of course, only be done for the FA algorithms. Figure 

													<InternalRef
RefID="F7">7(a)</InternalRef>
 shows the MSE of Λ as given by each algorithm. It also shows the MSE after performing varimax, quartimax, equamax, tanh, and procrustes rotation. Most FA algorithms perform equally well in predicting the values of the loadings of Λ. This is probably due to the fact that the hemoglobin factor loadings matrix is not sparse enough. Algorithms Z and U depend less on sparsity and match the performance of algorithms F and W on this dataset. However, once we perform varimax rotation the performance of all the algorithms improves.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F7">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Reconstruction of the factor loadings matrix for the Hemoglobin data</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-7" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Reconstruction of the factor loadings matrix for the Hemoglobin data</Emphasis>
 . Mean square errors (MSEs) for (a) and (c) the factor loadings matrix Λ, and (b) and (d) the factors matrix 

																<Emphasis
Type="Italic">F</Emphasis>
 . The positions of the zero entries in the loadings matrix are not given a priori. FA stands for the output of a given FA algorithm. On this output, a number of factor rotation methods (varimax (V), quartimax (Q), equamax (E), tanh (T) and procrustes (P)) are evaluated based on the MSE. (c) and (d) show the performance of algorithms F and W under different priors regarding the loadings matrix (for further details see section 

																<Emphasis
Type="Italic">Hemoglobin dataset</Emphasis>
 ).
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>The classical FA algorithm (M) performs best according to the MSE of Λ. However, comparing the MSE of the factors (Figure 

													<InternalRef
RefID="F7">7(b)</InternalRef>
 ) its performance is worse. This is also apparent by looking at the factor profiles (Figure 

													<InternalRef
RefID="F8">8</InternalRef>
 ). Classical FA optimises the joint likelihood of the loadings matrix and noise covariance matrix (under a suitable constraint that guarantees identifiability), which amounts to integrating out the factors. All other algorithms (with the exception of Z) represent the factors explicitly. This explains why classical FA is doing better in reconstructing the loadings but worse in reconstructing factors compared to the other algorithms.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F8">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Reconstruction of the factors matrix for the Hemoglobin data</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-8" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Reconstruction of the factors matrix for the Hemoglobin data</Emphasis>
 . Shown are (a) the reconstructed profiles given by algorithm Z, (b) the reconstructed profiles given by algorithm U, (c) the reconstructed profiles given by algorithm F, (d) the reconstructed profiles given by algorithm W, and (e) the reconstructed profiles given by algorithm M. The positions of the zero entries in the loadings matrix are not given a priori. The light gray curves are the profiles given by the 20 different Gibbs sampling runs, and the black curves are the average profiles. We also plot with gray the true profiles for an easier comparison. These profiles are obtained after performing varimax rotation on the factor loadings matrix.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>Algorithm F and W perform quite well on both the reconstruction of the Λ and the factor profiles. Their performance is also improved by using any of the four rotation methods. Varimax rotation also improves the performance of algorithms Z and U. Again procrustes rotation shows that we can rotate the estimated Λ to match the true Λ very closely. However, as shown again by the MSE on the factors, the best rotation for Λ is not necessarily the best rotation for the factors.</Para>


												<Para>Figure 

													<InternalRef
RefID="F7">7(a)</InternalRef>
 shows the result for algorithm F when entries of the loadings matrix are restricted to stay close to 0 by a strong prior (shape parameter 10 for 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 , scale parameter 0.01, see methods section). We also investigated the performance of algorithm F under a vaguer prior on matrix entries (shape parameter 1 for 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 , scale parameter 0.01). As shown in Figure 

													<InternalRef
RefID="F7">7(c)</InternalRef>
 and 

													<InternalRef
RefID="F7">7(d)</InternalRef>
 , this setting (Fu) performs better, but once the loadings matrix is rotated, the improvement is not as significant. Similarly, we set the prior probability 

													<Emphasis Type="Italic">π</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 (see methods section) that an entry of the loadings matrix is nonzero to 1 in algorithm W (Wu) and to 0.2 (W). As expected, since the connectivity is not sparse for the hemoglobin data, the difference is small (Figure 

													<InternalRef
RefID="F7">7(c)</InternalRef>
 ). With rotation (except the procrustes rotation) the sparse prior seems to do considerably better though. Finally, the algorithm Ws (with prior probability 0.2) that we have suggested in order to avoid the combinatorial problem of algorithm W gives good results and comparable to the ones by W.
												</Para>


												<Para>Figure 

													<InternalRef
RefID="F8">8</InternalRef>
 shows the reconstructed factor profiles after varimax rotation on the Λ without using prior information on nonzero entries. It also demonstrates that it is now harder to reconstruct the factor profiles, as seen in the greater variability of profiles from different MCMC runs when compared to Figure 

													<InternalRef
RefID="F6">6</InternalRef>
 . All the profiles shown have very similar likelihoods, indicating that the overall distribution is multimodal. Algorithms F and W perform quite well. Algorithms Z and U reconstruct the second and third factors quite well but not as well the first one. As mentioned above the reconstruction of the factor profiles by algorithm M are quite poor, while algorithm F seems to find the best factor profiles.
												</Para>


											</Section3>


											<Section3 ID="Sec_86120">

												<Heading>Escherichia coli dataset</Heading>


												<Para>We evaluated the FA algorithms as well as the algorithm by Boulesteix and Strimmer 

													<CitationRef
CitationID="B8">8</CitationRef>
 (S, as implemented in the R package 

													<Emphasis
Type="Italic">plsgenomics</Emphasis>
 ) on an 

													<Emphasis
Type="Italic">E. coli</Emphasis>
 dataset from Kao et al. 

													<CitationRef
CitationID="B6">6</CitationRef>
 . These data consist of 25 time points for 100 genes. The first time point was ignored since all the values are zero. A matrix that indicates possible interactions between 16 TFs and the 100 genes has been suggested by Kao et al. 

													<CitationRef
CitationID="B6">6</CitationRef>
 based on RegulonDB 

													<CitationRef
CitationID="B18">18</CitationRef>
 and the current literature. We will refer to this matrix as the Kao connectivity matrix. This matrix also indicates whether a TF inhibits or activates a given gene. Each FA algorithm is run 10 times. The following results refer to an average value over these runs. The classical FA is not used in this analysis since the number of cases (24) is smaller than the number of observed variables (100).
												</Para>


												<Para>Since the GNCA algorithm requires prior knowledge of zeros in the factor loadings matrix, for comparison we also run the FA algorithms of Fokoue 

													<CitationRef
CitationID="B16">16</CitationRef>
 and West 

													<CitationRef
CitationID="B3">3</CitationRef>
 providing prior information on zeros in the factor loadings matrix. Here, algorithms F and Ws treat the connectivity matrix simply as indicating whether there is a relationship or not between a gene and a TF and ignore the information on activation or inhibition. However, one could also include a more detailed prior information. We consider two different prior matrices for the GNCA algorithm: one where a simplified connectivity matrix that only indicates whether an interaction exists or not, and one with extra information on inhibition and activation. For each of the two different prior matrices, we run the GNCA algorithm 10 times, and we only consider the run with the least sum squared error.
												</Para>


												<Para>Figure 

													<InternalRef
RefID="F9">9(a)</InternalRef>
 shows that all the algorithms produce very similar TF profiles, that is, given the connectivity matrix, FA algorithms reconstruct TF profiles as well as GNCA. The second column in Table 

													<InternalRef
RefID="T1">1</InternalRef>
 shows the MSE deviation of profiles of algorithms F and Ws from profiles of GNCA which uses information on activation and inhibition. The MSE for GNCA is a consequence of using only connectivity information and no details on activation or inhibition. This small value of MSE suggests that convergence to a similar solution for the 

													<Emphasis
Type="Italic">E. coli</Emphasis>
 dataset is given regardless whether extra information on activation and inhibition is provided or not. The comparatively high MSE of algorithm S is due mainly to a few factors which are reconstructed as fiat.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F9">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Reconstruction of the factor profiles for the E. coli data</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-9" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Reconstruction of the factor profiles for the E. coli data</Emphasis>
 . a) prior connectivity structure is given and (b) no prior connectivity structure is given. Red lines correspond to algorithm GNCA, black lines correspond to GNCA where inhibition and activation information is also given, blue lines are for algorithm Z, cyan lines are for algorithm U, green lines correspond to algorithm F, purple lines are for algorithm Ws, and brown lines are for algorithm S.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Table Float="No" ID="T1">

													<Caption Language="En">

														<CaptionNumber>Table 1</CaptionNumber>


														<CaptionContent>

															<SimplePara>MSEs of the reconstructed factor profiles for the E. coli data</SimplePara>


														</CaptionContent>


													</Caption>


													<tgroup cols="3">

														<colspec colname="c0"
colnum="0" />


														<colspec colname="c1"
colnum="1" />


														<colspec colname="c2"
colnum="2" />


														<thead>

															<row>

																<entry colname="c0">

																	<SimplePara> 

																		<Emphasis
Type="Bold">algorithms</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara> 

																		<Emphasis
Type="Bold">MSE (with prior information)</Emphasis>

																	</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara> 

																		<Emphasis
Type="Bold">MSE (without prior information)</Emphasis>

																	</SimplePara>


																</entry>


															</row>


														</thead>


														<tbody>

															<row>

																<entry colname="c0">

																	<SimplePara />


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Z</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>-</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>0.017</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>U</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>-</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>0.008</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>F</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>0.005</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>0.010</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>Ws</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>0.003</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>0.014</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>S</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>0.020</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>-</SimplePara>


																</entry>


															</row>


															<row>

																<entry colname="c0">

																	<SimplePara>GNCA</SimplePara>


																</entry>


																<entry colname="c1">

																	<SimplePara>0.0004</SimplePara>


																</entry>


																<entry colname="c2">

																	<SimplePara>-</SimplePara>


																</entry>


															</row>


														</tbody>


													</tgroup>


													<tfooter>

														<SimplePara>MSEs of the reconstructed factor matrices from the factor matrix obtained from GNCA with activation and inhibition information. The second column contains the MSEs when the zero positions in the loadings matrix are fixed. The third column contains the MSEs when no information regarding those positions is given. – indicates that the algorithm was not tested.</SimplePara>


													</tfooter>


												</Table>


												<Para>Figure 

													<InternalRef
RefID="F9">9(b)</InternalRef>
 shows the results when no prior information on connectivity is provided. For comparison, we match the resulting TF profiles with those of GNCA by minimum MSE and add the plots of the GNCA TF profiles from Figure 

													<InternalRef
RefID="F9">9(a)</InternalRef>
 . As is evident, the FA algorithms in the case of the 

													<Emphasis
Type="Italic">E. coli</Emphasis>
 dataset are still capable of reconstructing important aspects of the TF profiles even without any prior information on the connectivity. This is encouraging since prior information on TF binding is sometimes limited, difficult to obtain, or not always reliable. The profiles are slightly rougher than the ones inferred given the connectivity matrix and the FA algorithms show greater variability. However, it is still impressive how all FA algorithms are able to reconstruct the main trends of the TF profiles. The third column in Table 

													<InternalRef
RefID="T1">1</InternalRef>
 shows the MSE deviation of profiles of algorithms Z, U, F and Ws from profiles of GNCA with activation and inhibition information. The MSE is about twice as large if no prior information is available.
												</Para>


												<Para>Finally, we analyse the inferred factor loadings matrix in greater detail. Such an evaluation is complicated by the fact that the true connectivity matrix is not fully known. For evaluating the learned loadings matrix, we treat the Kao connectivity matrix (Figure 

													<InternalRef
RefID="F1">1(a)</InternalRef>
 ) as showing true interactions and true missing interactions. However, we should keep in mind that the latter is based on partial biological information and not necessarily complete. Figure 

													<InternalRef
RefID="F10">10(a)</InternalRef>
 shows a ROC curve for each algorithm. The true positive (TP) rate is the proportion of entries above a specified cutoff among entries which are nonzero according to the Kao connectivity matrix. The false positive (FP) rate is the proportion of entries above a specified cutoff among entries which are zero according to the Kao connectivity matrix. On average all algorithms give very similar performance. The lack of differences between the algorithms that implicitly consider sparsity, F and Ws, compared to the algorithms that do not, Z and U, could be due to the lack of detailed information in the Kao connectivity matrix. That is, this matrix has only 0,1 entries and actually some of the 1 entries could be very close to zero or exactly zero and in contrast some zero values could be nonzero. Figure 

													<InternalRef
RefID="F10">10(a)</InternalRef>
 also shows a ROC curve that is based on merging the information gain by each algorithm. That is, we derive a combined factor loadings matrix by averaging the loading matrices derived by each algorithm. This combined loadings matrix gives a ROC curve that is better than any other ROC curve alone.
												</Para>


												<Figure Category="Standard"
Float="No" ID="F10">

													<Caption Language="En">

														<CaptionContent>

															<SimplePara>Reconstruction of the factor loadings matrix for the E. coli data</SimplePara>


														</CaptionContent>


													</Caption>


													<MediaObject>

														<ImageObject Color="Color"
FileRef="1471-2105-8-61-10" Format="GIF" Rendition="Preview"
Type="Linedraw" />


														<TextObject>

															<Para> 

																<Emphasis
Type="Bold">Reconstruction of the factor loadings matrix for the E. coli data</Emphasis>
 . Shown for the 

																<Emphasis
Type="Italic">E. coli</Emphasis>
 dataset are (a) the ROC curve of each FA algorithm for the factor loadings matrix, and (b) the ROC curve of each FA algorithm for the factor loadings matrix after applying procrustes rotation method. The true positive (TP) rate is plotted against the false positive (FP) rate for a given cutoff value.
															</Para>


														</TextObject>


													</MediaObject>


												</Figure>


												<Para>We also plot, in Figure 

													<InternalRef
RefID="F10">10(b)</InternalRef>
 , the ROC curve of each algorithm after applying procrustes rotation to the factor loadings matrix. Here, we use the Kao connectivity matrix as the target matrix for the procrustes rotation. The ROC curves have greatly improved indicating that an appropriate rotation of the learned loadings matrix for each algorithm can lead to a connectivity matrix that is very close to the Kao connectivity matrix. Again the combined loadings matrix gives a ROC curve that outperforms each of the ROC curves given by the FA algorithms.
												</Para>


											</Section3>


										</Section2>


									</Section1>


									<Section1 ID="Sec_57123">

										<Heading>Conclusion</Heading>


										<Para>We discussed and compared the performance of five factor analysis algorithms presented previously in the literature. Only one of these algorithms has been previously applied to biological data. We investigated the applicability of the algorithms on microarray data from 

											<Emphasis Type="Italic">E. coli</Emphasis>
 , on data from hemoglobin spectroscopic measurements and on simulated data. In a gene regulatory context, we aim to identify regulatory relationships between genes and TFs and to reconstruct transcription factor activity profiles. That is, the expression levels of regulated genes are the observed variables and the TFs are the unobserved variables. Even after imposing a correlation structure on the factors, this is still an underdetermined problem. If, however, we assume that the connectivity matrix is sparse, that is, that most genes are regulated by a small number of TFs and most TFs regulate only a small number of genes, estimation of TF profiles and loadings becomes possible.
										</Para>


										<Para>The sparsity requirement is implicit in the algorithms by Fokoue 

											<CitationRef
CitationID="B16">16</CitationRef>
 and West 

											<CitationRef
CitationID="B3">3</CitationRef>
 , and thus these algorithms are shown to perform very well on sparse simulated networks where the underlying relationships are linear. However, we show that the performance of the algorithms by Ghahramani and Hinton 

											<CitationRef
CitationID="B13">13</CitationRef>
 , and Utsugi and Kumagai 

											<CitationRef
CitationID="B14">14</CitationRef>
 is also very satisfactory after an orthogonal rotation of the loadings matrix. On the 

											<Emphasis Type="Italic">E. coli</Emphasis>
 data, we see that all the FA algorithms reconstruct the factor loadings matrix and the factors profiles equally well. Moreover, we show, using the 

											<Emphasis Type="Italic">E. coli</Emphasis>
 data, that such algorithms can reconstruct the underlying TF profiles to an acceptable degree even without any prior knowledge of the connectivity structure. In contrast, algorithms such as the GNCA algorithm of Tran et al. 

											<CitationRef
CitationID="B7">7</CitationRef>
 , depend heavily on prior connectivity information. Finally, we show that integrating results from several FA algorithms results in a connectivity matrix which has a better true positive rate given a specified false positive rate than each algorithm separately. Our analysis demonstrates the usefulness of FA algorithms for biological problems where prior information regarding the system under study is not fully available.
										</Para>


										<Para>The FA algorithms discussed here ignore any time series information. We are currently working on an extension of the above methods to integrate time correlation. We expect that such correlation will smooth TF activity profiles further.</Para>


									</Section1>


									<Section1 ID="Sec_48427">

										<Heading>Methods</Heading>


										<Para>For completeness and to show commonalities and differences between the approaches to FA analysis discussed in this paper, we describe them in some detail in this section. We conclude this section with a short description of matrix rotation methods.</Para>


										<Section2 ID="Sec_99034">

											<Heading>Factors F</Heading>


											<Para>The factors are assumed to be normally distributed with mean zero and covariance matrix Σ 

												<Emphasis Type="Italic"> 

													<Subscript>f</Subscript>

												</Emphasis>
 . That is,
											</Para>


											<Para> 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 ~NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@(0, Σ 

												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 )
											</Para>


											<Para>To resolve identifiability problems (we will return to this issue later), we set Σ 

												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 equal to the identity matrix 

												<Emphasis Type="Italic">I</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">K</Emphasis>

												</Subscript>
 as suggested by Ghahramani and Hinton 

												<CitationRef
CitationID="B13">13</CitationRef>
 , Utsugi and Kumagai 

												<CitationRef
CitationID="B14">14</CitationRef>
 , and Fokoue 

												<CitationRef
CitationID="B16">16</CitationRef>
 . Sabatti and James 

												<CitationRef
CitationID="B4">4</CitationRef>
 choose Σ 

												<Emphasis Type="Italic">f</Emphasis>
 =σf2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemOzaygabaGaeGOmaidaaaaa@30EA@ 

												<Emphasis Type="Italic">I</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">K</Emphasis>

												</Subscript>
 whereσf2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemOzaygabaGaeGOmaidaaaaa@30EA@is a constant value. Finally, West 

												<CitationRef
CitationID="B3">3</CitationRef>
 assigns a more general prior, Σ 

												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 = diag(σf12MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemOzay2aaSbaaWqaaiabigdaXaqabaaaleaacqaIYaGmaaaaaa@3212@,...,σfK2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGaemOzay2aaSbaaWqaaiabdUealbqabaaaleaacqaIYaGmaaaaaa@3241@).
											</Para>


											<Para>The posterior probability of the factors is now derived as</Para>


											<Para> 

												<Emphasis Type="Italic">p</Emphasis>
 ( 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 | 

												<Emphasis Type="Italic">x</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 , Λ, 

												<Emphasis Type="Italic">μ</Emphasis>
 , Ψ) ∝ 

												<Emphasis Type="Italic">p</Emphasis>
 ( 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 ) 

												<Emphasis Type="Italic">p</Emphasis>
 ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 | 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 , Λ, 

												<Emphasis Type="Italic">μ</Emphasis>
 , Ψ) =NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@( 

												<Emphasis Type="Italic">f</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 |mf*MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGTbqBdaqhaaWcbaGaemOzaygabaGaeiOkaOcaaaaa@306D@,Σf*MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHJoWudaqhaaWcbaGaemOzaygabaGaeiOkaOcaaaaa@308E@)
											</Para>


											<Para>where the posterior mean and variance are given by</Para>


											<Para>Σf*MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHJoWudaqhaaWcbaGaemOzaygabaGaeiOkaOcaaaaa@308E@= (Σ 

												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 + Λ'Ψ 

												<Superscript>-1</Superscript>
 Λ) 

												<Superscript>-1</Superscript>

											</Para>


											<Para>mf*MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGTbqBdaqhaaWcbaGaemOzaygabaGaeiOkaOcaaaaa@306D@=Σf*MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHJoWudaqhaaWcbaGaemOzaygabaGaeiOkaOcaaaaa@308E@Λ'Ψ 

												<Superscript>-1</Superscript>
 ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Superscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Superscript>
 - 

												<Emphasis Type="Italic">μ</Emphasis>
 )
											</Para>


											<Para>We can now integrate 

												<Emphasis Type="Italic">F</Emphasis>
 out of equation 4 to get the complete density of the data
											</Para>


											<Para>The EM algorithm of Ghahramani and Hinton 

												<CitationRef
CitationID="B13">13</CitationRef>
 consists of two steps: a) the E-step which calculates the expected values and the second moments of the factors for each case 

												<Emphasis Type="Italic">n</Emphasis>
 given the current Λ and Ψ as given below
											</Para>


											<Para>and b) the M-step which calculates the values of Λ and Ψ given the expected values of the factors that were computed in the E-step.</Para>


										</Section2>


										<Section2 ID="Sec_76287">

											<Heading>Mean vector μ</Heading>


											<Para>The prior probability assigned to the mean vector 

												<Emphasis Type="Italic">μ</Emphasis>
 is the Gaussian distribution with a mean vector 

												<Emphasis Type="Italic">m</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">μ</Emphasis>

												</Subscript>
 and a covariance matrix Σ 

												<Subscript> 

													<Emphasis Type="Italic">μ</Emphasis>

												</Subscript>

											</Para>


											<Para> 

												<Emphasis Type="Italic">μ</Emphasis>
 ~NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@( 

												<Emphasis Type="Italic">m</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">μ</Emphasis>

												</Subscript>
 , Σ 

												<Subscript> 

													<Emphasis Type="Italic">μ</Emphasis>

												</Subscript>
 )
											</Para>


											<Para>By using the above prior, we derive the following posterior distribution for 

												<Emphasis Type="Italic">μ</Emphasis>

											</Para>


											<Para>where</Para>


											<Para>West 

												<CitationRef
CitationID="B3">3</CitationRef>
 , Sabatti and James 

												<CitationRef
CitationID="B4">4</CitationRef>
 , and Fokoue 

												<CitationRef
CitationID="B16">16</CitationRef>
 suggest to centralise the data prior to the use of the FA model, and they also assume that 

												<Emphasis Type="Italic">μ</Emphasis>
 = 0. We also suggest to standardise (centralise and scale by standard deviation) the data prior to the analysis.
											</Para>


											<Para>Utsugi and Kumagai 

												<CitationRef
CitationID="B14">14</CitationRef>
 use a different prior covariance matrix that ties the mean to the error term. That is,
											</Para>


											<Para>where 

												<Emphasis Type="Italic">m</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">μ</Emphasis>

												</Subscript>
 is set to zero since they also centralize the data prior to the use of the FA model. The posterior distribution of 

												<Emphasis Type="Italic">μ</Emphasis>
 , given the above prior is derived in the next section together with Λ.
											</Para>


										</Section2>


										<Section2 ID="Sec_68650">

											<Heading>Factor loadings matrix Λ</Heading>


											<Para>The main differences between the existing FA models lies in the assignment of the prior distribution of the factor loadings matrix Λ or in the prior distribution of its parameters. Let us discuss each of these priors separately.</Para>


											<Section3 ID="Sec_27872">

												<Heading>Normal prior on Λ and Gamma prior on Λ's covariance parameter</Heading>


												<Para>Fokoue 

													<CitationRef
CitationID="B16">16</CitationRef>
 uses the prior suggested by Tipping 

													<CitationRef
CitationID="B20">20</CitationRef>
 in the context of 

													<Emphasis
Type="Italic">Relevance Vector Machines</Emphasis>
 to impose sparsity in the Λ matrix. That is, independent Gaussian priors are assigned to each element 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 of Λ.
												</Para>


												<Para> 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 | 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 ~NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@(0,δpk−1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF0oazdaqhaaWcbaGaemiCaaNaem4AaSgabaGaeyOeI0IaeGymaedaaaaa@332A@)
												</Para>


												<Para>To each 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 , a Gamma prior is assigned as follows
												</Para>


												<Para> 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 | 

													<Emphasis Type="Italic">α</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">δ</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">β</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">δ</Emphasis>

													</Subscript>
 ~GMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFge=raaa@382C@( 

													<Emphasis Type="Italic">α</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">δ</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">β</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">δ</Emphasis>

													</Subscript>
 )
												</Para>


												<Para>where a Gamma distribution with shape parameter 

													<Emphasis Type="Italic">α</Emphasis>
 and a scale parameter 

													<Emphasis Type="Italic">β</Emphasis>
 is defined as
												</Para>


												<Para>In the context of biological data, we suspect that each gene is regulated by only a small number of TFs. Thus, we aim to identify a sparse factor loadings matrix that faithfully describes the relationship between the transcription factors and the regulated genes. The suggested prior leads to a Student 

													<Emphasis Type="Italic">t</Emphasis>
 -distribution for each row of Λ. In two dimensions, such distribution assigns most probability mass to the origin where both 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>
 1
													</Subscript>
 and 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>
 2
													</Subscript>
 are zero and along the spines where one of the coefficients 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 is zero.
												</Para>


												<Para>We suggest that this type of prior on Λ is also applicable to biological data. We have also further extended this prior to include an extra level of hyperparameters for increased flexibility and for an easier assignment of the hyperparameters. Thus, the parameter 

													<Emphasis Type="Italic">β</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">δ</Emphasis>

													</Subscript>
 has also a Gamma prior of the form
												</Para>


												<Para>The posterior probability of each row Λ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 of Λ is given by
												</Para>


												<Para> 

													<Emphasis Type="Italic">p</Emphasis>
 (Λ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 | 

													<Emphasis Type="Italic">X</Emphasis>
 , 

													<Emphasis Type="Italic">F</Emphasis>
 , 

													<Emphasis Type="Italic">μ</Emphasis>
 , Ψ, Δ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 ) ∝ 

													<Emphasis Type="Italic">p</Emphasis>
 (Λ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 | Δ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 ) 

													<Emphasis Type="Italic">p</Emphasis>
 ( 

													<Emphasis Type="Italic">X</Emphasis>
 | 

													<Emphasis Type="Italic">F</Emphasis>
 , Λ, 

													<Emphasis Type="Italic">μ</Emphasis>
 , Ψ) =NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@(Λ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 |mΛpMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGTbqBdaWgaaWcbaGaeu4MdW0aaSbaaWqaaiabdchaWbqabaaaleqaaaaa@3151@,ΣΛpMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHJoWudaWgaaWcbaGaeu4MdW0aaSbaaWqaaiabdchaWbqabaaaleqaaaaa@3172@)   (7)
												</Para>


												<Para>where</Para>


												<Para>Δ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 = diag(δp1−1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF0oazdaqhaaWcbaGaemiCaaNaeGymaedabaGaeyOeI0IaeGymaedaaaaa@32BB@,... ,δpK−1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWF0oazdaqhaaWcbaGaemiCaaNaem4saSeabaGaeyOeI0IaeGymaedaaaaa@32EA@), and 

													<Emphasis Type="Italic">A</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 is a row vector that corresponds to the 

													<Emphasis Type="Italic">p</Emphasis>


													<Superscript> 

														<Emphasis
Type="Italic">th</Emphasis>

													</Superscript>
 row of 

													<Emphasis Type="Italic">A</Emphasis>
 .
												</Para>


												<Para>The posterior distribution of 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 is also a Gamma distribution given by
												</Para>


												<Para>Finally, the posterior distribution of the scale parameter 

													<Emphasis Type="Italic">β</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">δ</Emphasis>

													</Subscript>
 of Δ is given by
												</Para>


											</Section3>


											<Section3 ID="Sec_69049">

												<Heading>Mixture prior on Λ</Heading>


												<Para>West 

													<CitationRef
CitationID="B3">3</CitationRef>
 has suggested a mixture prior on the elements 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 that also induces sparsity on the factor loadings matrix Λ. Thus each element 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 has the following prior
												</Para>


												<Para>where 

													<Emphasis Type="Italic">δ</Emphasis>


													<Subscript>0</Subscript>
 is the unit point mass at zero, and 

													<Emphasis Type="Italic">π</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 indicates the probability of 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 to be different from zero. We set 

													<Emphasis Type="Italic">π</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 to 0.2 in the case of unknown connectivity and to 0 and 1 in the case the connectivity is known. An auxiliary variable is usually used to enable the calculation of the posterior probabilities. Thus, let us introduce a matrix of indicator variables 

													<Emphasis Type="Italic">Z</Emphasis>
 with each element 

													<Emphasis Type="Italic">z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 , corresponding to each element 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 . The prior probability on 

													<Emphasis Type="Italic">Z</Emphasis>
 is a product of independent Bernoulli distributions as follows
												</Para>


												<Para>The 

													<Emphasis Type="Italic">z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 variables are called 

													<Emphasis
Type="Italic">indicators</Emphasis>
 , since they indicate whether the value of 

													<Emphasis Type="Italic">λ</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 is to be drawn from the normal distribution or set to zero. That is,
												</Para>


												<Para>The posterior probability of the vector variable 

													<Emphasis Type="Italic">Z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 = ( 

													<Emphasis Type="Italic">z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>
 1
													</Subscript>
 ,..., 

													<Emphasis Type="Italic">z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pK</Emphasis>

													</Subscript>
 ) does not have a known form (see equation 8). Thus we have to calculate equation 8 for all possible configurations of 

													<Emphasis Type="Italic">Z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 and then use the multinomial probability distribution to sample a new configuration for 

													<Emphasis Type="Italic">Z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 . This is a combinatorial problem, and thus as the number of hidden variables increases, the computational cost increases exponentially.
												</Para>


												<Para>where 

													<Emphasis Type="Italic">F</Emphasis>
 [ 

													<Emphasis Type="Italic">Z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 ] denotes the submatrix of 

													<Emphasis Type="Italic">F</Emphasis>
 obtained by removing those rows of 

													<Emphasis Type="Italic">F</Emphasis>
 corresponding to 

													<Emphasis Type="Italic">z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 = 0, 

													<Emphasis Type="Italic">K</Emphasis>
 ' is the number of factors for which 

													<Emphasis Type="Italic">z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">pk</Emphasis>

													</Subscript>
 = 1, and 

													<Emphasis Type="Italic">I</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">K</Emphasis>
 '
													</Subscript>
 is the identity matrix of 

													<Emphasis Type="Italic">K</Emphasis>
 ' dimensions. We also tested a version Ws of this algorithm in which equation 8 (with 

													<Emphasis Type="Italic">K</Emphasis>
 = 1) is applied to each entry of the matrix individually, that is, without the need of a combinatorial evaluation of all possible 0,1 vectors 

													<Emphasis Type="Italic">Z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 .
												</Para>


												<Para>The posterior distribution of each row Λ 

													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 of Λ is the same as in equation 7 but 

													<Emphasis Type="Italic">F</Emphasis>
 is now replaced by 

													<Emphasis Type="Italic">F</Emphasis>
 [ 

													<Emphasis Type="Italic">Z</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">p</Emphasis>

													</Subscript>
 ] andΔp−1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHuoardaqhaaWcbaGaemiCaahabaGaeyOeI0IaeGymaedaaaaa@3185@byσλ−2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFdpWCdaqhaaWcbaGae83UdWgabaGaeyOeI0IaeGOmaidaaaaa@3231@ 

													<Emphasis Type="Italic">I</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">K'</Emphasis>

													</Subscript>
 .
												</Para>


											</Section3>


											<Section3 ID="Sec_39711">

												<Heading>Normal prior with the covariance parameter depending on Ψ</Heading>


												<Para>Let us denote each column of the Λ matrix with Λ 

													<Superscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Superscript>
 where 

													<Emphasis Type="Italic">k</Emphasis>
 = 1,..., 

													<Emphasis Type="Italic">K</Emphasis>
 . A convenient conjugate prior for Λ 

													<Superscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Superscript>
 is the Gaussian distribution. Utsugi and Kumagai 

													<CitationRef
CitationID="B14">14</CitationRef>
 set the mean of this distribution to zero and the covariance matrix toα2−1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFXoqydaqhaaWcbaGaeGOmaidabaGaeyOeI0IaeGymaedaaaaa@314E@Ψ. That is,
												</Para>


												<Para>Λ 

													<Superscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Superscript>
 ~NMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtaaa@383A@(0,α2−1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFXoqydaqhaaWcbaGaeGOmaidabaGaeyOeI0IaeGymaedaaaaa@314E@Ψ)
												</Para>


												<Para>where Ψ is the covariance of the noise. Thus, if the data are noisy then the above prior assigns large magnitude to the vector Λ 

													<Superscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Superscript>
 , while free of noise data suggest small magnitudes for Λ 

													<Superscript> 

														<Emphasis
Type="Italic">k</Emphasis>

													</Superscript>
 .
												</Para>


												<Para>The posterior distribution of the combined matrixΛ¯MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuqHBoatgaqeaaaa@2E39@= [ 

													<Emphasis Type="Italic">μ</Emphasis>
 , Λ] (see equation 6 for the prior on 

													<Emphasis Type="Italic">μ</Emphasis>
 ) is given
												</Para>


												<Para>by</Para>


												<Para>where ⊗ is the Kronecker tensor product,</Para>


												<Para>and 

													<Emphasis Type="Italic">I</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">K</Emphasis>

													</Subscript>
 is a 

													<Emphasis Type="Italic">K</Emphasis>
 dimensional vector of ones.
												</Para>


												<Para>Moreover, Utsugi and Kumagai 

													<CitationRef
CitationID="B14">14</CitationRef>
 suggest the use of a Gamma hyperprior on the parameters 

													<Emphasis Type="Italic">α</Emphasis>


													<Subscript>1</Subscript>
 and 

													<Emphasis Type="Italic">α</Emphasis>


													<Subscript>2</Subscript>
 . That is,
												</Para>


												<Para>The posterior distributions of those hyperparameters are also Gamma distributions given by</Para>


											</Section3>


										</Section2>


										<Section2 ID="Sec_80531">

											<Heading>Noise covariance matrix Ψ</Heading>


											<Para>A convenient conjugate prior is assigned to the inverse of the noise covariance matrix Ψ so that its posterior distribution has a known form. Thus, the prior on eachψp−2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFipqEdaqhaaWcbaGaemiCaahabaGaeyOeI0IaeGOmaidaaaaa@31F6@is a Gamma distribution given by</Para>


											<Para>The Gamma posterior distribution of 

												<Emphasis Type="Italic">ψ</Emphasis>


												<Superscript>-2</Superscript>
 in West 

												<CitationRef
CitationID="B3">3</CitationRef>
 , Sabatti and James 

												<CitationRef
CitationID="B4">4</CitationRef>
 , and Fokoue 

												<CitationRef
CitationID="B16">16</CitationRef>
 is given by
											</Para>


											<Para>where</Para>


											<Para>While the Gamma posterior distribution of 

												<Emphasis Type="Italic">ψ</Emphasis>


												<Superscript>-2</Superscript>
 in Utsugi and Kumagai 

												<CitationRef
CitationID="B14">14</CitationRef>
 has a more complicated form since Ψ is tied to the covariance matrices of both 

												<Emphasis Type="Italic">μ</Emphasis>
 , and Λ. Thus, it has the following form
											</Para>


											<Para>where</Para>


											<Para>where 

												<Emphasis Type="Italic">I</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">p</Emphasis>

												</Subscript>
 is the identity matrix of 

												<Emphasis Type="Italic">P</Emphasis>
 × 

												<Emphasis Type="Italic">P</Emphasis>
 dimensions and
											</Para>


											<Para>West 

												<CitationRef
CitationID="B3">3</CitationRef>
 , and Sabatti and James 

												<CitationRef
CitationID="B4">4</CitationRef>
 use a common variance 

												<Emphasis Type="Italic">ψ</Emphasis>


												<Superscript>-2</Superscript>
 for all dimensions 

												<Emphasis Type="Italic">P</Emphasis>
 , while Ghahramani and Hinton 

												<CitationRef
CitationID="B13">13</CitationRef>
 , Utsugi and Kumagai 

												<CitationRef
CitationID="B14">14</CitationRef>
 , and Fokoue 

												<CitationRef
CitationID="B16">16</CitationRef>
 allow the model to estimate a different varianceψp−2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFipqEdaqhaaWcbaGaemiCaahabaGaeyOeI0IaeGOmaidaaaaa@31F6@in each dimension 

												<Emphasis Type="Italic">p</Emphasis>
 . We also suggest the use of a second level of hyperpriors on the scale parameter of 

												<Emphasis Type="Italic">ψ</Emphasis>


												<Superscript>-2</Superscript>
 since it gives a greater flexibility to the model. The cost of this greater flexibility is that more parameters have to be estimated, but this disadvantage is compensated for by the easier assignment of the hyperparameters and the better estimation of the noise covariance matrix. We assign a Gamma prior on 

												<Emphasis Type="Italic">β</Emphasis>


												<Subscript>Ψ</Subscript>
 with parametersαβΨMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaiiGacqWFXoqydaWgaaWcbaGae8NSdi2aaSbaaWqaaiabfI6azbqabaaaleqaaaaa@31E1@and1/ββΨMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqaIXaqmcqGGVaWliiGacqWFYoGydaWgaaWcbaGae8NSdi2aaSbaaWqaaiabfI6azbqabaaaleqaaaaa@33B9@. The posterior distribution is given by
											</Para>


										</Section2>


										<Section2 ID="Sec_87342">

											<Heading>Rotation of Λ matrix</Heading>


											<Para>We are usually interested in those rotations that result in interpretable factor loadings matrix. For example, a matrix that has as few nonzero loadings as possible. In a biological context that means that each gene is regulated by a small number of TFs. The algorithms of West 

												<CitationRef
CitationID="B3">3</CitationRef>
 and Fokoue 

												<CitationRef
CitationID="B16">16</CitationRef>
 implicitly look for sparse matrices. However, this is not true for the classical FA algorithm and the algorithms of Ghahramani and Hinton 

												<CitationRef
CitationID="B13">13</CitationRef>
 , and Utsugi and Kumagai 

												<CitationRef
CitationID="B14">14</CitationRef>
 . As shown in the results section, the performance of these algorithms can be improved by applying an additional orthogonal rotation 

												<Emphasis Type="Italic">Q</Emphasis>
 on the learned factor loadings matrix that leads to a sparse one Λ 

												<Subscript> 

													<Emphasis
Type="Italic">rot</Emphasis>

												</Subscript>
 , Λ 

												<Subscript> 

													<Emphasis
Type="Italic">rot</Emphasis>

												</Subscript>
 = Λ 

												<Emphasis Type="Italic">Q</Emphasis>
 .
											</Para>


											<Para>Since different orthogonal rotation methods have different constraints as we discuss next, they can lead to different factor loadings matrix. Thus, a unique solution can not be achieved if a prior information regarding the position of the zeros in the factor loadings matrix is not given.</Para>


											<Para>A number of metrics can be used as a measure of sparsity. For example, the 

												<Emphasis
Type="Italic">varimax</Emphasis>
 rotation 

												<CitationRef
CitationID="B21">21</CitationRef>
 maximizes the row variances of the squares of the loadings.
											</Para>


											<Para>Similarly, the 

												<Emphasis
Type="Italic">quartimax</Emphasis>
 rotation maximizes the column variances of the squares of the loadings (using that the sum of squares along columns is constant).
											</Para>


											<Para>The 

												<Emphasis
Type="Italic">equamax</Emphasis>
 rotation is something between the varimax and quartimax rotation and gives better results for dense matrices.
											</Para>


											<Para>We suggest a new method, the tanh rotation. It penalizes small deviations from zero but keeps the penalty constant for values far from zero.</Para>


											<Para>where the parameter 

												<Emphasis Type="Italic">α</Emphasis>
 determines the steepness of the tanh function.
											</Para>


											<Para>Finally, the 

												<Emphasis
Type="Italic">procrustes</Emphasis>
 rotation 

												<CitationRef
CitationID="B22">22</CitationRef>
 results in a factor loadings matrix Λ 

												<Subscript> 

													<Emphasis
Type="Italic">rot</Emphasis>

												</Subscript>
 by minimizing the sum of squared differences to a target matrix 

												<Emphasis Type="Italic">T</Emphasis>
 ,
											</Para>


											<Para>Thus, if the true factor loadings matrix is known, the procrustes method can be used to identify the best possible rotation. However, since this is not usually true for real data, the procrustes method can be used, for example, when assessing FA methods on synthetic data. That is, in this case the target matrix is the true matrix that we try to infer.</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_78125">

										<Heading>Authors' contributions</Heading>


										<Para>Both IP and LW contributed to this paper, and also read and approved the final manuscript.</Para>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/1471-2105/8/61"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Ming</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Abuja</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Kriegman</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">Face detection using mixtures of linear subspaces</ArticleTitle>


												<JournalTitle>Proceedings Fourth International Conference on Automatic Face and Gesture Recognition</JournalTitle>


												<VolumeID>4</VolumeID>


												<FirstPage>70</FirstPage>


												<LastPage>76</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>O</Initials>


													<FamilyName>Aguilar</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>West</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">Bayesian dynamic factor models and portfolio allocation</ArticleTitle>


												<JournalTitle>Journal of Business and Economic Statistics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>338</FirstPage>


												<LastPage>357</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>West</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Bayesian factor regression models in the "Large p, Small n" paradigm</ArticleTitle>


												<JournalTitle>Bayesian statistics</JournalTitle>


												<VolumeID>7</VolumeID>


												<FirstPage>733</FirstPage>


												<LastPage>742</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Sabatti</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>James</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Bayesian sparse hidden components analysis for transcription regulation networks</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>22</VolumeID>


												<FirstPage>739</FirstPage>


												<LastPage>746</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Liao</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Boscolo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Tran</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Sabatti</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Roychowdhury</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Network componenet analysis: Reconstruction of regulatory signals in biological systems</ArticleTitle>


												<JournalTitle>PNAS</JournalTitle>


												<VolumeID>100</VolumeID>


												<FirstPage>15522</FirstPage>


												<LastPage>15527</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Kao</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Yang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Boscolo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Sabatti</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Roychowdhury</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Liao</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Transcriptome-based determination of multiple transcription regulator activities in Escherichia coli by using network component analysis</ArticleTitle>


												<JournalTitle>PNAS</JournalTitle>


												<VolumeID>101</VolumeID>


												<FirstPage>641</FirstPage>


												<LastPage>646</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Tran</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Brynildsen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Kao</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Suen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Liao</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">gNCA: A framework for determining transcription factor activity based on transcriptome: identiflability and numerical implementation</ArticleTitle>


												<JournalTitle>Metabolic Engineering</JournalTitle>


												<VolumeID>7</VolumeID>


												<FirstPage>128</FirstPage>


												<LastPage>141</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>AL</Initials>


													<FamilyName>Boulesteix</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Strimmer</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Predicting transcription factor activities from combined analysis of microarray and ChIP data: a partial least squares approach</ArticleTitle>


												<JournalTitle>Theoretical Biology and Medical Modelling</JournalTitle>


												<VolumeID>2</VolumeID>


												<FirstPage>23</FirstPage>


												<LastPage>34</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Liebermeister</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Linear modes of gene expression determined by independent component analysis</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>51</FirstPage>


												<LastPage>60</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>AM</Initials>


													<FamilyName>Martoglio</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Miskin</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Smith</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>MacKay</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">A decomposition model to track gene expression signatures: preview on observer-independent classification of ovarian cancer</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>1617</FirstPage>


												<LastPage>1624</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Frigyesi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Veerla</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Lindgren</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Höglund</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Independent component analysis reveals new and biologically significant structures in microarray data</ArticleTitle>


												<JournalTitle>BMC Bioinformatics</JournalTitle>


												<VolumeID>7</VolumeID>


												<FirstPage>290</FirstPage>


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Hinton</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Dayan</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Revow</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">Modelling the manifolds of images of handwritten digits</ArticleTitle>


												<JournalTitle>IEEE transactions on Neural Networks</JournalTitle>


												<VolumeID>8</VolumeID>


												<FirstPage>65</FirstPage>


												<LastPage>74</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Ghahramani</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Hinton</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">The EM algorithm for mixtures of factor analyzers</ArticleTitle>


												<JournalTitle>Technical Report CRG-TR-96-1</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Utsugi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Kumagai</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">Bayesian analysis of mixtures of factor analyzers</ArticleTitle>


												<JournalTitle>Neural Computation</JournalTitle>


												<VolumeID>13</VolumeID>


												<FirstPage>993</FirstPage>


												<LastPage>1002</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR15">

											<CitationNumber>15.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Sabatti</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Rohlin</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Lange</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Liao</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Vocabulon: a dictionary model approach for reconstruction and localization of transcription factor binding sites</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>922</FirstPage>


												<LastPage>931</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR16">

											<CitationNumber>16.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Fokoue</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Stochastic determination of the intrinsic structure in Bayesian factor analysis</ArticleTitle>


												<JournalTitle>Technical Report 17</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR17">

											<CitationNumber>17.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>I</Initials>


													<FamilyName>Pournara</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Reconstructing gene regulatory networks by passive and active Bayesian learning</ArticleTitle>


												<JournalTitle>PhD thesis</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR18">

											<CitationNumber>18.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Salgado</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Santos-Zavaleta</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Gama-Castro</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Millan-Zarate</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Diaz-Peredo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>F</Initials>


													<FamilyName>Sanchez-Solano</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Prez-Rueda</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>C</Initials>


													<FamilyName>Bonavides-Martinez</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Collado-Vides</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">RegulonDB (version 3.2): transcriptional regulation and operon organization in Escherichia coli K-12</ArticleTitle>


												<JournalTitle>Nucleic Acids Res</JournalTitle>


												<VolumeID>29</VolumeID>


												<FirstPage>72</FirstPage>


												<LastPage>74</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR19">

											<CitationNumber>19.</CitationNumber>


											<BibArticle>

												<Year />


												<ArticleTitle
Language="En">Factor analysis EM software</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR20">

											<CitationNumber>20.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tipping</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">Sparse Bayesian Learning and the Relevance Vector Machine</ArticleTitle>


												<JournalTitle>Journal of Machine Learning Research</JournalTitle>


												<VolumeID>1</VolumeID>


												<FirstPage>211</FirstPage>


												<LastPage>244</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR21">

											<CitationNumber>21.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Kaiser</FamilyName>


												</BibAuthorName>


												<Year>1958</Year>


												<ArticleTitle
Language="En">The varimax criterion for analytic rotation in factor analysis</ArticleTitle>


												<JournalTitle>Psychometrika</JournalTitle>


												<VolumeID>23</VolumeID>


												<FirstPage>187</FirstPage>


												<LastPage>200</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR22">

											<CitationNumber>22.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Schönemann</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Carroll</FamilyName>


												</BibAuthorName>


												<Year>1970</Year>


												<ArticleTitle
Language="En">Fitting one matrix to another under choice of a central dilation and a rigid motion</ArticleTitle>


												<JournalTitle>Psychometrika</JournalTitle>


												<VolumeID>35</VolumeID>


												<FirstPage>245</FirstPage>


												<LastPage>256</LastPage>


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:Authors>

						<meta:Author>Pournara, Iosifina</meta:Author>

						<meta:Author>Wernisch, Lorenz</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution geo="-0.0889470,51.5133300,0">

							<meta:OrgName>University of London</meta:OrgName>

							<meta:GeoOrg>-0.0889470,51.5133300,0#University of London</meta:GeoOrg>

							<meta:Country>United Kingdom</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2007-02-23</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1471-2105-8-61</meta:DOI>

					<meta:Title>Factor analysis for gene regulatory networks and transcription factor activity profiles</meta:Title>

					<meta:ISXN>1471-2105</meta:ISXN>

					<meta:Journal>BMC Bioinformatics</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>61</meta:ArticleFirstPage>

					<meta:Publication>BMC Bioinformatics</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Bioinformatics</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Microarrays</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Combinatorial Libraries</meta:Subject>

						<meta:Subject Priority="6"
Type="Secondary">Algorithms</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images>

				<Image Id="5-10.1186_1471-2105-8-61-1" xml:lang="en"
language="en">

					<Caption>

						<p>Distributions of genes and TFs for the simulated networks</p>


					</Caption>

					<FullText>

						<p>
Figure 2 shows the distributions of the genes and TFs for three
networks with densities of 15, 25 and 40 percent.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-2.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-3" xml:lang="en"
language="en">

					<Caption>

						<p>Convergence test and processing time</p>


					</Caption>

					<FullText>

						<p>

							<p>Figure 4(a) shows the change in the log likelihood for a chosen
representative run over 3000 cycles of the Gibbs sampling for
algorithms U, F and W.</p>


						</p>

						<p>
Finally, Figure 4(b) shows the average time consumed by each
algorithm.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-4.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-4" xml:lang="en"
language="en">

					<Caption>

						<p>Reconstruction of the factor loadings matrix for the Hemoglobin data</p>


					</Caption>

					<FullText>

						<p>

							<p>As shown in Figure 5(a) , the MSE in the estimation of Λ is
approximately equal for all algorithms except GNCA 

								<sub>r</sub>
 ,
and it is very similar before and after procrustes rotation.
							</p>


						</p>

						<p>
Figure 5(b) shows the MSE in the estimation of the factor profiles,
and these profiles are plotted in Figure 6 .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-5.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-5" xml:lang="en"
language="en">

					<Caption>

						<p>Reconstruction of the factors matrix for the Hemoglobin data</p>


					</Caption>

					<FullText>

						<p>
Figure 5(b) shows the MSE in the estimation of the factor profiles,
and these profiles are plotted in Figure 6 .
						</p>

						<p>
It also demonstrates that it is now harder to reconstruct the
factor profiles, as seen in the greater variability of profiles
from different MCMC runs when compared to Figure 6 .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-6.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-7" xml:lang="en"
language="en">

					<Caption>

						<p>Reconstruction of the factors matrix for the Hemoglobin data</p>


					</Caption>

					<FullText>

						<p>
This is also apparent by looking at the factor profiles (Figure 8
).
						</p>

						<p>

							<p>Figure 8 shows the reconstructed factor profiles after varimax
rotation on the Λ without using prior information on nonzero
entries.</p>


						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-8.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-9" xml:lang="en"
language="en">

					<Caption>

						<p>Reconstruction of the factor loadings matrix for the E. coli data</p>


					</Caption>

					<FullText>

						<p>
Figure 10(a) shows a ROC curve for each algorithm.
						</p>

						<p>
Figure 10(a) also shows a ROC curve that is based on merging the
information gain by each algorithm.
						</p>

						<p>

							<p>We also plot, in Figure 10(b) , the ROC curve of each algorithm
after applying procrustes rotation to the factor loadings
matrix.</p>


						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-10.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-0" xml:lang="en"
language="en">

					<Caption>

						<p>Factor loadings matrix of the E. coli network</p>


					</Caption>

					<FullText>

						<p>
The matrix is reproduced in Figure 1(a) .
						</p>

						<p>
It is very sparse with most genes regulated by 1 to 3 TFs, and with
only a few TFs regulating a larger number of genes as shown in
Figures 1(b) and 1(c) ..
						</p>

						<p>
coli network of Figure 1 ..
						</p>

						<p>
For evaluating the learned loadings matrix, we treat the Kao
connectivity matrix (Figure 1(a) ) as showing true interactions and
true missing interactions.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-1.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-2" xml:lang="en"
language="en">

					<Caption>

						<p>Evaluation of the FA algorithms on E. coli simulated networks</p>


					</Caption>

					<FullText>

						<p>

							<p>Figure 3(a) shows the MSE for the Λ matrix for all the FA
algorithms and for different network densities.</p>


						</p>

						<p>

							<p>Figure 3(a) also shows the MSE for the varimax and procrustes
rotated matrix Λ 

								<sub>rot</sub>
 .
							</p>


						</p>

						<p>
Figure 3(b) shows again that algorithms Z and U perform similarly
regardless of the number of cases in the dataset.
						</p>

						<p>

							<p>Figure 3(c) shows the results for different values of snr .</p>


						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-3.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-6" xml:lang="en"
language="en">

					<Caption>

						<p>Reconstruction of the factor loadings matrix for the Hemoglobin data</p>


					</Caption>

					<FullText>

						<p>
Figure 7(a) shows the MSE of Λ as given by each algorithm.
						</p>

						<p>
However, comparing the MSE of the factors (Figure 7(b) ) its
performance is worse.
						</p>

						<p>

							<p>Figure 7(a) shows the result for algorithm F when entries of the
loadings matrix are restricted to stay close to 0 by a strong prior
(shape parameter 10 for δ 

								<sub>pk</sub>
 , scale parameter 0.01, see
methods section).
							</p>


						</p>

						<p>
As shown in Figure 7(c) and 7(d) , this setting (Fu) performs
better, but once the loadings matrix is rotated, the improvement is
not as significant.
						</p>

						<p>
As expected, since the connectivity is not sparse for the
hemoglobin data, the difference is small (Figure 7(c) ).
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-7.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-8" xml:lang="en"
language="en">

					<Caption>

						<p>Reconstruction of the factor profiles for the E. coli data</p>


					</Caption>

					<FullText>

						<p>

							<p>Figure 9(a) shows that all the algorithms produce very similar
TF profiles, that is, given the connectivity matrix, FA algorithms
reconstruct TF profiles as well as GNCA.</p>


						</p>

						<p>

							<p>Figure 9(b) shows the results when no prior information on
connectivity is provided.</p>


						</p>

						<p>
For comparison, we match the resulting TF profiles with those of
GNCA by minimum MSE and add the plots of the GNCA TF profiles from
Figure 9(a) .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-61-9.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-61-10" xml:lang="en"
language="en">

					<Caption>

						<p>MSEs of the reconstructed factor profiles for the E. coli data</p>


					</Caption>

					<FullText>

						<p>
The second column in Table 1 shows the MSE deviation of profiles of
algorithms F and Ws from profiles of GNCA which uses information on
activation and inhibition.
						</p>

						<p>
The third column in Table 1 shows the MSE deviation of profiles of
algorithms Z, U, F and Ws from profiles of GNCA with activation and
inhibition information.
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T1">

							<Caption Language="En">

								<CaptionNumber>Table 1</CaptionNumber>


								<CaptionContent>

									<SimplePara>MSEs of the reconstructed factor profiles for the E. coli data</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="3">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">algorithms</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis
Type="Bold">MSE (with prior information)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis
Type="Bold">MSE (without prior information)</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Z</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>-</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.017</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>U</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>-</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.008</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>F</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.005</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.010</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>Ws</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.003</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>0.014</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>S</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.020</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>-</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>GNCA</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>0.0004</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>-</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>MSEs of the reconstructed factor matrices from the factor matrix obtained from GNCA with activation and inhibition information. The second column contains the MSEs when the zero positions in the loadings matrix are fixed. The third column contains the MSEs when no information regarding those positions is given. – indicates that the algorithm was not tested.</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<Authors>

						<Author>Pournara, Iosifina</Author>

						<Author>Wernisch, Lorenz</Author>

					</Authors>

					<Institutions>

						<Institution>School of Crystallography, Birkbeck College, University of London, London, UK</Institution>

					</Institutions>

					<ArticleTitle>Factor analysis for gene regulatory networks and transcription factor activity profiles</ArticleTitle>

					<DOI>10.1186/1471-2105-8-61</DOI>

					<PubDate>2007-02-23</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>gene</Keyword>

						<Keyword>regulatory</Keyword>

						<Keyword>Factor</Keyword>

						<Keyword>profiles</Keyword>

						<Keyword>factor</Keyword>

						<Keyword>analysis</Keyword>

						<Keyword>activity</Keyword>

						<Keyword>transcription</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Pournara and Wernisch; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-61.xml</ArticleURI>

					<Provider>Springer</Provider>

					<DateLoaded>2009-11-26T18:11:36.684Z</DateLoaded>

				</Image>

			</Images>

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>


					<PublisherName>Springer US</PublisherName>



					<PublisherLocation>Boston</PublisherLocation>



					<PublisherURL>http://www.springer-ny.com</PublisherURL>


				</PublisherInfo>

				<Journal OutputMedium="All">


					<JournalInfo
JournalProductType="NonStandardArchiveJournal" NumberingStyle="ContentOnly">


						<JournalID>11265</JournalID>



						<JournalPrintISSN>1939-8018</JournalPrintISSN>



						<JournalElectronicISSN>1939-8115</JournalElectronicISSN>



						<JournalTitle>Journal of Signal Processing Systems</JournalTitle>



						<JournalSubTitle>for Signal, Image, and Video Technology
(formerly the Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology)</JournalSubTitle>



						<JournalAbbreviatedTitle>J  Sign Process Syst Sign Image Video Technol</JournalAbbreviatedTitle>



						<JournalSubjectGroup>


							<JournalSubject
Type="Primary">Engineering</JournalSubject>



							<JournalSubject
Type="Secondary">Computer Imaging, Vision, Pattern Recognition and Graphics</JournalSubject>



							<JournalSubject
Type="Secondary">Pattern Recognition</JournalSubject>



							<JournalSubject
Type="Secondary">Image Processing and Computer Vision</JournalSubject>



							<JournalSubject
Type="Secondary">Electrical Engineering</JournalSubject>



							<JournalSubject
Type="Secondary">Circuits and Systems</JournalSubject>



							<JournalSubject
Type="Secondary">Signal, Image and Speech Processing</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>



					<Volume OutputMedium="All">


						<VolumeInfo TocLevels="0" VolumeType="Regular">


							<VolumeIDStart>60</VolumeIDStart>



							<VolumeIDEnd>60</VolumeIDEnd>



							<VolumeIssueCount>3</VolumeIssueCount>


						</VolumeInfo>



						<Issue IssueType="Regular" OutputMedium="All">


							<IssueInfo IssueType="Regular" TocLevels="0">


								<IssueIDStart>1</IssueIDStart>



								<IssueIDEnd>1</IssueIDEnd>



								<IssueArticleCount>10</IssueArticleCount>



								<IssueHistory>


									<OnlineDate>


										<Year>2010</Year>



										<Month>7</Month>



										<Day>15</Day>


									</OnlineDate>



									<PrintDate>


										<Year>2010</Year>



										<Month>4</Month>



										<Day>14</Day>


									</PrintDate>



									<CoverDate>


										<Year>2010</Year>



										<Month>7</Month>


									</CoverDate>



									<PricelistYear>2010</PricelistYear>


								</IssueHistory>



								<IssueCopyright>


									<CopyrightHolderName>Springer Science+Business Media, LLC</CopyrightHolderName>



									<CopyrightYear>2010</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>



							<Article ID="s11265-009-0405-9" OutputMedium="All">


								<ArticleInfo ArticleCitation="ArticleFirstPage"
ArticleType="OriginalPaper" ContainsESM="No" Language="En"
NumberingStyle="ContentOnly" TocLevels="0">


									<ArticleID>405</ArticleID>



									<ArticleDOI>10.1007/s11265-009-0405-9</ArticleDOI>



									<ArticleSequenceNumber>7</ArticleSequenceNumber>



									<ArticleTitle Language="En"
OutputMedium="All">On the Performance Analysis of the Least Mean M-Estimate and Normalized Least Mean M-Estimate Algorithms with Gaussian Inputs and Additive Gaussian and Contaminated Gaussian Noises</ArticleTitle>



									<ArticleFirstPage>81</ArticleFirstPage>



									<ArticleLastPage>103</ArticleLastPage>



									<ArticleHistory>


										<RegistrationDate>


											<Year>2009</Year>



											<Month>8</Month>



											<Day>25</Day>


										</RegistrationDate>



										<Received>


											<Year>2009</Year>



											<Month>3</Month>



											<Day>7</Day>


										</Received>



										<Revised>


											<Year>2009</Year>



											<Month>8</Month>



											<Day>24</Day>


										</Revised>



										<Accepted>


											<Year>2009</Year>



											<Month>8</Month>



											<Day>24</Day>


										</Accepted>



										<OnlineDate>


											<Year>2009</Year>



											<Month>10</Month>



											<Day>17</Day>


										</OnlineDate>


									</ArticleHistory>



									<ArticleCopyright>


										<CopyrightHolderName>Springer Science+Business Media, LLC</CopyrightHolderName>



										<CopyrightYear>2009</CopyrightYear>


									</ArticleCopyright>



									<ArticleGrants Type="OpenChoice">


										<MetadataGrant Grant="OpenAccess" />



										<AbstractGrant Grant="OpenAccess" />



										<BodyPDFGrant Grant="OpenAccess" />



										<BodyHTMLGrant Grant="OpenAccess" />



										<BibliographyGrant Grant="OpenAccess" />



										<ESMGrant Grant="OpenAccess" />


									</ArticleGrants>


								</ArticleInfo>



								<ArticleHeader>


									<AuthorGroup>


										<Author AffiliationIDS="Aff1">


											<AuthorName DisplayOrder="Western">


												<GivenName>S.</GivenName>



												<GivenName>C.</GivenName>



												<FamilyName>Chan</FamilyName>


											</AuthorName>



											<Contact>


												<Email>scchan@eee.hku.hk</Email>


											</Contact>



											<Biography>


												<FormalPara RenderingStyle="Style1">


													<Heading>S. C. Chan</Heading>



													<Para>(S’87–M’92) received the B.Sc. (Eng.) and Ph.D. degrees from the University of Hong Kong in 1986 and 1992, respectively. He joined City Polytechnic of Hong Kong in 1990 as an Assistant Lecturer and later as a University Lecturer. Since 1994, he has been with the Department of Electrical and Electronic Engineering, the University of Hong Kong, and is now a Professor. He held visiting positions in Microsoft Corporation, Redmond, USA, Microsoft Research Asia, University of Texas at Arlington, and Nanyang Technological University. Dr. Chan’s main research area is signal processing and applications. His previous research covers fast transform algorithms, filter design and realization, multirate signal processing, adaptive and communications signal processing, data compression and image-based rendering. Dr. Chan has served in a number of professional and administrative committees. He is currently a member of the Digital Signal Processing Technical Committee of the IEEE Circuits and Systems Society, Associate Editors for IEEE Trans. Circuits and Systems I and Journal of Signal Processing systems, and was the Chairman of the IEEE HK Chapter of Signal Processing. He was the special session chairman of ICASSP2003 and is an organizing committee member of the IEEE 2010 ICIP.

														<Figure Category="Standard"
Float="No" ID="Figa">


															<MediaObject ID="MO1">


																<ImageObject
Color="BlackWhite" FileRef="MediaObjects/11265_2009_405_Figa_HTML.jpg"
Format="JPEG" Rendition="HTML" Type="Halftone" />


															</MediaObject>


														</Figure>


													</Para>


												</FormalPara>


											</Biography>


										</Author>



										<Author AffiliationIDS="Aff1"
CorrespondingAffiliationID="Aff1">


											<AuthorName DisplayOrder="Western">


												<GivenName>Y.</GivenName>



												<FamilyName>Zhou</FamilyName>


											</AuthorName>



											<Contact>


												<Email>yizhou@eee.hku.hk</Email>


											</Contact>



											<Biography>


												<FormalPara RenderingStyle="Style1">


													<Heading>Y. Zhou</Heading>



													<Para>received the B.Sc., M.Sc. degrees in 1997 and 1999 respectively, both in control engineering from Harbin Institute of Technology, Harbin, China, and Ph.D. degree in electrical engineering in 2006, from the University of Hong Kong, Hong Kong, China. From July 2006 to May 2009, he was with the Department of Electrical and Electronic Engineering, the University of Hong Kong as a postdoctoral research fellow. His research interests are adaptive signal processing, system identification, statistical signal processing and estimation theory. Dr. Zhou joined the Key Laboratory of Noise and Vibration Research, Institute of Acoustics, Chinese Academy of Sciences in August 2009, where he is now an associate professor and engaged in research on statistical signal processing theory and techniques applied to acoustics. E-mail: yizhou@mail.ioa.ac.cn

														<Figure Category="Standard"
Float="No" ID="Figb">


															<MediaObject ID="MO2">


																<ImageObject
Color="Color" FileRef="MediaObjects/11265_2009_405_Figb_HTML.jpg"
Format="JPEG" Rendition="HTML" Type="Halftone" />


															</MediaObject>


														</Figure>


													</Para>


												</FormalPara>


											</Biography>


										</Author>



										<Affiliation ID="Aff1">


											<OrgDivision>Department of Electrical and Electronic Engineering</OrgDivision>



											<OrgName>The University of Hong Kong</OrgName>



											<OrgAddress>


												<Street>Pokfulam Road</Street>



												<City>Hong Kong</City>



												<Country>Hong Kong</Country>


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>



									<Abstract ID="Abs1" Language="En"
OutputMedium="All">


										<Heading>Abstract</Heading>



										<Para>This paper studies the convergence analysis of the least mean M-estimate (LMM) and normalized least mean M-estimate (NLMM) algorithms with Gaussian inputs and additive Gaussian and contaminated Gaussian noises. These algorithms are based on the M-estimate cost function and employ error nonlinearity to achieve improved robustness in impulsive noise environment over their conventional LMS and NLMS counterparts. Using the Price’s theorem and an extension of the method proposed in Bershad (

											<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing</Emphasis>
, ASSP-34(4), 793–806, 

											<CitationRef
CitationID="CR17">1986</CitationRef>
; 

											<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing</Emphasis>
, 35(5), 636–644, 

											<CitationRef
CitationID="CR18">1987</CitationRef>
), we first derive new expressions of the decoupled difference equations which describe the mean and mean square convergence behaviors of these algorithms for Gaussian inputs and additive Gaussian noise. These new expressions, which are expressed in terms of the generalized Abelian integral functions, closely resemble those for the LMS algorithm and allow us to interpret the convergence performance and determine the step size stability bound of the studied algorithms. Next, using an extension of the Price’s theorem for Gaussian mixture, similar results are obtained for additive contaminated Gaussian noise case. The theoretical analysis and the practical advantages of the LMM/NLMM algorithms are verified through computer simulations.
										</Para>


									</Abstract>



									<KeywordGroup Language="En" OutputMedium="All">


										<Heading>Keywords</Heading>



										<Keyword>Adaptive filtering</Keyword>



										<Keyword>Robust statistics</Keyword>



										<Keyword>Least mean square/M-estimate</Keyword>



										<Keyword>Impulsive noise</Keyword>


									</KeywordGroup>



									<ArticleNote Type="PresentedAt">


										<SimplePara>Part of this work was presented at 

											<Emphasis
Type="Italic">IEEE ISSPIT</Emphasis>
 2007 [46].
										</SimplePara>


									</ArticleNote>


								</ArticleHeader>



								<Body>


									<Section1 ID="Sec1" Type="Introduction">


										<Heading>Introduction</Heading>



										<Para>Adaptive filters are frequently employed to handle filtering problems in which the statistics of the underlying signals are either unknown 

											<Emphasis Type="Italic">a priori</Emphasis>
, or in some cases, slowly-varying. Many adaptive filtering algorithms have been proposed and they are usually variants of the well known least mean square (LMS) [

											<CitationRef
CitationID="CR1">1</CitationRef>
] and the recursive least squares (RLS) [

											<CitationRef
CitationID="CR2">2</CitationRef>
] algorithms. An important variant of the LMS algorithm is the normalized least mean square (NLMS) algorithm [

											<CitationRef
CitationID="CR3">3</CitationRef>
, 

											<CitationRef
CitationID="CR4">4</CitationRef>
], where the step size is normalized with respect to the energy of the input vector. Due to the numerical stability and computational simplicity of the LMS and the NLMS algorithms, they have been widely used in various applications [

											<CitationRef
CitationID="CR5">5</CitationRef>
].
										</Para>



										<Para>Other studies on the variants of the LMS and NLMS algorithms have also been very active, among which their robust counterparts in impulsive noise environment are of great practical interests. This is because their performances will deteriorate significantly when the additive noise is of impulsive nature [

											<CitationRef
CitationID="CR6">6</CitationRef>
, 

											<CitationRef
CitationID="CR7">7</CitationRef>
], as both the LMS and NLMS algorithms assume that the additive noise is Gaussian distributed. Considerable efforts have been devoted to combat this adverse effect of impulsive noise on various adaptive filtering algorithms [

											<CitationRef
CitationID="CR7">7</CitationRef>
–

											<CitationRef
CitationID="CR12">12</CitationRef>
]. Among them, the least mean M-estimate (LMM) [

											<CitationRef
CitationID="CR7">7</CitationRef>
] algorithm, and its step size normalized version, the normalized least mean M-estimate (NLMM) algorithm are two efficient generalizations of the LMS family. Like the recursive least M-estimate (RLM) algorithm [

											<CitationRef
CitationID="CR12">12</CitationRef>
], which is a robust variant of the RLS algorithm, they are derived from the concept of robust statistics techniques [

											<CitationRef
CitationID="CR13">13</CitationRef>
] where the M-estimate function [

											<CitationRef
CitationID="CR13">13</CitationRef>
] is minimized instead of the MSE to improve the robustness to impulsive noise. A common and important feature of these robust algorithms is the use of error nonlinearity to suppress the adverse effect of impulsive errors with large amplitude.
										</Para>



										<Para>Because of the importance of these adaptive filtering algorithms, their convergence performance analyses have been long standing research problems. The convergence behavior of the LMS algorithm for Gaussian inputs was thoroughly studied in the classical work of Widrow et al. [

											<CitationRef
CitationID="CR1">1</CitationRef>
], in which the widely used concept of independence assumption was first introduced. Other related studies of the LMS algorithm with independent Gaussian inputs include [

											<CitationRef
CitationID="CR14">14</CitationRef>
–

											<CitationRef
CitationID="CR16">16</CitationRef>
]. On the other hand, the NLMS algorithm generally possesses an improved convergence speed over the LMS algorithm, but its analysis is more complicated due to the step size normalization. In [

											<CitationRef
CitationID="CR17">17</CitationRef>
] and [

											<CitationRef
CitationID="CR18">18</CitationRef>
], the mean and mean square behaviors of the NLMS algorithm for Gaussian inputs were studied. Analysis for independent Gaussian inputs in [

											<CitationRef
CitationID="CR19">19</CitationRef>
] also revealed some of the advantages of the NLMS algorithm over the LMS algorithm. Due to the difficulties in evaluating the expectations involved in the difference equations for the mean weight-error vector and its covariance matrix, and hence in deriving the general expressions for these equations, the works in [

											<CitationRef
CitationID="CR17">17</CitationRef>
] and [

											<CitationRef
CitationID="CR18">18</CitationRef>
] only concentrated on certain special cases of eigenvalue distribution of the input autocorrelation matrix. In [

											<CitationRef
CitationID="CR20">20</CitationRef>
] and [

											<CitationRef
CitationID="CR21">21</CitationRef>
], simplified input data models were introduced to facilitate the analysis so that useful analytical expressions can still be derived. In [

											<CitationRef
CitationID="CR22">22</CitationRef>
–

											<CitationRef
CitationID="CR24">24</CitationRef>
], the averaging principle was invoked to simplify the expectations involved in the difference equations. Basically, the normalization term is assumed to vary slowly with respect to the input correlation term and the power of the input vector is assumed to be either chi-square distributed with 

											<Emphasis Type="Italic">L</Emphasis>
 degrees of freedom [

											<CitationRef
CitationID="CR22">22</CitationRef>
] or integrated otherwise [

											<CitationRef
CitationID="CR24">24</CitationRef>
]. Recently, Sayed et al. [

											<CitationRef
CitationID="CR25">25</CitationRef>
] proposed a unified framework based on the concept of energy conservation for analyzing the convergence of adaptive filtering algorithms. It has been applied to different adaptive filtering algorithms with satisfactory results [

											<CitationRef
CitationID="CR26">26</CitationRef>
, 

											<CitationRef
CitationID="CR27">27</CitationRef>
]. Other related works for the normalized sign-sign algorithm [

											<CitationRef
CitationID="CR28">28</CitationRef>
] and the NLMS algorithm in Gaussian noise environment with white input signal can be found in [

											<CitationRef
CitationID="CR29">29</CitationRef>
].
										</Para>



										<Para>Convergence behavior of the RLM algorithm in contaminated Gaussian (CG) noise [

											<CitationRef
CitationID="CR30">30</CitationRef>
] using the modified Huber (MH) error nonlinearity was studied in [

											<CitationRef
CitationID="CR12">12</CitationRef>
]. However, a detailed analysis of the LMM and NLMM algorithms both in Gaussian and CG additive noises is still unavailable, probably due to the complicated error nonlinearity and robust statistics involved, and the difficulty in analyzing the NLMS algorithm as mentioned above.
										</Para>



										<Para>In this paper, we study the performance of a class of LMS and NLMS algorithms with error nonlinearity and Gaussian inputs and additive Gaussian as well as CG noise. In particular, the LMM and NLMM algorithms with MH error nonlinearity and adaptive threshold selection (ATS) will be studied in detail. It also includes a variety of other interesting algorithms and provides valuable insights into the advantages of using ATS and the effectiveness of the LMM/NLMM algorithms in impulsive noise environment. The analysis is divided into two main parts:

											<OrderedList>


												<ListItem>


													<ItemNumber>(1)</ItemNumber>



													<ItemContent>


														<Para>We extend the framework of [

															<CitationRef
CitationID="CR17">17</CitationRef>
] and [

															<CitationRef
CitationID="CR18">18</CitationRef>
] to analyze the NLMS algorithm with a class of error nonlinearity, which we called the M-nonlinearity, in Gaussian noise. This includes most M-estimate functions and several other commonly used error nonlinearities. Using the conventional Price’s theorem [

															<CitationRef
CitationID="CR31">31</CitationRef>
–

															<CitationRef
CitationID="CR33">33</CitationRef>
], new decoupled difference equations describing the mean and mean square convergence behaviors are derived. The final results closely resemble the classical results for LMS in [

															<CitationRef
CitationID="CR1">1</CitationRef>
] and can be easily reduced to related works such as [

															<CitationRef
CitationID="CR34">34</CitationRef>
, 

															<CitationRef
CitationID="CR35">35</CitationRef>
]. Moreover, it is found that the nonlinearity will in general slow down the adaptation rate and the normalization process will always speed up the maximum convergence rate of the NLMS algorithms over their LMS counterparts if the eigenvalues of the input autocorrelation matrices are unequal. The convergence performances of the LMM and NLMM algorithms are studied in detail. The theoretical and practical importance of ATS is also explained and analyzed.
														</Para>


													</ItemContent>


												</ListItem>



												<ListItem>


													<ItemNumber>(2)</ItemNumber>



													<ItemContent>


														<Para>Instead of using the Price’s theorem for Gaussian variates as an approximation in analyzing the RLM algorithm with MH nonlinearity in CG noise as in [

															<CitationRef
CitationID="CR12">12</CitationRef>
], we show that it is also applicable to Gaussian mixtures with very slight modification. In fact, the Price’s theorem is applicable to each independent Gaussian component as pointed out, without proof, in a short note by Price [

															<CitationRef
CitationID="CR36">36</CitationRef>
]. Using this result and those in (

															<Emphasis
Type="Bold">1</Emphasis>
) above, the mean and mean square convergence behaviors of the LMS and NLMS algorithms with M-nonlinearity in CG noise are derived. The LMM and NLMM algorithms are then analyzed in detail. The analytical results suggest that the M-nonlinearity with ATS helps to suppress the impulsive noise in exchange for a slightly slower adaptation rate than that in the Gaussian noise case.
														</Para>


													</ItemContent>


												</ListItem>


											</OrderedList>


										</Para>



										<Para>Another key to the above analysis is the introduction of certain special functions called the generalized Abelian integral functions [

											<CitationRef
CitationID="CR37">37</CitationRef>
], which are generalizations of the Abelian integral functions [

											<CitationRef
CitationID="CR38">38</CitationRef>
]. They allow us to obtain decoupled analytical formulas by evaluating the expectations involved in the difference equations and help us interpret the convergence behaviors of the NLMS algorithms with M-nonlinearity. Particularly, these analytical results can be reduced to those for the conventional NLMS algorithms in Gaussian additive noise which agree with [

											<CitationRef
CitationID="CR17">17</CitationRef>
, 

											<CitationRef
CitationID="CR18">18</CitationRef>
], except that new expressions for the excess mean square error (EMSE), stability bound and difference equations in terms of the generalized Abelian integrals are obtained. For clarity of presentation and its practical importance, this particular case is separately treated in the companion paper [

											<CitationRef
CitationID="CR37">37</CitationRef>
]. All the above results can also be readily generalized to the simpler LMS case. Moreover, the new results also agree with the conventional LMS algorithm [

											<CitationRef
CitationID="CR1">1</CitationRef>
], the LMS algorithms with dual sign (DS) nonlinearity [

											<CitationRef
CitationID="CR34">34</CitationRef>
] and the error function (EF) nonlinearity [

											<CitationRef
CitationID="CR35">35</CitationRef>
]. Monte Carlo simulation results confirm that the NLMM algorithm offers improved robustness to impulsive noise over the NLMS algorithm, and are in good agreement with the theoretical analysis.
										</Para>



										<Para>The rest of this paper is organized as follows: In Section 

											<InternalRef RefID="Sec2">2</InternalRef>
, the NLMS algorithm is briefly reviewed and the NLMM algorithm is introduced. In Section 

											<InternalRef RefID="Sec5">3</InternalRef>
, the proposed convergence performance analysis is presented. Simulation results are given in Section 

											<InternalRef RefID="Sec12">4</InternalRef>
 and conclusions are drawn in Section 

											<InternalRef RefID="Sec13">5</InternalRef>
.
										</Para>


									</Section1>



									<Section1 ID="Sec2">


										<Heading>Normalized Least Mean M-Estimate Algorithm</Heading>



										<Section2 ID="Sec3">


											<Heading>The NLMS Algorithm</Heading>



											<Para>Consider the adaptive system identification problem in Fig. 

												<InternalRef
RefID="Fig1">1</InternalRef>
 where an adaptive filter with coefficient or weight vector of order 

												<Emphasis Type="Italic">L</Emphasis>
, 

												<InlineEquation ID="IEq1">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq1.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ W(n) = {\left[ {{w_1}(n),{w_2}(n), \cdots, {w_L}(n)} \right]^T} $$</EquationSource>


												</InlineEquation>
, is used to model an unknown system with impulse response 

												<InlineEquation ID="IEq2">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq2.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ W* = {\left[ {{w_1},{w_2}, \cdots, {w_L}} \right]^T} $$</EquationSource>


												</InlineEquation>
. Here, (∙)

												<Superscript>


													<Emphasis Type="Italic">T</Emphasis>


												</Superscript>
 denotes the transpose of a vector or a matrix. The unknown system and the adaptive filter are simultaneously excited by the same input 

												<Emphasis Type="Italic">x</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
). The output of the unknown system 

												<Emphasis Type="Italic">d</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) is assumed to be corrupted by a measurement noise 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) to form the desired signal 

												<Emphasis Type="Italic">d</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) for the adaptive filter. The estimation error is given by 

												<InlineEquation ID="IEq3">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq3.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ e(n) = d(n) - y(n) $$</EquationSource>


												</InlineEquation>
. In the LMS algorithm, the MSE is minimized by updating the weight vector in the negative direction of the instantaneous gradient of the MSE with respect to the weight vector, −2

												<Emphasis Type="Italic">e</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
)

												<Emphasis Type="BoldItalic">X</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
). This gives

												<Equation ID="Equ1">


													<EquationNumber>1</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ1.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ W\left( {n + 1} \right) = W(n) + \mu e(n)X(n), $$</EquationSource>


												</Equation>
where 

												<InlineEquation ID="IEq4">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq4.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ e(n) = d(n) - {W^T}(n)X(n) $$</EquationSource>


												</InlineEquation>
, 

												<InlineEquation ID="IEq5">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq5.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ X(n) = {\left[ {x(n),x\left( {n - 1} \right), \ldots, \,x\left( {n - L + 1} \right)} \right]^T} $$</EquationSource>


												</InlineEquation>
 is the input vector at time instant 

												<Emphasis Type="Italic">n</Emphasis>
 and 

												<Emphasis Type="Italic">μ</Emphasis>
 is a constant step size parameter chosen to reduce the gradient noise and to control the convergence rate and steady state error of the algorithm. In the NLMS algorithm, the step size is normalized by the energy of the input vector, 

												<Emphasis Type="BoldItalic">X</Emphasis>



												<Superscript>


													<Emphasis Type="Italic">T</Emphasis>


												</Superscript>
(

												<Emphasis Type="Italic">n</Emphasis>
)

												<Emphasis Type="BoldItalic">X</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
), which gives:

												<Equation ID="Equ2">


													<EquationNumber>2</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ2.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ W\left( {n + 1} \right) = W(n) + \frac{{\mu e(n)X(n)}}{{\varepsilon + {X^T}(n)X(n)}}. $$</EquationSource>


												</Equation>



												<Figure Category="Standard"
Float="Yes" ID="Fig1">


													<Caption Language="En">


														<CaptionNumber>Figure 1</CaptionNumber>



														<CaptionContent>


															<SimplePara>Adaptive system identification.</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO3">


														<ImageObject
Color="BlackWhite" FileRef="MediaObjects/11265_2009_405_Fig1_HTML.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>


												</Figure>
The step size 

												<Emphasis Type="Italic">μ</Emphasis>
 in the NLMS algorithm is a positive constant which should be chosen in the range 0
 &lt; 

												<Emphasis Type="Italic">μ</Emphasis>
 &lt; 
2 to ensure convergence of the algorithm. ε is a small positive value used to avoid division by zero.
											</Para>



											<Para>When the additive noise 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) is of impulsive nature, the performance of the NLMS algorithm which is based on the MSE criterion will deteriorate significantly. Recently, it is pointed out in [

												<CitationRef
CitationID="CR29">29</CitationRef>
] and [

												<CitationRef
CitationID="CR39">39</CitationRef>
] that the normalization mechanism of the NLMS algorithm can provide some degree of protection against impulsive noise. However, as will be illustrated in the subsequent computer experiments, the NLMS algorithm is still sensitive to consecutive impulses corrupting the desired signals. This motivates us to consider the NLMM algorithm.
											</Para>


										</Section2>



										<Section2 ID="Sec4">


											<Heading>The Normalized Least Mean M-Estimate Algorithm</Heading>



											<Para>Many techniques have been employed to reduce the hostile effect on the system due to impulsive interference. Examples include algorithms based on median filtering [

												<CitationRef
CitationID="CR8">8</CitationRef>
, 

												<CitationRef
CitationID="CR9">9</CitationRef>
], nonlinear clipping approaches [

												<CitationRef
CitationID="CR10">10</CitationRef>
, 

												<CitationRef
CitationID="CR11">11</CitationRef>
], and M-estimation-based algorithms such as the LMM [

												<CitationRef
CitationID="CR7">7</CitationRef>
] and RLM [

												<CitationRef
CitationID="CR12">12</CitationRef>
] algorithms. In particular, the last two algorithms were developed by minimizing the robust M-estimate cost functions instead of the conventional MSE criterion. Their improved robustness to impulsive noise and performance comparison with other relevant algorithms were thoroughly discussed in [

												<CitationRef
CitationID="CR7">7</CitationRef>
] and [

												<CitationRef
CitationID="CR12">12</CitationRef>
]. In the following, the concept of M-estimate cost function will be briefly reviewed and the NLMM algorithm will be derived.
											</Para>



											<Para>In the LMM algorithm [

												<CitationRef
CitationID="CR7">7</CitationRef>
], the least mean M-estimate distortion measure 

												<InlineEquation ID="IEq6">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq6.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {J_\rho } = E\left[ {\rho \left( {e(n)} \right)} \right] $$</EquationSource>


												</InlineEquation>
 is minimized, where 

												<Emphasis Type="Italic">ρ</Emphasis>
(

												<Emphasis Type="Italic">e</Emphasis>
) is an M-estimate function, which can be chosen as the Hampel’s three parts redescending function [

												<CitationRef
CitationID="CR40">40</CitationRef>
] or the modified Huber (MH) M-estimate function [

												<CitationRef
CitationID="CR13">13</CitationRef>
] as shown in Fig. 

												<InternalRef
RefID="Fig2">2a</InternalRef>
:

												<Equation ID="Equ3">


													<EquationNumber>3</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ3.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ {\rho_{\text{MH}}}(e) = \left\{ {\begin{array}{*{20}{c}} {\begin{array}{*{20}{c}} {{e^2}/2,} \hfill &amp; {0 \leqslant \left| e \right|&lt;\xi } \hfill \\ \end{array} } \hfill \\ {\begin{array}{*{20}{c}} {{\xi^2}/2.} \hfill &amp; {\text{otherwise}} \hfill \\ \end{array} } \hfill \\ \end{array} } \right., $$</EquationSource>


												</Equation>
where 

												<Emphasis Type="Italic">ξ</Emphasis>
 is the threshold parameter used to suppress the effect of outlier when the estimation error 

												<Emphasis Type="Italic">e</Emphasis>
 is very large. Notice that when 

												<InlineEquation ID="IEq7">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq7.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ \rho (e) = {e^2}/2 $$</EquationSource>


												</InlineEquation>
 it reduces to the conventional MSE criterion. Like the LMS algorithm, 

												<InlineEquation ID="IEq8">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq8.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {J_\rho } $$</EquationSource>


												</InlineEquation>
 is minimized by updating 

												<Emphasis Type="BoldItalic">W</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) in the negative direction of the instantaneous gradient vector 

												<InlineEquation ID="IEq9">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq9.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {\widehat\nabla_{W\rho }} $$</EquationSource>


												</InlineEquation>
. Hence, the gradient vector, 

												<InlineEquation ID="IEq10">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq10.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {\nabla_W}\left( {{J_\rho }} \right) $$</EquationSource>


												</InlineEquation>
, is approximated by

												<Equation ID="Equ4">


													<EquationNumber>4</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ4.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\nabla_W}\left( {{J_\rho }} \right) = E\left[ { - \frac{{\partial \left( {\rho \left( {e(n)} \right)} \right)}}{{\partial W}}} \right] \approx {{\widehat\nabla }_{W\rho }} = - \frac{\partial }{{\partial W}}\rho \left( {e(n)} \right)} \\ { = - q\left( {e(n)} \right)e(n)X(n),} \\ \end{array} $$</EquationSource>


												</Equation>
where 

												<InlineEquation ID="IEq11">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq11.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ q(e) = \psi (e)/e = \left( {\partial \rho (e)/\partial e} \right)/e $$</EquationSource>


												</InlineEquation>
 and 

												<Emphasis Type="Italic">ψ</Emphasis>
(

												<Emphasis Type="Italic">e</Emphasis>
) is the score function. For the MH function:

												<Equation ID="Equ5">


													<EquationNumber>5</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ5.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ {q_{\text{MH}}}(e) = \frac{{{\psi_{\text{MH}}}(e)}}{e} = \left\{ {\begin{array}{*{20}{c}} {\begin{array}{*{20}{c}} {1,} \hfill &amp; {0 \leqslant \left| e \right|&lt;\xi } \hfill \\ \end{array} } \hfill \\ {\begin{array}{*{20}{c}} {0.} \hfill &amp; {\text{otherwise}} \hfill \\ \end{array} } \hfill \\ \end{array} } \right. $$</EquationSource>


												</Equation>
which is depicted in Fig. 

												<InternalRef
RefID="Fig2">2b</InternalRef>
.

												<Figure Category="Standard"
Float="Yes" ID="Fig2">


													<Caption Language="En">


														<CaptionNumber>Figure 2</CaptionNumber>



														<CaptionContent>


															<SimplePara>


																<Emphasis
Type="Bold">a</Emphasis>
 The modified Huber M-estimate function 

																<Emphasis
Type="Italic">ρ</Emphasis>



																<Subscript>MH</Subscript>
(

																<Emphasis
Type="Italic">e</Emphasis>
); 

																<Emphasis
Type="Bold">b</Emphasis>



																<Emphasis
Type="Italic">ψ</Emphasis>



																<Subscript>MH</Subscript>
(

																<Emphasis
Type="Italic">e</Emphasis>
), the score function of 

																<Emphasis
Type="Italic">ρ</Emphasis>



																<Subscript>MH</Subscript>
(

																<Emphasis
Type="Italic">e</Emphasis>
).
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO4">


														<ImageObject
Color="BlackWhite" FileRef="MediaObjects/11265_2009_405_Fig2_HTML.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>


												</Figure>


											</Para>



											<Para>Finally, we obtain the LMM algorithm as follows

												<Equation ID="Equ6">


													<EquationNumber>6</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ6.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ W\left( {n + 1} \right) = W(n) - \mu {\widehat\nabla_{{\mathbf{W}}\rho }} = W(n) + \mu \psi \left( {e(n)} \right)X(n). $$</EquationSource>


												</Equation>


											</Para>



											<Para>In general, when 

												<Emphasis Type="Italic">e</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) is smaller than 

												<Emphasis Type="Italic">ξ</Emphasis>
, the weight function 

												<Emphasis Type="Italic">q</Emphasis>
(

												<Emphasis Type="Italic">e</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
)) is equal to one and (

												<InternalRef
RefID="Equ6">6</InternalRef>
) becomes identical to the LMS algorithm. When 

												<Emphasis Type="Italic">e</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) is larger than certain thresholds, say 

												<Emphasis Type="Italic">ξ</Emphasis>
 in the MH function, 

												<Emphasis Type="Italic">q</Emphasis>
(

												<Emphasis Type="Italic">e</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
)) will become zero and prevent the weight vector from updating. Thus the LMM algorithm can effectively reduce the adverse effect of large estimation error on the update of the filter coefficients. Another interpretation of (

												<InternalRef
RefID="Equ6">6</InternalRef>
) is that the error term is passed through a nonlinear device 

												<Emphasis Type="Italic">ψ</Emphasis>
(

												<Emphasis Type="Italic">e</Emphasis>
). This type of adaptive filtering algorithms has been studied previously for the error-function nonlinearity [

												<CitationRef
CitationID="CR35">35</CitationRef>
] and the sign nonlinearity [

												<CitationRef
CitationID="CR34">34</CitationRef>
]. The former concludes that the nonlinearity will slow down the convergence rate, while the latter is mainly introduced to reduce the implementation complexity. Both of them did not recognize the robustness of this class of algorithms to impulsive outliers. This was later studied by Koike in [

												<CitationRef
CitationID="CR10">10</CitationRef>
, 

												<CitationRef
CitationID="CR29">29</CitationRef>
] and using the clipping nonlinearity, and [

												<CitationRef
CitationID="CR6">6</CitationRef>
, 

												<CitationRef
CitationID="CR7">7</CitationRef>
, 

												<CitationRef
CitationID="CR41">41</CitationRef>
] using M-estimation. In [

												<CitationRef
CitationID="CR7">7</CitationRef>
], the threshold parameter 

												<Emphasis Type="Italic">ξ</Emphasis>
 in the MH function is continuously updated using a technique called adaptive threshold selection (ATS), which greatly improves the convergence speed and steady state error.
											</Para>



											<FormalPara RenderingStyle="Style1">


												<Heading>Adaptive Threshold Selection (ATS)</Heading>



												<Para>In ATS, 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) is assumed to be Gaussian distributed except being corrupted occasionally by additive impulsive noise. By using the following robust variance estimate

													<Equation ID="Equ7">


														<EquationNumber>7</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ7.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) = {\lambda_\sigma }\widehat\sigma_e^2\left( {n - 1} \right) + {c_1}\left( {{\text{1}} - {\lambda_\sigma }} \right){\text{med}}\left( {{A_e}(n)} \right){\text{,}} $$</EquationSource>


													</Equation>



													<InlineEquation ID="IEq12">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq12.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


													</InlineEquation>
, the variance of the “impulse-free” error 

													<InlineEquation ID="IEq13">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq13.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \hat e(n) $$</EquationSource>


													</InlineEquation>
, can be estimated. Hence, it can be used to detect and reject the adverse outliers in 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
). The forgetting factor 

													<InlineEquation ID="IEq14">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq14.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\lambda_\sigma } $$</EquationSource>


													</InlineEquation>
 is a positive real number close to but smaller than one. med(∙) is the median operator. 

													<InlineEquation ID="IEq15">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq15.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_e}(n) = \left\{ {{e^2}(n), \cdots, {e^2}\left( {n - {N_w} + 1} \right)} \right\} $$</EquationSource>


													</InlineEquation>
. 

													<Emphasis Type="Italic">c</Emphasis>



													<Subscript>1</Subscript>
 = 2.13 is the correction factor for median estimation and 

													<Emphasis Type="Italic">N</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">w</Emphasis>


													</Subscript>
 is the length of the data set. The probability of 

													<InlineEquation ID="IEq16">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq16.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| {\hat e(n)} \right| $$</EquationSource>


													</InlineEquation>
 greater than a given threshold 

													<Emphasis Type="Italic">T</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">h</Emphasis>


													</Subscript>
 is given by

													<Equation ID="Equ8">


														<EquationNumber>8</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ8.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\theta_T}(n) = {P_r}\left\{ {\left| {e(n)} \right| &gt; {T_h}} \right\} = {\text{erfc}}\left( {\tfrac{{{T_h}}}{{\sqrt 2 {{\widehat\sigma }_e}(n)}}} \right), $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq17">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq17.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\text{erfc}}(x) = \left( {2/\sqrt \pi } \right)\int_x^\infty {{e^{ - {t^2}}}} dt $$</EquationSource>


													</InlineEquation>
 is the complementary error function. Let 

													<InlineEquation ID="IEq18">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq18.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\theta_\xi } = {P_r}\left\{ {\left| {e(n)} \right| &gt; \xi } \right\} $$</EquationSource>


													</InlineEquation>
 be the probabilities that 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) is greater than 

													<Emphasis Type="Italic">ξ</Emphasis>
, the value of 

													<Emphasis Type="Italic">ξ</Emphasis>
 can be determined by appropriate selection of 

													<InlineEquation ID="IEq19">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq19.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\theta_\xi } $$</EquationSource>


													</InlineEquation>
 according to (

													<InternalRef
RefID="Equ8">8</InternalRef>
). For example, if 

													<InlineEquation ID="IEq20">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq20.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\theta_\xi } $$</EquationSource>


													</InlineEquation>
, is chosen as 0.01, there will be 99% confidence to reject it when 

													<InlineEquation ID="IEq21">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq21.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| {\hat e(n)} \right| &gt; \xi $$</EquationSource>


													</InlineEquation>
. Accordingly, 

													<Emphasis Type="Italic">ξ</Emphasis>
 can be obtained from (

													<InternalRef
RefID="Equ8">8</InternalRef>
) as

													<Equation ID="Equ9">


														<EquationNumber>9</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ9.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \xi = {k_\xi }{\widehat\sigma_e}(n) = 2.576{\widehat\sigma_e}(n). $$</EquationSource>


													</Equation>


												</Para>



												<Para>Similar expressions for the Hampel’s three parts re-descending function can be found in [

													<CitationRef
CitationID="CR7">7</CitationRef>
, 

													<CitationRef
CitationID="CR12">12</CitationRef>
]. This technique, which is referred to as adaptive threshold selection (ATS), is very important both theoretically and practically to the LMM and NLMM algorithms as we shall further elaborate in Sections 

													<InternalRef
RefID="Sec5">3</InternalRef>
 and 

													<InternalRef
RefID="Sec12">4</InternalRef>
. Some commonly used M-estimate functions, which we called the M-nonlinearity, and useful properties pertaining to their analyses in Section 

													<InternalRef
RefID="Sec5">3</InternalRef>
 are also summarized in Appendix 

													<InternalRef
RefID="Sec17">C</InternalRef>
.
												</Para>



												<Para>The computational complexity of the LMM algorithm per iteration is of order 

													<Emphasis Type="Italic">O</Emphasis>
(

													<Emphasis Type="Italic">L</Emphasis>
) with additional 

													<Emphasis Type="Italic">O</Emphasis>
(

													<Emphasis Type="Italic">N</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">w</Emphasis>


													</Subscript>
log

													<Emphasis Type="Italic">N</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">w</Emphasis>


													</Subscript>
) operations for performing the median filtering [

													<CitationRef
CitationID="CR7">7</CitationRef>
]. The window length 

													<Emphasis Type="Italic">N</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">w</Emphasis>


													</Subscript>
 is usually chosen between 5 and 9.
												</Para>



												<Para>If the step size 

													<Emphasis Type="Italic">μ</Emphasis>
 in (

													<InternalRef
RefID="Equ6">6</InternalRef>
) for the LMM algorithm is again normalized by the squared norm of the input vector as in the NLMS algorithm, the following NLMM algorithm is obtained

													<Equation ID="Equ10">


														<EquationNumber>10</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ10.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ W\left( {n + 1} \right) = W(n) + \frac{{\mu \psi \left( {e(n)} \right)X(n)}}{{\varepsilon + {X^T}(n)X(n)}}, $$</EquationSource>


													</Equation>


												</Para>



												<Para>where 

													<InlineEquation ID="IEq22">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq22.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \psi \left( {e(n)} \right) = q\left( {e(n)} \right)e(n) $$</EquationSource>


													</InlineEquation>
. Alternatively, (

													<InternalRef
RefID="Equ10">10</InternalRef>
) can be derived using the Least Perturbation Property (LPP) [

													<CitationRef
CitationID="CR5">5</CitationRef>
] originally proposed for the NLMS algorithm. Given the 

													<Emphasis
Type="Italic">a prior</Emphasis>
 output estimation error 

													<InlineEquation ID="IEq23">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq23.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ e(n) = d(n) - {X^T}(n)W(n) $$</EquationSource>


													</InlineEquation>
. The NLMS algorithm can be derived by seeking a 

													<Emphasis
Type="BoldItalic">W</Emphasis>
(

													<Emphasis Type="Italic">n </Emphasis>
+ 1) such that its deviation from 

													<Emphasis
Type="BoldItalic">W</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) is minimized in the 2-norm, while satisfying a constraint between the a posteriori output estimation error 

													<InlineEquation ID="IEq24">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq24.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {e_p}(n) = d(n) - {X^T}(n)W\left( {n + 1} \right) $$</EquationSource>


													</InlineEquation>
 and the prior output estimation error as follows:

													<Equation ID="Equa">


														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equa.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \mathop {\min }\limits_{{\mathbf{W}}\left( {n + 1} \right)} {\left\| {W\left( {n + 1} \right) - W(n)} \right\|^2}, $$</EquationSource>


													</Equation>
subject to

													<Equation ID="Equ11">


														<EquationNumber>11</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ11.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {e_p}(n) = \left( {1 - \frac{{\mu {{\left\| {X(n)} \right\|}^2}}}{{\varepsilon + {{\left\| {X(n)} \right\|}^2}}}} \right)e(n). $$</EquationSource>


													</Equation>
The solution of this problem yields the NLMS conventional update in (

													<InternalRef
RefID="Equ2">2</InternalRef>
). In M-estimation, the 

													<Emphasis
Type="Italic">a prior</Emphasis>
 output estimation error 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) is replaced by the score function 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)) to suppress the adverse effect of impulsive noise. Substituting this into the constraint above and solving for 

													<Emphasis
Type="BoldItalic">W</Emphasis>
(

													<Emphasis Type="Italic">n </Emphasis>
+ 1) will yield the update in (

													<InternalRef
RefID="Equ10">10</InternalRef>
) above, since the two optimization problems have the same structure except for that 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) is now replaced by 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)).
												</Para>



												<Para>As will be shown in the performance analysis in Section 

													<InternalRef
RefID="Sec5">3</InternalRef>
 and the simulation results in Section 

													<InternalRef
RefID="Sec12">4</InternalRef>
, this normalization brings faster convergence speed when the input is highly colored. Compared to the LMM algorithm for real inputs, the computational complexity is increased by one multiplication and two additions for updating 

													<Emphasis
Type="BoldItalic">X</Emphasis>



													<Superscript>


														<Emphasis
Type="Italic">T</Emphasis>


													</Superscript>
(

													<Emphasis Type="Italic">n</Emphasis>
)

													<Emphasis
Type="BoldItalic">X</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
), and one addition and one division for evaluating 

													<InlineEquation ID="IEq25">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq25.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \mu /\left( {\varepsilon + {X^T}(n)X(n)} \right) $$</EquationSource>


													</InlineEquation>
. The performance advantage of the NLMS algorithm for white inputs has been also analyzed in detail by Douglas et al

													<Emphasis Type="Italic">.</Emphasis>
 [

													<CitationRef
CitationID="CR42">42</CitationRef>
].
												</Para>


											</FormalPara>


										</Section2>


									</Section1>



									<Section1 ID="Sec5">


										<Heading>Mean and Mean Square Convergence Analysis</Heading>



										<Para>In this section, we first present a detailed convergence performance analysis of the NLMS algorithms with M-nonlinearity 

											<Emphasis Type="Italic">ψ</Emphasis>
(

											<Emphasis Type="Italic">e</Emphasis>
) for Gaussian input and independent white Gaussian additive noise. More precisely, by extending the approach in [

											<CitationRef
CitationID="CR7">7</CitationRef>
, 

											<CitationRef
CitationID="CR12">12</CitationRef>
] and using the conventional Price’s theorem, it is possible to derive analytical expressions for modeling their mean and mean square convergence behaviors. Next, we extend the Price’s theorem to mixture Gaussian processes and, by using the above results, present a detailed convergence performance analysis of these NLMS algorithms for Gaussian inputs and independent CG additive noise. In particular, the LMM and NLMM algorithms with MH function and ATS will be studied in detail. The final results are new expressions for EMSE, stability bound and difference equations describing the convergence behaviors of the various algorithms in terms of the generalized Abelian integral functions. They are similar to the classical results of the LMS algorithm by Widrow et al

											<Emphasis Type="Italic">.</Emphasis>
 [

											<CitationRef
CitationID="CR1">1</CitationRef>
], which allow us to clearly interpret and compare the convergence behaviors of this class of algorithms. To simplify the analysis, the following assumptions are made:
										</Para>



										<FormalPara RenderingStyle="Style1">


											<Heading>Assumption 1:</Heading>



											<Para>The input signal 

												<Emphasis Type="Italic">x</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) is an ergodic process which is Gaussian distributed with zero mean and autocorrelation matrix 

												<InlineEquation ID="IEq26">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq26.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {R_{XX}} = E\left[ {X(n){X^T}(n)} \right] $$</EquationSource>


												</InlineEquation>
.
											</Para>


										</FormalPara>



										<FormalPara RenderingStyle="Style1">


											<Heading>Assumption 2:</Heading>



											<Para>The additive noise 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) is assumed to be a Gaussian noise for the analysis in section A below (

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) = 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>


													<Emphasis Type="Italic">g</Emphasis>


												</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
)). For the analysis in Section B below, 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) is modeled as a CG noise which is a frequently used model for analyzing impulsive noise. More precisely, it is given by:

												<Equation ID="Equ12">


													<EquationNumber>12</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ12.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ {\eta_0}(n) = {\eta_g}(n) + {\eta_{im}}(n) = {\eta_g}(n) + b(n){\eta_w}(n), $$</EquationSource>


												</Equation>
where 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>


													<Emphasis Type="Italic">g</Emphasis>


												</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) and 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>


													<Emphasis Type="Italic">w</Emphasis>


												</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) are both independent and identically distributed (i.i.d.) zero mean Gaussian sequences with respective variance 

												<InlineEquation ID="IEq27">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq27.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ \sigma_g^2 $$</EquationSource>


												</InlineEquation>
 and 

												<InlineEquation ID="IEq28">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq28.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ \sigma_w^2 $$</EquationSource>


												</InlineEquation>
. 

												<Emphasis Type="Italic">b</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) is an i.i.d. Bernoulli random sequence whose value at any time instant is either zero or one, with occurrence probabilities 

												<InlineEquation ID="IEq29">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq29.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {P_r}\left( {b(n) = 1} \right) = {p_r} $$</EquationSource>


												</InlineEquation>
 and 

												<InlineEquation ID="IEq30">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq30.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {P_r}\left( {b(n) = 0} \right) = 1 - {p_r} $$</EquationSource>


												</InlineEquation>
. The variances of the random processes 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>


													<Emphasis Type="Italic">im</Emphasis>


												</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) and 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) are then given by 

												<InlineEquation ID="IEq31">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq31.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ \sigma_{im}^2 = {p_r}\sigma_w^2 $$</EquationSource>


												</InlineEquation>
 and 

												<InlineEquation ID="IEq32">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq32.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ \sigma_{{\eta_0}}^2 = \sigma_g^2 + \sigma_{im}^2 = \sigma_g^2 + {p_r}\sigma_w^2 $$</EquationSource>


												</InlineEquation>
. The ratio 

												<InlineEquation ID="IEq33">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq33.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {r_{im}} = \sigma_{im}^2/\sigma_g^2 = {p_r}\sigma_w^2/\sigma_g^2 $$</EquationSource>


												</InlineEquation>
 is a measure of the impulsive characteristic of the CG noise. 

												<InlineEquation ID="IEq34">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq34.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ \sigma_\Sigma^2 = \sigma_g^2 + \sigma_w^2 $$</EquationSource>


												</InlineEquation>
. Accordingly, the probability distribution function (PDF) of this CG distribution is given by

												<Equation ID="Equ13">


													<EquationNumber>13</EquationNumber>



													<MediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ13.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</MediaObject>



													<EquationSource
Format="TEX">$$ {f_{{\eta_0}}}\left( \eta \right) = \tfrac{{1 - {p_r}}}{{\sqrt {2\pi \sigma_g^2} }}\exp \left( { - \tfrac{{{\eta^2}}}{{2\sigma_g^2}}} \right) + \tfrac{{{p_r}}}{{\sqrt {2\pi \sigma_\Sigma^2} }}\exp \left( { - \tfrac{{{\eta^2}}}{{2\sigma_\Sigma^2}}} \right). $$</EquationSource>


												</Equation>


											</Para>


										</FormalPara>



										<FormalPara RenderingStyle="Style1">


											<Heading>Assumption 3:</Heading>



											<Para>


												<Emphasis Type="BoldItalic">W</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
), 

												<Emphasis Type="Italic">x</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) and 

												<Emphasis Type="Italic">η</Emphasis>



												<Subscript>0</Subscript>
(

												<Emphasis Type="Italic">n</Emphasis>
) are statistically independent (the independent assumption [

												<CitationRef
CitationID="CR1">1</CitationRef>
]). Although this assumption is not completely valid in general applications, it is a good approximation and is commonly used to simplify the convergence analysis of numerous adaptive filtering algorithms. Moreover, we denote 

												<InlineEquation ID="IEq35">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq35.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ W* = R_{XX}^{ - 1}{P_{dX}} $$</EquationSource>


												</InlineEquation>
 as the optimal Wiener solution, where 

												<InlineEquation ID="IEq36">


													<InlineMediaObject>


														<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq36.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


													</InlineMediaObject>



													<EquationSource
Format="TEX">$$ {P_{dX}} = E\left[ {d(n)X(n)} \right] $$</EquationSource>


												</InlineEquation>
 is the ensemble-averaged cross-correlation vector between 

												<Emphasis Type="BoldItalic">X</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
) and 

												<Emphasis Type="Italic">d</Emphasis>
(

												<Emphasis Type="Italic">n</Emphasis>
).
											</Para>


										</FormalPara>



										<Section2 ID="Sec6">


											<Heading>Convergence Behaviors of the NLMS Algorithms with M-Nonlinearity and Gaussian Inputs and Noises</Heading>



											<Section3 ID="Sec7">


												<Heading>Mean Behavior</Heading>



												<Para>From (

													<InternalRef
RefID="Equ10">10</InternalRef>
), the weight-error vector 

													<InlineEquation ID="IEq37">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq37.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ v(n) = W* - W(n) $$</EquationSource>


													</InlineEquation>
 can be written as:

													<Equation ID="Equ14">


														<EquationNumber>14</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ14.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ v\left( {n + 1} \right) = v(n) - \frac{{\mu \psi \left( {e(n)} \right)X(n)}}{{\varepsilon + {X^T}(n)X(n)}}. $$</EquationSource>


													</Equation>


												</Para>



												<Para>If 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)) = 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
), (

													<InternalRef
RefID="Equ14">14</InternalRef>
) will reduce to the conventional NLMS algorithm (

													<InternalRef
RefID="Equ2">2</InternalRef>
). Taking expectation on both sides of (

													<InternalRef
RefID="Equ14">14</InternalRef>
), we get

													<Equation ID="Equ15">


														<EquationNumber>15</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ15.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E\left[ {v\left( {n + 1} \right)} \right] = E\left[ {v(n)} \right] - \mu {H_1}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq38">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq38.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {H_1} = E\left[ {\psi \left( {e(n)} \right)X(n)/\left( {\varepsilon + {X^T}(n)X(n)} \right)} \right] $$</EquationSource>


													</InlineEquation>
 and 

													<Emphasis Type="Italic">E</Emphasis>
[∙] denotes the expectation over 

													<InlineEquation ID="IEq39">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq39.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left\{ {v(n),X(n),{\eta_g}(n)} \right\} $$</EquationSource>


													</InlineEquation>
 and is written more clearly as 

													<InlineEquation ID="IEq40">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq40.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {E_{\left\{ {v,X,{\eta_g}} \right\}}}\left[ \cdot \right] $$</EquationSource>


													</InlineEquation>
. Since 

													<Emphasis
Type="BoldItalic">X</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) and 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">g</Emphasis>


													</Subscript>
(

													<Emphasis Type="Italic">n</Emphasis>
) are stationary, we can drop the time index 

													<Emphasis Type="Italic">n</Emphasis>
 in the expectation to get 

													<InlineEquation ID="IEq41">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq41.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {H_1} = E\left[ {\psi (e)X/\left( {\varepsilon + {X^T}X} \right)} \right] $$</EquationSource>


													</InlineEquation>
.
												</Para>



												<Para>In the conventional NLMS algorithm [

													<CitationRef
CitationID="CR17">17</CitationRef>
] and [

													<CitationRef
CitationID="CR18">18</CitationRef>
], similar difference equation for the mean weight-error vector (c.f. [

													<CitationRef
CitationID="CR18">18</CitationRef>
, Eq. 

													<InternalRef
RefID="Equ11">11</InternalRef>
]) was obtained:

													<Equation ID="Equ16">


														<EquationNumber>16</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ16.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E\left[ {v\left( {n + 1} \right)} \right] = \left( {I - \mu {F_\varepsilon }} \right)E\left[ {v(n)} \right], $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq42">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq42.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {F_\varepsilon } = E\left[ {X(n){X^T}(n)/\left( {\varepsilon + {X^T}(n)X(n)} \right)} \right] $$</EquationSource>


													</InlineEquation>
 and 

													<Emphasis
Type="BoldItalic">I</Emphasis>
 is the identity matrix. For convenience, the variables have been renamed according to the notation in this paper. Moreover, 

													<InlineEquation ID="IEq43">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq43.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {F_\varepsilon } $$</EquationSource>


													</InlineEquation>
 was further diagonalized to 

													<InlineEquation ID="IEq44">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq44.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {H_\varepsilon } $$</EquationSource>


													</InlineEquation>
 whose 

													<Emphasis Type="Italic">i</Emphasis>
-th element is 

													<InlineEquation ID="IEq45">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq45.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {{H_\varepsilon }} \right]_{i,i}} = \int_0^\infty {\frac{{\exp \left( { - \beta \varepsilon } \right)}}{{{{\left| {I + 2\beta {R_{XX}}} \right|}^{1/2}}}}} \frac{{{\lambda_i}}}{{1 + 2\beta {\lambda_i}}}d\beta $$</EquationSource>


													</InlineEquation>
 (c. f. [

													<CitationRef
CitationID="CR18">18</CitationRef>
, Eq. 

													<InternalRef
RefID="Equ14">14</InternalRef>
]), where λ

													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
 is the 

													<Emphasis Type="Italic">i</Emphasis>
-th eigenvalue of 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
. It was evaluated analytically in [

													<CitationRef
CitationID="CR17">17</CitationRef>
] for three important cases with different eigenvalue distributions (in [

													<CitationRef
CitationID="CR18">18</CitationRef>
], only the first case was elaborated): (1) white input signal with 

													<InlineEquation ID="IEq46">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq46.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\lambda_1} = \ldots = {\lambda_L} $$</EquationSource>


													</InlineEquation>
; (2) two signal subspaces with equal powers 

													<InlineEquation ID="IEq47">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq47.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\lambda_1} = \ldots = {\lambda_K} = a $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq48">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq48.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\lambda_{K + 1}} = \ldots = {\lambda_L} = b $$</EquationSource>


													</InlineEquation>
; (3) distinct pairs, 

													<InlineEquation ID="IEq49">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq49.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\lambda_1} = {\lambda_2},\,{\lambda_3} = {\lambda_4},\, \ldots, \,{\lambda_{L - 1}} = {\lambda_L} $$</EquationSource>


													</InlineEquation>
 (assuming 

													<Emphasis Type="Italic">L</Emphasis>
 is even). Besides these three special cases, no general solution to 

													<InlineEquation ID="IEq50">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq50.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {H_\varepsilon } $$</EquationSource>


													</InlineEquation>
 was provided. Therefore, general closed-form formulas for modeling the mean and mean square behavior of NLMS algorithm were unavailable in [

													<CitationRef
CitationID="CR17">17</CitationRef>
] and [

													<CitationRef
CitationID="CR18">18</CitationRef>
]. To our best knowledge, no such analytical solution has ever appeared in the literature.
												</Para>



												<Para>Here, we pursue another direction by treating some of these integrals involved as special functions and carry them throughout the analysis. Furthermore, using the Price’s theorem, the expectation involving the M-nonlinearity and ATS can be decoupled from the rest of the equations. The final formulas containing these special integral functions still allow us to clearly interpret the convergence behavior of the NLMS algorithms with error nonlinearity. More precisely, it is shown in Appendix 

													<InternalRef
RefID="Sec14">A</InternalRef>
 (

													<InternalRef
RefID="Equ76">A-14</InternalRef>
) that

													<Equation ID="Equ17">


														<EquationNumber>17</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ17.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {H_1} \approx {A_\psi }\left( {\sigma_e^2} \right)U\Lambda {D_\Lambda }{U^T}E\left[ {v(n)} \right]. $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq51">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq51.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) = \overline {\psi \prime } \left( {\sigma_e^2} \right) = \tfrac{1}{{\sqrt {2\pi } {\sigma_e}}}\int_{ - \infty }^\infty {\psi \prime (e)\exp \left( { - {e^2}/\left( {2\sigma_e^2} \right)} \right)de} $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq52">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq52.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {R_{XX}} = U\Lambda {U^T} $$</EquationSource>


													</InlineEquation>
 is the eigenvalue decomposition of 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
 and 

													<InlineEquation ID="IEq53">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq53.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \Lambda = {\text{diag}}\left( {{\lambda_1},{\lambda_2}, \cdots, {\lambda_L}} \right) $$</EquationSource>


													</InlineEquation>
 contains its eigenvalues. 

													<InlineEquation ID="IEq54">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq54.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {D_\Lambda } $$</EquationSource>


													</InlineEquation>
 is a diagonal matrix with the 

													<Emphasis Type="Italic">i</Emphasis>
-th diagonal entry given by (A-16): 

													<InlineEquation ID="IEq55">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq55.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {{D_\Lambda }} \right]_{i,i}} = {I_i}\left( \Lambda \right) = \int_0^\infty {exp\left( { - \beta \varepsilon } \right)\left[ {\mathop \Pi \limits_{k = 1}^L {{\left( {2\beta {\lambda_k} + 1} \right)}^{ - 1/2}}} \right]{{\left( {2\beta {\lambda_i} + 1} \right)}^{ - 1}}d\beta } $$</EquationSource>


													</InlineEquation>
, which is a generalized Abelian integral function, whereas the conventional Abelian integral has the form 

													<InlineEquation ID="IEq56">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq56.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {I_{\text{a}}}(x) = \int_0^x {{{\left[ {q\left( \beta \right)} \right]}^{ - 1/2}}d\beta } $$</EquationSource>


													</InlineEquation>
 with 

													<Emphasis Type="Italic">q</Emphasis>
(β) being a polynomial in 

													<Emphasis Type="Italic">β</Emphasis>
. It is also similar to 

													<InlineEquation ID="IEq57">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq57.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {{H_\varepsilon }} \right]_{i,i}} $$</EquationSource>


													</InlineEquation>
 in [

													<CitationRef
CitationID="CR18">18</CitationRef>
].
												</Para>



												<Para>Substituting (

													<InternalRef
RefID="Equ17">17</InternalRef>
) into (

													<InternalRef
RefID="Equ15">15</InternalRef>
), we obtain

													<Equation ID="Equ18">


														<EquationNumber>18</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ18.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E\left[ {v\left( {n + 1} \right)} \right] = \left( {I - \mu {A_\psi }\left( {\sigma_e^2(n)} \right)U\Lambda {D_\Lambda }{U^T}} \right)E\left[ {v(n)} \right], $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq58">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq58.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(n) = E\left[ {{v^T}(n){R_{XX}}v(n)} \right] + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
 and the approximate sign has been replaced by the equality sign. (

													<InternalRef
RefID="Equ18">18</InternalRef>
) can also be written in the natural coordinate 

													<Emphasis
Type="BoldItalic">V</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) = 

													<Emphasis
Type="BoldItalic">U</Emphasis>



													<Superscript>


														<Emphasis
Type="Italic">T</Emphasis>


													</Superscript>



													<Emphasis
Type="BoldItalic">v</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) as

													<Equation ID="Equ19">


														<EquationNumber>19</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ19.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E\left[ {V\left( {n + 1} \right)} \right] = \left( {I - \mu {A_\psi }\left( {\sigma_e^2(n)} \right)\Lambda {D_\Lambda }} \right)E\left[ {V(n)} \right], $$</EquationSource>


													</Equation>
which is equivalent to 

													<Emphasis Type="Italic">L</Emphasis>
 scalar first order finite difference equations as follows:

													<Equation ID="Equ20">


														<EquationNumber>20</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ20.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E{\left[ {V\left( {n + 1} \right)} \right]_i} = \left( {1 - \mu {A_\psi }\left( {\sigma_e^2(n)} \right){\lambda_i}{I_i}\left( \Lambda \right)} \right)E{\left[ {V(n)} \right]_i}, $$</EquationSource>


													</Equation>
where 

													<Emphasis Type="Italic">E</Emphasis>
[

													<Emphasis
Type="BoldItalic">V</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)]

													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
 is the 

													<Emphasis Type="Italic">i</Emphasis>
-th element of the vector 

													<Emphasis Type="Italic">E</Emphasis>
[

													<Emphasis
Type="BoldItalic">V</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)].
												</Para>



												<Para>Though the maximum possible step size is in general difficult to obtain, a sufficient condition for the algorithm to converge is 

													<InlineEquation ID="IEq59">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq59.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| {1 - \mu {A_\psi }\left( {\sigma_e^2} \right){\lambda_i}{I_i}\left( \Lambda \right)} \right|&lt;1 $$</EquationSource>


													</InlineEquation>
, for all 

													<Emphasis Type="Italic">i</Emphasis>
. If 

													<InlineEquation ID="IEq60">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq60.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline {\psi \prime } \left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 is bounded above by a constant 

													<InlineEquation ID="IEq61">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq61.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\overline {\psi \prime }_{\max }} = {A_{\psi \_\max }} $$</EquationSource>


													</InlineEquation>
, then a conservative maximum step size bound is

													<Equation ID="Equ21">


														<EquationNumber>21</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ21.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \mu&lt;\frac{2}{{{A_{\psi \_\max }}{\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)}} $$</EquationSource>


													</Equation>
which yields good estimates in practical algorithms such as the LMM, NLMM, and those in [

													<CitationRef
CitationID="CR34">34</CitationRef>
, 

													<CitationRef
CitationID="CR35">35</CitationRef>
].
												</Para>



												<FormalPara RenderingStyle="Style1">


													<Heading>Remarks</Heading>



													<Para>


														<OrderedList>


															<ListItem>


																<ItemNumber>(R-A1):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">LMS algorithms with error nonlinearity</Emphasis>
:
																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>
As pointed out at the end of Appendix 

														<InternalRef
RefID="Sec14">A</InternalRef>
, our analysis will reduce to the LMS case when 

														<InlineEquation ID="IEq62">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq62.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {D_\Lambda } = I $$</EquationSource>


														</InlineEquation>
 and Eq. (

														<InternalRef
RefID="Equ18">18</InternalRef>
) strictly holds for general error nonlinearity 

														<Emphasis
Type="Italic">ψ</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
). Except for the conventional LS error criterion, (

														<InternalRef
RefID="Equ19">19</InternalRef>
) or (

														<InternalRef
RefID="Equ20">20</InternalRef>
) are generally a set of nonlinear difference equations due to the error nonlinearity. A general solution is rather difficult to obtain because the term 

														<InlineEquation ID="IEq63">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq63.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) = \overline {\psi \prime } \left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 is dependent on the MSE. For the DS nonlinearity, (

														<InternalRef
RefID="Equ19">19</InternalRef>
) agrees with the results in [

														<CitationRef
CitationID="CR34">34</CitationRef>
], and moreover, Mathews et al. showed that the resulting equation is convergent using the Doob’s theorem [

														<CitationRef
CitationID="CR43">43</CitationRef>
, 

														<CitationRef
CitationID="CR44">44</CitationRef>
]. (

														<InternalRef
RefID="Equ19">19</InternalRef>
) also agrees with the result in [

														<CitationRef
CitationID="CR35">35</CitationRef>
] for LMS algorithm with EF nonlinearity, in which the difference equation was approximated with a differential equation and it was showed that there are two possible adapting phases, the nonlinear and linear ones. If the nonlinearity 

														<Emphasis
Type="Italic">ψ</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
) is known, then it may be possible to derive its convergence behavior by converting it to a nonlinear ordinary differential equation (ODE). 

														<InlineEquation ID="IEq64">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq64.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 for some commonly used error nonlinearities are summarized in Table 

														<InternalRef
RefID="Tab1">1</InternalRef>
.

														<Table Float="Yes" ID="Tab1">


															<Caption Language="En">


																<CaptionNumber>Table 1</CaptionNumber>



																<CaptionContent>


																	<SimplePara>List of 

																		<InlineEquation
ID="IEq65">


																			<InlineMediaObject>


																				<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq65.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																			</InlineMediaObject>



																			<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_\varepsilon^2} \right) $$</EquationSource>


																		</InlineEquation>
, 

																		<InlineEquation
ID="IEq66">


																			<InlineMediaObject>


																				<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq66.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																			</InlineMediaObject>



																			<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_\varepsilon^2} \right) $$</EquationSource>


																		</InlineEquation>
 and 

																		<InlineEquation
ID="IEq67">


																			<InlineMediaObject>


																				<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq67.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																			</InlineMediaObject>



																			<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_\varepsilon^2} \right) $$</EquationSource>


																		</InlineEquation>
 for three related algorithms.
																	</SimplePara>


																</CaptionContent>


															</Caption>



															<tgroup align="left" cols="5">


																<colspec align="left"
colname="c1" colnum="1" />



																<colspec align="left"
colname="c2" colnum="2" />



																<colspec align="left"
colname="c3" colnum="3" />



																<colspec align="left"
colname="c4" colnum="4" />



																<colspec align="left"
colname="c5" colnum="5" />



																<thead>


																	<row>


																		<entry colname="c1">


																			<SimplePara>Nonlinearity</SimplePara>


																		</entry>



																		<entry colname="c2">


																			<SimplePara>


																				<Emphasis
Type="Italic">ψ</Emphasis>
(ε)
																			</SimplePara>


																		</entry>



																		<entry colname="c3">


																			<SimplePara>


																				<InlineEquation
ID="IEq68">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq68.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_\varepsilon^2} \right) $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c4">


																			<SimplePara>


																				<InlineEquation
ID="IEq69">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq69.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_\varepsilon^2} \right) $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c5">


																			<SimplePara>


																				<InlineEquation
ID="IEq70">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq70.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_\varepsilon^2} \right) $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>


																	</row>


																</thead>



																<tbody>


																	<row>


																		<entry colname="c1">


																			<SimplePara>Modified Huber</SimplePara>


																		</entry>



																		<entry colname="c2">


																			<SimplePara>


																				<InlineEquation
ID="IEq71">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq71.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {\psi_{\text{MH}}}\left( \varepsilon \right) = \left\{ {\begin{array}{*{20}{c}} {\varepsilon, } &amp; {\left| \varepsilon \right| \leqslant \xi } \\ {0,} &amp; {\text{otherwise}} \\ \end{array} } \right. $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c3">


																			<SimplePara>


																				<InlineEquation
ID="IEq72">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq72.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ \frac{2}{{\sqrt {2\pi } {\sigma_\varepsilon }}}\left[ {\int_0^\xi {\exp \left( { - \tfrac{{{e^2}}}{{2\sigma_\varepsilon^2}}} \right)de - } \xi \exp \left( { - \tfrac{{{\xi^2}}}{{2\sigma_\varepsilon^2}}} \right)} \right] $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c4">


																			<SimplePara>


																				<InlineEquation
ID="IEq73">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq73.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ \tfrac{{2{\sigma_\varepsilon }}}{{\sqrt {2\pi } }}\left[ {\int_0^\xi {\exp \left( { - \tfrac{{{e^2}}}{{2\sigma_\varepsilon^2}}} \right)de - } \xi \exp \left( { - \tfrac{{{\xi^2}}}{{2\sigma_\varepsilon^2}}} \right)} \right] $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c5">


																			<SimplePara>


																				<InlineEquation
ID="IEq74">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq74.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {\tfrac{2}{{\sqrt {2\pi } {\sigma_\varepsilon }}}\left[ {\int_0^\xi {\exp \left( { - \tfrac{{{e^2}}}{{2\sigma_\varepsilon^2}}} \right)de - } \xi \exp \left( { - \tfrac{{{\xi^2}}}{{2\sigma_\varepsilon^2}}} \right)} \right]} \hfill \\ { - \left( {\tfrac{{{\xi^3}}}{{\sqrt {2\pi } \sigma_\varepsilon^3}}} \right)\exp \left( { - \tfrac{{{\xi^2}}}{{2\sigma_\varepsilon^2}}} \right)} \hfill \\ \end{array} $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>


																	</row>



																	<row>


																		<entry colname="c1">


																			<SimplePara>Error function</SimplePara>


																		</entry>



																		<entry colname="c2">


																			<SimplePara>


																				<InlineEquation
ID="IEq75">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq75.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {\psi_{\text{EF}}}\left( \varepsilon \right) = \int_0^\varepsilon {\exp \left( { - {u^2}/2\sigma_y^2} \right)du} $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c3">


																			<SimplePara>


																				<InlineEquation
ID="IEq76">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq76.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {\sigma_y}/{\left( {\sigma_y^2 + \sigma_\varepsilon^2} \right)^{1/2}} $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c4">


																			<SimplePara>


																				<InlineEquation
ID="IEq77">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq77.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ \sigma_y^2{\sin^{ - 1}}\left( {\tfrac{1}{{1 + (\sigma_y^2/\sigma_\varepsilon^2)}}} \right) $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c5">


																			<SimplePara>


																				<InlineEquation
ID="IEq78">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq78.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {\left[ {\left( {1 + 2\sigma_\varepsilon^2/\sigma_y^2} \right)\left( {1 + \sigma_\varepsilon^2/\sigma_y^2} \right)} \right]^{ - 1}} $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>


																	</row>



																	<row>


																		<entry colname="c1">


																			<SimplePara>Dual sign</SimplePara>


																		</entry>



																		<entry colname="c2">


																			<SimplePara>


																				<InlineEquation
ID="IEq79">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq79.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {\psi_{\text{DS}}}\left( \varepsilon \right) = \left\{ {\begin{array}{*{20}{c}} {{\rm sgn} \left( \varepsilon \right),} &amp; {\left| \varepsilon \right| \leqslant \tau } \\ {L{\rm sgn} \left( \varepsilon \right),} &amp; {\left| \varepsilon \right| &gt; \tau } \\ \end{array} } \right. $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c3">


																			<SimplePara>


																				<InlineEquation
ID="IEq80">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq80.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ \sqrt {\tfrac{2}{\pi }} \tfrac{1}{{{\sigma_\varepsilon }}}\left[ {1 + \left( {L - 1} \right)\exp \left( { - {\tau^2}/\left( {2\sigma_\varepsilon^2} \right)} \right)} \right] $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c4">


																			<SimplePara>


																				<InlineEquation
ID="IEq81">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq81.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ {L^2} - \left( {{L^2} - 1} \right)\sqrt {\tfrac{2}{\pi }} \int_0^{\tau /\sigma_\varepsilon^2} {\exp \left( { - {u^2}/2} \right)du} $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>



																		<entry colname="c5">


																			<SimplePara>


																				<InlineEquation
ID="IEq82">


																					<InlineMediaObject>


																						<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq82.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																					</InlineMediaObject>



																					<EquationSource
Format="TEX">$$ \tfrac{1}{2}\exp \left( { - \left( {{\tau^2}/2\sigma_\varepsilon^3} \right)} \right)\left( {{L^2} - 1} \right)\left( {\tau /\sigma_\varepsilon^3} \right) $$</EquationSource>


																				</InlineEquation>


																			</SimplePara>


																		</entry>


																	</row>


																</tbody>


															</tgroup>


														</Table>


													</Para>



													<Para>For most M-estimate functions, 

														<Emphasis
Type="Italic">ψ</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
) = 

														<Emphasis
Type="Italic">q</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
)

														<Emphasis
Type="Italic">e</Emphasis>
. 

														<Emphasis
Type="Italic">q</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
) is equal to one when 

														<InlineEquation ID="IEq83">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq83.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \left| e \right| $$</EquationSource>


														</InlineEquation>
 is less than a certain threshold 

														<Emphasis
Type="Italic">ξ</Emphasis>
 and will gradually decrease to reduce its sensitivity to impulses with large amplitude. Hence, 

														<InlineEquation ID="IEq84">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq84.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ 0 \leqslant \psi \prime (e) \leqslant 1 $$</EquationSource>


														</InlineEquation>
 and it is approximately equal to one when 

														<InlineEquation ID="IEq85">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq85.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \left| e \right| $$</EquationSource>


														</InlineEquation>
 is less than 

														<Emphasis
Type="Italic">ξ</Emphasis>
. Specifically, for the LMM and NLMM algorithms using MH nonlinearity, it can be shown from (

														<InternalRef
RefID="Equ99">C-5</InternalRef>
) and (

														<InternalRef
RefID="Equ5">5</InternalRef>
) that:

														<Equation ID="Equb">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equb.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {A_{\text{MH}}}\left( {\sigma_e^2} \right) = {\text{erf}}\left( {\tfrac{\xi }{{\sqrt 2 {\sigma_e}}}} \right) - \tfrac{{2\xi }}{{\sqrt {2\pi } {\sigma_e}}}\exp \left( { - \tfrac{{{\xi^2}}}{{2\sigma_e^2}}} \right), $$</EquationSource>


														</Equation>
with 

														<InlineEquation ID="IEq86">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq86.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \mathop {\lim }\limits_{\sigma_e^2 \to 0} {A_{\text{MH}}}\left( {\sigma_e^2} \right) \to 1 $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq87">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq87.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \mathop {\lim }\limits_{\sigma_e^2 \to \infty } {A_{\text{MH}}}\left( {\sigma_e^2} \right) \to 0 $$</EquationSource>


														</InlineEquation>
. For sufficiently small step size 

														<Emphasis
Type="Italic">μ</Emphasis>
, the algorithm will converge and 

														<InlineEquation ID="IEq88">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq88.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 will decrease. Because when 

														<InlineEquation ID="IEq89">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq89.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 is much larger than 

														<InlineEquation ID="IEq90">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq90.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\xi^2} $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq91">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq91.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_{\text{MH}}}\left( {\sigma_e^2} \right) \to 0 $$</EquationSource>


														</InlineEquation>
, 

														<Emphasis
Type="Italic">ξ</Emphasis>
 should be carefully chosen otherwise the step size can be unnecessarily small as adaptation starts. This is because 

														<InlineEquation ID="IEq92">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq92.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 can be much larger than 

														<InlineEquation ID="IEq93">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq93.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\xi^2} $$</EquationSource>


														</InlineEquation>
 initially, which depends in turn on the initial condition. If 

														<InlineEquation ID="IEq94">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq94.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_{\text{MH}}}\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 is not made adaptive, an inappropriately chosen 

														<Emphasis
Type="Italic">ξ</Emphasis>
 may suppress the signal component, instead of just the outliers. This will cause 

														<InlineEquation ID="IEq95">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq95.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_{\text{MH}}}\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 to increase gradually and lead to the so called “nonlinear adaptation” problem encountered by the EF nonlinearity in [

														<CitationRef
CitationID="CR35">35</CitationRef>
]. When the algorithm is nearly converged, 

														<InlineEquation ID="IEq96">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq96.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 will be approximately constant and the asymptotic rate of convergence is 

														<InlineEquation ID="IEq97">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq97.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ 1 - \mu {A_{\text{MH}}}\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
, which is the so called “linear adaptation”.
													</Para>



													<Para>Since nonlinear adaptation is typically slow, it should be avoided. In the LMM and NLMM algorithms, 

														<Emphasis
Type="Italic">ξ</Emphasis>
 is chosen to be a multiple of the estimated σ

														<Subscript>


															<Emphasis
Type="Italic">e</Emphasis>


														</Subscript>
 as shown in (

														<InternalRef
RefID="Equ9">9</InternalRef>
). If 

														<InlineEquation ID="IEq98">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq98.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \hat \sigma_e^2 \approx \sigma_e^2 $$</EquationSource>


														</InlineEquation>
, this helps to avoid significant signal suppression by maintaining a fairly stationary 

														<InlineEquation ID="IEq99">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq99.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_{\text{MH}}}\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 as follows:

														<Equation ID="Equc">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equc.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {A_{\text{MH}}} \approx {\text{erf}}\left( {\tfrac{{{k_\xi }}}{{\sqrt 2 }}} \right) - \tfrac{{2{k_\xi }}}{{\sqrt {2\pi } }}\exp \left( { - \tfrac{{k_\xi^2}}{2}} \right), $$</EquationSource>


														</Equation>
which is approximately constant and is slightly less than one. From (

														<InternalRef
RefID="Equ20">20</InternalRef>
), we can see that the degradation in mean convergence over their LMS and NLMS counterparts is therefore minimal. When an impulse with large amplitude is encountered, we will momentarily have 

														<InlineEquation ID="IEq100">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq100.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 &gt; &gt; k_\xi^2\hat \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 and the measurement will be discarded to improve robustness.
													</Para>



													<Para>In Appendix 

														<InternalRef
RefID="Sec17">C</InternalRef>
, it is further shown that for most M-estimate functions and nonlinearities (c.f. (C-1) for the definition of M-nonlinearities), 

														<InlineEquation ID="IEq101">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq101.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 is approximately independent of 

														<InlineEquation ID="IEq102">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq102.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 if ATS mentioned above is used. Consequently, the convergence rate is approximately constant and similar to that of the LMS and NLMS algorithms. Because of its good performance, ATS is highly recommended in practical applications.

														<OrderedList>


															<ListItem>


																<ItemNumber>(R-A2):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">NLMS algorithms with error nonlinearity:</Emphasis>


																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>


													</Para>



													<Para>In the normalized case, (

														<InternalRef
RefID="Equ19">19</InternalRef>
) provides a good approximation at the steady state of the algorithm and for the NLMS algorithms using the M-nonlinearities together with ATS, which we have elaborated above (c.f. the approximation used in Appendix 

														<InternalRef
RefID="Sec14">A</InternalRef>
 and the justification at Appendix 

														<InternalRef
RefID="Sec17">C</InternalRef>
). It should be noted that no such approximation is used in the variants of the LMS algorithms [

														<CitationRef
CitationID="CR34">34</CitationRef>
, 

														<CitationRef
CitationID="CR35">35</CitationRef>
] mentioned above. If ATS is not employed, (

														<InternalRef
RefID="Equ19">19</InternalRef>
) is only an approximation and nonlinear adaptation is likely to be encountered, depending on the clipping level of the nonlinearity and variances of the error signals. The adaptation is typically slow and accurate analytical solution is very difficult to obtain. When the algorithm is nearly converged, 

														<InlineEquation ID="IEq103">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq103.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 will be approximately constant and (

														<InternalRef
RefID="Equ19">19</InternalRef>
) will be a good approximation. The asymptotic rate of convergence is 

														<InlineEquation ID="IEq104">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq104.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ 1 - \mu {A_{\text{MH}}}\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
, which is the familiar “linear adaptation” phase. To avoid slow nonlinear adaptation, ATS is again recommended in practical applications. A conservative maximum step size can also be estimated from (

														<InternalRef
RefID="Equ21">21</InternalRef>
). Because of its importance, the main focus of this paper is those algorithms employing ATS.

														<OrderedList>


															<ListItem>


																<ItemNumber>(R-A3):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">Comparison of LMS and NLMS algorithms with M-nonlinearity and ATS</Emphasis>
:
																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>


													</Para>



													<Para>Next, we briefly compare the LMS/NLMS-based algorithms. It can be seen that with M-nonlinearity and ATS, the convergence rate of the NLMS-based algorithms is 

														<InlineEquation ID="IEq105">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq105.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ 1 - \mu {A_\psi }\left( {\sigma_e^2(n)} \right){I_i}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
, where 

														<InlineEquation ID="IEq106">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq106.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2(n)} \right) $$</EquationSource>


														</InlineEquation>
is approximately constant and usually has a value slightly smaller than one when no impulses are encountered. As a result, compared with the conventional LMS-based algorithms, the step size of the normalized algorithms is changed by a factor of 

														<InlineEquation ID="IEq107">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq107.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ 1/\left( {{A_\psi }\left( {\sigma_e^2(n)} \right){I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)} \right) \approx 1/{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
. Since the maximum of the product

														<Equation ID="Equd">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equd.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\lambda_i}{I_i}\left( \Lambda \right) = {\lambda_i}\int_0^\infty {exp\left( { - \beta \varepsilon } \right)\left[ {\mathop \Pi \limits_{k = 1}^L {{\left( {2\beta {\lambda_k} + 1} \right)}^{ - 1/2}}} \right]{{\left( {2\beta {\lambda_i} + 1} \right)}^{ - 1}}d\beta } } \\ { = \int_0^\infty {exp\left( { - \beta \varepsilon } \right)\left[ {\Pi_{k = 1}^L{{\left( {2\beta {\lambda_k} + 1} \right)}^{ - 1/2}}} \right]{{\left[ {2\beta + {{\left( {{\lambda_i}} \right)}^{ - 1}}} \right]}^{ - 1}}d\beta }, } \\ \end{array} $$</EquationSource>


														</Equation>
also achieves its maximum at the largest eigenvalue λ

														<Subscript>max</Subscript>
 with the corresponding value of 

														<Emphasis
Type="Italic">I</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">i</Emphasis>


														</Subscript>
(Λ) given by 

														<InlineEquation ID="IEq108">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq108.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_{i\_{\lambda_{\max }}}}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
, the fastest convergence rate of the normalized algorithm occurs when 

														<InlineEquation ID="IEq109">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq109.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \mu = 1/\left( {{A_\psi }\left( {\sigma_e^2} \right){\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)} \right) $$</EquationSource>


														</InlineEquation>
 and it is limited by the mode corresponding to the smallest eigenvalue λ

														<Subscript>min</Subscript>
 with the corresponding value of 

														<Emphasis
Type="Italic">I</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">i</Emphasis>


														</Subscript>
(Λ) given by 

														<InlineEquation ID="IEq110">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq110.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_{i\_{\lambda_{\min }}}}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
. That is

														<Equation ID="Equ22">


															<EquationNumber>22</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ22.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ 1 - \frac{{2{\lambda_{\min }}{I_{i\_{\lambda_{\min }}}}\left( \Lambda \right)}}{{{\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)}}. $$</EquationSource>


														</Equation>
From the definition of 

														<Emphasis
Type="Italic">I</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">i</Emphasis>


														</Subscript>
(Λ), it can be shown that 

														<InlineEquation ID="IEq111">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq111.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_{i\_{\lambda_{\min }}}}\left( \Lambda \right)/{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right) \geqslant 1 $$</EquationSource>


														</InlineEquation>
. In other words, the eigenvalue spread λ

														<Subscript>max</Subscript>
/λ

														<Subscript>min</Subscript>
 is reduced by a factor 

														<InlineEquation ID="IEq112">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq112.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)/{I_{i\_{\lambda_{\min }}}}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
 after the normalization. Therefore, under the stated assumptions, the maximum convergence rate of the normalized algorithms using ATS is faster than the LMS-based algorithms if the eigenvalues are unequal. Similar conclusion is obtained for the conventional NLMS algorithm [

														<CitationRef
CitationID="CR37">37</CitationRef>
, 

														<CitationRef
CitationID="CR46">46</CitationRef>
].
													</Para>


												</FormalPara>


											</Section3>



											<Section3 ID="Sec8">


												<Heading>Mean Square Behavior</Heading>



												<Para>Post-multiplying (

													<InternalRef
RefID="Equ14">14</InternalRef>
) by its transpose and taking expectation gives

													<Equation ID="Equ23">


														<EquationNumber>23</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ23.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\mathbf{\Xi }}\left( {n + 1} \right) = {\mathbf{\Xi }}(n) - {M_1} - {M_2} + {M_3}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq113">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq113.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathbf{\Xi }}(n) = E\left[ {v(n){v^T}(n)} \right] $$</EquationSource>


													</InlineEquation>
, 

													<Equation ID="Equ24">


														<EquationNumber>24</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ24.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {M_1} = \mu {E_{\left\{ {\mathbf{v}} \right\}}}\left[ {H{v^T}} \right] \approx \mu {A_\psi }\left( {\sigma_e^2} \right)U\Lambda {D_\Lambda }{U^T}{\mathbf{\Xi }}\left( {n - 1} \right), $$</EquationSource>


													</Equation>



													<Equation ID="Equ25">


														<EquationNumber>25</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ25.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {M_2} = M_1^T \approx \mu {A_\psi }\left( {\sigma_e^2} \right){\mathbf{\Xi }}(n)U{D_\Lambda }\Lambda {U^T}, $$</EquationSource>


													</Equation>
and

													<Equation ID="Equ26">


														<EquationNumber>26</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ26.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {M_3} = {\mu^2}E\left[ {{\psi^2}(e)X{X^T}/{{\left( {\varepsilon + {X^T}X} \right)}^2}} \right]. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Note, the expressions in (

													<InternalRef
RefID="Equ24">24</InternalRef>
) and (

													<InternalRef
RefID="Equ25">25</InternalRef>
) are obtained by using previous result in (

													<InternalRef
RefID="Equ18">18</InternalRef>
), and (

													<InternalRef
RefID="Equ26">26</InternalRef>
) is obtained from the stationarity and independence assumptions. 

													<Emphasis
Type="BoldItalic">M</Emphasis>



													<Subscript>3</Subscript>
 is evaluated in Appendix 

													<InternalRef
RefID="Sec16">B</InternalRef>
 to be

													<Equation ID="Equ27">


														<EquationNumber>27</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ27.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{M_3} \approx 2{\mu^2}{C_\psi }\left( {\sigma_e^2} \right)U\left\{ {\Lambda \left[ {\left( {{U^T}{\mathbf{\Xi }}(n)U} \right) \circ I\left( \Lambda \right)} \right]\Lambda } \right\}{U^T}} \\ { + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)U{{\overline D }_2}{U^T} + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\sigma_g^2U\Lambda I\prime \left( \Lambda \right){U^T}.} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq114">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq114.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) = \tfrac{d}{{d\sigma_e^2}}E\left[ {{\psi^2}(e)} \right] $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq115">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq115.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2} \right) = {B_\psi }\left( {\sigma_e^{\text{2}}} \right)/\sigma_e^{\text{2}} $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq116">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq116.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^{\text{2}}} \right) = E\left[ {{\psi^2}(e)} \right] = \tfrac{1}{{\sqrt {2\pi } {\sigma_e}}}\int_{ - \infty }^\infty {{\psi^2}(e)\exp \left( { - \tfrac{{{e^2}}}{{2\sigma_e^2}}} \right)de} $$</EquationSource>


													</InlineEquation>
. ○ denotes element-wise product of two matrices (Hadamard product), and 

													<Emphasis
Type="BoldItalic">I</Emphasis>
(Λ) and 

													<Emphasis
Type="BoldItalic">I</Emphasis>



													<Emphasis Type="Italic">′</Emphasis>
(Λ) are defined in (

													<InternalRef
RefID="Equ90">B-14</InternalRef>
) and (

													<InternalRef
RefID="Equ93">B-16</InternalRef>
). The diagonal matrix 

													<InlineEquation ID="IEq117">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq117.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\overline D_2} $$</EquationSource>


													</InlineEquation>
 results from (

													<InternalRef
RefID="Equ91">B-14b</InternalRef>
) and its 

													<Emphasis Type="Italic">i</Emphasis>
-th element is 

													<InlineEquation ID="IEq118">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq118.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {{{\overline D }_2}} \right]_{i,i}} = \mathop \Sigma \limits_k {\lambda_k}{\lambda_i}{I_{ki}}\left( \Lambda \right){\left[ {{U^T}{\mathbf{\Xi }}(n)U} \right]_{k,k}} $$</EquationSource>


													</InlineEquation>
. For a given distortion measure 

													<Emphasis Type="Italic">ρ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
) and hence 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
), these integrals can either be computed analytically or numerically.
												</Para>



												<Para>Substituting (

													<InternalRef
RefID="Equ24">24</InternalRef>
)–(

													<InternalRef
RefID="Equ27">27</InternalRef>
) into (

													<InternalRef
RefID="Equ23">23</InternalRef>
), and using the natural coordinate 

													<InlineEquation ID="IEq119">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq119.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathbf{\Phi }}(n) = {U^T}{\mathbf{\Xi }}(n)U $$</EquationSource>


													</InlineEquation>
, one gets

													<Equation ID="Equ28">


														<EquationNumber>28</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ28.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\mathbf{\Phi }}\left( {n + 1} \right) \approx {\mathbf{\Phi }}(n) - \mu {A_\psi }\left( {\sigma_e^2} \right)\Lambda {D_\Lambda }{\mathbf{\Phi }}(n) - \mu {A_\psi }\left( {\sigma_e^2} \right){\mathbf{\Phi }}(n){D_\Lambda }\Lambda } \\ { + 2{\mu^2}{C_\psi }\left( {\sigma_e^2} \right)\left[ {\Lambda \left( {{\mathbf{\Phi }}(n) \circ I\left( \Lambda \right)} \right)\Lambda } \right] + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right){{\mathop D\limits^\sim }_2}} \\ { + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\sigma_g^2\Lambda I\prime \left( \Lambda \right),} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq120">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq120.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {{{\mathop D\limits^\sim }_2}} \right]_{i,i}} = \mathop \Sigma \limits_k {\lambda_k}{\lambda_i}{I_{ki}}\left( \Lambda \right){\left[ {{\mathbf{\Phi }}(n)} \right]_{k,k}} $$</EquationSource>


													</InlineEquation>
. (

													<InternalRef
RefID="Equ28">28</InternalRef>
) can be written as the following scalar form:

													<Equation ID="Equ29">


														<EquationNumber>29</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ29.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\Phi_{i,i}}\left( {n + 1} \right) \approx \left( {1 - 2\mu {A_\psi }\left( {\sigma_e^2} \right){\lambda_i}{I_i}\left( \Lambda \right) + 2{\mu^2}{C_\psi }\left( {\sigma_e^2} \right)\lambda_i^2{I_{ii}}\left( \Lambda \right)} \right){\Phi_{i,i}}(n)} \hfill \\ { + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\mathop \Sigma \limits_k {\lambda_k}{\lambda_i}{I_{ki}}\left( \Lambda \right){\Phi_{k,k}}(n) + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\sigma_g^2{\lambda_i}I_i^\prime \left( \Lambda \right).} \hfill \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>To study the step size bound for mean square convergence, we first note that the EMSE at time instant 

													<Emphasis Type="Italic">n</Emphasis>
 is given by 

													<InlineEquation ID="IEq121">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq121.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\text{EMSE}}(n) = {\text{Tr}}\left( {{\mathbf{\Phi }}(n)\Lambda } \right) $$</EquationSource>


													</InlineEquation>
. Assuming that algorithm converges, it is shown in Appendix 

													<InternalRef
RefID="Sec16">B</InternalRef>
 that the last two terms on the right hand side of (

													<InternalRef
RefID="Equ29">29</InternalRef>
) are tightly upper bounded by 

													<InlineEquation ID="IEq122">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq122.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\sigma_e^2\left( \infty \right){\lambda_i}I_i^\prime \left( \Lambda \right) $$</EquationSource>


													</InlineEquation>
 at the steady state. Therefore, from (

													<InternalRef
RefID="Equ28">28</InternalRef>
), one gets an upper bound for the EMSE as

													<Equation ID="Equ30">


														<EquationNumber>30</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ30.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) = {\text{Tr}}\left( {{\mathbf{\Phi }}\left( \infty \right)\Lambda } \right) \approx \tfrac{1}{2}\mu \sigma_e^2\left( \infty \right){\phi_{{\text{NLMS}}\_\psi }}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq123">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq123.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\phi_{{\text{NLMS}}\_\psi }} = {S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)\mathop \Sigma \nolimits_{i = 1}^L \tfrac{{{\lambda_i}I_i^\prime \left( \Lambda \right)}}{{{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right) - \mu {C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){\lambda_i}{I_{ii}}\left( \Lambda \right)}} $$</EquationSource>


													</InlineEquation>
. Using the fact that 

													<InlineEquation ID="IEq124">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq124.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2\left( \infty \right) = {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
, one gets

													<Equation ID="Equ31">


														<EquationNumber>31</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ31.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) = \frac{{\tfrac{1}{2}\mu \sigma_g^2{\phi_{{\text{NLMS}}\_\psi }}}}{{1 - \tfrac{1}{2}\mu {\phi_{{\text{NLMS}}\_\psi }}}}. $$</EquationSource>


													</Equation>
which is a nonlinear equation in 

													<InlineEquation ID="IEq125">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq125.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2\left( \infty \right) $$</EquationSource>


													</InlineEquation>
 and hence 

													<InlineEquation ID="IEq126">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq126.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) $$</EquationSource>


													</InlineEquation>
. It can be seen that 

													<InlineEquation ID="IEq127">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq127.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) $$</EquationSource>


													</InlineEquation>
 is unbounded when either its denominator becomes zero or when 

													<InlineEquation ID="IEq128">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq128.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\phi_{{\text{NLMS}}\_\psi }} $$</EquationSource>


													</InlineEquation>
 becomes unbounded due to any of the denominators of its partial sum becomes zero. These two conditions allow us to determine the following conditions for the maximum step size:

													<Equation ID="Equ32">


														<EquationNumber>32a</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ32.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \mu&lt;2/{\phi_{{\text{NLMS}}\_\psi }}, $$</EquationSource>


													</Equation>



													<Equation ID="Equ33">


														<EquationNumber>32b</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ33.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ 0&lt;\mu&lt;{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right)/\left[ {{C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){\lambda_i}{I_{ii}}\left( \Lambda \right)} \right]. $$</EquationSource>


													</Equation>


												</Para>



												<Para>For the conventional LMS algorithm, 

													<InlineEquation ID="IEq129">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq129.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi },{C_\psi },{I_i}\left( \Lambda \right),I_i^\prime \left( \Lambda \right) $$</EquationSource>


													</InlineEquation>
, and 

													<Emphasis Type="Italic">I</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">ii</Emphasis>


													</Subscript>
(Λ) are all equal to one, (

													<InternalRef
RefID="Equ32">32a</InternalRef>
) and (

													<InternalRef
RefID="Equ33">32b</InternalRef>
) are identical to the necessary and sufficient conditions for the mean square convergence of the LMS algorithm previously obtained in [

													<CitationRef
CitationID="CR14">14</CitationRef>
]. Similar results are obtained in [

													<CitationRef
CitationID="CR15">15</CitationRef>
] by solving the difference equation in 

													<Emphasis Type="Italic">Φ</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) and in [

													<CitationRef
CitationID="CR16">16</CitationRef>
] by a matrix analysis technique. Furthermore, a lower bound of the maximum step size for mean square convergence is obtained in [

													<CitationRef
CitationID="CR15">15</CitationRef>
]. Here, we extend this approach to study the stability bound of our algorithms. To this end, we write (

													<InternalRef
RefID="Equ32">32a</InternalRef>
) in full as follows

													<Equation ID="Equ34">


														<EquationNumber>33</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ34.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)\mathop \Sigma \nolimits_{i = 1}^L \tfrac{{\mu {\lambda_i}I_i^\prime \left( \Lambda \right)}}{{{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right) - \mu {C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){\lambda_i}{I_{ii}}\left( \Lambda \right)}}&lt;2. $$</EquationSource>


													</Equation>
For convenience, rewrite (

													<InternalRef
RefID="Equ34">33</InternalRef>
) as

													<Equation ID="Equ35">


														<EquationNumber>34</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ35.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \mathop \Sigma \nolimits_{i = 1}^L \frac{{\mu {\lambda_i}{c_i}}}{{1 - \mu {\lambda_i}{d_i}}}&lt;2, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq130">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq130.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {c_i} = I_i^\prime \left( \Lambda \right)/\left( {{A_\psi }\left( {\sigma_e^2\left( \infty \right){I_i}\left( \Lambda \right)} \right)} \right. $$</EquationSource>


													</InlineEquation>
, and 

													<InlineEquation ID="IEq131">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq131.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {d_i} = {C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_{ii}}\left( \Lambda \right)/\left( {{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right)} \right) $$</EquationSource>


													</InlineEquation>
.
												</Para>



												<Para>The right hand side has singularities at 1/(λ

													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>



													<Emphasis Type="Italic">d</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
). Between 0 and 

													<InlineEquation ID="IEq132">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq132.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \mathop {\min }\limits_i \left( {1/\left( {{\lambda_i}{d_i}} \right)} \right) $$</EquationSource>


													</InlineEquation>
, it is a monotonic increasing function of 

													<Emphasis Type="Italic">μ</Emphasis>
. Therefore, it will reach the critical value of two on the right hand side of the equality at the smallest root 

													<Emphasis Type="Italic">μ</Emphasis>



													<Subscript>max</Subscript>
 of the equation

													<Equation ID="Equ36">


														<EquationNumber>35</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ36.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \mathop \Sigma \nolimits_{i = 1}^L \frac{{\mu {\lambda_i}{c_i}}}{{1 - \mu {\lambda_i}{d_i}}} = 2, $$</EquationSource>


													</Equation>
before 

													<InlineEquation ID="IEq133">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq133.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \mathop {\min }\limits_i \left( {1/\left( {{\lambda_i}{d_i}} \right)} \right) $$</EquationSource>


													</InlineEquation>
. Any positive step size 

													<Emphasis Type="Italic">μ</Emphasis>
 below 

													<Emphasis Type="Italic">μ</Emphasis>



													<Subscript>max</Subscript>
 will ensure the mean square convergence of the LMS algorithm.
												</Para>



												<Para>Let 

													<Emphasis Type="Italic">u</Emphasis>
 = 2 

													<Emphasis Type="Italic">μ</Emphasis>



													<Superscript>−1</Superscript>
 and rewrite (

													<InternalRef
RefID="Equ36">35</InternalRef>
) as

													<Equation ID="Equ37">


														<EquationNumber>36</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ37.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \ell (u) - \mathop \Sigma \nolimits_{i = 1}^L {\lambda_i}{c_i}{l_i}(u) = \mathop \Pi \limits_{i = 1}^L \left( {u - {{\bar u}_i}} \right) = \mathop \Sigma \nolimits_{i = 1}^L {\left( { - 1} \right)}^{L-i} \;{b_{L - i}}{u^i} = 0, $$</EquationSource>


													</Equation>
where, 

													<InlineEquation ID="IEq134">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq134.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \ell (u) = \mathop \Pi \limits_{i = 1}^L \left( {u - 2{\lambda_i}{d_i}} \right) $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq135">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq135.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {l_i}(u) = \ell (u)/\left( {u - 2{\lambda_i}{d_i}} \right) $$</EquationSource>


													</InlineEquation>
, and 

													<InlineEquation ID="IEq136">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq136.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline u_i^{ - 1} $$</EquationSource>


													</InlineEquation>
 are the roots of (

													<InternalRef
RefID="Equ37">36</InternalRef>
). The largest root of (

													<InternalRef
RefID="Equ37">36</InternalRef>
) (smallest root of (

													<InternalRef
RefID="Equ35">34</InternalRef>
)) is upper bounded (lower bounded) by [

													<CitationRef
CitationID="CR47">47</CitationRef>
]

													<Equation ID="Equ38">


														<EquationNumber>37</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ38.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {u_{N\_\max }} \leqslant \tfrac{1}{L}\left[ {{s_1} + \sqrt {\left( {L - 1} \right)\left( {L{s_2} - s_1^2} \right)} } \right], $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq137">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq137.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {s_1} = \mathop \Sigma \nolimits_{i = 1}^L {\overline u_i} = {b_1} $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq138">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq138.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {s_2} = \mathop \Sigma \nolimits_{i = 1}^L \bar u_i^2 = b_1^2 - {2b_2} $$</EquationSource>


													</InlineEquation>
.
												</Para>



												<Para>By comparing the coefficients on different sides of (

													<InternalRef
RefID="Equ37">36</InternalRef>
), one also gets

													<Equation ID="Eque">


														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Eque.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {b_1} = \mathop \Sigma \nolimits_{i = 1}^L {\lambda_i}\left( {2{d_i} + {c_i}} \right) $$</EquationSource>


													</Equation>
and

													<Equation ID="Equ39">


														<EquationNumber>38</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ39.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {b_2} = 4\mathop \Sigma \limits_{1 \leqslant i \ne j \leqslant L} {\lambda_i}{\lambda_j}{d_i}{d_j} + \mathop \Sigma \nolimits_{i = 1}^L {\lambda_i}{c_i}\left( {2\mathop \Sigma \limits_{1 \leqslant j \ne i \leqslant L} {\lambda_j}{d_j}} \right). $$</EquationSource>


													</Equation>


												</Para>



												<Para>From (

													<InternalRef
RefID="Equ38">37</InternalRef>
), a more convenient lower bound of 

													<Emphasis Type="Italic">μ</Emphasis>



													<Subscript>max</Subscript>
 can be obtained as follows

													<Equation ID="Equ40">


														<EquationNumber>39</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ40.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\mu_{\max }} \geqslant \tfrac{{2L}}{{{b_1} + \sqrt {{{\left( {L - 1} \right)}^2}\left. {b_1^2 -2L(L-1){b_2}} \right)} }}} \\ { \geqslant \tfrac{{2L}}{{{b_1} + \sqrt {{{\left( {L - 1} \right)}^2}b_1^2} }} = 2/{b_1} \equiv {\mu_B}} \\ { = \tfrac{{2{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)}}{{{S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)\mathop \Sigma \nolimits_{i = 1}^L {\lambda_i}\left[ {I_i^\prime \left( \Lambda \right)/\left( {{I_i}\left( \Lambda \right)} \right) + 2{C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_{ii}}\left( \Lambda \right)/{I_i}\left( \Lambda \right)} \right]}}.} \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>It can be seen from (

													<InternalRef
RefID="Equ40">39</InternalRef>
) that 

													<InlineEquation ID="IEq139">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq139.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq140">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq140.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq141">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq141.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) $$</EquationSource>


													</InlineEquation>
 also depend on 

													<InlineEquation ID="IEq142">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq142.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2\left( \infty \right) $$</EquationSource>


													</InlineEquation>
 and the exact step size bound is still very difficult to obtain analytically. For M-nonlinearities with ATS, they are approximately constant and the step size bound can be determined accordingly from 

													<Emphasis Type="Italic">μ</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">B</Emphasis>


													</Subscript>
. Alternatively, one can replace 

													<InlineEquation ID="IEq143">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq143.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq144">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq144.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq145">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq145.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) $$</EquationSource>


													</InlineEquation>
 by their worse case values to estimate the step size bound 

													<Emphasis Type="Italic">μ</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">B</Emphasis>


													</Subscript>
.
												</Para>



												<Para>In passing, we find from simulation results that the term 

													<InlineEquation ID="IEq146">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq146.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \mu_k^2\Sigma {\lambda_k}{\lambda_i}{I_{ki}}\left( \Lambda \right){\Phi_{k,k}}(n) $$</EquationSource>


													</InlineEquation>
 in (

													<InternalRef
RefID="Equ29">29</InternalRef>
) is very small for small EMSE. Consequently, (

													<InternalRef
RefID="Equ29">29</InternalRef>
) can be approximated as:

													<Equation ID="Equf">


														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equf.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\Phi_{i,i}}\left( {n + 1} \right) \approx \left( {1 - 2\mu {A_\psi }\left( {\sigma_e^2} \right){\lambda_i}{I_i}\left( \Lambda \right) + 2{\mu^2}{C_\psi }\left( {\sigma_e^2} \right)\lambda_i^2{I_{ii}}\left( \Lambda \right)} \right){\Phi_{i,i}}(n)} \\ { + {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\sigma_g^2{\lambda_i}I_i^\prime \left( \Lambda \right).} \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>For notation convenience, we have dropped the time index 

													<Emphasis Type="Italic">n</Emphasis>
 in 

													<InlineEquation ID="IEq147">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq147.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(n) = E\left[ {{v^T}(n){R_{XX}}v(n)} \right] + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
. The algorithm will converge when 

													<InlineEquation ID="IEq148">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq148.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| {1 - 2\mu {A_\psi }\left( {\sigma_e^2} \right){\lambda_i}{I_i}\left( \Lambda \right) + 2{\mu^2}{C_\psi }\left( {\sigma_e^2} \right)\lambda_i^2{I_{ii}}\left( \Lambda \right)} \right|&lt;1 $$</EquationSource>


													</InlineEquation>
, which gives (

													<InternalRef
RefID="Equ33">32b</InternalRef>
): 

													<InlineEquation ID="IEq149">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq149.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \mu&lt;{A_\psi }\left( {\sigma_e^2} \right){I_i}\left( \Lambda \right)/\left( {{C_\psi }\left( {\sigma_e^2} \right){\lambda_i}{I_{ii}}\left( \Lambda \right)} \right) $$</EquationSource>


													</InlineEquation>
 for all 

													<Emphasis Type="Italic">i.</Emphasis>
 That is, the results of (

													<InternalRef
RefID="Equ32">32a</InternalRef>
) and (

													<InternalRef
RefID="Equ33">32b</InternalRef>
) are very close to each other. From the definition of 

													<Emphasis Type="Italic">I</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
(Λ) and 

													<Emphasis Type="Italic">I</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">ii</Emphasis>


													</Subscript>
(Λ), we then have 

													<InlineEquation ID="IEq150">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq150.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {I_i}\left( \Lambda \right)/\left( {{\lambda_i}{I_{ii}}\left( \Lambda \right)} \right) = 2/\left( {1 - I_i^{\prime \prime }\left( \Lambda \right)/{I_i}\left( \Lambda \right)} \right) &gt; 2 $$</EquationSource>


													</InlineEquation>
, where 

													<InlineEquation ID="IEq151">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq151.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ I_i^{\prime \prime }\left( \Lambda \right) = \int_0^\infty {\exp \left( { - \beta \varepsilon } \right)\left[ {\mathop \Pi \limits_{k = 1}^L {{\left( {2\beta {\lambda_k} + 1} \right)}^{ - 1/2}}} \right]} {\left( {2\beta {\lambda_i} + 1} \right)^{ - 2}}d\beta $$</EquationSource>


													</InlineEquation>
. Therefore, a conservative stability bound for small EMSE is 

													<InlineEquation ID="IEq152">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq152.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mu_B}&lt;2{A_\psi }\left( {\sigma_e^2} \right)/{C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
. For M-nonlinearities with ATS, 

													<InlineEquation ID="IEq153">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq153.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mu_B}&lt;2{A_\psi }/{C_\psi } $$</EquationSource>


													</InlineEquation>
, which is a useful “rule of thumb” step size bound because it does not require any prior knowledge of the input signal.
												</Para>



												<FormalPara RenderingStyle="Style1">


													<Heading>Remarks</Heading>



													<Para>


														<OrderedList>


															<ListItem>


																<ItemNumber>(R-A4):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">LMS algorithms with error nonlinearity:</Emphasis>


																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>
When 

														<InlineEquation ID="IEq154">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq154.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_i}\left( \Lambda \right) = I_i^\prime \left( \Lambda \right) = {I_{ii}}\left( \Lambda \right) = 1 $$</EquationSource>


														</InlineEquation>
, our analysis will reduce to that for the LMS algorithm with general nonlinearity. (

														<InternalRef
RefID="Equ31">31</InternalRef>
) will reduce to

														<Equation ID="Equ41">


															<EquationNumber>40</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ41.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{LMS}}\_\psi }}\left( \infty \right) \approx \frac{{\tfrac{1}{2}\mu \sigma_g^2{\phi_{{\text{LMS}}\_\psi }}}}{{1 - \tfrac{1}{2}\mu {\phi_{{\text{LMS}}\_\psi }}}}, $$</EquationSource>


														</Equation>
where 

														<InlineEquation ID="IEq155">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq155.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\phi_{{\text{LMS}}\_\psi }} = {S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)\mathop \Sigma \nolimits_{i = 1}^L \tfrac{{{\lambda_i}}}{{{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) - \mu {C_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){\lambda_i}}} $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq156">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq156.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right) = \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq157">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq157.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq158">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq158.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 for some related algorithms are summarized in Table 

														<InternalRef
RefID="Tab1">1</InternalRef>
. For the DS algorithm [

														<CitationRef
CitationID="CR34">34</CitationRef>
], (

														<InternalRef
RefID="Equ41">40</InternalRef>
) will reduce to [

														<CitationRef
CitationID="CR34">34</CitationRef>
, Eq. (

														<InternalRef
RefID="Equ24">24</InternalRef>
)]. If 

														<Emphasis
Type="Italic">μ</Emphasis>
 is sufficiently small, then the contribution of the weight-error vector to 

														<InlineEquation ID="IEq159">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq159.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2\left( \infty \right) $$</EquationSource>


														</InlineEquation>
 can be ignored. Accordingly, 

														<InlineEquation ID="IEq160">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq160.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2\left( \infty \right) \approx \sigma_g^2 $$</EquationSource>


														</InlineEquation>
 and (

														<InternalRef
RefID="Equ31">31</InternalRef>
) can be simplified to 

														<InlineEquation ID="IEq161">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq161.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{LMS}}\_\psi }}\left( \infty \right) \approx \tfrac{{\mu {S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)\sigma_g^2}}{{2{A_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)}}\mathop \Sigma \nolimits_{i = 1}^L {\lambda_i} $$</EquationSource>


														</InlineEquation>
. This agrees with [

														<CitationRef
CitationID="CR34">34</CitationRef>
, Eq. (

														<InternalRef
RefID="Equ25">25</InternalRef>
)] for the DS algorithm. Further, if the input is white, it will reduce to [

														<CitationRef
CitationID="CR35">35</CitationRef>
 Eq. 

														<InternalRef
RefID="Equ46">45</InternalRef>
] for the EF nonlinearity case.
													</Para>



													<Para>For M-nonlinearities with ATS, 

														<InlineEquation ID="IEq162">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq162.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq163">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq163.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq164">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq164.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 are approximately constant (c.f. Appendix 

														<InternalRef
RefID="Sec17">C</InternalRef>
). Denote them by 

														<InlineEquation ID="IEq165">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq165.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi } $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq166">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq166.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi } $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq167">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq167.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi } $$</EquationSource>


														</InlineEquation>
, respectively. Then, (

														<InternalRef
RefID="Equ30">30</InternalRef>
) becomes

														<Equation ID="Equ42">


															<EquationNumber>41</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ42.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{LMS}}\_\psi }}\left( \infty \right) \approx \frac{{\tfrac{1}{2}\mu \sigma_g^2{\phi_{{\text{LMS}}\_\psi }}}}{{1 - \tfrac{1}{2}\mu {\phi_{{\text{LMS}}\_\psi }}}}, $$</EquationSource>


														</Equation>
where 

														<InlineEquation ID="IEq168">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq168.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\phi_{{\text{LMS}}\_\psi }} = {S_\psi }\mathop \Sigma \nolimits_{i = 1}^L \tfrac{{{\lambda_i}}}{{{A_\psi } - \mu {\lambda_i}{C_\psi }}} $$</EquationSource>


														</InlineEquation>
. The stability bound from (32) is 

														<InlineEquation ID="IEq169">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq169.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \mu&lt;\min \left( {2/{\phi_{{\text{LMS}}\_\psi }},\,{A_\psi }/\left( {{C_\psi }{\lambda_i}} \right)} \right) $$</EquationSource>


														</InlineEquation>
, from which we can obtain its lower bound from (

														<InternalRef
RefID="Equ39">38</InternalRef>
) as follows:

														<Equation ID="Equ43">


															<EquationNumber>42</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ43.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\mu_{B\_{\text{LMS}}\_\psi }} = \frac{{2{A_\psi }}}{{{S_\psi }\mathop \Sigma \nolimits_{i = 1}^L {\lambda_i}\left( {1 + 2{C_\psi }} \right)}}. $$</EquationSource>


														</Equation>
If 

														<Emphasis
Type="Italic">ψ</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
) = 

														<Emphasis
Type="Italic">e</Emphasis>
 and 

														<InlineEquation ID="IEq170">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq170.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi } = {S_\psi } = {C_\psi } = 1 $$</EquationSource>


														</InlineEquation>
 we obtain the traditional LMS algorithm. (

														<InternalRef
RefID="Equ41">40</InternalRef>
) and (

														<InternalRef
RefID="Equ43">42</InternalRef>
) will respectively reduce to the EMSE(∞) and stability bound for the conventional LMS algorithm derived in [

														<CitationRef
CitationID="CR15">15</CitationRef>
]. For the LMM algorithm using MH-nonlinearity with a practical value of 

														<InlineEquation ID="IEq171">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq171.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {k_\xi } = 2.576 $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq172">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq172.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi } $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq173">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq173.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi } $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq174">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq174.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi } $$</EquationSource>


														</InlineEquation>
 are quite close to one, and its performance is therefore similar to that of the conventional LMS algorithm.

														<OrderedList>


															<ListItem>


																<ItemNumber>(R-A5):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">NLMS algorithms with error nonlinearity:</Emphasis>


																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>


													</Para>



													<Para>For the normalized version, (

														<InternalRef
RefID="Equ31">31</InternalRef>
) will be a good approximation at the steady state of the algorithm and for the NLMS algorithms using M-nonlinearities and ATS (c.f. the approximation used in Appendix 

														<InternalRef
RefID="Sec16">B</InternalRef>
). For the latter, (

														<InternalRef
RefID="Equ31">31</InternalRef>
) can be simplified to

														<Equation ID="Equ44">


															<EquationNumber>43</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ44.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) \approx \tfrac{{\mu \sigma_e^2\left( \infty \right)}}{2}{\phi_{{\text{NLMS}}\_\psi }}, $$</EquationSource>


														</Equation>
where 

														<InlineEquation ID="IEq175">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq175.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\phi_{{\text{NLMS}}\_\psi }} = \tfrac{{{S_\psi }}}{{{A_\psi }}}\mathop \Sigma \nolimits_{i = 1}^L \tfrac{{{\lambda_i}I_i^\prime \left( \Lambda \right)}}{{{I_i}\left( \Lambda \right) - \mu {\lambda_i}{I_{ii}}\left( \Lambda \right)\left( {{C_\psi }/{A_\psi }} \right)}} $$</EquationSource>


														</InlineEquation>
. Solving (

														<InternalRef
RefID="Equ44">43</InternalRef>
) yields

														<Equation ID="Equ45">


															<EquationNumber>44</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ45.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) \approx \frac{{\tfrac{1}{2}\mu \sigma_g^2{\phi_{{\text{NLMS}}\_\psi }}}}{{1 - \tfrac{1}{2}\mu {\phi_{{\text{NLMS}}\_\psi }}}}. $$</EquationSource>


														</Equation>


													</Para>



													<Para>The stability bound from (

														<InternalRef
RefID="Equ32">32</InternalRef>
) satisfies: 

														<InlineEquation ID="IEq176">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq176.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \mu&lt;2/{\phi_{{\text{NLMS}}\_\psi }},0&lt;\mu&lt;{A_\psi }/\left. {\left( {{C_\psi }{\lambda_i}} \right)} \right) $$</EquationSource>


														</InlineEquation>
, from which we can obtain its lower bound from (

														<InternalRef
RefID="Equ40">39</InternalRef>
) as follows:

														<Equation ID="Equ46">


															<EquationNumber>45</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ46.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\mu_{B\_{\text{NLMS}}\_\psi }} = \tfrac{{2{A_\psi }}}{{{S_\psi }\mathop \Sigma \nolimits_{i = 1}^L {\lambda_i}\left[ {I_i^\prime \left( \Lambda \right)/\left( {{I_i}\left( \Lambda \right)} \right) + 2{C_\psi }{I_{ii}}\left( \Lambda \right)/{I_i}\left( \Lambda \right)} \right]}}. $$</EquationSource>


														</Equation>


													</Para>



													<Para>For 

														<Emphasis
Type="Italic">ψ</Emphasis>
(

														<Emphasis
Type="Italic">e</Emphasis>
) = 

														<Emphasis
Type="Italic">e</Emphasis>
 and 

														<InlineEquation ID="IEq177">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq177.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi } = {S_\psi } = {C_\psi } = 1 $$</EquationSource>


														</InlineEquation>
 we obtain the traditional NLMS algorithm. (

														<InternalRef
RefID="Equ44">43</InternalRef>
) and (

														<InternalRef
RefID="Equ46">45</InternalRef>
) will reduce to the EMSE(∞) and stability bound for the conventional NLMS algorithm derived in [

														<CitationRef
CitationID="CR37">37</CitationRef>
]. For the NLMM algorithm using MH- nonlinearity with a practical value of 

														<InlineEquation ID="IEq178">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq178.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {k_\xi } = 2.576 $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq179">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq179.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi } $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq180">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq180.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi } $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq181">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq181.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi } $$</EquationSource>


														</InlineEquation>
 are quite close to one, and its performance is therefore similar to that of the conventional NLMS algorithm.
													</Para>


												</FormalPara>


											</Section3>


										</Section2>



										<Section2 ID="Sec9">


											<Heading>Convergence Behaviors of the NLMS Algorithm with Error Nonlinearity and Gaussian Input and CG Noise</Heading>



											<Para>We now analyze the mean and mean square behaviors of the various algorithms studied above in Section 

												<InternalRef
RefID="Sec6">3.1</InternalRef>
 in CG noise environment. From the previous analysis, we note that the assumption of Gaussian input and additive noise allows us to use the Price’s theorem to approximately decouple the effect of the nonlinearity. For most M-estimate functions which suppress outliers with large amplitude, the convergence rate will be slightly impaired with a similar EMSE, after using ATS. We shall show in the following that this will be paid off by their improved robustness to impulsive noise. Apparently, the Price’s theorem does not apply to Gaussian mixture. However, as pointed out in [

												<CitationRef
CitationID="CR36">36</CitationRef>
] and to be explained below, it is actually applicable to individual components of the mixture. Moreover, for the LMM and NLMM algorithms, the error signal is nearly Gaussian distributed after the impulsive component is suppressed. This also explains why the approximations in [

												<CitationRef
CitationID="CR12">12</CitationRef>
, 

												<CitationRef
CitationID="CR41">41</CitationRef>
] give excellent agreement with Monte Carlo simulations. A detailed analysis will be given below.
											</Para>



											<Section3 ID="Sec10">


												<Heading>Mean Behavior</Heading>



												<Para>Assume the additive noise 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0</Subscript>
 is now a CG noise as defined in (

													<InternalRef
RefID="Equ12">12</InternalRef>
), it is a Gaussian mixture consisting of two components 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0</Subscript>
_

													<Subscript>1</Subscript>
 and 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0</Subscript>
_

													<Subscript>2</Subscript>
, each with zero mean and variance 

													<InlineEquation ID="IEq182">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq182.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_1^2 = \sigma_g^2 $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq183">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq183.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_2^2 = \sigma_\Sigma^2 $$</EquationSource>


													</InlineEquation>
, respectively. The occurrence probability of the impulsive noise is 

													<Emphasis Type="Italic">p</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">r</Emphasis>


													</Subscript>
. Accordingly,

													<Equation ID="Equ47">


														<EquationNumber>46</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ47.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{E_{\left\{ {v,X,{\eta_0}} \right\}}}\left[ {f\left( {X(n),e(n)} \right)} \right] = \left( {1 - {p_r}} \right){E_{\left\{ {v,X,{\eta_{0\_1}}} \right\}}}\left[ {f\left( {X(n),e(n)} \right)} \right]} \\ { + {p_r}{E_{\left\{ {v,X,{\eta_{0\_2}}} \right\}}}\left[ {f\left( {X(n),e(n)} \right)} \right],} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<Emphasis Type="Italic">f</Emphasis>
(

													<Emphasis
Type="BoldItalic">X</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
), 

													<Emphasis Type="Italic">e</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)) is an arbitrary quantity whose statistical average is to be evaluated. Since 

													<Emphasis
Type="BoldItalic">X</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
), 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0_1</Subscript>
, and 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0_2</Subscript>
 are Gaussian distributed, each of the expectation on the right hand side can be evaluated using the Price’s theorem. Consequently, the results in Section 

													<InternalRef
RefID="Sec6">3.1</InternalRef>
 can be carried forward to the CG case by firstly changing the noise power respectively to 

													<InlineEquation ID="IEq184">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq184.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_g^2 $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq185">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq185.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_\Sigma^2 $$</EquationSource>


													</InlineEquation>
, and then combining the two results using (

													<InternalRef
RefID="Equ47">46</InternalRef>
).
												</Para>



												<Para>Recall the relation of the mean weight-error vector in (

													<InternalRef
RefID="Equ15">15</InternalRef>
):

													<Equation ID="Equ48">


														<EquationNumber>47</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ48.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E\left[ {v\left( {n + 1} \right)} \right] = E\left[ {v(n)} \right] - \mu H\prime $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq186">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq186.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ H\prime = {E_{\left\{ {v,X,{\eta_0}} \right\}}}\left[ {\frac{{\psi \left( {e(n)} \right)X(n)}}{{\varepsilon + {X^T}(n)X(n)}}} \right] = \left( {1 - {p_r}} \right)H_1^\prime + {p_r}H_2^\prime $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq187">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq187.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\rm H}_1^\prime $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq188">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq188.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\rm H}_2^\prime $$</EquationSource>


													</InlineEquation>
 are the expectation of the term inside the brackets above with respect to {

													<Emphasis
Type="BoldItalic">v</Emphasis>
, 

													<Emphasis
Type="BoldItalic">X</Emphasis>
, 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0_1</Subscript>
}, and {

													<Emphasis
Type="BoldItalic">v</Emphasis>
, 

													<Emphasis
Type="BoldItalic">X</Emphasis>
, 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>0_2</Subscript>
}. From (

													<InternalRef
RefID="Equ17">17</InternalRef>
), we have 

													<InlineEquation ID="IEq189">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq189.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ H_i^\prime \approx {A_\psi }\left( {\sigma_{{e_i}}^2} \right)U\Lambda {D_\Lambda }{U^T}E\left[ {v(n)} \right] $$</EquationSource>


													</InlineEquation>
, 

													<Emphasis Type="Italic">i </Emphasis>
= 1, 2, where 

													<InlineEquation ID="IEq190">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq190.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_{{e_1}}^2 = \sigma_{{e_g}}^2 $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq191">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq191.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_{{e_2}}^2 = \sigma_{{e_\Sigma }}^2 $$</EquationSource>


													</InlineEquation>
. Hence

													<Equation ID="Equ49">


														<EquationNumber>48</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ49.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ H\prime \approx {\tilde A_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right)U\Lambda {D_\Lambda }{U^T}E\left[ {v(n)} \right], $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq192">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq192.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_{{e_g}}^2 = E\left[ {{v^T}(n){R_{XX}}v(n)} \right] + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq193">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq193.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_{{e_\Sigma }}^2 = E\left[ {{v^T}(n){R_{XX}}v(n)} \right] + \sigma_\Sigma^2 $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq194">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq194.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\tilde A_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) = \left( {1 - {p_r}} \right){A_\psi }\left( {\sigma_{{e_g}}^2} \right) + {p_r}{A_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


													</InlineEquation>
, and 

													<Emphasis
Type="BoldItalic">U</Emphasis>
, Λ, and 

													<InlineEquation ID="IEq195">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq195.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {D_\Lambda } $$</EquationSource>


													</InlineEquation>
 have been defined in Section 

													<InternalRef
RefID="Sec6">3.1</InternalRef>
. Substituting (

													<InternalRef
RefID="Equ18">18</InternalRef>
) into (

													<InternalRef
RefID="Equ47">46</InternalRef>
) and using the natural coordinate 

													<Emphasis
Type="BoldItalic">V</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) = 

													<Emphasis
Type="BoldItalic">U</Emphasis>



													<Superscript>


														<Emphasis
Type="Italic">T</Emphasis>


													</Superscript>



													<Emphasis
Type="BoldItalic">v</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
), one gets

													<Equation ID="Equ50">


														<EquationNumber>49</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ50.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ E{\left[ {V\left( {n + 1} \right)} \right]_i} = \left( {1 - \mu {{\mathop A\limits^\sim }_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right){\lambda_i}{I_i}\left( \Lambda \right)} \right)E{\left[ {V(n)} \right]_i}. $$</EquationSource>


													</Equation>


												</Para>



												<Para>For notational convenience, we have replaced the approximate symbol by the equality symbol. This yields the same form as (

													<InternalRef
RefID="Equ20">20</InternalRef>
), except for 

													<InlineEquation ID="IEq196">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq196.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


													</InlineEquation>
. Similar argument regarding the mean convergence in Section 

													<InternalRef
RefID="Sec7">3.1.1</InternalRef>
 also applies to (

													<InternalRef
RefID="Equ50">49</InternalRef>
). A sufficient condition for the algorithm to converge is 

													<InlineEquation ID="IEq197">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq197.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| {1 - \mu {{\mathop A\limits^\sim }_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right){\lambda_i}{I_i}\left( \Lambda \right)} \right|&lt;1 $$</EquationSource>


													</InlineEquation>
, for all 

													<Emphasis Type="Italic">i</Emphasis>
. If 

													<InlineEquation ID="IEq198">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq198.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 is bounded above by 

													<InlineEquation ID="IEq199">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq199.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_{\psi \_\max }} $$</EquationSource>


													</InlineEquation>
, then following the argument in Section 

													<InternalRef
RefID="Sec7">3.1.1</InternalRef>
, the following conservative maximum step size is obtained:

													<Equation ID="Equg">


														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equg.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\mu_{\max }}&lt;2/\left( {{{\mathop A\limits^\sim }_{\psi \_\max }}{\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)} \right). $$</EquationSource>


													</Equation>


												</Para>



												<FormalPara RenderingStyle="Style1">


													<Heading>Remarks</Heading>



													<Para>


														<OrderedList>


															<ListItem>


																<ItemNumber>(R-B1):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">LMS and NLMS algorithms</Emphasis>


																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>


													</Para>



													<Para>Compared with the Gaussian case, the convergence rate of the conventional LMS and NLMS algorithms without error nonlinearity remains unchanged since 

														<InlineEquation ID="IEq200">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq200.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) = 1 $$</EquationSource>


														</InlineEquation>
, though the EMSE will be increased significantly as shown below in Section 

														<InternalRef
RefID="Sec11">3.2.2</InternalRef>
. All the conclusions in (R-A1) and (R-A2) apply.
													</Para>



													<Para>For general nonlinearity without ATS, both 

														<InlineEquation ID="IEq201">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq201.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_g}}^2 $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq202">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq202.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_\Sigma }}^2 $$</EquationSource>


														</InlineEquation>
 can be very large due to the large value of 

														<InlineEquation ID="IEq203">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq203.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_\Sigma }}^2 $$</EquationSource>


														</InlineEquation>
 and the slow decay of the EMSE 

														<InlineEquation ID="IEq204">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq204.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ E\left[ {{v^T}(n){R_{XX}}v(n)} \right] $$</EquationSource>


														</InlineEquation>
, as the gain 

														<InlineEquation ID="IEq205">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq205.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) = \left( {1 - {p_r}} \right){A_\psi }\left( {\sigma_{{e_g}}^2} \right) + {p_r}{A_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


														</InlineEquation>
 can be very small initially. This leads to nonlinear adaptation and slow convergence. Near convergence, 

														<InlineEquation ID="IEq206">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq206.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_g}}^2 $$</EquationSource>


														</InlineEquation>
 and hence 

														<InlineEquation ID="IEq207">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq207.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


														</InlineEquation>
 will become stable. The convergence is exponential and the rate for the 

														<Emphasis
Type="Italic">i</Emphasis>
-th mode is approximately 

														<InlineEquation ID="IEq208">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq208.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ 1 - \mu {\mathop A\limits^\sim_\psi }\left( \infty \right){\lambda_i}{I_i}\left( \Lambda \right) $$</EquationSource>


														</InlineEquation>
. The actual steady state value of 

														<InlineEquation ID="IEq209">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq209.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi }\left( {\sigma_{{e_g}}^2\left( \infty \right),\sigma_{{e_\Sigma }}^2\left( \infty \right)} \right) $$</EquationSource>


														</InlineEquation>
 will depend on the EMSE and the nonlinearity. Normally, the second term 

														<InlineEquation ID="IEq210">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq210.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {p_r}\overline {\psi \prime } \left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


														</InlineEquation>
 will be much smaller than the first due to the clipping property of the nonlinearity. The “asymptotic convergence rate” of the NLMS algorithm with general nonlinearity will still be faster than their LMS counterparts if the eigenvalues are unequal.

														<OrderedList>


															<ListItem>


																<ItemNumber>(R-B2):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">LMS and NLMS with M-nonlinearity and ATS</Emphasis>


																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>


													</Para>



													<Para>For the LMM/NLMM algorithms with M-nonlinearity and ATS, this degradation is again not very serious. To see this, consider the MH function where 

														<InlineEquation ID="IEq211">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq211.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_{\text{MH}}}\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


														</InlineEquation>
 is given by

														<Equation ID="Equ51">


															<EquationNumber>50</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ51.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_{\text{MH}}}\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) = \left( {1 - {p_r}} \right){A_{\text{MH}}}\left( {\sigma_{{e_g}}^2} \right) + {p_r}{A_{\text{MH}}}\left( {\sigma_{{e_\Sigma }}^2} \right). $$</EquationSource>


														</Equation>


													</Para>



													<Para>Using the 

														<Emphasis
Type="Italic">ξ</Emphasis>
 update in (

														<InternalRef
RefID="Equ9">9</InternalRef>
), the first term will approach 

														<InlineEquation ID="IEq212">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq212.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \left( {1 - {p_r}} \right){A_{\text{MH}}}\left( {\sigma_{{e_g}}^2} \right) $$</EquationSource>


														</InlineEquation>
 while the second term will be close to zero if 

														<InlineEquation ID="IEq213">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq213.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_g}}^2&lt;&lt; \sigma_{{e_\Sigma }}^2 $$</EquationSource>


														</InlineEquation>
 as explained at the end of Appendix 

														<InternalRef
RefID="Sec17">C</InternalRef>
. Hence,

														<InlineEquation ID="IEq214">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq214.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_{\text{MH}}} \approx \left( {1 - {p_r}} \right){A_{\text{MH}}}\left( {\sigma_{{e_g}}^2} \right) $$</EquationSource>


														</InlineEquation>
, which is a constant close to one if 

														<Emphasis
Type="Italic">p</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">r</Emphasis>


														</Subscript>
 is not too large.
													</Para>



													<Para>The fastest convergence rate of these NLMS algorithms with M-nonlinearity occurs when 

														<InlineEquation ID="IEq215">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq215.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \mu = 1/\left( {{{\mathop A\limits^\sim }_\psi }{\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)} \right) $$</EquationSource>


														</InlineEquation>
 and it is limited by the mode corresponding to the smallest eigenvalue. That is

														<Equation ID="Equ52">


															<EquationNumber>51</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ52.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ 1 - \frac{{2{{\mathop A\limits^\sim }_\psi }{\lambda_{\min }}{I_{i\_{\lambda_{\min }}}}\left( \Lambda \right)}}{{{{\mathop A\limits^\sim }_\psi }{\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)}} = 1 - \frac{{2{\lambda_{\min }}{I_{i\_{\lambda_{\min }}}}\left( \Lambda \right)}}{{{\lambda_{\max }}{I_{i\_{\lambda_{\max }}}}\left( \Lambda \right)}}. $$</EquationSource>


														</Equation>


													</Para>



													<Para>Therefore, in additive CG noise, the maximum convergence rate of the NLMS algorithm with M-nonlinearity will also be faster than its un-normalized counterpart if the eigenvalues are unequal.</Para>


												</FormalPara>


											</Section3>



											<Section3 ID="Sec11">


												<Heading>Mean Square Behavior</Heading>



												<Para>For convenience, we drop the argument 

													<InlineEquation ID="IEq216">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq216.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


													</InlineEquation>
 in 

													<InlineEquation ID="IEq217">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq217.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi }\left( {\sigma_{{e_g}}^2,\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


													</InlineEquation>
 and similar quantities. From (

													<InternalRef
RefID="Equ23">23</InternalRef>
), we have

													<Equation ID="Equ53">


														<EquationNumber>52</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ53.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\mathbf{\Xi }}\left( {n + 1} \right) = {\mathbf{\Xi }}(n) - M_1^\prime - M_2^\prime + M_3^\prime, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq218">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq218.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ M_1^\prime = \mu {\mathop A\limits^\sim_\psi }U\Lambda {D_\Lambda }{U^T}{\mathbf{\Xi }}(n) $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq219">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq219.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ M_2^\prime = \mu {\mathop A\limits^\sim_\psi }{\mathbf{\Xi }}(n)U{D_\Lambda }\Lambda {U^T} $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq220">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq220.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ M_3^\prime = {\mu^2}{E_{\left\{ {v,X,{\eta_0}} \right\}}}\left[ {{{\left[ {\psi (e)/\left( {\varepsilon + {X^T}X} \right)} \right]}^2}X{X^T}} \right]. $$</EquationSource>


													</InlineEquation>
 Note, the expressions for 

													<InlineEquation ID="IEq221">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq221.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \,M_1^\prime $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq222">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq222.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \,M_2^\prime $$</EquationSource>


													</InlineEquation>
 are obtained by using the previous result in (

													<InternalRef
RefID="Equ49">48</InternalRef>
). Using (

													<InternalRef
RefID="Equ47">46</InternalRef>
), we have

													<Equation ID="Equ54">


														<EquationNumber>53</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ54.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ M_3^\prime = \left( {1 - {p_r}} \right)M_{3\_1}^\prime + {p_r}M_{3\_2}^\prime, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq223">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq223.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ M_{3\_i}^\prime = {\mu^2}{E_{\left\{ {v,X,{\eta_{0\_i}}} \right\}}}\left[ {{{\left[ {\psi (e)/\left( {\varepsilon + {X^T}X} \right)} \right]}^2}X{X^T}} \right] $$</EquationSource>


													</InlineEquation>
, for 

													<Emphasis Type="Italic">i </Emphasis>
= 1, 2, and from (

													<InternalRef
RefID="Equ27">27</InternalRef>
):

													<Equation ID="Equ55">


														<EquationNumber>54</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ55.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {M_3^\prime \approx 2{\mu^2}{{\tilde C}_\psi }U\left[ {\Lambda \left( {{\mathbf{\Xi }}(n) \circ I\left( \Lambda \right)} \right)\Lambda } \right]{U^T} + {\mu^2}{{\tilde B}_\psi }U\Lambda I\prime \left( \Lambda \right){U^T}} \\ { + {\mu^2}{{\tilde S}_\psi }U{{\overline D }_2}{U^T},} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq224">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq224.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_\psi } = \left( {1 - {p_r}} \right){C_\psi }\left( {\sigma_{{e_g}}^2} \right) + {p_r}{C_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


													</InlineEquation>
 and

													<Equation ID="Equh">


														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equh.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\mathop S\limits^\sim_\psi } = \left( {1 - {p_r}} \right){S_\psi }\left( {\sigma_{{e_g}}^2} \right) + {p_r}{S_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


													</Equation>



													<Equation ID="Equi">


														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equi.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\mathop B\limits^\sim_\psi } = \left( {1 - {p_r}} \right){S_\psi }\left( {\sigma_{{e_g}}^2} \right)\sigma_g^2 + {p_r}{S_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right)\sigma_\Sigma^2. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Substituting (

													<InternalRef
RefID="Equ54">53</InternalRef>
) into (

													<InternalRef
RefID="Equ52">51</InternalRef>
) and using the natural coordinate 

													<InlineEquation ID="IEq225">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq225.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathbf{\Phi }}(n) = {U^T}{\mathbf{\Xi }}(n)U $$</EquationSource>


													</InlineEquation>
, one gets

													<Equation ID="Equ56">


														<EquationNumber>55</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ56.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\mathbf{\Phi }}\left( {n + 1} \right) \approx {\mathbf{\Phi }}(n) - \mu {{\mathop A\limits^\sim }_\psi }\Lambda {D_\Lambda }{\mathbf{\Phi }}(n) - \mu {{\mathop A\limits^\sim }_\psi }{\mathbf{\Phi }}(n){D_\Lambda }\Lambda } \hfill \\ { + 2{{\mathop C\limits^\sim }_\psi }{\mu^2}\left[ {\Lambda \left( {{\mathbf{\Phi }}(n) \circ I\left( \Lambda \right)} \right)\Lambda } \right] + {\mu^2}{{\mathop B\limits^\sim }_\psi }\Lambda I\prime \left( \Lambda \right) + {\mu^2}{{\mathop S\limits^\sim }_\psi }{{\mathop D\limits^\sim }_2}\left( \Lambda \right).} \hfill \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>The 

													<Emphasis Type="Italic">i</Emphasis>
-th diagonal value of 

													<Emphasis Type="Italic">Φ</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) is

													<Equation ID="Equ57">


														<EquationNumber>56</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ57.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{\Phi_{i,i}}\left( {n + 1} \right) \approx \left( {1 - 2\mu {{\mathop A\limits^\sim }_\psi }{\lambda_i}{I_i}\left( \Lambda \right) + 2{\mu^2}{{\mathop C\limits^\sim }_\psi }\lambda_i^2{I_{ii}}\left( \Lambda \right)} \right){\Phi_{i,i}}(n)} \hfill \\ { + {\mu^2}{{\mathop S\limits^\sim }_\psi }\mathop \Sigma \limits_k {\lambda_k}{\lambda_i}{I_{ki}}\left( \Lambda \right){\Phi_{k,k}}(n) + {\mu^2}{{\mathop B\limits^\sim }_\psi }{\lambda_i}I_i^\prime \left( \Lambda \right).} \hfill \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>This has the same form as (

													<InternalRef
RefID="Equ28">28</InternalRef>
) except for the constants 

													<InlineEquation ID="IEq226">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq226.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq227">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq227.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop S\limits^\sim_\psi } $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq228">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq228.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop B\limits^\sim_\psi } $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq229">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq229.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_\psi } $$</EquationSource>


													</InlineEquation>
. As mentioned previously, the last two terms on (

													<InternalRef
RefID="Equ56">55</InternalRef>
) is upper bounded at the steady state by 

													<InlineEquation ID="IEq230">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq230.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mu^2}{\mathop B\limits^\sim_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){\lambda_i}I_i^\prime \left( \Lambda \right) $$</EquationSource>


													</InlineEquation>
, where 

													<InlineEquation ID="IEq231">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq231.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop B\limits^\sim_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) = E\left[ {\sigma_e^2\left( \infty \right){S_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)} \right] $$</EquationSource>


													</InlineEquation>
 and the expectation, is taken over the CG noise. Consequently, we have

													<Equation ID="Equ58">


														<EquationNumber>57</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ58.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) = {\text{Tr}}\left( {{\mathbf{\Phi }}\left( \infty \right)\Lambda } \right) \approx \frac{{\mu {{\mathop B\limits^\sim }_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)}}{{2{{\mathop S\limits^\sim }_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)}}{\mathop \phi \limits^\sim_{{\text{NLMS}}\_\psi }}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq232">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq232.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mathop \phi \limits^\sim_{{\text{NLMS}}\_\psi }} = {\mathop S\limits^\sim_\psi }\left( {\sigma_e^2\left( \infty \right)} \right)\mathop \Sigma \nolimits_{i = 1}^L \frac{{{\lambda_i}I_i^\prime \left( \Lambda \right)}}{{{{\mathop A\limits^\sim }_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){I_i}\left( \Lambda \right) - \mu {{\mathop C\limits^\sim }_\psi }\left( {\sigma_e^2\left( \infty \right)} \right){\lambda_i}{I_{ii}}\left( \Lambda \right)}} $$</EquationSource>


													</InlineEquation>
. In general, (

													<InternalRef
RefID="Equ58">57</InternalRef>
) is a nonlinear equation in the EMSE.
												</Para>



												<FormalPara RenderingStyle="Style1">


													<Heading>Remarks</Heading>



													<Para>


														<OrderedList>


															<ListItem>


																<ItemNumber>(R-B3):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">LMS and NLMS algorithms</Emphasis>
.
																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>
For NLMS algorithm, 

														<InlineEquation ID="IEq233">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq233.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } = {\mathop C\limits^\sim_\psi } = {\mathop S\limits^\sim_\psi } = 1 $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq234">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq234.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop B\limits^\sim_\psi } = \left( {1 - {p_r}} \right)\sigma_g^2 + {p_r}\sigma_\Sigma^2 = \sigma_g^2 + {p_r}\sigma_w^2 = \sigma_{{\eta_0}}^2 $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq235">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq235.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop B\limits^\sim_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) = \sigma_e^2\left( \infty \right) $$</EquationSource>


														</InlineEquation>
. Using the fact that 

														<InlineEquation ID="IEq236">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq236.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2\left( \infty \right) = {\text{EMS}}{{\text{E}}_{\text{NLMS}}}\left( \infty \right) + \sigma_{{\eta_0}}^2 $$</EquationSource>


														</InlineEquation>
, one gets

														<Equation ID="Equ59">


															<EquationNumber>58</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ59.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{\text{NLMS}}}\left( \infty \right) \approx \frac{{\frac{1}{2}\mu \sigma_{{\eta_0}}^2{\phi_{\text{NLMS}}}}}{{1 - \frac{1}{2}\mu {\phi_{\text{NLMS}}}}}. $$</EquationSource>


														</Equation>


													</Para>



													<Para>Similar results are obtained for the LMS algorithm with 

														<InlineEquation ID="IEq237">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq237.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_i}\left( \Lambda \right) = {I'_i}\left( \Lambda \right) = {I_{ii}}\left( \Lambda \right) = 1 $$</EquationSource>


														</InlineEquation>
. The step size bounds are the same as the Gaussian case.

														<OrderedList>


															<ListItem>


																<ItemNumber>(R-B4):</ItemNumber>



																<ItemContent>


																	<Para>


																		<Emphasis
Type="Bold">LMS and NLMS algorithms with M-nonlinearities and ATS</Emphasis>


																	</Para>


																</ItemContent>


															</ListItem>


														</OrderedList>


													</Para>



													<Para>In this case, 

														<InlineEquation ID="IEq238">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq238.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq239">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq239.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq240">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq240.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop S\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
 are nearly constant at the steady state. The step size bounds are similar to Eqs. (

														<InternalRef
RefID="Equ43">42</InternalRef>
) and (

														<InternalRef
RefID="Equ46">45</InternalRef>
) for the Gaussian noise case, except that 

														<InlineEquation ID="IEq241">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq241.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi } $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq242">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq242.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi } $$</EquationSource>


														</InlineEquation>
 are now replaced by 

														<InlineEquation ID="IEq243">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq243.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq244">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq244.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
 respectively.
													</Para>



													<Para>If 

														<InlineEquation ID="IEq245">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq245.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_g}}^2&lt;&lt; \sigma_{{e_\Sigma }}^2 $$</EquationSource>


														</InlineEquation>
, the parameter 

														<InlineEquation ID="IEq246">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq246.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {k_\xi } $$</EquationSource>


														</InlineEquation>
 in ATS, which is chosen as a reasonable multiple of the “impulse free” variance of the estimation error, can be selected to reduce the adverse effect of the impulses. Consider for example 

														<InlineEquation ID="IEq247">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq247.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } = \left( {1 - {p_r}} \right){A_\psi }\left( {\sigma_{{e_g}}^2} \right) + {p_r}{A_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


														</InlineEquation>
. As mentioned at the end of Appendix 

														<InternalRef
RefID="Sec17">C</InternalRef>
 and remarks (R-B2) above, the term 

														<InlineEquation ID="IEq248">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq248.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_{{e_\Sigma }}^2} \right) $$</EquationSource>


														</InlineEquation>
 is usually small since the nonlinearity is designed to clip at this high noise level arising from the impulsive noise. Moreover the first term is nearly a constant. As a result, 

														<InlineEquation ID="IEq249">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq249.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } \approx \left( {1 - {p_r}} \right){A_\psi } $$</EquationSource>


														</InlineEquation>
, and similarly 

														<InlineEquation ID="IEq250">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq250.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop S\limits^\sim_\psi } \approx \left( {1 - {p_r}} \right){S_\psi } $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq251">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq251.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_\psi } = \left( {1 - {p_r}} \right){C_\psi } $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq252">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq252.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop B\limits^\sim_\psi }\left( {\sigma_e^2\left( \infty \right)} \right) \approx \left( {1 - {p_r}} \right){S_\psi }\sigma_{{e_g}}^2\left( \infty \right) $$</EquationSource>


														</InlineEquation>
. If ATS is not employed, both 

														<InlineEquation ID="IEq253">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq253.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq254">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq254.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_\psi } $$</EquationSource>


														</InlineEquation>
, and hence the MSE, will be significantly affected by the impulses. Moreover, complicated nonlinear effects will be encountered, which will generally lead to slower convergence and higher EMSE to be discussed in the sequel.
													</Para>



													<Para>By noting that 

														<InlineEquation ID="IEq255">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq255.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_{{e_g}}^2\left( \infty \right) = {\text{EMS}}{{\text{E}}_{{\text{NLMM}}\_\psi }} + \sigma_g^2 $$</EquationSource>


														</InlineEquation>
, (

														<InternalRef
RefID="Equ58">57</InternalRef>
) can be simplified to

														<Equation ID="Equ60">


															<EquationNumber>59</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ60.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\text{EMS}}{{\text{E}}_{{\text{NLMS}}\_\psi }}\left( \infty \right) \approx \frac{{\frac{1}{2}\mu \sigma_g^2{{\mathop \phi \limits^\sim }_{{\text{NLMS}}\_\psi }}}}{{1 - \frac{1}{2}\mu {{\mathop \phi \limits^\sim }_{{\text{NLMS}}\_\psi }}}}, $$</EquationSource>


														</Equation>
where 

														<InlineEquation ID="IEq256">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq256.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop \phi \limits^\sim_{{\text{NLMS}}\_\psi }} = {\mathop S\limits^\sim_\psi }\mathop \Sigma \nolimits_{i = 1}^L \frac{{{\lambda_i}I_i^\prime \left( \Lambda \right)}}{{{{\mathop A\limits^\sim }_\psi }{I_i}\left( \Lambda \right) - \mu {{\mathop C\limits^\sim }_\psi }{\lambda_i}{I_{ii}}\left( \Lambda \right)}} \approx {S_\psi }\mathop \Sigma \nolimits_{i = 1}^L \frac{{{\lambda_i}I_i^\prime \left( \Lambda \right)}}{{{A_\psi }{I_i}\left( \Lambda \right) - \mu {C_\psi }{\lambda_i}{I_{ii}}\left( \Lambda \right)}} $$</EquationSource>


														</InlineEquation>
, which is identical to that in Gaussian noise alone (c.f. Eq. (

														<InternalRef
RefID="Equ42">41</InternalRef>
)). This also holds true for the LMS-based algorithms where 

														<InlineEquation ID="IEq257">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq257.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_i}\left( \Lambda \right) = I_i^\prime \left( \Lambda \right) = {I_{ii}}\left( \Lambda \right) = 1 $$</EquationSource>


														</InlineEquation>
. The step size for convergence is also identical to the Gaussian case.
													</Para>



													<Para>For the NLMM algorithm using the MH function and ATS, the values of 

														<InlineEquation ID="IEq258">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq258.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_{\text{MH}}} $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq259">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq259.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop S\limits^\sim_{\text{MH}}} $$</EquationSource>


														</InlineEquation>
 and 

														<InlineEquation ID="IEq260">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq260.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_{\text{MH}}} $$</EquationSource>


														</InlineEquation>
 can be computed with the help of Table 

														<InternalRef
RefID="Tab1">1</InternalRef>
. Assuming 

														<InlineEquation ID="IEq261">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq261.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\sigma_g}&lt;\xi&lt;&lt; {\sigma_\Sigma } $$</EquationSource>


														</InlineEquation>
, then 

														<InlineEquation ID="IEq262">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq262.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop A\limits^\sim_{\text{MH}}} \approx {\mathop S\limits^\sim_{\text{MH}}} \approx \left( {1 - {p_r}} \right){\text{erf}}\left( {\frac{{{k_\xi }}}{{\sqrt 2 }}} \right) $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq263">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq263.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_{\text{MH}}} \approx {\mathop A\limits^\sim_{\text{MH}}} - \left( {1 - {p_r}} \right)\left( {\frac{{k_\xi^3}}{{\sqrt {2\pi } }}} \right)\exp \left( { - \frac{{k_\xi^2}}{2}} \right) $$</EquationSource>


														</InlineEquation>
. For practical values of 

														<InlineEquation ID="IEq264">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq264.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {k_\xi } = 2.576 $$</EquationSource>


														</InlineEquation>
, the second term of 

														<InlineEquation ID="IEq265">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq265.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop C\limits^\sim_{\text{MH}}} $$</EquationSource>


														</InlineEquation>
 above is small and 

														<InlineEquation ID="IEq266">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq266.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\mathop \phi \limits^\sim_{{\text{NLMS}}\_\psi }} $$</EquationSource>


														</InlineEquation>
 in (

														<InternalRef
RefID="Equ59">58</InternalRef>
) will be approximately given by

														<Equation ID="Equ61">


															<EquationNumber>60</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ61.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\mathop \phi \limits^\sim_{{\text{NLMS}}\_\psi }} \approx \mathop \Sigma \nolimits_{i = 1}^L \tfrac{{{\lambda_i}I\prime \left( \Lambda \right)}}{{I\prime \left( \Lambda \right) - \mu {\lambda_i}{I_{ii}}\left( \Lambda \right)}}, $$</EquationSource>


														</Equation>
which is identical to that of the NLMS algorithm in Gaussian noise alone. This approximation is more accurate when the step size is small. This applies to the LMM algorithm with 

														<InlineEquation ID="IEq267">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq267.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {I_i}\left( \Lambda \right) = I_i^\prime \left( \Lambda \right) = {I_{ii}}\left( \Lambda \right) = 1 $$</EquationSource>


														</InlineEquation>
.
													</Para>



													<Para>In [

														<CitationRef
CitationID="CR37">37</CitationRef>
], the performance of the NLMS algorithm in Gaussian noise was analyzed and it is shown that 

														<InlineEquation ID="IEq268">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq268.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \frac{1}{2}\mu \sigma_g^2 $$</EquationSource>


														</InlineEquation>
 serves as a useful lower bound for estimating the EMSE (misadjustment) of the NLMS algorithm. It is attractive because it does not require the knowledge of the eigenvalues or eigenvalue spread of 

														<Emphasis
Type="BoldItalic">R</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">XX</Emphasis>


														</Subscript>
. A similar upper bound can be estimated empirically as well. From the above analysis for the NLMM algorithm, we see that 

														<InlineEquation ID="IEq269">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq269.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \frac{1}{2}\mu \sigma_g^2 $$</EquationSource>


														</InlineEquation>
 also serves as a useful lower bound for the EMSE of the NLMM algorithm in both Gaussian and CG noises since its performance is similar to the NLMS algorithm in Gaussian noise alone in view of Eq. (

														<InternalRef
RefID="Equ61">60</InternalRef>
). This will be illustrated by computer simulations in the next section.
													</Para>



													<Para>Finally, we note that the increase in EMSE of NLMS algorithm over the NLMM algorithm in CG noise environment is

														<Equation ID="Equ62">


															<EquationNumber>61</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ62.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ \Delta {\text{EMS}}{{\text{E}}_{\text{NLMS}}}\left( \infty \right) \approx \frac{{\tfrac{1}{2}\mu {\phi_{\text{NLMS}}}}}{{1 - \tfrac{1}{2}\mu {\phi_{\text{NLMS}}}}}{p_r}\left( {\sigma_\Sigma^2 - \sigma_g^2} \right). $$</EquationSource>


														</Equation>


													</Para>



													<Para>It is clear that the NLMM algorithm, which is based on robust statistics, offers a substantially lower steady state error than the NLMS algorithm in impulsive noise environment. Similar observation applies to the LMM algorithm.</Para>


												</FormalPara>


											</Section3>


										</Section2>


									</Section1>



									<Section1 ID="Sec12">


										<Heading>Simulation Results</Heading>



										<Para>In this section we shall first examine the performance of the NLMM and other related algorithms and then verify the analytical results obtained in Section 

											<InternalRef RefID="Sec5">3</InternalRef>
. Since the comparison of the LMM algorithm with other robust algorithms is available in [

											<CitationRef
CitationID="CR7">7</CitationRef>
], we will only compare the NLMM algorithm with the LMM and NLMS algorithms in our experiments. All simulations are carried out using the system identification model shown in Fig. 

											<InternalRef RefID="Fig1">1</InternalRef>
 and all the learning curves are obtained by averaging the results of 

											<Emphasis Type="Italic">K </Emphasis>
= 
200 independent runs. The unknown system to be estimated is an FIR filter with order 

											<Emphasis Type="Italic">L</Emphasis>
. Its weight vector 

											<Emphasis Type="BoldItalic">W</Emphasis>



											<Superscript>*</Superscript>
 is randomly generated and normalized to unit energy. The input signal 

											<Emphasis Type="Italic">x</Emphasis>
(

											<Emphasis Type="Italic">n</Emphasis>
) is obtained from a first order AR process 

											<InlineEquation ID="IEq270">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq270.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ x(n) = ax\left( {n - 1} \right) + v(n), $$</EquationSource>


											</InlineEquation>
 where 

											<InlineEquation ID="IEq271">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq271.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ 0&lt;a&lt;1 $$</EquationSource>


											</InlineEquation>
 is the correlation coefficient. 

											<Emphasis Type="Italic">v</Emphasis>
(

											<Emphasis Type="Italic">n</Emphasis>
) is an additive white Gaussian noise (AWGN) sequence with zero mean and variance 

											<InlineEquation ID="IEq272">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq272.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \sigma_v^2 $$</EquationSource>


											</InlineEquation>
 and 

											<InlineEquation ID="IEq273">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq273.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \sigma_v^2 = 1/\left( {1 - {a^2}} \right) $$</EquationSource>


											</InlineEquation>
. The additive noise is either the Gaussian noise 

											<Emphasis Type="Italic">η</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">g</Emphasis>


											</Subscript>
(

											<Emphasis Type="Italic">n</Emphasis>
) with zero mean and variance 

											<InlineEquation ID="IEq274">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq274.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \sigma_g^2 $$</EquationSource>


											</InlineEquation>
, or the CG noise sequence 

											<Emphasis Type="Italic">η</Emphasis>



											<Subscript>0</Subscript>
(

											<Emphasis Type="Italic">n</Emphasis>
) generated from (

											<InternalRef RefID="Equ12">12</InternalRef>
) with a probability 

											<InlineEquation ID="IEq275">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq275.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ P\left( {b(n) = 1} \right) = {p_r} $$</EquationSource>


											</InlineEquation>
.

											<OrderedList>


												<ListItem>


													<ItemNumber>Experiment A:</ItemNumber>



													<ItemContent>


														<Para>


															<Emphasis
Type="Bold">Convergence performance in Gaussian and CG noise</Emphasis>


														</Para>


													</ItemContent>


												</ListItem>


											</OrderedList>


										</Para>



										<Para>In this experiment, the convergence performance and the robustness of the NLMM algorithm to impulsive noise are evaluated. The system order is 

											<Emphasis Type="Italic">L</Emphasis>
 = 8 and the correlation coefficient of the input AR process is 

											<Emphasis Type="Italic">a</Emphasis>
 = 
0.9. The impulse occurrence probability and the impulsive characteristic ratio are respectively 

											<Emphasis Type="Italic">p</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">r</Emphasis>


											</Subscript>
 = 0.01 and 

											<Emphasis Type="Italic">r</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">im</Emphasis>


											</Subscript>
 = 
300. For illustration purpose, the impulsive noise is applied to the system after time instant 

											<Emphasis Type="Italic">n</Emphasis>
 = 2,000. From 

											<Emphasis Type="Italic">n</Emphasis>
 = 1 to 

											<Emphasis Type="Italic">n</Emphasis>
 = 1,999, the additive noise is 

											<Emphasis Type="Italic">η</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">g</Emphasis>


											</Subscript>
(

											<Emphasis Type="Italic">n</Emphasis>
) with 

											<InlineEquation ID="IEq276">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq276.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \sigma_g^2 = {10^{ - 4}} $$</EquationSource>


											</InlineEquation>
. From 

											<Emphasis Type="Italic">n</Emphasis>
 = 2,000 onwards, a CG noise 

											<Emphasis Type="Italic">η</Emphasis>



											<Subscript>0</Subscript>
(

											<Emphasis Type="Italic">n</Emphasis>
) generated by (

											<InternalRef RefID="Equ12">12</InternalRef>
) is applied. The step size of the NLMM, NLMS and LMM algorithms are set to 

											<InlineEquation ID="IEq277">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq277.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ {\mu_{\text{NLMM}}} = {\mu_{\text{NLMS}}} = 0.1 $$</EquationSource>


											</InlineEquation>
 and 

											<Emphasis Type="Italic">μ</Emphasis>



											<Subscript>LMM</Subscript>
 = 
0.025, which enables all algorithms to reach a similar steady state EMSE. The small positive constant ε used to prevent division by zero in (

											<InternalRef RefID="Equ2">2</InternalRef>
) and (

											<InternalRef RefID="Equ10">10</InternalRef>
) for the NLMS and NLMM algorithms is set to 0.0001. The threshold parameter 

											<Emphasis Type="Italic">ξ</Emphasis>
 of the M-estimate function in the NLMM and LMM algorithms is calculated from (

											<InternalRef RefID="Equ9">9</InternalRef>
) with 

											<InlineEquation ID="IEq278">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq278.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


											</InlineEquation>
 being estimated from (

											<InternalRef RefID="Equ7">7</InternalRef>
). The forgetting factor 

											<InlineEquation ID="IEq279">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq279.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ {\lambda_\sigma } $$</EquationSource>


											</InlineEquation>
 in (

											<InternalRef RefID="Equ7">7</InternalRef>
) is 0.95. The window length 

											<Emphasis Type="Italic">N</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">w</Emphasis>


											</Subscript>
 for the NLMM and the LMM algorithms is chosen to be 9. For better visualization, the locations of the impulses are fixed whereas their amplitudes are varied according to 

											<Emphasis Type="Italic">η</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">w</Emphasis>


											</Subscript>
(

											<Emphasis Type="Italic">n</Emphasis>
). This is realized by generating one fixed Bernoulli sequence 

											<InlineEquation ID="IEq280">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq280.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \overline b (n) $$</EquationSource>


											</InlineEquation>
 with 

											<Emphasis Type="Italic">p</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">r</Emphasis>


											</Subscript>
 = 
0.01 and using it in all of the independent runs. Consequently, the locations of the impulses are fixed at 

											<Emphasis Type="Italic">n</Emphasis>
 = 2,482, 

											<Emphasis Type="Italic">n</Emphasis>
 = 3,475 and 

											<Emphasis Type="Italic">n</Emphasis>
 = 
4,486 so as to visualize more clearly the impact of the impulsive noise on the desired signal. Figure 

											<InternalRef RefID="Fig3">3</InternalRef>
 depicts the performance of all the algorithms, which shows they can reach a similar steady state MSE value. The NLMM and NLMS algorithms have almost identical initial convergence performance. It can be also seen that the NLMM and LMM algorithms are very robust to the impulses in the desired signal. In contrast, the performance of the NLMS algorithm is severely deteriorated by these impulses.

											<Figure Category="Standard" Float="Yes"
ID="Fig3">


												<Caption Language="En">


													<CaptionNumber>Figure 3</CaptionNumber>



													<CaptionContent>


														<SimplePara>The convergence performance and the robustness of (1) LMM, (2) NLMM and (3) NLMS algorithms to impulses in desired signal (MSE vs. 

															<Emphasis
Type="Italic">n</Emphasis>
).
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO5">


													<ImageObject Color="BlackWhite"
FileRef="MediaObjects/11265_2009_405_Fig3_HTML.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>Simulations for 

											<Emphasis Type="Italic">p</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">r</Emphasis>


											</Subscript>
 larger than 0.01 were also conducted and similar results are obtained. In general, the performance of the algorithms will be degraded gradually as 

											<Emphasis Type="Italic">p</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">r</Emphasis>


											</Subscript>
 increases. More simulation results concerning the effects of using different parameter values of SNR, 

											<Emphasis Type="Italic">μ</Emphasis>



											<Subscript>NLMM</Subscript>
, 

											<Emphasis Type="Italic">N</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">w</Emphasis>


											</Subscript>
, 

											<InlineEquation ID="IEq281">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq281.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ {k_\xi } $$</EquationSource>


											</InlineEquation>
 are available in [

											<CitationRef
CitationID="CR41">41</CitationRef>
]. The results show that the NLMM algorithm has improved robustness to impulses and is not too sensitive to these parameters if they are reasonably chosen as suggested.

											<OrderedList>


												<ListItem>


													<ItemNumber>Experiment B:</ItemNumber>



													<ItemContent>


														<Para>


															<Emphasis
Type="Bold">Verification of convergence performance analysis</Emphasis>


														</Para>


													</ItemContent>


												</ListItem>


											</OrderedList>


										</Para>



										<Para>In this experiment, the theoretical analysis presented in Section 

											<InternalRef RefID="Sec5">3</InternalRef>
 will be verified through extensive simulations. For the mean convergence, the norm of the mean square weight-error vector is used as the performance measure:

											<Equation ID="Equj">


												<MediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_Equj.gif" Format="GIF" Rendition="HTML"
Type="Linedraw" />


												</MediaObject>



												<EquationSource
Format="TEX">$$ {\left\| {{v_A}(n)} \right\|_2} = \sqrt {\Sigma_{i = 1}^L{{\left[ {\tfrac{1}{K}\Sigma_{j = 1}^Kv_i^{(j)}(n)} \right]}^2}}, \,i = 1, \cdots, L,\,j = 1, \cdots, K, $$</EquationSource>


											</Equation>
where 

											<InlineEquation ID="IEq282">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq282.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ v_i^{(j)}(n) $$</EquationSource>


											</InlineEquation>
 is the 

											<Emphasis Type="Italic">i</Emphasis>
-th component of the weight-error vector 

											<Emphasis Type="BoldItalic">v</Emphasis>
(

											<Emphasis Type="Italic">n</Emphasis>
) at time 

											<Emphasis Type="Italic">n</Emphasis>
 in the 

											<Emphasis Type="Italic">j</Emphasis>
-th independent run. For the mean square convergence results, 

											<InlineEquation ID="IEq283">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq283.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ {\text{EMSE}}(n) = {\text{Tr}}\left( {{\mathbf{\Phi }}(n)\Lambda } \right) $$</EquationSource>


											</InlineEquation>
is used as the performance measure. The values of the special integral functions appearing in the analytical results, i. e., 

											<Emphasis Type="Italic">I</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">i</Emphasis>


											</Subscript>
(Λ), 

											<InlineEquation ID="IEq284">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq284.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ I_i^\prime \left( \Lambda \right) $$</EquationSource>


											</InlineEquation>
, and 

											<Emphasis Type="Italic">I</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">ii</Emphasis>


											</Subscript>
(Λ), are evaluated numerically using the method introduced in [

											<CitationRef
CitationID="CR45">45</CitationRef>
]. Unlike experiment A, the impulsive noise sequence used in these algorithms is applied to the tested algorithms throughout the whole adapting process. The locations of impulses in the desired signal are not fixed for each independent run. Different system order 

											<Emphasis Type="Italic">L</Emphasis>
, input correlation factor 

											<Emphasis Type="Italic">a</Emphasis>
, algorithm step size 

											<Emphasis Type="Italic">μ</Emphasis>
 and other key parameters 

											<InlineEquation ID="IEq285">


												<InlineMediaObject>


													<ImageObject Color="BlackWhite"
FileRef="11265_2009_405_Article_IEq285.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</InlineMediaObject>



												<EquationSource
Format="TEX">$$ \sigma_g^2 $$</EquationSource>


											</InlineEquation>
, 

											<Emphasis Type="Italic">r</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">im</Emphasis>


											</Subscript>
, and 

											<Emphasis Type="Italic">p</Emphasis>



											<Subscript>


												<Emphasis Type="Italic">r</Emphasis>


											</Subscript>
 are employed in different experiments. For succinct description, their values are summarized in Table 

											<InternalRef RefID="Tab2">2</InternalRef>
. The following four experiments are conducted:

											<OrderedList>


												<ListItem>


													<ItemNumber>Ex.1</ItemNumber>



													<ItemContent>


														<Para>NLMS algorithm with CG noise. The theoretical results are calculated from (

															<InternalRef
RefID="Equ50">49</InternalRef>
), (

															<InternalRef
RefID="Equ57">56</InternalRef>
) and (

															<InternalRef
RefID="Equ59">58</InternalRef>
). Figure 

															<InternalRef
RefID="Fig4">4a</InternalRef>
 shows that for mean convergence there is a good agreement between theoretical and experimental results. It can be seen from Fig. 

															<InternalRef
RefID="Fig4">4b</InternalRef>
 that the EMSE as given in (

															<InternalRef
RefID="Equ59">58</InternalRef>
) is also close to the true EMSE, which shows that the performance of the NLMS is substantially affected by CG noise. The results concerning NLMS algorithm with Gaussian noise has been detailed in [

															<CitationRef
CitationID="CR37">37</CitationRef>
] so that they are omitted here.
														</Para>


													</ItemContent>


												</ListItem>



												<ListItem>


													<ItemNumber>Ex.2</ItemNumber>



													<ItemContent>


														<Para>NLMM algorithm with CG and Gaussian noises. The theoretical results of the former are calculated from (

															<InternalRef
RefID="Equ50">49</InternalRef>
), (

															<InternalRef
RefID="Equ57">56</InternalRef>
) and (

															<InternalRef
RefID="Equ61">60</InternalRef>
) while those for Gaussian noise are calculated from (

															<InternalRef
RefID="Equ20">20</InternalRef>
), (

															<InternalRef
RefID="Equ29">29</InternalRef>
) and (

															<InternalRef
RefID="Equ45">44</InternalRef>
). All theoretical and experimental results are depicted in Figs. 

															<InternalRef
RefID="Fig5">5</InternalRef>
, 

															<InternalRef
RefID="Fig6">6</InternalRef>
 and 

															<InternalRef
RefID="Fig7">7</InternalRef>
 respectively and they are found to match each other very well. To evaluate the effectiveness and robustness of the ATS proposed in (

															<InternalRef
RefID="Equ7">7</InternalRef>
), (

															<InternalRef
RefID="Equ9">9</InternalRef>
), the “impulse-free” variance 

															<InlineEquation ID="IEq286">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq286.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
 estimated from (

															<InternalRef
RefID="Equ7">7</InternalRef>
), 

															<InlineEquation ID="IEq287">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq287.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ k_\xi^2\widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
, and 

															<InlineEquation ID="IEq288">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq288.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
 for one independent run are plotted in sub-figures (c) at Figs. 

															<InternalRef
RefID="Fig5">5</InternalRef>
, 

															<InternalRef
RefID="Fig6">6</InternalRef>
, 

															<InternalRef
RefID="Fig7">7</InternalRef>
 and 

															<InternalRef
RefID="Fig8">8</InternalRef>
. It can be seen that the estimated “impulse-free” variance is able to follow the true value and impulses with large amplitudes can be effectively detected. As pointed out at the end of Section 

															<InternalRef
RefID="Sec5">3</InternalRef>
, an estimate of the lower bound of the EMSE for the NLMM algorithm is 

															<InlineEquation ID="IEq289">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq289.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \frac{1}{2}\mu \sigma_g^2 $$</EquationSource>


															</InlineEquation>
. To account for the approximation error in this formula, we also empirically estimated an upper bound 

															<InlineEquation ID="IEq290">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq290.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ CF \cdot \frac{1}{2}\mu \sigma_g^2 $$</EquationSource>


															</InlineEquation>
 from extensive simulation results by means of a constant correction factor 

															<Emphasis
Type="Italic">CF</Emphasis>
, which was found to be 1.7. Due to space limitation, we only plot these bounds in Fig. 

															<InternalRef
RefID="Fig5">5b</InternalRef>
 curve (

															<InternalRef
RefID="Equ4">4</InternalRef>
) and Fig. 

															<InternalRef
RefID="Fig7">7b</InternalRef>
 curve (

															<InternalRef
RefID="Equ3">3</InternalRef>
). It can be seen that these estimates give satisfactory bounds for the EMSE.
														</Para>


													</ItemContent>


												</ListItem>



												<ListItem>


													<ItemNumber>Ex.3</ItemNumber>



													<ItemContent>


														<Para>LMM algorithm with CG noise, which is a special case of Ex. 1 with all special integrals equal to one. Figure 

															<InternalRef
RefID="Fig8">8</InternalRef>
 illustrates the good performance of the algorithm and an agreement between the theoretical and simulation results.
														</Para>


													</ItemContent>


												</ListItem>


											</OrderedList>



											<Table Float="Yes" ID="Tab2">


												<Caption Language="En">


													<CaptionNumber>Table 2</CaptionNumber>



													<CaptionContent>


														<SimplePara>List of parameters in experiment B.</SimplePara>


													</CaptionContent>


												</Caption>



												<tgroup align="left" cols="10">


													<colspec align="left"
colname="c1" colnum="1" />



													<colspec align="left"
colname="c2" colnum="2" />



													<colspec align="char" char="."
colname="c3" colnum="3" />



													<colspec align="char" char="."
colname="c4" colnum="4" />



													<colspec align="char" char="."
colname="c5" colnum="5" />



													<colspec align="char" char="."
colname="c6" colnum="6" />



													<colspec align="char" char="."
colname="c7" colnum="7" />



													<colspec align="char" char="("
colname="c8" colnum="8" />



													<colspec align="char" char="("
colname="c9" colnum="9" />



													<colspec align="left"
colname="c10" colnum="10" />



													<thead>


														<row>


															<entry colname="c1"
morerows="1">


																<SimplePara>Exps.</SimplePara>


															</entry>



															<entry colname="c2"
morerows="1">


																<SimplePara>


																	<Emphasis
Type="Italic">L</Emphasis>


																</SimplePara>


															</entry>



															<entry colname="c3"
morerows="1">


																<SimplePara>


																	<Emphasis
Type="Italic">a</Emphasis>


																</SimplePara>


															</entry>



															<entry colname="c4"
morerows="1">


																<SimplePara>


																	<Emphasis
Type="Italic">μ</Emphasis>


																</SimplePara>


															</entry>



															<entry colname="c5"
morerows="1">


																<SimplePara>


																	<InlineEquation
ID="IEq291">


																		<InlineMediaObject>


																			<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq291.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																		</InlineMediaObject>



																		<EquationSource
Format="TEX">$$ \sigma_g^2 $$</EquationSource>


																	</InlineEquation>


																</SimplePara>


															</entry>



															<entry colname="c6"
morerows="1">


																<SimplePara>


																	<Emphasis
Type="Italic">r</Emphasis>



																	<Subscript>


																		<Emphasis
Type="Italic">im</Emphasis>


																	</Subscript>


																</SimplePara>


															</entry>



															<entry colname="c7"
morerows="1">


																<SimplePara>


																	<Emphasis
Type="Italic">p</Emphasis>



																	<Subscript>


																		<Emphasis
Type="Italic">r</Emphasis>


																	</Subscript>


																</SimplePara>


															</entry>



															<entry nameend="c10"
namest="c8">


																<SimplePara>Fig. and curve index</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c8">


																<SimplePara>Mean</SimplePara>


															</entry>



															<entry colname="c9">


																<SimplePara>Mean square</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>


																	<Emphasis
Type="Italic">e</Emphasis>



																	<Superscript>2</Superscript>
(

																	<Emphasis
Type="Italic">n</Emphasis>
), 

																	<InlineEquation
ID="IEq292">


																		<InlineMediaObject>


																			<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq292.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																		</InlineMediaObject>



																		<EquationSource
Format="TEX">$$ \hat \sigma_e^2 $$</EquationSource>


																	</InlineEquation>


																</SimplePara>


															</entry>


														</row>


													</thead>



													<tbody>


														<row>


															<entry colname="c1"
morerows="3">


																<SimplePara>(1). NLMS with CG noise</SimplePara>


															</entry>



															<entry colname="c2">


																<SimplePara>16</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.2</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>4 (a), (1)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>4 (b), (1)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>N/A</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>16</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.3</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.15</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−5</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>100</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.01</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>4 (a), (2)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>4 (b), (2)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>N/A</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>16</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.6</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.1</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−4</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>200</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.015</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>4 (a), (3)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>4 (b), (3)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>N/A</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>16</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.05</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>4 (a), (4)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>4 (b), (4)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>N/A</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c1"
morerows="11">


																<SimplePara>(2). NLMM with CG noise</SimplePara>


															</entry>



															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.2</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>5 (a), (1)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>5 (b), (1)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.1</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>5 (a), (2)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>5 (b), (2)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>5 (c), (2)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.05</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>5 (a), (3)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>5 (b), (3)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>5 (a), (4)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>5 (b), (4)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.01</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>5 (a), (5)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>5 (b), (5)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>5 (c), (5)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.007</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>5 (a), (6)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>5 (b), (6)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.3</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>6 (a), (1)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>6 (b), (1)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>6 (c), (1)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.15</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>6 (a), (2)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>6 (b), (2)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.1</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>6 (a), (3)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>6 (b), (3)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.08</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>6 (a), (4)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>6 (b), (4)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.05</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>6 (a), (5)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>6 (b), (5)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.03</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>6 (a), (6)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>6 (b), (6)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>6 (c), (6)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c1"
morerows="3">


																<SimplePara>(3). NLMM with Gaussian noise</SimplePara>


															</entry>



															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.2</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>7 (a), (1)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>7 (b), (1)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.3</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.15</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−5</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>7 (a), (2)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>7 (b), (2)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>7 (c), (2)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.6</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.1</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−4</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>7 (a), (3)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>7 (b), (3)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>8</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.05</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>N/A</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>7 (a), (4)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>7 (b), (4)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>7 (c), (4)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c1"
morerows="3">


																<SimplePara>(4). LMM with CG noise</SimplePara>


															</entry>



															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.01</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−6</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>50</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.02</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>8 (a), (1)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>8 (b), (1)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>8 (c), (1)</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.3</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.008</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−5</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>100</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.015</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>8 (a), (2)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>8 (b), (2)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.6</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.006</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−4</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>200</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.01</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>8 (a), (3)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>8 (b), (3)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>Omitted</SimplePara>


															</entry>


														</row>



														<row>


															<entry colname="c2">


																<SimplePara>24</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c3">


																<SimplePara>0.9</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c4">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c5">


																<SimplePara>10

																	<Superscript>−3</Superscript>


																</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c6">


																<SimplePara>400</SimplePara>


															</entry>



															<entry align="char"
char="." colname="c7">


																<SimplePara>0.005</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c8">


																<SimplePara>8 (a), (4)</SimplePara>


															</entry>



															<entry align="char"
char="(" colname="c9">


																<SimplePara>8 (b), (4)</SimplePara>


															</entry>



															<entry colname="c10">


																<SimplePara>8 (c), (4)</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>



											<Figure Category="Standard" Float="Yes"
ID="Fig4">


												<Caption Language="En">


													<CaptionNumber>Figure 4</CaptionNumber>



													<CaptionContent>


														<SimplePara>The mean (

															<Emphasis
Type="Bold">a</Emphasis>
) and mean square (

															<Emphasis
Type="Bold">b</Emphasis>
) convergence performance of the NLMS algorithm with CG noise. Learning curves: (1) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.2, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005; (2) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.3, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.15, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−5</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 =100, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.01; (3) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.6, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.1, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−4</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 200, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.015; (4) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.05, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02.
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO6">


													<ImageObject Color="Color"
FileRef="MediaObjects/11265_2009_405_Fig4_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig5">


												<Caption Language="En">


													<CaptionNumber>Figure 5</CaptionNumber>



													<CaptionContent>


														<SimplePara>The mean (

															<Emphasis
Type="Bold">a</Emphasis>
) and mean square (

															<Emphasis
Type="Bold">b</Emphasis>
) convergence performance of the NLMM algorithm with CG noise; 

															<Emphasis
Type="Bold">c</Emphasis>
 Illustration of (i) 

															<InlineEquation ID="IEq293">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq293.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
, (ii) 

															<InlineEquation ID="IEq294">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq294.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
 and (iii) 

															<InlineEquation ID="IEq295">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq295.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ k_\xi^2\widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
. Learning curves: (1) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.2, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (2) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.01, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 =50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (3) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.05, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (4) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.02, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005; (5) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.01, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005; (6) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.07, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005.
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO7">


													<ImageObject Color="BlackWhite"
FileRef="MediaObjects/11265_2009_405_Fig5_HTML.gif" Format="GIF"
Rendition="HTML" Type="Linedraw" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig6">


												<Caption Language="En">


													<CaptionNumber>Figure 6</CaptionNumber>



													<CaptionContent>


														<SimplePara>The mean (

															<Emphasis
Type="Bold">a</Emphasis>
) and mean square (

															<Emphasis
Type="Bold">b</Emphasis>
) convergence performance of the NLMM algorithm with CG noise; 

															<Emphasis
Type="Bold">c</Emphasis>
 Illustration of (i) 

															<InlineEquation ID="IEq296">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq296.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
, (ii) 

															<InlineEquation ID="IEq297">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq297.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
 and (iii) 

															<InlineEquation ID="IEq298">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq298.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ k_\xi^2\widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
. Learning curves: (1) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.3, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (2) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.15, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 =50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (3) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.1, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (4) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.08, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005; (5) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.05, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005; (6) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.03, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005.
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO8">


													<ImageObject Color="BlackWhite"
FileRef="MediaObjects/11265_2009_405_Fig6_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig7">


												<Caption Language="En">


													<CaptionNumber>Figure 7</CaptionNumber>



													<CaptionContent>


														<SimplePara>The mean (

															<Emphasis
Type="Bold">a</Emphasis>
) and mean square (

															<Emphasis
Type="Bold">b</Emphasis>
) convergence performance of the NLMM algorithm with Gaussian noise; 

															<Emphasis
Type="Bold">c</Emphasis>
 Illustration of (i) 

															<InlineEquation ID="IEq299">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq299.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
, (ii) 

															<InlineEquation ID="IEq300">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq300.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
 and (iii) 

															<InlineEquation ID="IEq301">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq301.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ k_\xi^2\widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
. Learning curves: (1) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.02, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
; (2) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.3, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.15, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−5</Superscript>
; (3) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.6, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.01, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−4</Superscript>
; (4) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.05, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
.
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO9">


													<ImageObject Color="BlackWhite"
FileRef="MediaObjects/11265_2009_405_Fig7_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig8">


												<Caption Language="En">


													<CaptionNumber>Figure 8</CaptionNumber>



													<CaptionContent>


														<SimplePara>The mean (

															<Emphasis
Type="Bold">a</Emphasis>
) and mean square (

															<Emphasis
Type="Bold">b</Emphasis>
) convergence performance of the LMM algorithm with CG noise; 

															<Emphasis
Type="Bold">c</Emphasis>
 Illustration of (i) 

															<InlineEquation ID="IEq302">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq302.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
, (ii) 

															<InlineEquation ID="IEq303">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq303.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ \widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
 and (iii) 

															<InlineEquation ID="IEq304">


																<InlineMediaObject>


																	<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq304.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


																</InlineMediaObject>



																<EquationSource
Format="TEX">$$ k_\xi^2\widehat\sigma_e^2(n) $$</EquationSource>


															</InlineEquation>
. Learning curves: (1) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.01, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−6</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 50, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.02; (2) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.3, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.008, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−5</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 100, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.015; (3) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.6, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.006, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−4</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 200, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.01; (4) 

															<Emphasis
Type="Italic">a</Emphasis>
 = 0.9, 

															<Emphasis
Type="Italic">μ</Emphasis>
 = 0.005, 

															<Emphasis
Type="Italic">σ</Emphasis>



															<Superscript>2</Superscript>



															<Subscript>


																<Emphasis
Type="Italic">g</Emphasis>


															</Subscript>
 = 10

															<Superscript>−3</Superscript>
, 

															<Emphasis
Type="Italic">r</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">im</Emphasis>


															</Subscript>
 = 400, 

															<Emphasis
Type="Italic">p</Emphasis>



															<Subscript>


																<Emphasis
Type="Italic">r</Emphasis>


															</Subscript>
 = 0.005.
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO10">


													<ImageObject Color="BlackWhite"
FileRef="MediaObjects/11265_2009_405_Fig8_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


												</MediaObject>


											</Figure>


										</Para>


									</Section1>



									<Section1 ID="Sec13" Type="Conclusion">


										<Heading>Conclusions</Heading>



										<Para>A new convergence performance analysis of the LMS and NLMS algorithms with error nonlinearity in Gaussian and CG noises is presented. The approach relies on the use of the Price theorem and its extension to signals with mixture Gaussian distributions, and the use of generalized Abelian integral functions in evaluating the mathematical expectation involved. New formulas for the EMSE, stability bound and difference equations describing both the transient and the steady state convergence behaviors of the algorithms are obtained. Simulation results show good agreement with the theoretical results.</Para>


									</Section1>


								</Body>



								<BodyRef
FileRef="BodyRef/PDF/11265_2009_Article_405.pdf" TargetType="OnlinePDF" />



								<ArticleBackmatter>


									<Appendix ID="App1">


										<Section1 ID="Sec14">


											<Heading>Appendix</Heading>



											<Section2 ID="Sec15">


												<Heading>Appendix A: Evaluation of 

													<Emphasis
Type="BoldItalic">H</Emphasis>



													<Subscript>1</Subscript>


												</Heading>



												<Para>We now extend the approach in [

													<CitationRef
CitationID="CR17">17</CitationRef>
, 

													<CitationRef
CitationID="CR18">18</CitationRef>
] and [

													<CitationRef
CitationID="CR12">12</CitationRef>
] to evaluate 

													<Emphasis
Type="BoldItalic">H</Emphasis>



													<Subscript>1</Subscript>
. Using the independence assumption 3, we further have 

													<Emphasis
Type="BoldItalic">H</Emphasis>



													<Subscript>1</Subscript>
 = 

													<Emphasis Type="Italic">E</Emphasis>



													<Subscript>{

														<Emphasis
Type="BoldItalic">v</Emphasis>
}
													</Subscript>
[

													<Emphasis
Type="BoldItalic">H</Emphasis>
], where 

													<InlineEquation ID="IEq305">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq305.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ H = {E_{\left\{ {X,{\eta_g}} \right\}}}\left[ {\psi (e)X/\left( {\varepsilon + {X^T}X} \right)|v} \right] $$</EquationSource>


													</InlineEquation>
. As 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">g</Emphasis>


													</Subscript>
(

													<Emphasis Type="Italic">n</Emphasis>
) and 

													<Emphasis Type="Italic">x</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
) are also assumed to be statistically independent, and 

													<Emphasis
Type="BoldItalic">X</Emphasis>
 are jointly Gaussian with autocorrelation matrix 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
, one gets

													<Equation ID="Equ63">


														<EquationNumber>A-1</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ63.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ H = {C_R}\iint\limits_{L + 1{\text{fold}}} {\frac{{\psi (e)X}}{{\varepsilon + {X^T}X}}}exp\left( { - \tfrac{1}{2}{X^T}R_{XX}^{ - 1}X} \right){f_{{\eta_g}}}\left( {{\eta_g}} \right)d{\eta_g}dX, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq306">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq306.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_R} = {\left( {2\pi } \right)^{ - L/2}}|{R_{XX}}{|^{ - 1/2}} $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq307">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq307.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {f_{{\eta_g}}}\left( {{\eta_g}} \right) $$</EquationSource>


													</InlineEquation>
 is the PDF of the Gaussian noise 

													<InlineEquation ID="IEq308">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq308.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\eta_g}.\left| \cdot \right| $$</EquationSource>


													</InlineEquation>
 denotes the determinant of a matrix. Similar to [

													<CitationRef
CitationID="CR17">17</CitationRef>
] and [

													<CitationRef
CitationID="CR18">18</CitationRef>
], let us consider the integral

													<Equation ID="Equ64">


														<EquationNumber>A-2</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ64.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {F\left( \beta \right) = {C_R}\iint\limits_{L + 1{\text{ fold}}} {\frac{{\psi (e)X\exp \left( { - \beta \left( {\varepsilon + {X^T}X} \right)} \right)}}{{\varepsilon + {X^T}X}}}} \\ { \cdot \exp \left( { - \tfrac{1}{2}{X^T}R_{XX}^{ - 1}X} \right){f_{{\eta_g}}}\left( {{\eta_g}} \right)d{\eta_g}dX.} \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>It can be seen that 

													<Emphasis
Type="BoldItalic">H</Emphasis>
 = 

													<Emphasis
Type="BoldItalic">F</Emphasis>
(0). Differentiating (

													<InternalRef
RefID="Equ64">A-2</InternalRef>
) with respect to 

													<Emphasis Type="Italic">β</Emphasis>
, one gets

													<Equation ID="Equ65">


														<EquationNumber>A-3</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ65.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {\frac{{dF\left( \beta \right)}}{{d\beta }} = - \exp \left( { - \beta \varepsilon } \right){C_R}\iint\limits_{L + 1{\text{ fold}}} {{\text{ }}\left( {\psi (e)X} \right)}\exp \left( { - \frac{1}{2}{X^T}{B^{ - 1}}(n)X} \right)} \\ { \cdot {f_{{\eta_g}}}\left( {{\eta_g}} \right)d{\eta_g}dX,} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq309">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq309.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ B(n) = {\left( {2\beta I + R_{XX}^{ - 1}} \right)^{ - 1}} $$</EquationSource>


													</InlineEquation>
. For notation convenience, we shall simply write 

													<Emphasis
Type="BoldItalic">B</Emphasis>
 for 

													<Emphasis
Type="BoldItalic">B</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
). To evaluate the integral, we use the eigenvalue decomposition of 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
 = 

													<Emphasis
Type="BoldItalic">U</Emphasis>
Λ

													<Emphasis
Type="BoldItalic">U</Emphasis>



													<Superscript>


														<Emphasis
Type="Italic">T</Emphasis>


													</Superscript>
, where Λ is a diagonal matrix whose elements are the eigenvalues of 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
 and 

													<Emphasis
Type="BoldItalic">U</Emphasis>
 is a unitary matrix. The matrix 

													<Emphasis
Type="BoldItalic">B</Emphasis>



													<Superscript>−1</Superscript>
 can be written as

													<Equation ID="Equ66">


														<EquationNumber>A-4</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ66.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {B^{ - 1}} = U\left( {2\beta I + {\Lambda^{ - 1}}} \right){U^T} = U{D^{ - 1}}{U^T}, $$</EquationSource>


													</Equation>
where 

													<Emphasis
Type="BoldItalic">D</Emphasis>
 is a diagonal matrix with the 

													<Emphasis Type="Italic">i</Emphasis>
-th diagonal entry given by 

													<InlineEquation ID="IEq310">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq310.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {d_{ii}}(n) = {\left( {2\beta + \lambda_i^{ - 1}} \right)^{ - 1}} $$</EquationSource>


													</InlineEquation>
. Noting that the determinants of 

													<Emphasis
Type="BoldItalic">U</Emphasis>
 and 

													<Emphasis
Type="BoldItalic">D</Emphasis>
 are respectively 1 and 

													<InlineEquation ID="IEq311">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq311.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left| D \right|^{ - 1/2}} = \prod\limits_{i = 1}^L {{{\left( {2\beta + \lambda_i^{ - 1}} \right)}^{1/2}}} $$</EquationSource>


													</InlineEquation>
, one can rewrite (

													<InternalRef
RefID="Equ65">A-3</InternalRef>
) as follows

													<Equation ID="Equ67">


														<EquationNumber>A-5</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ67.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \frac{{dF\left( \beta \right)}}{{d\beta }} = - \gamma \left( \beta \right){E_{\left\{ {X,{\eta_g}} \right\}}}{\left. {\left[ {\psi (e)X\left| v \right.} \right]} \right|_{E\left[ {X{X^T}} \right] = B}} = - \gamma \left( \beta \right){L_1}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq312">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq312.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_B} = {\left( {2\pi } \right)^{ - L/2}}{\left| B \right|^{ - 1/2}} $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq313">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq313.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \gamma \left( \beta \right) = \exp \left( { - \beta \varepsilon } \right)\prod\limits_{i = 1}^L {{{\left( {2\beta {\lambda_i} + 1} \right)}^{\frac{1}{2}}}} $$</EquationSource>


													</InlineEquation>
, and 

													<InlineEquation ID="IEq314">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq314.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {L_1} = {E_{\left\{ {X,{\eta_g}} \right\}}}\left[ {\psi (e)X\left| v \right.} \right]\left| {_{E\left[ {X{X^T}} \right] = B}} \right. $$</EquationSource>


													</InlineEquation>
 is the expectation of 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
)

													<Emphasis
Type="BoldItalic">X</Emphasis>
 conditioned on 

													<Emphasis
Type="BoldItalic">v</Emphasis>
 when 

													<Emphasis Type="Italic">x</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
, 

													<Emphasis Type="Italic">x</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">j</Emphasis>


													</Subscript>
∈

													<Emphasis
Type="BoldItalic">X</Emphasis>
 are jointly Gaussian with correlation matrix 

													<Emphasis
Type="BoldItalic">B</Emphasis>
. We shall evaluate 

													<Emphasis
Type="BoldItalic">L</Emphasis>



													<Subscript>1</Subscript>
 by considering its 

													<Emphasis Type="Italic">i-</Emphasis>
th element as follows:

													<Equation ID="Equ68">


														<EquationNumber>A-6</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ68.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {L_{1,i}} = {E_{\left\{ {X,{\eta_g}} \right\}}}{\left. {\left[ {\psi (e){x_i}\left| v \right.} \right]} \right|_{E\left[ {X{X^T}} \right] = B}} \cdot $$</EquationSource>


													</Equation>


												</Para>



												<Para>Using the Price’s theorem, we have

													<Equation ID="Equ69">


														<EquationNumber>A-7</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ69.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {\frac{{\partial {L_{1,i}}}}{{\partial {r_{{x_i}e}}}} = {E_{\left\{ {X,{\eta_g}} \right\}}}{{\left. {\left[ {\frac{{d\psi (e)}}{{de}}\left| v \right.} \right]} \right|}_{E\left[ {X{X^T}} \right] = B}}} \\ { = \frac{1}{{\sqrt {2\pi } {\sigma_e}}}\int_{ - \infty }^\infty {\psi \prime (e)\exp \left( { - \frac{{{e^2}}}{{2\sigma_e^2}}} \right)de = } \overline {\psi \prime } \left( {\sigma_e^2} \right),} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq315">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq315.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline {\psi \prime } \left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 is the average of 

													<Emphasis Type="Italic">ψ</Emphasis>
′(

													<Emphasis Type="Italic">e</Emphasis>
), and 

													<Emphasis Type="Italic">e</Emphasis>
 = 

													<Emphasis
Type="BoldItalic">X</Emphasis>



													<Superscript>


														<Emphasis
Type="Italic">T</Emphasis>


													</Superscript>



													<Emphasis Type="BoldItalic">v 
													</Emphasis>
+ 

													<Emphasis Type="Italic">η</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">g</Emphasis>


													</Subscript>
, which is Gaussian distributed with zero mean and variance 

													<InlineEquation ID="IEq316">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq316.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(v) = E\left[ {{v^T}X{X^T}v\left| v \right.} \right]\left| {_{E\left[ {X{X^T}} \right] = B}} \right. + \sigma_g^2 = {v^T}Bv + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
. To simplify notation, we shall write 

													<InlineEquation ID="IEq317">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq317.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline {\psi \prime } \left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 as 

													<InlineEquation ID="IEq318">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq318.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
. Integrating (

													<InternalRef
RefID="Equ69">A-7</InternalRef>
) with respect to 

													<InlineEquation ID="IEq319">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq319.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {r_{{x_i}e}} $$</EquationSource>


													</InlineEquation>
, one gets

													<Equation ID="Equ70">


														<EquationNumber>A-8</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ70.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {L_{1,i}} = {A_\psi }\left( {\sigma_e^2} \right){r_{{x_i}e}}, $$</EquationSource>


													</Equation>
where the constant of integration is zero because 

													<InlineEquation ID="IEq320">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq320.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {r_{{x_i}e}} $$</EquationSource>


													</InlineEquation>
 is equal to zero when 

													<Emphasis Type="Italic">x</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
 and 

													<Emphasis Type="Italic">e</Emphasis>
 are uncorrelated. Since

													<Equation ID="Equ71">


														<EquationNumber>A-9</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ71.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{r_{{x_i}e}} = {{\left. {E\left[ {{x_i}e\left| v \right.} \right]} \right|}_{E\left[ {X{X^T}} \right] = B}}} \\ { = E{{\left. {\left[ {{x_i}\left( {{X^T}v + {\eta_g}} \right)\left| v \right.} \right]} \right|}_{E\left[ {X{X^T}} \right] = B}} = {B_i}v,} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<Emphasis
Type="BoldItalic">B</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">i</Emphasis>


													</Subscript>
 is the 

													<Emphasis Type="Italic">i</Emphasis>
-th row of 

													<Emphasis
Type="BoldItalic">B</Emphasis>
, we have

													<Equation ID="Equ72">


														<EquationNumber>A-10</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ72.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {L_1} = {A_\psi }\left( {\sigma_e^2} \right)Bv(n). $$</EquationSource>


													</Equation>


												</Para>



												<Para>Substituting (

													<InternalRef
RefID="Equ72">A-10</InternalRef>
) into (

													<InternalRef
RefID="Equ67">A-5</InternalRef>
) and integrating yield

													<Equation ID="Equ73">


														<EquationNumber>A-11</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ73.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ F(\beta ) = \left( { - {\smallint^\beta }{A_\psi }\left( {\sigma_e^2} \right)\gamma \left( \beta \right)Bd\beta } \right){\mathbf{v}}(n) \approx {A_\psi }(\overline {\sigma_e^2} )I\left( \beta \right){\mathbf{v}}(n), $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq321">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq321.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ I\left( \beta \right) = U\left( { - {\smallint^\beta }\gamma \left( \beta \right)D(n)d\beta } \right){U^T} $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq322">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq322.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {d_{ii}}(n) = {\lambda_i}/\left( {2\beta {\lambda_i} + 1} \right) $$</EquationSource>


													</InlineEquation>
. The constant of integration is equal to zero because of the boundary condition 

													<Emphasis
Type="BoldItalic">F</Emphasis>
(∞) = 0. Here, we have assumed that 

													<InlineEquation ID="IEq323">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq323.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline {\psi \prime } \left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 depends weakly on 

													<Emphasis Type="Italic">β</Emphasis>
 and is taken outside of the integral using mean value theorem with some appropriate mean value 

													<InlineEquation ID="IEq324">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq324.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\overline {\sigma_e^2} } \right) $$</EquationSource>


													</InlineEquation>
. Note, the dependence of 

													<InlineEquation ID="IEq325">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq325.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }(\sigma_e^2) = \overline {\psi \prime } (\sigma_e^2) $$</EquationSource>


													</InlineEquation>
 on 

													<Emphasis Type="Italic">β</Emphasis>
 is mainly through 

													<InlineEquation ID="IEq326">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq326.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 = {v^T}Bv + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
. From (

													<InternalRef
RefID="Equ66">A-4</InternalRef>
), we can see that as 

													<Emphasis Type="Italic">β</Emphasis>
 various from 0 to ∞, 

													<InlineEquation ID="IEq327">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq327.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 will decrease from 

													<InlineEquation ID="IEq328">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq328.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(n) = {v^T}{R_{XX}}v + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
 to 

													<InlineEquation ID="IEq329">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq329.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_g^2 $$</EquationSource>


													</InlineEquation>
. Here, we shall use the mean value of the upper bound: 

													<InlineEquation ID="IEq330">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq330.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(n) = E\left[ {{v^T}{R_{XX}}v} \right] + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
, which is the mean MSE at time 

													<Emphasis Type="Italic">n</Emphasis>
 for 

													<InlineEquation ID="IEq331">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq331.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline {\sigma_e^2} $$</EquationSource>


													</InlineEquation>
. This is also a good approximation at the steady state of the algorithm where 

													<InlineEquation ID="IEq332">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq332.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 = {v^T}Bv + \sigma_g^2 \approx \sigma_g^2 $$</EquationSource>


													</InlineEquation>
. This is also commonly used as in [

													<CitationRef
CitationID="CR34">34</CitationRef>
, 

													<CitationRef
CitationID="CR35">35</CitationRef>
]. For the effect of the nonlinearity 

													<InlineEquation ID="IEq333">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq333.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
, first we note from (

													<InternalRef
RefID="Equ69">A-7</InternalRef>
) that it can be rewritten as 

													<InlineEquation ID="IEq334">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq334.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) = \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {\psi \prime \left( {{\sigma_e}u} \right)\exp \left( { - \frac{{{u^2}}}{2}} \right)} du $$</EquationSource>


													</InlineEquation>
. For most practical nonlinearities, it is shown in Appendix 

													<InternalRef
RefID="Sec17">C</InternalRef>
 that 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
) can be written as 

													<InlineEquation ID="IEq335">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq335.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ eq\left( {e/\xi } \right) $$</EquationSource>


													</InlineEquation>
 with threshold parameter 

													<Emphasis Type="Italic">ξ</Emphasis>
. Moreover, it can be shown in Appendix 

													<InternalRef
RefID="Sec17">C</InternalRef>
 that if the ATS in (

													<InternalRef
RefID="Equ7">7</InternalRef>
)–(

													<InternalRef
RefID="Equ9">9</InternalRef>
) is used to update 

													<Emphasis Type="Italic">ξ</Emphasis>
 for these nonlinearities (collectively called the M-nonlinearity) and the estimated error variance 

													<InlineEquation ID="IEq336">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq336.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \widehat\sigma_e^2 $$</EquationSource>


													</InlineEquation>
 is close to the “impulse-free” variance, then 

													<InlineEquation ID="IEq337">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq337.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 is approximately independent of 

													<InlineEquation ID="IEq338">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq338.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 and hence of 

													<Emphasis Type="Italic">β</Emphasis>
.
												</Para>



												<Para>For general nonlinearity, 

													<Emphasis Type="Italic">ψ</Emphasis>
(

													<Emphasis Type="Italic">e</Emphasis>
) is usually equal to one when 

													<InlineEquation ID="IEq339">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq339.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| e \right| $$</EquationSource>


													</InlineEquation>
 is smaller than 

													<Emphasis Type="Italic">ξ</Emphasis>
, and gradually decreases to zero. For sufficiently large 

													<Emphasis Type="Italic">ξ</Emphasis>
, the main contribution is due to the first term in (

													<InternalRef
RefID="Equ99">C-5</InternalRef>
) because of the exponential decay of the Gaussian PDF. Hence, 

													<InlineEquation ID="IEq340">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq340.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) \approx \frac{2}{{\sqrt {2\pi } }}\int_0^{\xi /{\sigma_e}} {\exp \left( { - \frac{{{u^2}}}{2}} \right)} du $$</EquationSource>


													</InlineEquation>
. The derivative of 

													<InlineEquation ID="IEq341">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq341.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 with respect to 

													<InlineEquation ID="IEq342">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq342.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 is approximately 

													<InlineEquation ID="IEq343">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq343.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ - \frac{{2\xi }}{{\sqrt {2\pi \sigma_e^2} }}\exp \left( { - \frac{{{\xi^2}}}{{2\sigma_e^2}}} \right) $$</EquationSource>


													</InlineEquation>
. For sufficiently large 

													<Emphasis Type="Italic">ξ</Emphasis>
, the exponential factor decreases faster than the other factor and the variations of 

													<InlineEquation ID="IEq344">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq344.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 with respect to 

													<InlineEquation ID="IEq345">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq345.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 is small. This also justifies the approximation used above. To efficiently suppress outliers, 

													<Emphasis Type="Italic">ξ</Emphasis>
 however cannot be very large so that it is important to adapt it with respect to 

													<InlineEquation ID="IEq346">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq346.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
, which will help to reduce the variation of 

													<InlineEquation ID="IEq347">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq347.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 with respect to 

													<InlineEquation ID="IEq348">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq348.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
. These are also the cases which are of practical interests. In conclusion, (

													<InternalRef
RefID="Equ73">A-11</InternalRef>
) is a good approximation for commonly used M-estimate functions and practical nonlinearities with ATS or at the steady state of the algorithm.
												</Para>



												<Para>To evaluate the term 

													<Emphasis
Type="BoldItalic">I</Emphasis>
(0) and hence 

													<InlineEquation ID="IEq349">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq349.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ H = F(0) = {A_\psi }\left( {\sigma_e^2} \right)I(0)v(n) $$</EquationSource>


													</InlineEquation>
, we note from (

													<InternalRef
RefID="Equ66">A-4</InternalRef>
) that 

													<Emphasis
Type="BoldItalic">B</Emphasis>
 = 

													<Emphasis
Type="BoldItalic">UD</Emphasis>
(

													<Emphasis Type="Italic">n</Emphasis>
)

													<Emphasis
Type="BoldItalic">U</Emphasis>



													<Superscript>


														<Emphasis
Type="Italic">T</Emphasis>


													</Superscript>
 and thus

													<Equation ID="Equ74">


														<EquationNumber>A-12</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ74.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ I(0) = U\left( { - {\smallint^0}\gamma \left( \beta \right)D(n)d\beta } \right){U^T} = U\Lambda {D_\Lambda }{U^T}. $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq350">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq350.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {D_\Lambda } = {\text{diag}}\left( {{I_1}\left( \Lambda \right),...,{I_L}\left( \Lambda \right)} \right) $$</EquationSource>


													</InlineEquation>
 and

													<Equation ID="Equ75">


														<EquationNumber>A-13</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ75.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {I_i}(\Lambda ) = \int_0^\infty {exp\left( { - \beta \varepsilon } \right)\left[ {\mathop \Pi \limits_{k = 1}^L {{\left( {2\beta {\lambda_k} + 1} \right)}^{ - 1/2}}} \right]{{\left( {2\beta {\lambda_i} + 1} \right)}^{ - 1}}d\beta }, $$</EquationSource>


													</Equation>



													<InlineEquation ID="IEq351">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq351.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ i = 1,2, \cdots, L $$</EquationSource>


													</InlineEquation>
. Finally, we have the following desired result

													<Equation ID="Equ76">


														<EquationNumber>A-14</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ76.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {H_1} = E\left[ H \right] = E\left[ {F(0)} \right] \approx {A_\psi }\left( {\sigma_e^2} \right)U\Lambda {D_\Lambda }{U^T}E\left[ {v(n)} \right]. $$</EquationSource>


													</Equation>


												</Para>



												<Para>For the LMS-like algorithms, 

													<Emphasis
Type="BoldItalic">B</Emphasis>
 will be equal to 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
. Accordingly, (

													<InternalRef
RefID="Equ76">A-14</InternalRef>
) will be modified by replacing 

													<InlineEquation ID="IEq352">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq352.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {D_\Lambda } $$</EquationSource>


													</InlineEquation>
 with the identity matrix and 

													<InlineEquation ID="IEq353">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq353.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(v) $$</EquationSource>


													</InlineEquation>
 by 

													<InlineEquation ID="IEq354">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq354.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {v^T}{R_{XX}}v + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
.
												</Para>


											</Section2>



											<Section2 ID="Sec16">


												<Heading>Appendix B: Evaluation of 

													<Emphasis
Type="BoldItalic">s</Emphasis>



													<Subscript>3</Subscript>
 and 

													<Emphasis
Type="BoldItalic">M</Emphasis>



													<Subscript>3</Subscript>


												</Heading>



												<Para>Firstly, we note that 

													<InlineEquation ID="IEq355">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq355.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {M_3} = {\mu^2}{E_{\left\{ v \right\}}}\left[ {{s_3}} \right] $$</EquationSource>


													</InlineEquation>
, where 

													<InlineEquation ID="IEq356">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq356.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {s_3} = {E_{\{ X,{\eta_g}\} }}\left[ {{\psi^2}(e)X{X^T}/{{(\varepsilon + {X^T}X)}^2}|v} \right] $$</EquationSource>


													</InlineEquation>
. As in Appendix 

													<InternalRef
RefID="Sec14">A</InternalRef>
, we can write 

													<Emphasis
Type="BoldItalic">s</Emphasis>



													<Subscript>3</Subscript>
 as:

													<Equation ID="Equ77">


														<EquationNumber>B-1</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ77.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {s_3} = {C_R}\iint\limits_{L + 1{\text{ fold}}} {\frac{{{\psi^2}(e)X{X^T}}}{{{{\left( {\varepsilon + {X^T}X} \right)}^2}}}}exp\left( { - \tfrac{1}{2}{X^T}R_{XX}^{ - 1}X} \right){f_{{\eta_g}}}\left( {{\eta_g}} \right)d{\eta_g}dX. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Let us define

													<Equation ID="Equ78">


														<EquationNumber>B-2</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ78.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {\overline F \left( \beta \right) = {C_R}\iint\limits_{L + 1{\text{ fold}}} {\frac{{{\psi^2}(e)X{X^T}exp\left( { - \beta \left( {\varepsilon + {X^T}X} \right)} \right)}}{{{{\left( {\varepsilon + {X^T}X} \right)}^2}}}}} \\ { \times \exp ( - \tfrac{1}{2}{X^T}R_{XX}^{ - 1}X){f_{{\eta_g}}}({\eta_g})d{\eta_g}dX.} \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>Comparing (

													<InternalRef
RefID="Equ78">B-2</InternalRef>
) with (

													<InternalRef
RefID="Equ77">B-1</InternalRef>
), it can be seen that 

													<InlineEquation ID="IEq357">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq357.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {s_3} = \overline F (0) $$</EquationSource>


													</InlineEquation>
. To evaluate 

													<InlineEquation ID="IEq358">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq358.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline F \left( \beta \right) $$</EquationSource>


													</InlineEquation>
, differentiating (

													<InternalRef
RefID="Equ78">B-2</InternalRef>
) twice with respect to 

													<Emphasis Type="Italic">β</Emphasis>
, one gets

													<Equation ID="Equ79">


														<EquationNumber>B-3</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ79.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{d^2}\overline F \left( \beta \right)/d{\beta^2} = {C_R}exp\left( { - \beta \varepsilon } \right)\iint\limits_{L + 1{\text{ fold}}} {{\psi^2}(e)X{X^T}}exp\left( { - \tfrac{1}{2}{X^T}{B^{ - 1}}X} \right){f_{{\eta_g}}}\left( {{\eta_g}} \right)d{\eta_g}dX} \\ { = \gamma \left( \beta \right)\left[ {{C_B}\iint\limits_{L + 1{\text{ fold}}} {{\psi^2}(e)X{X^T}exp\left( { - \frac{1}{2}{X^T}{B^{ - 1}}X} \right){f_{{\eta_g}}}\left( {{\eta_g}} \right)d{\eta_g}d\left. X \right]}} \right]} \\ { = \gamma \left( \beta \right){L_3},} \\ \end{array} $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq359">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq359.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {L_3} = {E_{\left\{ {X,{\eta_g}} \right\}}}{\left. {\left[ {{\psi^2}(e)X{X^T}|v} \right]} \right|_{E\left[ {X{X^T}} \right] = B}} $$</EquationSource>


													</InlineEquation>
, 

													<Emphasis Type="Italic">γ</Emphasis>
(

													<Emphasis Type="Italic">β</Emphasis>
), 

													<Emphasis Type="Italic">C</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">B</Emphasis>


													</Subscript>
, and the correlation matrix 

													<Emphasis
Type="BoldItalic">B</Emphasis>
 have been defined in Appendix 

													<InternalRef
RefID="Sec14">A</InternalRef>
. Let’s evaluate the (

													<Emphasis Type="Italic">i</Emphasis>
, 

													<Emphasis Type="Italic">j</Emphasis>
)-th element of 

													<Emphasis
Type="BoldItalic">L</Emphasis>



													<Subscript>3</Subscript>
 as follows:

													<Equation ID="Equ80">


														<EquationNumber>B-4</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ80.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {L_{3,i,j}} = {E_{\left\{ {X,{\eta_g}} \right\}}}{\left. {\left[ {{\psi^2}(e){x_i}{x_j}\left| v \right.} \right]} \right|_B}. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Using Price’s theorem, we have

													<Equation ID="Equ81">


														<EquationNumber>B-5</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ81.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {\frac{{\partial {L_{3,i,j}}}}{{\partial {r_{3,i,j}}}} = {E_{\left\{ {X,{\eta_g}} \right\}}}{{\left. {\left[ {{\psi^2}(e)\left| v \right.} \right]} \right|}_B}} \\ {{\text{ }} = \frac{1}{{\sqrt {2\pi } {\sigma_e}}}\int_{ - \infty }^\infty {{\psi^2}(e)\exp \left( { - \frac{{{e^2}}}{{2\sigma_e^2}}} \right)de} = \overline {{\psi^2}} \left( {\sigma_e^2} \right).} \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>For notational simplicity, we shall write 

													<InlineEquation ID="IEq360">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq360.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline {{\psi^2}} \left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 as 

													<InlineEquation ID="IEq361">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq361.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
. Integrating with respect to 

													<InlineEquation ID="IEq362">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq362.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {r_{x_i} {x_j}} $$</EquationSource>


													</InlineEquation>
 gives

													<Equation ID="Equ82">


														<EquationNumber>B-6</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ82.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {L_{3,i,j}} = {B_\psi }\left( {\sigma_e^2} \right){r_{{x_i}{x_j}}} + {c_{i,j}}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq363">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq363.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {c_{i,j}} = E{\left. {\left[ {{\psi^2}(e){x_i}{x_j}} \right]} \right|_{{r_{{x_i}{x_j} = 0}}}} $$</EquationSource>


													</InlineEquation>
 is the integration constant. Using Price’s theorem again, we have

													<Equation ID="Equ83">


														<EquationNumber>B-7</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ83.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \frac{{\partial {c_{i,j}}}}{{\partial {r_{{x_i}e}}}} = E{\left. {\left[ {\frac{{d{\psi^2}(e)}}{{de}}{x_j}} \right]} \right|_{{r_{{x_i}{x_j} = 0}}}} = E\left[ {\frac{{{d^2}{\psi^2}(e)}}{{d{e^2}}}} \right]{r_{{x_j}e}} = 2{C_\psi }\left( {\sigma_e^2} \right){r_{{x_j}e}}. $$</EquationSource>


													</Equation>


												</Para>



												<Para>(From Price’s theorem, 

													<InlineEquation ID="IEq364">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq364.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \tfrac{1}{2}E\left[ {\tfrac{{{d^2}{\psi^2}(e)}}{{d{e^2}}}} \right] = \tfrac{d}{{d\sigma_e^2}}E\left[ {{\psi^2}(e)} \right] $$</EquationSource>


													</InlineEquation>
, which is written as 

													<InlineEquation ID="IEq365">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq365.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 here) Integrating again, one gets

													<Equation ID="Equ84">


														<EquationNumber>B-8</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ84.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {c_{i,j}} = 2{C_\psi }\left( {\sigma_e^2} \right){r_{{x_j}e}}{r_{{x_i}e}}. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Combining (

													<InternalRef
RefID="Equ82">B-6</InternalRef>
) and (

													<InternalRef
RefID="Equ84">B-8</InternalRef>
), we have

													<Equation ID="Equ85">


														<EquationNumber>B-9</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ85.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {L_3} = {B_\psi }\left( {\sigma_e^2} \right)B + 2{C_\psi }\left( {\sigma_e^2} \right)Bv{v^T}B. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Substituting (

													<InternalRef
RefID="Equ85">B-9</InternalRef>
) into (

													<InternalRef
RefID="Equ79">B-3</InternalRef>
) gives

													<Equation ID="Equ86">


														<EquationNumber>B-10</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ86.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \frac{{{d^2}\overline F \left( \beta \right)}}{{d{\beta^2}}} = \gamma \left( \beta \right){B_\psi }\left( {\sigma_e^2} \right)B + 2\gamma \left( \beta \right){C_\psi }\left( {\sigma_e^2} \right)Bv{v^T}B. $$</EquationSource>


													</Equation>


												</Para>



												<Para>Integrating (

													<InternalRef
RefID="Equ86">B-10</InternalRef>
) with respect to 

													<Emphasis Type="Italic">β</Emphasis>
 yields

													<Equation ID="Equ87">


														<EquationNumber>B-11</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ87.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{s_3} = \int_0^\infty {\int_{{\beta_1}}^\infty {\gamma \left( {{\beta_2}} \right){B_\psi }\left( {\sigma_e^2} \right)Bd{\beta_2}d{\beta_1}} } } \\ { + 2\int_0^\infty {\int_{{\beta_1}}^\infty {\gamma \left( {{\beta_2}} \right){C_\psi }\left( {\sigma_e^2} \right)Bv{v^T}Bd{\beta_2}d{\beta_1}} }, } \\ \end{array} $$</EquationSource>


													</Equation>
with the boundary conditions: 

													<InlineEquation ID="IEq366">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq366.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \overline F \left( \infty \right) = 0 $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq367">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq367.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \partial \overline F \left( \beta \right)/{\left. {\partial \beta } \right|_{\beta = \infty }} = 0 $$</EquationSource>


													</InlineEquation>
. For NLMS algorithm with general nonlinearity, the integrals are rather difficult to evaluate because of the presence of 

													<InlineEquation ID="IEq368">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq368.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
.
												</Para>



												<Para>Fortunately, it can be shown in Appendix 

													<InternalRef
RefID="Sec17">C</InternalRef>
 that for M-nonlinearities which include commonly used M-estimate functions and several practical nonlinearities, 

													<InlineEquation ID="IEq369">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq369.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right)/\sigma_e^2 = {S_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq370">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq370.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 are approximately independent of 

													<InlineEquation ID="IEq371">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq371.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 due to the use of ATS in (

													<InternalRef
RefID="Equ7">7</InternalRef>
)–(

													<InternalRef
RefID="Equ9">9</InternalRef>
) to update 

													<Emphasis Type="Italic">ξ</Emphasis>
 and thus the estimated error variance 

													<InlineEquation ID="IEq372">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq372.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 is close to the “impulse-free” variance. Similar to the case of 

													<InlineEquation ID="IEq373">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq373.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {A_\psi }\left( {\hat\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
, if 

													<Emphasis Type="Italic">ξ</Emphasis>
 is sufficiently large, then this is also a reasonable approximation, especially at the steady state. Thus, 

													<InlineEquation ID="IEq374">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq374.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq375">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq375.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 can be taken outside of the integral if the above conditions are satisfied. This leaves the term 

													<InlineEquation ID="IEq376">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq376.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
, which is identical to the conventional analysis of the NLMS algorithm. Taking 

													<InlineEquation ID="IEq377">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq377.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 and 

													<InlineEquation ID="IEq378">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq378.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 outside the integral, one gets

													<Equation ID="Equ88">


														<EquationNumber>B-12</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ88.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {s_3} \approx {C_\psi }(\sigma_e^2){I_1} + {S_\psi }(\sigma_e^2){I_2} + {S_\psi }(\sigma_e^2)\sigma_g^2{I_3}. $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq379">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq379.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {I_1} = 2\int_0^\infty {\int_{{\beta_1}}^\infty {\gamma \left( {{\beta_2}} \right)Bv{v^T}Bd{\beta_2}d} {\beta_1}} $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq380">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq380.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {I_2} = \int_0^\infty {\int_{{\beta_1}}^\infty {\gamma \left( {{\beta_2}} \right)\left( {{v^T}Bv} \right)Bd{\beta_2}d} {\beta_1}} $$</EquationSource>


													</InlineEquation>
, and 

													<InlineEquation ID="IEq381">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq381.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {I_3} = \int_0^\infty {\int_{{\beta_1}}^\infty {\gamma \left( {{\beta_2}} \right)Bd{\beta_2}d} {\beta_1}} $$</EquationSource>


													</InlineEquation>
. The integrals 

													<Emphasis
Type="BoldItalic">I</Emphasis>



													<Subscript>1</Subscript>
 ~ 

													<Emphasis
Type="BoldItalic">I</Emphasis>



													<Subscript>3</Subscript>
 also appear in the analysis of the NLMS algorithm in Gaussian noise [

													<CitationRef
CitationID="CR37">37</CitationRef>
] and they have been evaluated to be

													<Equation ID="Equ89">


														<EquationNumber>B-13</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ89.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {I_1} = U{D_1}{U^T}, $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq382">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq382.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {D_1} = 2\Lambda \left[ {\left( {V{V^T}} \right) \circ I\left( \Lambda \right)} \right]\Lambda $$</EquationSource>


													</InlineEquation>
, 

													<InlineEquation ID="IEq383">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq383.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {I\left( \Lambda \right)} \right]_{i,j}} = {I_{ij}}\left( \Lambda \right) $$</EquationSource>


													</InlineEquation>
, ○ denotes element-wise product of two matrices,

													<Equation ID="Equ90">


														<EquationNumber>B-14a</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ90.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {I_{ij}}\left( \Lambda \right) = \int_0^\infty {\beta \exp \left( { - \beta \varepsilon } \right)\left[ {\mathop \Pi \limits_{k = 1}^L {{\left( {2\beta {\lambda_k} + 1} \right)}^{ - 1/2}}} \right]} {\left( {2\beta {\lambda_i} + 1} \right)^{ - 1}}{\left( {2\beta {\lambda_j} + 1} \right)^{ - 1}}d\beta, $$</EquationSource>


													</Equation>
and

													<Equation ID="Equ91">


														<EquationNumber>B-14b</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ91.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {I_2} = U{D_2}{U^T} $$</EquationSource>


													</Equation>
where 

													<Emphasis
Type="BoldItalic">D</Emphasis>



													<Subscript>2</Subscript>
 is a diagonal matrix and its 

													<Emphasis Type="Italic">i</Emphasis>
-th diagonal element is given by 

													<InlineEquation ID="IEq384">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq384.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\left[ {{D_2}} \right]_{i,i}} = \mathop \Sigma \limits_k {\lambda_k}{\lambda_i}{I_{ki}}\left( \Lambda \right){\left[ {V{V^T}} \right]_{k,k}} $$</EquationSource>


													</InlineEquation>
.

													<Equation ID="Equ92">


														<EquationNumber>B-15</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ92.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {I_3} = \int_0^\infty {\int_{{\beta_1}}^\infty {\gamma \left( {{\beta_2}} \right)UD(n){U^T}d{\beta_2}d{\beta_1}} } = U{D_3}(n){U^T}, $$</EquationSource>


													</Equation>
where 

													<Emphasis
Type="BoldItalic">D</Emphasis>



													<Subscript>3</Subscript>
(

													<Emphasis Type="Italic">n</Emphasis>
) = Λ

													<Emphasis
Type="BoldItalic">I</Emphasis>
′(Λ) and 

													<Emphasis
Type="BoldItalic">I</Emphasis>
′(Λ) is a diagonal matrix with its 

													<Emphasis Type="Italic">i</Emphasis>
-th diagonal element given by

													<Equation ID="Equ93">


														<EquationNumber>B-16</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ93.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ {I'_i}\left( \Lambda \right) = \int_0^\infty {\beta \exp \left( { - \beta \varepsilon } \right)\left[ {\mathop \Pi \limits_{k = 1}^L {{\left( {2\beta {\lambda_k} + 1} \right)}^{ - \tfrac{1}{2}}}} \right]{{\left( {2\beta {\lambda_i} + 1} \right)}^{ - 1}}} d\beta . $$</EquationSource>


													</Equation>


												</Para>



												<Para>Substituting (

													<InternalRef
RefID="Equ89">B-13</InternalRef>
)~(

													<InternalRef
RefID="Equ92">B-15</InternalRef>
) into (

													<InternalRef
RefID="Equ88">B-12</InternalRef>
) yields

													<Equation ID="Equ94">


														<EquationNumber>B-17</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ94.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{M_3} = {\mu^2}{E_{\left\{ v \right\}}}\left[ {{s_3}} \right] \approx 2{C_\psi }\left( {\sigma_e^2} \right){\mu^2}U\left\{ {\Lambda \left[ {\left( {{U^T}{\mathbf{\Xi }}(n)U} \right) \circ I\left( \Lambda \right)\Lambda } \right.} \right\}{U^T}} \\ { + {S_\psi }\left( {\sigma_e^2} \right){\mu^2}U{{\overline D }_2}{U^T} + {S_\psi }\left( {\sigma_e^2} \right){\mu^2}\sigma_g^2U\Lambda I\prime \left( \Lambda \right){U^T}.} \\ \end{array} $$</EquationSource>


													</Equation>


												</Para>



												<Para>This expression is somewhat complicated. To simplify the determination of the step size bound, we can alternatively adopt the argument in Appendix 

													<InternalRef
RefID="Sec14">A</InternalRef>
 and that in [

													<CitationRef
CitationID="CR37">37</CitationRef>
] by approximating 

													<InlineEquation ID="IEq385">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq385.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


													</InlineEquation>
 inside the integral by the mean value of its upper bound 

													<InlineEquation ID="IEq386">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq386.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \sigma_e^2(n) = E\left[ {{v^T}{R_{XX}}v} \right] + \sigma_g^2 $$</EquationSource>


													</InlineEquation>
, which is the mean MSE at time 

													<Emphasis Type="Italic">n</Emphasis>
. Although this approximation is not entirely valid for all 

													<Emphasis Type="Italic">n</Emphasis>
, 

													<InlineEquation ID="IEq387">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq387.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


													</InlineEquation>
 only dominates at the steady states where the other terms have died down. Therefore, the mean square behavior can still be satisfactorily described as shown in the simulation results. Moreover, in (

													<InternalRef
RefID="Equ94">B-17</InternalRef>
) since both terms are multiplied by 

													<Emphasis Type="Italic">μ</Emphasis>



													<Superscript>2</Superscript>
 which is usually small, they only affect the behavior at the steady state where the approximation is more likely to be valid. By doing so, the last two terms in (

													<InternalRef
RefID="Equ94">B-17</InternalRef>
) will be tightly bounded by 

													<InlineEquation ID="IEq388">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq388.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\mu^2}{S_\psi }\left( {\sigma_e^2} \right)\sigma_e^2(n)U\Lambda I\prime \left( \Lambda \right){U^T} $$</EquationSource>


													</InlineEquation>
.
												</Para>



												<Para>Finally, we note that for the LMS algorithm, the normalization term in (

													<InternalRef
RefID="Equ78">B-2</InternalRef>
) is missing and 

													<Emphasis
Type="BoldItalic">L</Emphasis>



													<Subscript>3</Subscript>
 is obtained by replacing 

													<Emphasis
Type="BoldItalic">B</Emphasis>
 above by 

													<Emphasis
Type="BoldItalic">R</Emphasis>



													<Subscript>


														<Emphasis
Type="Italic">XX</Emphasis>


													</Subscript>
.
												</Para>


											</Section2>



											<Section2 ID="Sec17">


												<Heading>Appendix C: M-Estimate Functions (Nonlinearities) and Properties</Heading>



												<Para>In addition to the modified Huber M-estimate function, the bi-weight family of functions and the Hampel’s three parts redescending function are also commonly used in robust statistics [

													<CitationRef
CitationID="CR40">40</CitationRef>
]. The score functions of these M-estimate functions can be written as

													<Equation ID="Equ95">


														<EquationNumber>C-1</EquationNumber>



														<MediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ95.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</MediaObject>



														<EquationSource
Format="TEX">$$ \rho_\xi^\prime (e) = {\psi_\xi }(e) = eq\left( {e/\xi } \right), $$</EquationSource>


													</Equation>
where 

													<InlineEquation ID="IEq389">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq389.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ q\left( {e/\xi } \right) $$</EquationSource>


													</InlineEquation>
 is the score function and 

													<Emphasis Type="Italic">ξ</Emphasis>
 is a threshold for controlling the tradeoff between degree of outlier suppression. We call those nonlinearities satisfying (C-1) the M-nonlinearity. As an M-estimate score function, 

													<InlineEquation ID="IEq390">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq390.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ {\psi_\xi }(e) $$</EquationSource>


													</InlineEquation>
 should be equal to 

													<Emphasis Type="Italic">e</Emphasis>
 when 

													<InlineEquation ID="IEq391">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq391.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| e \right| $$</EquationSource>


													</InlineEquation>
 is much smaller than 

													<Emphasis Type="Italic">ξ</Emphasis>
 and gradually decrease in magnitude as 

													<InlineEquation ID="IEq392">


														<InlineMediaObject>


															<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq392.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


														</InlineMediaObject>



														<EquationSource
Format="TEX">$$ \left| e \right| $$</EquationSource>


													</InlineEquation>
 increases, i.e. 

													<Emphasis Type="Italic">q</Emphasis>
(0) = 1. For redescending score function, 

													<Emphasis Type="Italic">ψ</Emphasis>
(∞) = 0 so that outliers with very large amplitude are completely ignored. The score functions for the bi-weight family of functions, the Hampel’s function and the MH nonlinearity are all redescending. More specifically:
												</Para>



												<Section3 ID="Sec18">


													<Heading>Bi-Weight Family of Functions</Heading>



													<Para>


														<Equation ID="Equ96">


															<EquationNumber>C-2</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ96.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\rho_{\text{BW}}}(e) = \left\{ {\begin{array}{*{20}{c}} {\left( {{\xi^2}/6} \right)\left\{ {1 - {{\left[ {1 - {{\left( {e/\xi } \right)}^2}} \right]}^3}} \right\},} &amp; {{\text{if}} \left| e \right| \leqslant \xi } \\ {1.} &amp; {{\text{if}} \left| e \right| &gt; \xi } \\ \end{array} } \right. $$</EquationSource>


														</Equation>



														<InlineEquation ID="IEq393">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq393.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \rho_{\text{BW}}^\prime (e) = {\psi_{\text{BW}}}(e) = e{\left[ {1 - {{\left( {e/\xi } \right)}^2}} \right]^2} $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq394">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq394.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ I\left( {\left| e \right| \leqslant \xi } \right) $$</EquationSource>


														</InlineEquation>
 and 0 elsewhere where 

														<Emphasis
Type="Italic">I</Emphasis>
(∙) is the indicator function.
													</Para>


												</Section3>



												<Section3 ID="Sec19">


													<Heading>Hampel’s Function</Heading>



													<Para>


														<Equation ID="Equ97">


															<EquationNumber>C-3</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ97.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\psi_{\text{HP}}}(e) = e{q_{\text{HP}}}\left( {e/\xi } \right), $$</EquationSource>


														</Equation>



														<Equation ID="Equk">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equk.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {q_{\text{HP}}}(e) = \left\{ {\begin{array}{*{20}{c}} {1,} &amp; {0 \leqslant \left| e \right|&lt;1} \\ {{\rm sgn} (e)/e,} &amp; {1 /1 \leqslant \left| e \right|&lt;{\xi_1}/\xi } \\ {{\rm sgn} (e)\left( {{\rm sgn} (e) - {\xi_2}} \right)/e\left( {{\xi_1} - {\xi_2}} \right),} &amp; {{\xi_1}/\xi \leqslant \left| e \right|&lt;{\xi_2}/\xi } \\ {0.} &amp; {\xi _2/\xi \leqslant \left| e \right|} \\ \end{array} } \right. $$</EquationSource>


														</Equation>


													</Para>



													<Para>The score function of the EF nonlinearity and the Huber function also satisfy (

														<InternalRef
RefID="Equ95">C-1</InternalRef>
), but they are not redescending. Therefore, they will be sensitive to outliers with large amplitude. More specifically, the weighting functions of the EF nonlinearity is:

														<Equation ID="Equl">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equl.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\psi_{\text{EF}}}(e) = e{q_{\text{EF}}}\left( {e/\xi } \right), $$</EquationSource>


														</Equation>
where 

														<InlineEquation ID="IEq395">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq395.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {q_{\text{EF}}}(e) = \frac{1}{e}\int_0^e {\exp \left( { - {u^2}/2} \right)} du $$</EquationSource>


														</InlineEquation>
. Note, for the sake of presentation we have used 

														<Emphasis
Type="Italic">ξ</Emphasis>
 instead of σ

														<Subscript>


															<Emphasis
Type="Italic">y</Emphasis>


														</Subscript>
 as shown in Table 

														<InternalRef
RefID="Tab1">1</InternalRef>
. This is also true for the quantizer function in which

														<Equation ID="Equm">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equm.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {\psi_{\text{Q}}}\left( {\mu e} \right) = \left( {\mu e} \right){q_{\text{Q}}}\left( {\mu e/\Delta } \right), $$</EquationSource>


														</Equation>
where 

														<InlineEquation ID="IEq396">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq396.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {q_{\text{Q}}}\left( {\mu e} \right) = \left\{ {\begin{array}{*{20}{c}} {0,} &amp; {\left| {\mu e} \right|&lt;1/2} \\ {\left( {1/\mu e} \right){\rm sgn} \left( {\mu e} \right),} &amp; {1/2 \leqslant \left| {\mu e} \right|&lt;1} \\ {1.} &amp; {\text{otherwise}} \\ \end{array} } \right. $$</EquationSource>


														</InlineEquation>


													</Para>



													<Para>We now show that 

														<InlineEquation ID="IEq397">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq397.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq398">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq398.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right)/\sigma_e^2 $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq399">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq399.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 are approximately independent of 

														<InlineEquation ID="IEq400">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq400.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
, if the score function has the form in (

														<InternalRef
RefID="Equ95">C-1</InternalRef>
) and the ATS is employed. First of all, we note from (

														<InternalRef
RefID="Equ95">C-1</InternalRef>
) that

														<Equation ID="Equ98">


															<EquationNumber>C-4</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ98.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ \psi_\xi^\prime (e) = q\left( {e/\xi } \right) + \left( {e/\xi } \right)q\prime \left( {e/\xi } \right). $$</EquationSource>


														</Equation>


													</Para>



													<Para>Using change of variable 

														<InlineEquation ID="IEq401">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq401.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ u = e/{\sigma_e} $$</EquationSource>


														</InlineEquation>
, one gets

														<Equation ID="Equ99">


															<EquationNumber>C-5</EquationNumber>



															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equ99.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ E\left[ {\psi_\xi^\prime (e)} \right] = \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {\left[ {q\left( {\frac{{u{\sigma_e}}}{\xi }} \right) + \left( {\frac{e}{\xi }} \right)q\prime \left( {\frac{{u{\sigma_e}}}{\xi }} \right)} \right]\exp \left( { - \frac{{{u^2}}}{2}} \right)du} . $$</EquationSource>


														</Equation>


													</Para>



													<Para>If ATS is employed, then 

														<InlineEquation ID="IEq402">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq402.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \xi = {k_\xi }{\widehat\sigma_e} $$</EquationSource>


														</InlineEquation>
. Assuming that 

														<InlineEquation ID="IEq403">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq403.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\widehat\sigma_e} \approx \kappa {\sigma_e} $$</EquationSource>


														</InlineEquation>
, i.e. the estimate “impulse free” noise standard deviation is 

														<Emphasis
Type="Italic">κ</Emphasis>
 times of 

														<Emphasis
Type="Italic">σ</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">e</Emphasis>


														</Subscript>
, then (

														<InternalRef
RefID="Equ99">C-5</InternalRef>
) becomes

														<Equation ID="Equn">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equn.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ E\left[ {\psi_\xi^\prime (e)} \right] = {A_\psi }\left( {\sigma_e^2} \right) \approx \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {\left( {q\left( {\frac{u}{{\kappa {k_\xi }}}} \right) + \left( {\frac{e}{\xi }} \right)q\prime \left( {\frac{u}{{\kappa {k_\xi }}}} \right)} \right)\exp \left( { - \frac{{{u^2}}}{2}} \right)du}, $$</EquationSource>


														</Equation>
which is almost independent of 

														<InlineEquation ID="IEq404">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq404.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
. For normal operation without outliers, 

														<Emphasis
Type="Italic">κ</Emphasis>
 ≈ 1. In the presence of impulses, the conditioned error variance 

														<Emphasis
Type="Italic">σ</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">e</Emphasis>


														</Subscript>
 will be much larger than the “impulse free” error variance and 

														<Emphasis
Type="Italic">κ</Emphasis>
 &gt;&gt; 1. Consequently, 

														<InlineEquation ID="IEq405">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq405.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 will be nearly zero. Similarly, we have

														<Equation ID="Equo">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equo.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{B_\psi }\left( {\sigma_e^2} \right) = \frac{1}{{\sqrt {2\pi } {\sigma_e}}}\int_{ - \infty }^\infty {{e^2}{q^2}\left( {e/\xi } \right)\exp \left( { - \frac{{{e^2}}}{{2\sigma_e^2}}} \right)du{\sigma_e}} } \\ { = \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {{u^2}\sigma_e^2{q^2}\left( {u{\sigma_e}/\xi } \right)\exp \left( { - \frac{{{u^2}}}{2}} \right)du} .} \\ \end{array} $$</EquationSource>


														</Equation>


													</Para>



													<Para>Using ATS and assuming 

														<InlineEquation ID="IEq406">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq406.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\widehat\sigma_e} \approx \kappa {\sigma_e} $$</EquationSource>


														</InlineEquation>
 yields 

														<InlineEquation ID="IEq407">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq407.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right) = \sigma_e^2{S_\psi }\left( {\sigma_e^2} \right) \approx \sigma_e^2{S_\psi }\left( \kappa \right) $$</EquationSource>


														</InlineEquation>
, where 

														<InlineEquation ID="IEq408">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq408.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi }\left( \kappa \right) = \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {{u^2}{q^2}\left( {\frac{u}{{{k_\xi }\kappa }}} \right)\exp \left( { - \frac{{{u^2}}}{2}} \right)du} $$</EquationSource>


														</InlineEquation>
 is independent of σ

														<Subscript>


															<Emphasis
Type="Italic">e</Emphasis>


														</Subscript>
. Note, 

														<InlineEquation ID="IEq409">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq409.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {B_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
 is proportional to the error power 

														<InlineEquation ID="IEq410">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq410.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
. Finally,

														<Equation ID="Equp">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equp.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ \begin{array}{*{20}{c}} {{C_\psi }\left( {\sigma_e^2} \right) \approx \frac{d}{{d\sigma_e^2}}E\left[ {{\psi^2}(e)} \right] = \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {{u^2}{q^2}\left( {\frac{{u{\sigma_e}}}{\xi }} \right)\exp \left( { - \frac{{{u^2}}}{2}} \right)du} } \\ { + \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {\left( {\frac{{{u^3}{\sigma_e}}}{\xi }} \right)q\left( {\frac{{u{\sigma_e}}}{\xi }} \right)q\prime \left( {\frac{{u{\sigma_e}}}{\xi }} \right)\exp \left( { - \frac{{{u^2}}}{2}} \right)du.} } \\ \end{array} $$</EquationSource>


														</Equation>


													</Para>



													<Para>If ATS is employed and assuming that 

														<InlineEquation ID="IEq411">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq411.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {\widehat\sigma_e} \approx \kappa {\sigma_e} $$</EquationSource>


														</InlineEquation>
, then

														<Equation ID="Equq">


															<MediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_Equq.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</MediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) \approx {S_\psi }\left( \kappa \right) + \frac{1}{{\sqrt {2\pi } }}\int_{ - \infty }^\infty {\left( {\frac{{{u^3}}}{{\kappa {k_\xi }}}} \right)q\left( {\frac{u}{{\kappa {k_\xi }}}} \right)q\prime \left( {\frac{u}{{\kappa {k_\xi }}}} \right)\exp \left( {\frac{{{u^2}}}{2}} \right)du}, $$</EquationSource>


														</Equation>


													</Para>



													<Para>which is again independent of 

														<Emphasis
Type="Italic">σ</Emphasis>



														<Subscript>


															<Emphasis
Type="Italic">e</Emphasis>


														</Subscript>
. In summary, if ATS is employed, 

														<InlineEquation ID="IEq412">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq412.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {A_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
, 

														<InlineEquation ID="IEq413">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq413.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {C_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
, and 

														<InlineEquation ID="IEq414">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq414.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ {S_\psi }\left( {\sigma_e^2} \right) $$</EquationSource>


														</InlineEquation>
depend very weakly on 

														<InlineEquation ID="IEq415">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq415.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 and they depend on the threshold parameters 

														<Emphasis
Type="Italic">k</Emphasis>
 and the ratio between 

														<InlineEquation ID="IEq416">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq416.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \sigma_e^2 $$</EquationSource>


														</InlineEquation>
 and the “impulse free” noise variance 

														<InlineEquation ID="IEq417">


															<InlineMediaObject>


																<ImageObject
Color="BlackWhite" FileRef="11265_2009_405_Article_IEq417.gif"
Format="GIF" Rendition="HTML" Type="Linedraw" />


															</InlineMediaObject>



															<EquationSource
Format="TEX">$$ \hat\sigma_e^2 $$</EquationSource>


														</InlineEquation>
, 

														<Emphasis
Type="Italic">κ</Emphasis>



														<Superscript>2</Superscript>
. For normal operation without outliers, 

														<Emphasis
Type="Italic">κ</Emphasis>
 ≈ 1. In case of impulsive outliers, κ

														<Superscript>2</Superscript>
 will be much larger than one, and their values will be very small.
													</Para>


												</Section3>


											</Section2>


										</Section1>


									</Appendix>



									<Bibliography ID="Bib1">


										<Heading>References</Heading>



										<Citation ID="CR1">


											<CitationNumber>1.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>B</Initials>



													<FamilyName>Widrow</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>J</Initials>



													<FamilyName>McCool</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>MG</Initials>



													<FamilyName>Larimore</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>CR</Initials>



													<FamilyName>Johnson</FamilyName>



													<Suffix>Jr</Suffix>


												</BibAuthorName>



												<Year>1976</Year>



												<ArticleTitle
Language="En">Stationary and nonstationary learning characteristics of the LMS adaptive filter</ArticleTitle>



												<JournalTitle>Proceedings of the IEEE</JournalTitle>



												<VolumeID>64</VolumeID>



												<FirstPage>1151</FirstPage>



												<LastPage>1162</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/PROC.1976.10286</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>421814</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Widrow, B., McCool, J., Larimore, M. G., &amp; Johnson, C. R., Jr. (1976). Stationary and nonstationary learning characteristics of the LMS adaptive filter. 

												<Emphasis
Type="Italic">Proceedings of the IEEE, 64</Emphasis>
, 1151–1162.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR2">


											<CitationNumber>2.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>RL</Initials>



													<FamilyName>Plackett</FamilyName>


												</BibAuthorName>



												<Year>1972</Year>



												<ArticleTitle
Language="En">The discovery of the method of least-squares</ArticleTitle>



												<JournalTitle>Biometrika</JournalTitle>



												<VolumeID>59</VolumeID>



												<IssueID>2</IssueID>



												<FirstPage>239</FirstPage>



												<LastPage>251</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0274.01024</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>326871</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Plackett, R. L. (1972). The discovery of the method of least-squares. 

												<Emphasis
Type="Italic">Biometrika, 59</Emphasis>
(2), 239–251.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR3">


											<CitationNumber>3.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>JI</Initials>



													<FamilyName>Nagumo</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>A</Initials>



													<FamilyName>Noda</FamilyName>


												</BibAuthorName>



												<Year>1967</Year>



												<ArticleTitle
Language="En">A learning method for system identification</ArticleTitle>



												<JournalTitle>IEEE Transactions on Automatic Control</JournalTitle>



												<VolumeID>AC-12</VolumeID>



												<FirstPage>282</FirstPage>



												<LastPage>287</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TAC.1967.1098599</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Nagumo, J. I., &amp; Noda, A. (1967). A learning method for system identification. 

												<Emphasis
Type="Italic">IEEE Transactions on Automatic Control, AC-12</Emphasis>
, 282–287.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR4">


											<CitationNumber>4.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>RR</Initials>



													<FamilyName>Bitmead</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>BDO</Initials>



													<FamilyName>Anderson</FamilyName>


												</BibAuthorName>



												<Year>1980</Year>



												<ArticleTitle
Language="En">Performance of adaptive estimation algorithms in dependent random environments</ArticleTitle>



												<JournalTitle>IEEE Transactions on Automatic Control</JournalTitle>



												<VolumeID>25</VolumeID>



												<FirstPage>788</FirstPage>



												<LastPage>794</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0459.93050</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.1109/TAC.1980.1102433</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>583456</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Bitmead, R. R., &amp; Anderson, B. D. O. (1980). Performance of adaptive estimation algorithms in dependent random environments. 

												<Emphasis
Type="Italic">IEEE Transactions on Automatic Control, 25</Emphasis>
, 788–794.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR5">


											<CitationNumber>5.</CitationNumber>



											<BibBook>


												<BibAuthorName>


													<Initials>S</Initials>



													<FamilyName>Haykin</FamilyName>


												</BibAuthorName>



												<Year>2001</Year>



												<BookTitle>Adaptive filter theory</BookTitle>



												<EditionNumber>4</EditionNumber>



												<PublisherName>Prentice-Hall</PublisherName>



												<PublisherLocation>Englewood Cliffs</PublisherLocation>


											</BibBook>



											<BibUnstructured>Haykin, S. (2001). 

												<Emphasis
Type="Italic">Adaptive filter theory</Emphasis>
 (4th ed.). Englewood Cliffs: Prentice-Hall.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR6">


											<CitationNumber>6.</CitationNumber>



											<BibUnstructured>Zou, Y. (2000). Robust statistical based adaptive filtering algorithms for impulsive noise suppression. Hong Kong: Ph.D. Dissertation, The Univ. Hong Kong.</BibUnstructured>


										</Citation>



										<Citation ID="CR7">


											<CitationNumber>7.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>Y</Initials>



													<FamilyName>Zou</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>SC</Initials>



													<FamilyName>Chan</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>TS</Initials>



													<FamilyName>Ng</FamilyName>


												</BibAuthorName>



												<Year>2000</Year>



												<ArticleTitle
Language="En">Least mean M-estimate algorithms for robust adaptive filtering in impulsive noise</ArticleTitle>



												<JournalTitle>IEEE Transactions on Circuits and Systems II</JournalTitle>



												<VolumeID>47</VolumeID>



												<IssueID>12</IssueID>



												<FirstPage>1564</FirstPage>



												<LastPage>1569</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/82.899657</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Zou, Y., Chan, S. C., &amp; Ng, T. S. (2000). Least mean M-estimate algorithms for robust adaptive filtering in impulsive noise. 

												<Emphasis
Type="Italic">IEEE Transactions on Circuits and Systems II, 47</Emphasis>
(12), 1564–1569.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR8">


											<CitationNumber>8.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>TI</Initials>



													<FamilyName>Haweel</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>PM</Initials>



													<FamilyName>Clarkson</FamilyName>


												</BibAuthorName>



												<Year>1992</Year>



												<ArticleTitle
Language="En">A class of order statistic LMS algorithms</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>40</VolumeID>



												<IssueID>1</IssueID>



												<FirstPage>44</FirstPage>



												<LastPage>53</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.157180</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Haweel, T. I., &amp; Clarkson, P. M. (1992). A class of order statistic LMS algorithms. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 40</Emphasis>
(1), 44–53.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR9">


											<CitationNumber>9.</CitationNumber>



											<BibUnstructured>Settineri, R., Najim, M., &amp; Ottaviani, D. (1996). Order statistic fast Kalman filter. In 

												<Emphasis
Type="Italic">Proceedings of IEEE International Symposium on Circuits and Systems</Emphasis>
 (Vol. 2, pp. 116–119).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR10">


											<CitationNumber>10.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>S</Initials>



													<FamilyName>Koike</FamilyName>


												</BibAuthorName>



												<Year>1997</Year>



												<ArticleTitle
Language="En">Adaptive threshold nonlinear algorithm for adaptive filters with robustness against impulsive noise</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>45</VolumeID>



												<IssueID>9</IssueID>



												<FirstPage>2391</FirstPage>



												<LastPage>2395</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.622963</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Koike, S. (1997). Adaptive threshold nonlinear algorithm for adaptive filters with robustness against impulsive noise. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 45</Emphasis>
(9), 2391–2395.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR11">


											<CitationNumber>11.</CitationNumber>



											<BibUnstructured>Weng, J. F., &amp; Leung, S. H. (1997). Adaptive nonlinear RLS algorithm for robust filtering in impulsive noise. In 

												<Emphasis
Type="Italic">Proceedings of IEEE International Symposium on Circuits and Systems</Emphasis>
 (Vol. 4, pp. 2337–2340).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR12">


											<CitationNumber>12.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>SC</Initials>



													<FamilyName>Chan</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>Y</Initials>



													<FamilyName>Zou</FamilyName>


												</BibAuthorName>



												<Year>2004</Year>



												<ArticleTitle
Language="En">A recursive least M-estimate algorithm for robust adaptive filtering in impulsive noise: fast algorithm and convergence performance analysis</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>52</VolumeID>



												<IssueID>4</IssueID>



												<FirstPage>975</FirstPage>



												<LastPage>991</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TSP.2004.823496</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>2055409</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Chan, S. C., &amp; Zou, Y. (2004). A recursive least M-estimate algorithm for robust adaptive filtering in impulsive noise: fast algorithm and convergence performance analysis. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 52</Emphasis>
(4), 975–991.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR13">


											<CitationNumber>13.</CitationNumber>



											<BibBook>


												<BibAuthorName>


													<Initials>PJ</Initials>



													<FamilyName>Huber</FamilyName>


												</BibAuthorName>



												<Year>1981</Year>



												<BookTitle>Robust statistics</BookTitle>



												<PublisherName>Wiley</PublisherName>



												<PublisherLocation>New York</PublisherLocation>



												<Occurrence Type="ZLBID">


													<Handle>0536.62025</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.1002/0471725250</Handle>


												</Occurrence>


											</BibBook>



											<BibUnstructured>Huber, P. J. (1981). 

												<Emphasis
Type="Italic">Robust statistics</Emphasis>
. New York: Wiley.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR14">


											<CitationNumber>14.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>LL</Initials>



													<FamilyName>Horowitz</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>KD</Initials>



													<FamilyName>Senne</FamilyName>


												</BibAuthorName>



												<Year>1981</Year>



												<ArticleTitle
Language="En">Performance advantage of complex LMS for controlling narrowband adaptive arrays</ArticleTitle>



												<JournalTitle>IEEE Transactions on Acoustics, Speech, and Signal Processing</JournalTitle>



												<VolumeID>29</VolumeID>



												<IssueID>3</IssueID>



												<FirstPage>722</FirstPage>



												<LastPage>736</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TASSP.1981.1163602</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Horowitz, L. L., &amp; Senne, K. D. (1981). Performance advantage of complex LMS for controlling narrowband adaptive arrays. 

												<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing, 29</Emphasis>
(3), 722–736.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR15">


											<CitationNumber>15.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>A</Initials>



													<FamilyName>Feuer</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>E</Initials>



													<FamilyName>Weinstein</FamilyName>


												</BibAuthorName>



												<Year>1985</Year>



												<ArticleTitle
Language="En">Convergence analysis of LMS filters with uncorrelated Gaussian data</ArticleTitle>



												<JournalTitle>IEEE Transactions on Acoustics, Speech, and Signal Processing</JournalTitle>



												<VolumeID>33</VolumeID>



												<IssueID>1</IssueID>



												<FirstPage>222</FirstPage>



												<LastPage>230</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TASSP.1985.1164493</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Feuer, A., &amp; Weinstein, E. (1985). Convergence analysis of LMS filters with uncorrelated Gaussian data. 

												<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing, 33</Emphasis>
(1), 222–230.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR16">


											<CitationNumber>16.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>JB</Initials>



													<FamilyName>Foley</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>FM</Initials>



													<FamilyName>Boland</FamilyName>


												</BibAuthorName>



												<Year>1988</Year>



												<ArticleTitle
Language="En">A note on the convergence analysis of LMS adaptive filters with Gaussian data</ArticleTitle>



												<JournalTitle>IEEE Transactions on Acoustics, Speech, and Signal Processing</JournalTitle>



												<VolumeID>36</VolumeID>



												<IssueID>7</IssueID>



												<FirstPage>1087</FirstPage>



												<LastPage>1089</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0653.93056</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.1109/29.1632</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Foley, J. B., &amp; Boland, F. M. (1988). A note on the convergence analysis of LMS adaptive filters with Gaussian data. 

												<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing, 36</Emphasis>
(7), 1087–1089.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR17">


											<CitationNumber>17.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>NJ</Initials>



													<FamilyName>Bershad</FamilyName>


												</BibAuthorName>



												<Year>1986</Year>



												<ArticleTitle
Language="En">Analysis of the normalized LMS algorithm with Gaussian inputs</ArticleTitle>



												<JournalTitle>IEEE Transactions on Acoustics, Speech, and Signal Processing</JournalTitle>



												<VolumeID>ASSP-34</VolumeID>



												<IssueID>4</IssueID>



												<FirstPage>793</FirstPage>



												<LastPage>806</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TASSP.1986.1164914</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Bershad, N. J. (1986). Analysis of the normalized LMS algorithm with Gaussian inputs. 

												<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-34</Emphasis>
(4), 793–806.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR18">


											<CitationNumber>18.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>NJ</Initials>



													<FamilyName>Bershad</FamilyName>


												</BibAuthorName>



												<Year>1987</Year>



												<ArticleTitle
Language="En">Behavior of the ε-normalized LMS algorithm with Gaussian inputs</ArticleTitle>



												<JournalTitle>IEEE Transactions on Acoustics, Speech, and Signal Processing</JournalTitle>



												<VolumeID>ASSP-35</VolumeID>



												<IssueID>5</IssueID>



												<FirstPage>636</FirstPage>



												<LastPage>644</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TASSP.1987.1165197</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Bershad, N. J. (1987). Behavior of the ε-normalized LMS algorithm with Gaussian inputs. 

												<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-35</Emphasis>
(5), 636–644.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR19">


											<CitationNumber>19.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>M</Initials>



													<FamilyName>Tarrab</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>A</Initials>



													<FamilyName>Feuer</FamilyName>


												</BibAuthorName>



												<Year>1988</Year>



												<ArticleTitle
Language="En">Convergence and performance analysis of the normalized LMS algorithm with uncorrelated Gaussian data</ArticleTitle>



												<JournalTitle>IEEE Transactions on Information Theory</JournalTitle>



												<VolumeID>IT-34</VolumeID>



												<IssueID>4</IssueID>



												<FirstPage>680</FirstPage>



												<LastPage>691</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/18.9768</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>966740</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Tarrab, M., &amp; Feuer, A. (1988). Convergence and performance analysis of the normalized LMS algorithm with uncorrelated Gaussian data. 

												<Emphasis
Type="Italic">IEEE Transactions on Information Theory, IT-34</Emphasis>
(4), 680–691.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR20">


											<CitationNumber>20.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>M</Initials>



													<FamilyName>Rupp</FamilyName>


												</BibAuthorName>



												<Year>1993</Year>



												<ArticleTitle
Language="En">The behavior of LMS and NLMS algorithms in the presence of spherically invariant processes</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>41</VolumeID>



												<IssueID>3</IssueID>



												<FirstPage>1149</FirstPage>



												<LastPage>1160</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0775.93314</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.205720</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Rupp, M. (1993). The behavior of LMS and NLMS algorithms in the presence of spherically invariant processes. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 41</Emphasis>
(3), 1149–1160.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR21">


											<CitationNumber>21.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>DTM</Initials>



													<FamilyName>Slock</FamilyName>


												</BibAuthorName>



												<Year>1993</Year>



												<ArticleTitle
Language="En">On the convergence behavior of the LMS and the normalized LMS algorithms</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>41</VolumeID>



												<IssueID>9</IssueID>



												<FirstPage>2811</FirstPage>



												<LastPage>2825</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0800.94093</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.236504</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Slock, D. T. M. (1993). On the convergence behavior of the LMS and the normalized LMS algorithms. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 41</Emphasis>
(9), 2811–2825.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR22">


											<CitationNumber>22.</CitationNumber>



											<BibUnstructured>Costal, M. H., &amp; Bermudez, J. C. M. (2002). An improved model for the normalized LMS algorithm with Gaussian inputs and large number of coefficients. In 

												<Emphasis
Type="Italic">Proceedings of IEEE ICASSP’2002</Emphasis>
 (Vol. 2, pp. 1385–1388).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR23">


											<CitationNumber>23.</CitationNumber>



											<BibUnstructured>Barrault, G., Costa, M. H., J. Bermudez, C. M. &amp; Lenzi, A. (2005). A new analytical model for the NLMS algorithm. In 

												<Emphasis
Type="Italic">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing</Emphasis>
 (Vol. 4, pp. 41–44).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR24">


											<CitationNumber>24.</CitationNumber>



											<BibUnstructured>Lobato, E. M., Tobias, O. J., &amp; Seara, R. (2006). Stochastic model for the NLMS algorithm with correlated Gaussian data. In 

												<Emphasis
Type="Italic">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing</Emphasis>
 (Vol. 3, pp. 760–763).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR25">


											<CitationNumber>25.</CitationNumber>



											<BibUnstructured>Sayed, A. H., &amp; Rupp, M. (1995). A time-domain feedback analysis of adaptive gradient algorithms via the small gain theorem. In 

												<Emphasis
Type="Italic">Proc. SPIE Conf. Advanced Signal Processing: Algorithms, Architectures, Implementations</Emphasis>
 (Vol. 2563, pp. 458–469). San Diego.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR26">


											<CitationNumber>26.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>NR</Initials>



													<FamilyName>Yousef</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>AH</Initials>



													<FamilyName>Sayed</FamilyName>


												</BibAuthorName>



												<Year>2001</Year>



												<ArticleTitle
Language="En">A unified approach to the steady-state and tracking analyses of adaptive filters</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>49</VolumeID>



												<IssueID>2</IssueID>



												<FirstPage>314</FirstPage>



												<LastPage>324</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.902113</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Yousef, N. R., &amp; Sayed, A. H. (2001). A unified approach to the steady-state and tracking analyses of adaptive filters. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 49</Emphasis>
(2), 314–324.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR27">


											<CitationNumber>27.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>TY</Initials>



													<FamilyName>Al-Naffouri</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>AH</Initials>



													<FamilyName>Sayed</FamilyName>


												</BibAuthorName>



												<Year>2003</Year>



												<ArticleTitle
Language="En">Transient analysis of data-normalized adaptive filters</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>51</VolumeID>



												<IssueID>3</IssueID>



												<FirstPage>639</FirstPage>



												<LastPage>663</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TSP.2002.808106</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Al-Naffouri, T. Y., &amp; Sayed, A. H. (2003). Transient analysis of data-normalized adaptive filters. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 51</Emphasis>
(3), 639–663.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR28">


											<CitationNumber>28.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>S</Initials>



													<FamilyName>Koike</FamilyName>


												</BibAuthorName>



												<Year>2006</Year>



												<ArticleTitle
Language="En">Convergence analysis of adaptive filters using normalized sign-sign algorithm</ArticleTitle>



												<JournalTitle>IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences</JournalTitle>



												<VolumeID>E88-A</VolumeID>



												<IssueID>11</IssueID>



												<FirstPage>3218</FirstPage>



												<LastPage>3224</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1093/ietfec/e88-a.11.3218</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Koike, S. (2006). Convergence analysis of adaptive filters using normalized sign-sign algorithm. 

												<Emphasis
Type="Italic">IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E88-A</Emphasis>
(11), 3218–3224.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR29">


											<CitationNumber>29.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>S</Initials>



													<FamilyName>Koike</FamilyName>


												</BibAuthorName>



												<Year>2006</Year>



												<ArticleTitle
Language="En">Performance analysis of the normalized LMS algorithm for complex-domain adaptive filters in the presence of impulse noise at filter input</ArticleTitle>



												<JournalTitle>IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences</JournalTitle>



												<VolumeID>E89-A</VolumeID>



												<IssueID>9</IssueID>



												<FirstPage>2422</FirstPage>



												<LastPage>2428</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1093/ietfec/e89-a.9.2422</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Koike, S. (2006). Performance analysis of the normalized LMS algorithm for complex-domain adaptive filters in the presence of impulse noise at filter input. 

												<Emphasis
Type="Italic">IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences, E89-A</Emphasis>
(9), 2422–2428.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR30">


											<CitationNumber>30.</CitationNumber>



											<BibUnstructured>Tukey, J. W. (1960). A survey of sampling from contaminated distributions in contributions to probability and statistics. I. Olkin (Ed.), Stanford University Press.</BibUnstructured>


										</Citation>



										<Citation ID="CR31">


											<CitationNumber>31.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>R</Initials>



													<FamilyName>Price</FamilyName>


												</BibAuthorName>



												<Year>1958</Year>



												<ArticleTitle
Language="En">A useful theorem for nonlinear devices having Gaussian inputs</ArticleTitle>



												<JournalTitle>IRE Transactions on Information Theory</JournalTitle>



												<VolumeID>IT-4</VolumeID>



												<FirstPage>69</FirstPage>



												<LastPage>72</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TIT.1958.1057444</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>122618</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Price, R. (1958). A useful theorem for nonlinear devices having Gaussian inputs. 

												<Emphasis
Type="Italic">IRE Transactions on Information Theory, IT-4</Emphasis>
, 69–72.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR32">


											<CitationNumber>32.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>EL</Initials>



													<FamilyName>McMahon</FamilyName>


												</BibAuthorName>



												<Year>1964</Year>



												<ArticleTitle
Language="En">An extension of Price’s theorem</ArticleTitle>



												<JournalTitle>IEEE Transactions on Information Theory</JournalTitle>



												<VolumeID>IT-10</VolumeID>



												<FirstPage>168</FirstPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TIT.1964.1053656</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>McMahon, E. L. (1964). An extension of Price’s theorem. 

												<Emphasis
Type="Italic">IEEE Transactions on Information Theory, IT-10</Emphasis>
, 168.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR33">


											<CitationNumber>33.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>A</Initials>



													<FamilyName>Papoulis</FamilyName>


												</BibAuthorName>



												<Year>1965</Year>



												<ArticleTitle
Language="En">Comment on ‘an extension of Price’s theorem’</ArticleTitle>



												<JournalTitle>IEEE Transactions on Information Theory</JournalTitle>



												<VolumeID>IT-11</VolumeID>



												<FirstPage>154</FirstPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TIT.1965.1053722</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Papoulis, A. (1965). Comment on ‘an extension of Price’s theorem’. 

												<Emphasis
Type="Italic">IEEE Transactions on Information Theory, IT-11</Emphasis>
, 154.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR34">


											<CitationNumber>34.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>VJ</Initials>



													<FamilyName>Mathews</FamilyName>


												</BibAuthorName>



												<Year>1991</Year>



												<ArticleTitle
Language="En">Performance analysis of adaptive filters equipped with the dual sign algorithm</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>39</VolumeID>



												<IssueID>1</IssueID>



												<FirstPage>85</FirstPage>



												<LastPage>91</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0731.93078</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.80768</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Mathews, V. J. (1991). Performance analysis of adaptive filters equipped with the dual sign algorithm. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 39</Emphasis>
(1), 85–91.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR35">


											<CitationNumber>35.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>NJ</Initials>



													<FamilyName>Bershad</FamilyName>


												</BibAuthorName>



												<Year>1988</Year>



												<ArticleTitle
Language="En">On Error-saturation nonlinearities in LMS adaptation</ArticleTitle>



												<JournalTitle>IEEE Transactions on Acoustics, Speech, and Signal Processing</JournalTitle>



												<VolumeID>ASSP-36</VolumeID>



												<IssueID>4</IssueID>



												<FirstPage>440</FirstPage>



												<LastPage>452</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/29.1548</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Bershad, N. J. (1988). On Error-saturation nonlinearities in LMS adaptation. 

												<Emphasis
Type="Italic">IEEE Transactions on Acoustics, Speech, and Signal Processing, ASSP-36</Emphasis>
(4), 440–452.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR36">


											<CitationNumber>36.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>R</Initials>



													<FamilyName>Price</FamilyName>


												</BibAuthorName>



												<Year>1964</Year>



												<ArticleTitle
Language="En">Comment on: ‘a useful theorem for nonlinear devices having Gaussian inputs’</ArticleTitle>



												<JournalTitle>IEEE Transactions on Information Theory</JournalTitle>



												<VolumeID>IT-10</VolumeID>



												<FirstPage>171</FirstPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TIT.1964.1053659</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>172779</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Price, R. (1964). Comment on: ‘a useful theorem for nonlinear devices having Gaussian inputs’. 

												<Emphasis
Type="Italic">IEEE Transactions on Information Theory, IT-10</Emphasis>
, 171.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR37">


											<CitationNumber>37.</CitationNumber>



											<BibUnstructured>Chan, S. C., &amp; Zhou, Y. (2009). Convergence behavior of NLMS algorithm for Gaussian inputs: Solutions using generalized Abelian integral functions and step size selection. To appear in 

												<Emphasis
Type="Italic">Journal of Signal Processing Systems</Emphasis>
, 2010.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR38">


											<CitationNumber>38.</CitationNumber>



											<BibBook>


												<BibAuthorName>


													<Initials>CL</Initials>



													<FamilyName>Siegel</FamilyName>


												</BibAuthorName>



												<Year>1988</Year>



												<BookTitle>Topics in complex function theory, vol. 2: Automorphic functions and Abelian Integrals</BookTitle>



												<PublisherName>Wiley</PublisherName>



												<PublisherLocation>New York</PublisherLocation>


											</BibBook>



											<BibUnstructured>Siegel, C. L. (1988). 

												<Emphasis
Type="Italic">Topics in complex function theory, vol. 2: Automorphic functions and Abelian Integrals</Emphasis>
. New York: Wiley.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR39">


											<CitationNumber>39.</CitationNumber>



											<BibUnstructured>Macleod, M. D. (2005). Robust normalized LMS filtering. In 

												<Emphasis
Type="Italic">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing</Emphasis>
 (Vol. 4, pp. 37–40).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR40">


											<CitationNumber>40.</CitationNumber>



											<BibBook>


												<BibAuthorName>


													<Initials>FR</Initials>



													<FamilyName>Hampel</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>EM</Initials>



													<FamilyName>Ronchetti</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>PJ</Initials>



													<FamilyName>Rousseeuw</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>WA</Initials>



													<FamilyName>Stahel</FamilyName>


												</BibAuthorName>



												<Year>2005</Year>



												<BookTitle>Robust statistics: The approach based on influence functions</BookTitle>



												<PublisherName>Wiley</PublisherName>



												<PublisherLocation>New York</PublisherLocation>


											</BibBook>



											<BibUnstructured>Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., &amp; Stahel, W. A. (2005). 

												<Emphasis
Type="Italic">Robust statistics: The approach based on influence functions</Emphasis>
. New York: Wiley.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR41">


											<CitationNumber>41.</CitationNumber>



											<BibUnstructured>Zhou, Y. (2006). Improved analysis and design of efficient adaptive transversal filtering algorithms with particular emphasis on noise, input and channel modeling. Ph.D. Dissertation, The Univ. Hong Kong, Hong Kong.</BibUnstructured>


										</Citation>



										<Citation ID="CR42">


											<CitationNumber>42.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>SC</Initials>



													<FamilyName>Douglas</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>THY</Initials>



													<FamilyName>Meng</FamilyName>


												</BibAuthorName>



												<Year>1994</Year>



												<ArticleTitle
Language="En">Normalized data nonlinearities for LMS adaptation</ArticleTitle>



												<JournalTitle>IEEE Transactions on Signal Processing</JournalTitle>



												<VolumeID>42</VolumeID>



												<IssueID>6</IssueID>



												<FirstPage>1352</FirstPage>



												<LastPage>1365</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/78.286952</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Douglas, S. C., &amp; Meng, T. H. Y. (1994). Normalized data nonlinearities for LMS adaptation. 

												<Emphasis
Type="Italic">IEEE Transactions on Signal Processing, 42</Emphasis>
(6), 1352–1365.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR43">


											<CitationNumber>43.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>A</Initials>



													<FamilyName>Gersho</FamilyName>


												</BibAuthorName>



												<Year>1984</Year>



												<ArticleTitle
Language="En">Adaptive filtering with binary reinforcement</ArticleTitle>



												<JournalTitle>IEEE Transactions on Information Theory</JournalTitle>



												<VolumeID>IT-30</VolumeID>



												<IssueID>2</IssueID>



												<FirstPage>191</FirstPage>



												<LastPage>199</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1109/TIT.1984.1056890</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>651811</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Gersho, A. (1984). Adaptive filtering with binary reinforcement. 

												<Emphasis
Type="Italic">IEEE Transactions on Information Theory, IT-30</Emphasis>
(2), 191–199.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR44">


											<CitationNumber>44.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>JL</Initials>



													<FamilyName>Doob</FamilyName>


												</BibAuthorName>



												<Year>1948</Year>



												<ArticleTitle
Language="En">Asymptotic properties of Markoff transition probabilities</ArticleTitle>



												<JournalTitle>Transactions of the American Mathematical Society</JournalTitle>



												<VolumeID>63</VolumeID>



												<FirstPage>393</FirstPage>



												<LastPage>421</LastPage>



												<Occurrence Type="ZLBID">


													<Handle>0041.45406</Handle>


												</Occurrence>



												<Occurrence Type="DOI">


													<Handle>10.2307/1990566</Handle>


												</Occurrence>



												<Occurrence Type="AMSID">


													<Handle>25097</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Doob, J. L. (1948). Asymptotic properties of Markoff transition probabilities. 

												<Emphasis
Type="Italic">Transactions of the American Mathematical Society, 63</Emphasis>
, 393–421.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR45">


											<CitationNumber>45.</CitationNumber>



											<BibBook>


												<BibAuthorName>


													<Initials>G</Initials>



													<FamilyName>Recktenwald</FamilyName>


												</BibAuthorName>



												<Year>2000</Year>



												<BookTitle>Numerical methods with MATLAB: Implementations and applications</BookTitle>



												<PublisherName>Prentice-Hall</PublisherName>



												<PublisherLocation>Englewood Cliffs</PublisherLocation>


											</BibBook>



											<BibUnstructured>Recktenwald, G. (2000). 

												<Emphasis
Type="Italic">Numerical methods with MATLAB: Implementations and applications</Emphasis>
. Englewood Cliffs: Prentice-Hall.
											</BibUnstructured>


										</Citation>



										<Citation ID="CR46">


											<CitationNumber>46.</CitationNumber>



											<BibUnstructured>Chan, S. C., &amp; Zhou, Y. (2007). On the convergence analysis of the normalized LMS and the normalized least mean M-estimate algorithms. In 

												<Emphasis
Type="Italic">Proc. IEEE International Symposium on Signal Processing and Information Technology</Emphasis>
 (pp. 1059–1065).
											</BibUnstructured>


										</Citation>



										<Citation ID="CR47">


											<CitationNumber>47.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>OL</Initials>



													<FamilyName>Jageman</FamilyName>


												</BibAuthorName>



												<Year>1975</Year>



												<ArticleTitle
Language="En">Nonstationary blocking in telephone traffic</ArticleTitle>



												<JournalTitle>Bell System Technical Journal</JournalTitle>



												<VolumeID>54</VolumeID>



												<IssueID>3</IssueID>



												<FirstPage>625</FirstPage>



												<LastPage>661</LastPage>



												<Occurrence Type="AMSID">


													<Handle>376131</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Jageman, O. L. (1975). Nonstationary blocking in telephone traffic. 

												<Emphasis
Type="Italic">Bell System Technical Journal, 54</Emphasis>
(3), 625–661.
											</BibUnstructured>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:DateLoaded>2010-10-08T18:44:55.893196+02:00</meta:DateLoaded>

					<meta:Authors>

						<meta:Author>Chan, S. C.</meta:Author>

						<meta:Author>Zhou, Y.</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution>

							<meta:OrgName>The University of Hong Kong</meta:OrgName>

							<meta:Country>Hong Kong</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2010-04-14</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1007/s11265-009-0405-9</meta:DOI>

					<meta:Title>On the Performance Analysis of the Least Mean M-Estimate and Normalized Least Mean M-Estimate Algorithms with Gaussian Inputs and Additive Gaussian and Contaminated Gaussian Noises</meta:Title>

					<meta:ISXN>1939-8115</meta:ISXN>

					<meta:PubName>Springer</meta:PubName>

					<meta:Journal>Journal of Signal Processing Systems</meta:Journal>

					<meta:Publication>Journal of Signal Processing Systems</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Engineering</meta:Subject>

						<meta:Subject
Type="Secondary">Computer Imaging, Vision, Pattern Recognition and Graphics</meta:Subject>

						<meta:Subject
Type="Secondary">Pattern Recognition</meta:Subject>

						<meta:Subject
Type="Secondary">Image Processing and Computer Vision</meta:Subject>

						<meta:Subject
Type="Secondary">Electrical Engineering</meta:Subject>

						<meta:Subject
Type="Secondary">Circuits and Systems</meta:Subject>

						<meta:Subject
Type="Secondary">Signal, Image and Speech Processing</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images />

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1472-6807</JournalID>


						<JournalPrintISSN>1472-6807</JournalPrintISSN>


						<JournalElectronicISSN>1472-6807</JournalElectronicISSN>


						<JournalTitle>BMC Structural Biology</JournalTitle>


						<JournalAbbreviatedTitle>BMC Struct Biol</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Protein Science</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Crystallography</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Mass Spectrometry</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Spectroscopy/Spectrometry</JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Biochemistry, general</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>10</VolumeIDStart>


							<VolumeIDEnd>10</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>Suppl 1</IssueIDStart>


								<IssueIDEnd>Suppl 1</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2010</Year>


										<Month>5</Month>


										<Day>17</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Wu et al; licensee BioMed Central Ltd.</CopyrightHolderName>


									<CopyrightYear>2010</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1472-6807-10-S1-S7</ArticleID>


									<ArticleDOI>10.1186/1472-6807-10-S1-S7</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Research</ArticleCategory>


									<ArticleFirstPage>S7</ArticleFirstPage>


									<ArticleLastPage>S7</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year />


											<Month />


											<Day />


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year />


											<Month />


											<Day />


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Wu et al; licensee BioMed Central Ltd.</CopyrightHolderName>


										<CopyrightYear>2010</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>10</VolumeIDStart>


										<VolumeIDEnd>10</VolumeIDEnd>


										<IssueIDStart>Suppl 1</IssueIDStart>


										<IssueIDEnd>Suppl 1</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Robert</GivenName>


												<FamilyName>Davis</FamilyName>


											</AuthorName>


											<Contact>

												<Email />


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Claus</GivenName>


												<FamilyName>Ernst</FamilyName>


											</AuthorName>


											<Contact>

												<Email>claus.ernst@wku.edu</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Di</GivenName>


												<FamilyName>Wu</FamilyName>


											</AuthorName>


											<Contact>

												<Email>di.wu@wku.edu</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>A protein structure can be determined by solving a so-called distance geometry problem whenever a set of inter-atomic distances is available and sufficient. However, the problem is intractable in general and has proved to be a NP hard problem. An updated geometric build-up algorithm (UGB) has been developed recently that controls numerical errors and is efficient in protein structure determination for cases where only sparse exact distance data is available. In this paper, the UGB method has been improved and revised with aims at solving distance geometry problems more efficiently and effectively.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Methods</Heading>


											<Para>An efficient algorithm (called the revised updated geometric build-up algorithm (RUGB)) to build up a protein structure from atomic distance data is presented and provides an effective way of determining a protein structure with sparse exact distance data. In the algorithm, the condition to determine an unpositioned atom iteratively is relaxed (when compared with the UGB algorithm) and data structure techniques are used to make the algorithm more efficient and effective. The algorithm is tested on a set of proteins selected randomly from the Protein Structure Database-PDB.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>We test a set of proteins selected randomly from the Protein Structure Database-PDB. We show that the numerical errors produced by the new RUGB algorithm are smaller when compared with the errors of the UGB algorithm and that the novel RUGB algorithm has a significantly smaller runtime than the UGB algorithm.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusions</Heading>


											<Para>The RUGB algorithm relaxes the condition for updating and incorporates the data structure for accessing neighbours of an atom. The revisions result in an improvement over the UGB algorithm in two important areas: a reduction on the overall runtime and decrease of the numeric error.</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Heading>Keywords</Heading>


										<Keyword>build-up</Keyword>


										<Keyword>structure</Keyword>


										<Keyword>via</Keyword>


										<Keyword>determination</Keyword>


										<Keyword>geometric</Keyword>


										<Keyword>efficient</Keyword>


										<Keyword>algorithm</Keyword>


										<Keyword>Protein</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_74553">

										<Heading>Introduction</Heading>


										<Para>Proteins are important bio-molecules in biological systems and activities. A protein is a polypeptide chain made of 20 different types of amino acids. An amino acid sequence determines the structure of the protein. Knowledge of the protein structure gives us insight into function of the protein and its dynamics. Therefore, it is always important to have an accurate protein structure in the highest resolution available. The distances between many pairs of atoms in a protein can often be determined based on our knowledge of chemistry (for example certain types of bond-lengths and bond angles) 

											<CitationRef
CitationID="B1">1</CitationRef>
 , or from nuclear magnetic resonance (NMR) experiments 

											<CitationRef
CitationID="B2">2</CitationRef>
 . If a sufficiently large set of inter-atomic distances can be obtained, then a protein structure can be determined by solving a so-called molecular distance geometry problem (MDGP) 

											<CitationRef
CitationID="B3">3</CitationRef>
 . MDGPs in their most general form are known to be computationally intractable (NP-hard) 

											<CitationRef
CitationID="B4">4</CitationRef>
 .
										</Para>


										<Para>In an experimental setting there are two additional restrictions: First, often only a small subset of all pair-wise distances may be available. Second, instead of a single distance, experiments might only yield a distance range for a pair of atoms (a lower bound and upper bound of a distance). Several algorithms have been developed as solutions or approximate solutions to MDGP. These algorithms include singular value decomposition 

											<CitationRef
CitationID="B3">3</CitationRef>
 , the embedding algorithm 

											<CitationRef
CitationID="B3">3</CitationRef>
 , the alternative project algorithm 

											<CitationRef
CitationID="B5">5</CitationRef>
 , the graph reduction algorithm 

											<CitationRef
CitationID="B6">6</CitationRef>
 , the multi-scaling algorithm 

											<CitationRef
CitationID="B7">7</CitationRef>
 , and the global optimization algorithm 

											<CitationRef
CitationID="B8">8</CitationRef>


											<CitationRef
CitationID="B9">9</CitationRef>
 . Many of these algorithms are computationally expensive, in particular if they attempt to solve the MDGP in a general form.
										</Para>


										<Para>In this paper we will only consider the MDGP in the case when exact distances are available. Furthermore we concentrate on a particular class of algorithms that are computationally quite fast and will often suffice to solve the MDGP problem. These are so called geometric build-up algorithms (GB) 

											<CitationRef
CitationID="B10">10</CitationRef>
 . A GB algorithm is based on the idea of iteratively adding one atom at a time to a list of positioned atoms.
										</Para>


										<Para>Here we will refer to a positioned atom as an atom with known coordinates in 3D space and an unpositioned atom as an atom where we do not know its 3D coordinates. It is well-known in geometry that in 3D an unpositioned point 

											<Emphasis Type="Italic">P</Emphasis>
 can be positioned when there exist four positioned non-planar points, each of which has a known distance to 

											<Emphasis Type="Italic">P</Emphasis>
 . It is easy to see that when all pair-wise distances between atoms are available, a set of four such atoms can always be found. Such a set of four atoms used to determine another atom is also called a 

											<Emphasis
Type="Italic">metric base</Emphasis>
 . The algorithm in the case when all distances are known has a linear running time because a metric base is easily found 

											<CitationRef
CitationID="B10">10</CitationRef>
 . However, such an ideal situation will hardly ever arise. Clearly in this case the MDGP is not a hard problem. The algorithm needs to be modified to determine a protein structure when only a sparse set of pair-wise distances is available. In such a case, finding a metric base to add an atom to the list of positioned atoms requires more work. The simplest idea would be to exhaustively search through all possible metric bases until one is found that allows the positioning of an atom. Theoretically, when a sparse distance data has sufficiently many entries a protein structure can be determined.
										</Para>


										<Para>A major problem in the geometric build up procedure is numerical stability when a protein has a large number of atoms. Due to computational round off or truncation, errors are introduced into the build-up coordinates and the iterative nature of the algorithm can cause these errors to accumulate. This problem has been solved by using an updated geometric build-up (UGB) algorithm 

											<CitationRef
CitationID="B11">11</CitationRef>
 . The updating reduces the accumulation of numerical error to a tolerable level. The UGB algorithm can solve the MDGP with high accuracy. The idea of the updating procedure is to re-compute the coordinates of the four atoms in the metric base whenever possible using the original, correct pair-wise distances. Therefore, the fresh coordinates of these four atoms, with a minimal numerical error, can be used to determine the unpositioned atom more accurately. The drawback of the UGB algorithm is the additional computational time it requires to select a metric base carefully and the updating procedure itself.
										</Para>


										<Section2 ID="Sec_40369">

											<Heading>Geometric build-up algorithms</Heading>


											<Para>Initially, four atoms that are not co-planar are selected such that all six inter-atomic distances between each pair of these four atoms are known. A set of coordinates for the four atoms is determined that satisfies the distances between them. We call atoms with fixed coordinates 

												<Emphasis
Type="Italic">positioned</Emphasis>
 atoms. Thus there are initially only four positioned atoms. Next, the GB algorithm increases the number of positioned atoms by determining the coordinates of an unpositioned atom that has four known distances to four distinct non-planar already positioned atoms. As before, we call these four already positioned atoms 

												<Emphasis
Type="Italic">a metric base</Emphasis>
 of the unpositioned atom. Using this procedure repeatedly all the coordinates of the remaining atoms can be determined using a distance from four positioned atoms. The algorithm can solve a MDGP even when only a sparse set of pair wise distances are available. In this case, the metric base of four base atoms may need to be changed frequently in the determination of the remaining atoms. This general geometric build-up algorithm is outlined in the Figure 

												<InternalRef RefID="F1">1</InternalRef>
 and also in reference 

												<CitationRef
CitationID="B12">12</CitationRef>
 . The geometric build-up (GB) algorithm will solve a MDGP when a sufficiently large set of exact pair wise distances is given. It is possible to formulate conditions that grantee a sufficiently large set of distances so that a geometric build-up algorithm will be successful, for details see 

												<CitationRef
CitationID="B13">13</CitationRef>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F1">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>The outline of the General Geometric Build-Up Algorithm for Solving MDGP [Dong and Wu 2002b]</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1472-6807-10-S1-S7-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>The outline of the General Geometric Build-Up Algorithm for Solving MDGP [Dong and Wu 2002b]</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para> 

												<Emphasis
Type="Bold">Definition 1.1</Emphasis>
 A set of points 

												<Emphasis Type="Italic">B</Emphasis>
 (with known coordinates) in a space (usually 

												<Emphasis Type="Italic">R 

													<Superscript>3</Superscript>

												</Emphasis>
 ) is a 

												<Emphasis
Type="Italic">metric basis</Emphasis>
 of a set 

												<Emphasis Type="Italic">S</Emphasis>
 of points provided the coordinates of each point of 

												<Emphasis Type="Italic">S</Emphasis>
 are uniquely determined by its known distances to the points in 

												<Emphasis Type="Italic">B</Emphasis>
 .
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Definition 1.2</Emphasis>
 A set of four points in 

												<Emphasis Type="Italic">R 

													<Superscript>3</Superscript>

												</Emphasis>
 is called 

												<Emphasis
Type="Italic">independent</Emphasis>
 if they are not co-planar.
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Definition 1.3</Emphasis>
 A point 

												<Emphasis Type="Italic">u 

													<Subscript>i</Subscript>

												</Emphasis>
 is called a 

												<Emphasis
Type="Italic">neighbouring point</Emphasis>
 of a point 

												<Emphasis Type="Italic">u 

													<Subscript>j</Subscript>

												</Emphasis>
 if 

												<Emphasis Type="Italic">u 

													<Subscript>i</Subscript>

												</Emphasis>
 has a known distance 

												<Emphasis Type="Italic">d 

													<Subscript>i,j</Subscript>

												</Emphasis>
 from 

												<Emphasis Type="Italic">u 

													<Subscript>j</Subscript>

												</Emphasis>
 .
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem 1.1</Emphasis>
 Given a set of distances among four non-coplanar points, then the coordinates of the four points can be uniquely determined up to a rigid motion that is a combination of a translation, a rotation and possibly a reflection.
											</Para>


											<Para> 

												<Emphasis Type="Bold">Proof.</Emphasis>
 The distances between the four points define a tetrahedron and therefore this is obvious.
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem 1.2</Emphasis>
 If the coordinates of four non-planar atoms 

												<Emphasis Type="Italic">x 

													<Subscript>i</Subscript>
 , i=1,2,3,4
												</Emphasis>
 and the distances 

												<Emphasis Type="Italic">d 

													<Subscript>i,j,</Subscript>
 , i=1,2,3,4
												</Emphasis>
 to a fifth atom 

												<Emphasis Type="Italic">x 

													<Subscript>j</Subscript>

												</Emphasis>
 are given, then the coordinates of the fifth atom 

												<Emphasis Type="Italic">x 

													<Subscript>j</Subscript>

												</Emphasis>
 can be determined uniquely. In other words, any four independent points in 

												<Emphasis Type="Italic">R 

													<Superscript>3</Superscript>

												</Emphasis>
 form a metric basis for 

												<Emphasis Type="Italic">R 

													<Superscript>3</Superscript>

												</Emphasis>
 .
											</Para>


											<Para> 

												<Emphasis Type="Bold">Proof.</Emphasis>
 While this theorem is geometrically obvious, we provide a short proof that will give us insight of how the coordinates of the fifth atom are actually computed. Let 

												<Emphasis Type="Italic">x 

													<Subscript>i</Subscript>
 =
												</Emphasis>
 ( 

												<Emphasis Type="Italic">u 

													<Subscript>i</Subscript>

												</Emphasis>
 , 

												<Emphasis Type="Italic">v 

													<Subscript>i</Subscript>

												</Emphasis>
 , 

												<Emphasis Type="Italic">w 

													<Subscript>i</Subscript>

												</Emphasis>
 ) 

												<Emphasis Type="Italic"> 

													<Superscript>T</Superscript>

												</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 = 1, 2, 3, 4, be the coordinate vectors of the first four atoms and 

												<Emphasis Type="Italic">x 

													<Subscript>j</Subscript>
 =
												</Emphasis>
 ( 

												<Emphasis Type="Italic">u 

													<Subscript>j</Subscript>

												</Emphasis>
 , 

												<Emphasis Type="Italic">v 

													<Subscript>j</Subscript>

												</Emphasis>
 , 

												<Emphasis Type="Italic">w 

													<Subscript>j</Subscript>

												</Emphasis>
 ) 

												<Emphasis Type="Italic"> 

													<Superscript>T</Superscript>

												</Emphasis>
 the coordinate vector of the fifth atom. We then have a set of equations,
											</Para>


											<Para>Square the equations and expand their left-hand-sides to obtain</Para>


											<Para>Subtract the first equation from the rest to reduce the equations to the following three,</Para>


											<Para>Define the matrix 

												<Emphasis Type="Italic">A</Emphasis>
 and the vector 

												<Emphasis Type="Italic">b</Emphasis>
 by:
											</Para>


											<Para>We can then write the above equations in the following matrix form.</Para>


											<Para>Since 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>2</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>3</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>4</Subscript>
 are not in the same plane, the matrix 

												<Emphasis Type="Italic">A</Emphasis>
 is nonsingular and therefore, the linear system of equations can be solved to obtain a unique solution for 

												<Emphasis Type="Italic">x 

													<Subscript>j</Subscript>

												</Emphasis>
 . Therefore, any four independent points in 

												<Emphasis Type="Italic">R 

													<Superscript>3</Superscript>

												</Emphasis>
 form a metric basis for 

												<Emphasis Type="Italic">R 

													<Superscript>3</Superscript>

												</Emphasis>
 .
											</Para>


											<Para>Note that the above algorithm (given in the proof of Theorem 1.2) shows that the coordinates of 

												<Emphasis Type="Italic">x 

													<Subscript>j</Subscript>

												</Emphasis>
 can be computed in a constant time. Therefore, the geometric build-up algorithm can determine a protein structure in a linear running time when all exact distances are available. Moreover, when all distances are available a single metric base can be used throughout the process because in each iteration as the four required distances will be available. However there is no guarantee that a solution to the MDGP can be found when only sparse distance data is available. If we assume that any initial four atoms will lead to a protein structure using the GB, the algorithm will require a 

												<Emphasis Type="Italic">O(n 

													<Superscript>3</Superscript>
 )
												</Emphasis>
 running time in a worst case analysis. There are three nested loops in the GB algorithm: A while-loop (while L is not empty, where L is the list of unpositioned atoms), within the while-loop a for-loop (check all remaining atoms in L to find which one can be determined with currently determined atoms), and within the for-loop finding four determined atoms with a distance from a given atom. Each step has in the worst case 

												<Emphasis Type="Italic">O(n)</Emphasis>
 many steps. Therefore, the worst case total running time is 

												<Emphasis Type="Italic">O(n 

													<Superscript>3</Superscript>
 )
												</Emphasis>
 .
											</Para>


											<Para>As shown in previous reports 

												<CitationRef
CitationID="B12">12</CitationRef>
 , sparse distance data can produce a large numerical rounding error that must be dealt with. In the case of given sparse distance data, almost always a new different metric base must be used in the determination of a single atom. Thus, the metric bases used in determination of unpositioned atoms contain rounding errors from earlier calculations. Therefore the errors introduced in previous steps accumulate. As a result, the matrix A in the proof of Theorem 1.2 is often not accurate and hence cannot be used to determine new coordinates of atoms accurately. In summary, the GB algorithm produces larger and larger rounding error in the coordinate determination of unpositioned atoms.
											</Para>


										</Section2>


										<Section2 ID="Sec_68823">

											<Heading>An updated geometric build-up algorithm (UGB)</Heading>


											<Para>This algorithm incorporates the idea of re-computing the coordinates of the four atoms in a metric base to minimize the rounding error. In many cases, there exist many options to select a metric base of four atoms that can determine an unpositioned atom. In the updated geometric build-up algorithm, four non-coplanar atoms with original distances among them are preferred. The reason is that a metric base forms a tetrahedron 

												<Emphasis Type="Italic">T</Emphasis>
 consisting of original distances that allows to position the atoms of the metric base relative to each other with minimal rounding error. The coordinates of the unpositioned atom can now be determined with minimal rounding error relative to the tetrahedron 

												<Emphasis Type="Italic">T</Emphasis>
 creating a complex consisting of 5 atoms whose edges form a complete graph 

												<Emphasis Type="Italic">K 

													<Subscript>5</Subscript>

												</Emphasis>
 .
											</Para>


											<Para>It is important to realize that the determination of the coordinates of the unpositioned atom is independent of the coordinates of other atoms obtained previously. After translation and rotation of the complete graph 

												<Emphasis Type="Italic">K 

													<Subscript>5</Subscript>

												</Emphasis>
 consisting of the five atoms (the metric base and unpositioned atom) will be put back into the protein structure in a way that will minimize the rounding errors of the positions of all 5 atoms simultaneously. The old already build-up coordinates of the four atoms in T (with their potential error) will be replaced by the new updated set. We call this procedure re-initializing the coordinates of the five atoms. This algorithm is outlined in the Figure 

												<InternalRef RefID="F2">2</InternalRef>
 and also in reference 

												<CitationRef
CitationID="B11">11</CitationRef>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F2">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>The outline of the updated geometric build-up algorithm for solving the molecular distance geometry problem with sparse exact distances [Wu and Wu]</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1472-6807-10-S1-S7-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>The outline of the updated geometric build-up algorithm for solving the molecular distance geometry problem with sparse exact distances [Wu and Wu]</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>There are two major steps in this algorithm. First, the positions of the four base atoms are recomputed based on Theorem 1.1. The new positions of the four base atoms are completely independent of their old positions, and this first step just guarantees that the four base atoms form a tetrahedron where the distances between the atoms as accurate as possible. Second, the translation vector and rotation matrix need to be found for re-initializing. This second step requires techniques used in computation of the Root Mean Square Deviation (RMSD).</Para>


											<Para>We explain the re-initialization step for a tetrahedron when all distances among four atoms are available. Let ( 

												<Emphasis Type="Italic">x 

													<Subscript>i</Subscript>
 , y 

													<Subscript>i</Subscript>
 , z 

													<Subscript>i</Subscript>

												</Emphasis>
 ) be coordinates of 

												<Emphasis Type="Italic">i 

													<Superscript>th</Superscript>

												</Emphasis>
 atom, 

												<Emphasis
Type="Italic">i=1,2,3,4,</Emphasis>
 four atoms and let 

												<Emphasis Type="Italic">d 

													<Subscript>ij</Subscript>

												</Emphasis>
 be the distance between 

												<Emphasis Type="Italic">i 

													<Superscript>th</Superscript>

												</Emphasis>
 and 

												<Emphasis Type="Italic">j 

													<Superscript>th</Superscript>

												</Emphasis>
 atoms, 

												<Emphasis
Type="Italic">i=1,2,3,4.</Emphasis>
 The initialization consists of the following steps. We put the first atom at the origin, the second atom on the 

												<Emphasis Type="Italic">x</Emphasis>
 -axis and the third atom into the 

												<Emphasis Type="Italic">xy</Emphasis>
 -plane. Then we can determine the position of the fourth atom. The formulas below explain the above steps and a more detailed explanation of the procedure is available in the reference 

												<CitationRef
CitationID="B11">11</CitationRef>
 ,
											</Para>


											<Para>We explain the standard RMSD steps for any two structures of embedded points with coordinate matrices 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 of an identical set of 

												<Emphasis Type="Italic">n</Emphasis>
 points. In our case 

												<Emphasis Type="Italic">n=4</Emphasis>
 , the matrix 

												<Emphasis Type="Italic">X</Emphasis>
 contains the old coordinates of metric base atoms and the matrix 

												<Emphasis Type="Italic">Y</Emphasis>
 contains the recomputed coordinates of the metric base atoms. First, we need to translate these two structures so that their geometric centers are both at the origin. This can be done using the following formulas,
											</Para>


											<Para>,for 

												<Emphasis Type="Italic">j</Emphasis>
 =1,2,3.
											</Para>


											<Para>for 

												<Emphasis Type="Italic">i</Emphasis>
 =1,2,…,n.
											</Para>


											<Para>Now, 

												<Emphasis Type="Italic">X 

													<Subscript>1</Subscript>

												</Emphasis>
 and 

												<Emphasis Type="Italic">Y 

													<Subscript>1</Subscript>

												</Emphasis>
 are the two translatedmatrices with the same geometric center at the origin. We can then find the rotation matrix 

												<Emphasis Type="Italic">Q</Emphasis>
 so that RMSD value of 

												<Emphasis Type="Italic">X 

													<Subscript>1</Subscript>

												</Emphasis>
 and 

												<Emphasis Type="Italic">Y 

													<Subscript>1</Subscript>

												</Emphasis>
 is minimized. This is formulated as, where 

												<Emphasis Type="Italic">Q</Emphasis>
 is a rotation matrix and || || 

												<Subscript>F</Subscript>
 is defined by, where is the distance between the two points 

												<Emphasis Type="Italic">X 

													<Subscript>i</Subscript>

												</Emphasis>
 and 

												<Emphasis Type="Italic">Y 

													<Subscript>i</Subscript>

												</Emphasis>
 . 

												<Emphasis Type="Italic">Q</Emphasis>
 can be computed through the following steps. Compute 

												<Emphasis Type="Italic">C= Y 

													<Subscript>1</Subscript>


													<Superscript>T</Superscript>
 X 

													<Subscript>1</Subscript>

												</Emphasis>
 ; then let 

												<Emphasis Type="Italic">UΣV 

													<Superscript>T</Superscript>

												</Emphasis>
 =C be the singular value decomposition of 

												<Emphasis Type="Italic">C.</Emphasis>
 That 

												<Emphasis Type="Italic">Q=UV 

													<Superscript>T</Superscript>

												</Emphasis>
 can be easily verified to be the solution to the above minimization problem. In the updated geometric build-up algorithm, the above computations will give the translation vectors 

												<Emphasis
Type="Italic">(xc(1),xc(2),xc(3))</Emphasis>
 and 

												<Emphasis
Type="Italic">(yc(1),yc(2),yc(3))</Emphasis>
 and the rotation matrix 

												<Emphasis Type="Italic">Q</Emphasis>
 . Applying this to the recomputed coordinates of four metric base atoms and the newly determined atom, the five atoms can be translated and rotated back to the protein structure. Compared to the general geometric build-up algorithm, in many cases, only the updated geometric build-up algorithm can determine protein structures completely and accurately when a sparse set of distance data is available 

												<CitationRef
CitationID="B9">9</CitationRef>
 . However, the algorithm has a drawback that a brute force search for a metric base of four atoms with known distances among them can take up to 

												<Emphasis Type="Italic">O(n 

													<Superscript>4</Superscript>
 )
												</Emphasis>
 (if one considers all 4 element subsets of 

												<Emphasis Type="Italic">n</Emphasis>
 points) and then the total running time can be 

												<Emphasis Type="Italic">O(n 

													<Superscript>6</Superscript>
 )
												</Emphasis>
 . The majority of this worst case running time is spent finding four atoms with all distances among them.
											</Para>


											<Para>In this paper, the UGB algorithm is improved by a revised updated geometric build-up algorithm (RUGB). This algorithm aims at reducing the computational complexity of the UGB algorithm. As we will show the RUGB algorithm also improves the numerical error over the performance of the UGB algorithm.</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_35650">

										<Heading>Methods</Heading>


										<Section2 ID="Sec_94850">

											<Heading>A revised updated geometric build-up algorithm (RUGB)</Heading>


											<Para>Although the updated geometric build-up algorithm UGB has shown the property of controlling numerical errors, the UGB algorithm requires searching for four atoms with distances among them as a metric base in every iteration. A revised updated geometric build-up algorithm is described in this paper. The algorithm is based on the regular updated geometric build-up algorithm and modified by adding a new data structure and relaxing the condition of a metric base. The first modification in the algorithm is that instead of requiring four metric base atoms with distances among them, this algorithm requires three metric base atoms with distances among them and one additional atom. The purpose of relaxing the condition is to cut down the time it takes to find a new metric base. The updating scheme can still be implemented with only three metric base atoms. However, using three atoms, with all distances among them, will result in two possible sets of coordinates for the position of an undetermined atom. In order to distinguish the correct solution from the incorrect solution we use the distance to a fourth determined atom that is not in coplanar with the first three base atoms. This strategy is also based on Theorem 1.2. The re-initialization and updating of the metric base of three atoms also follows the steps similar to those in UGB algorithm introduced in the previous section. In this case, three atoms rather than four atoms are considered.</Para>


											<Para>A second modification is the creation of a data structure that makes it easy to access all of the neighbouring atoms given by the original distance matrix for any atom. Here we refer to the degree of an atom as the number of neighbouring atoms and 

												<Emphasis Type="Italic">d 

													<Subscript>max</Subscript>

												</Emphasis>
 as the largest degree of all the atoms. Using the original distances we generate a list of adjacency arrays, whose lengths are bounded by 

												<Emphasis Type="Italic">d 

													<Subscript>max</Subscript>

												</Emphasis>
 . Then searching through these lists of neighbouring atoms, three metric base atoms and one additional atom can be much faster because 

												<Emphasis Type="Italic">d 

													<Subscript>max</Subscript>

												</Emphasis>
 is typically small when compared to 

												<Emphasis Type="Italic">n</Emphasis>
 the number of all atoms, especially as 

												<Emphasis Type="Italic">n</Emphasis>
 gets large. Recall that previously the UGB algorithm may require in the worst case an exhaustive search through all subsets of four atoms out of 

												<Emphasis Type="Italic">n</Emphasis>
 atoms. The relative size difference between the number of atoms 

												<Emphasis Type="Italic">n</Emphasis>
 and 

												<Emphasis Type="Italic">d 

													<Subscript>max</Subscript>

												</Emphasis>
 is illustrated in Table 

												<InternalRef RefID="T1">1</InternalRef>
 , consisting of the ten largest of the tested proteins. Note that in Table 

												<InternalRef RefID="T1">1</InternalRef>
 the size of 

												<Emphasis Type="Italic">d 

													<Subscript>max</Subscript>

												</Emphasis>
 is somewhat dependent on how the data set is generated. However we assume that this is still typical for a sparse data set. The revised geometric build-up algorithm UGB is outlined in Figure 

												<InternalRef RefID="F3">3</InternalRef>
 . Two theorems illustrate the computational complexity of the revised geometric build-up algorithm.
											</Para>


											<Table Float="No" ID="T1">

												<Caption Language="En">

													<CaptionNumber>Table 1</CaptionNumber>


													<CaptionContent>

														<SimplePara>The size of 

															<Emphasis Type="Italic">d 

																<Subscript>max</Subscript>

															</Emphasis>
 compared to the total number of atoms 

															<Emphasis
Type="Italic">n</Emphasis>

														</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="4">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Bold">PDB Name</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold"># Atoms</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara> 

																	<Emphasis Type="Italic"> 

																		<Emphasis
Type="Bold">d 

																			<Subscript>max</Subscript>

																		</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara> 

																	<Emphasis Type="Italic"> 

																		<Emphasis
Type="Bold">d 

																			<Subscript>max</Subscript>
 /n
																		</Emphasis>

																	</Emphasis>

																</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1VII.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>596</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>77</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.129195</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1HIP.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>617</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>37</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.059968</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1ULR.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>677</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>36</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.053176</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1BOM.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>700</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>69</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.098571</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1AIK.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>729</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>49</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.067215</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1CEU.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>854</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>65</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.076112</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1KVX.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>954</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>38</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.039832</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1VMP.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1166</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>74</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.063465</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1HSM.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1251</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>73</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.058353</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>'1HAA.pdb'</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>1310</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>69</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.052672</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


											<Figure Category="Standard" Float="No"
ID="F3">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>The outline of the revised updated geometric build-up algorithm (RUGB) for solving the molecular distance geometry problem with sparse exact distances</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1472-6807-10-S1-S7-3" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>The outline of the revised updated geometric build-up algorithm (RUGB) for solving the molecular distance geometry problem with sparse exact distances</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para> 

												<Emphasis
Type="Bold">Theorem 3.1</Emphasis>
 Assume that any four initial metric base atoms can lead to the complete determination of a protein structure given a sparse set of distance data, then a protein structure can be determined by the revised geometric build-up algorithm (RUGB) using 

												<Emphasis Type="Italic">O(n 

													<Superscript>2</Superscript>
 d 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 many steps, where 

												<Emphasis Type="Italic">n</Emphasis>
 is the number of atoms and 

												<Emphasis Type="Italic">d 

													<Subscript>max</Subscript>

												</Emphasis>
 is the largest degree of atoms
											</Para>


											<Para> 

												<Emphasis Type="Bold">Proof.</Emphasis>
 For any unpositioned atom 

												<Emphasis Type="Italic">A</Emphasis>
 , it will take 

												<Emphasis Type="Italic">O(d 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 many steps to know if there exist three neighbouring atoms 

												<Emphasis Type="Italic">x 

													<Subscript>1</Subscript>
 , x 

													<Subscript>2</Subscript>
 , x 

													<Subscript>3</Subscript>

												</Emphasis>
 , which have known distances between them. If it is the case, then it will take 

												<Emphasis Type="Italic">O(d 

													<Subscript>max</Subscript>
 )
												</Emphasis>
 many steps to know if there is any additional neighbouring atom 

												<Emphasis Type="Italic">x 

													<Subscript>4</Subscript>

												</Emphasis>
 of 

												<Emphasis Type="Italic">A</Emphasis>
 such that 

												<Emphasis Type="Italic">x 

													<Subscript>1</Subscript>
 , x 

													<Subscript>2</Subscript>
 , x 

													<Subscript>3</Subscript>

												</Emphasis>
 , and 

												<Emphasis Type="Italic">x 

													<Subscript>4</Subscript>

												</Emphasis>
 are non planar.
											</Para>


											<Para>If both a metric base of three atoms 

												<Emphasis Type="Italic">x 

													<Subscript>1</Subscript>
 , x 

													<Subscript>2</Subscript>
 , x 

													<Subscript>3</Subscript>

												</Emphasis>
 and an additional neighbouring atom 

												<Emphasis Type="Italic">x 

													<Subscript>4</Subscript>

												</Emphasis>
 can be found, then apply the updating strategy, which includes re-computing the coordinates of a metric base of three atoms 

												<Emphasis Type="Italic">x 

													<Subscript>1</Subscript>
 , x 

													<Subscript>2</Subscript>
 , x 

													<Subscript>3</Subscript>

												</Emphasis>
 , determining the coordinates of the unpositioned atom 

												<Emphasis Type="Italic">A</Emphasis>
 , updating coordinates by translation and rotation and using the additional atom 

												<Emphasis Type="Italic">x 

													<Subscript>4</Subscript>

												</Emphasis>
 to determine the correct position for 

												<Emphasis Type="Italic">A</Emphasis>
 . For any choice of the four atoms 

												<Emphasis Type="Italic">x 

													<Subscript>1</Subscript>
 , x 

													<Subscript>2</Subscript>
 , x 

													<Subscript>3</Subscript>

												</Emphasis>
 , and 

												<Emphasis Type="Italic">x 

													<Subscript>4</Subscript>

												</Emphasis>
 this can be done in constant time.
											</Para>


											<Para>Thus for an unpositioned atom 

												<Emphasis Type="Italic">A</Emphasis>
 the total running time will be at most 

												<Emphasis Type="Italic">O(d 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 regardless if the position of 

												<Emphasis Type="Italic">A</Emphasis>
 can be determined at this point. There are at most 

												<Emphasis Type="Italic">n</Emphasis>
 unpositioned atoms and in the worst case we have to look at all of them before we can add a single atom. Thus it may take 

												<Emphasis Type="Italic">O(nd 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 many steps to add a single atom. Since the size of the initial list L is 

												<Emphasis Type="Italic">n-4</Emphasis>
 initially, the total running time is 

												<Emphasis Type="Italic">O(n 

													<Superscript>2</Superscript>
 d 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 .
											</Para>


											<Para>Note that often in NMR structure determination, only distances less than 5Å can be obtained. Therefore, the typical distance matrix is sparse in realistic applications. However, the RUGB algorithm of Figure 

												<InternalRef RefID="F3">3</InternalRef>
 relies on the successful selection of the initial four metric base atoms. There is no guarantee that choosing any arbitrarily selected metric base for initialization, will result in the algorithm completely determining a protein structure. In such a case we can start over by selecting a different set of atoms for initialization. The following theorem analyzes the upper bound of computational complexity no matter whether a protein structure can be determined or a graph can be realized, using a revised geometric build-up algorithm.
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem 3.2</Emphasis>
 Given a sparse set of distance data for a protein, then it takes at most 

												<Emphasis Type="Italic">O(n 

													<Superscript>3</Superscript>
 d 

													<Subscript>max</Subscript>


													<Superscript>6</Superscript>
 )
												</Emphasis>
 to determine whether a protein structure can be solved using a revised geometric build-up algorithm.
											</Para>


											<Para> 

												<Emphasis Type="Bold">Proof.</Emphasis>
 In a protein structure, there are at most 

												<Emphasis Type="Italic">O(nd 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 many four atoms that are non co-planar and have distances among them. Any of these sets of four atoms can be considered an initial metric base. However, the worst case is all of them fail until the last one works or none of them work at all. Therefore, the upper bound of running time is 

												<Emphasis Type="Italic">O(nd 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 ) O(n 

													<Superscript>2</Superscript>
 d 

													<Subscript>max</Subscript>


													<Superscript>3</Superscript>
 )
												</Emphasis>
 = 

												<Emphasis Type="Italic">O(n 

													<Superscript>3</Superscript>
 d 

													<Subscript>max</Subscript>


													<Superscript>6</Superscript>
 )
												</Emphasis>
 .
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_62074">

										<Heading>Results</Heading>


										<Para>We tested the RUGB algorithm on a set of proteins. We also compared the results with results generated by the GB algorithm and the UGB algorithm. The testing data was prepared in the following way. A set of proteins with their structures were downloaded from the protein structure database PDB 

											<CitationRef
CitationID="B14">14</CitationRef>
 . For each protein, a structure file contains the 

											<Emphasis Type="Italic">(x,y,z)</Emphasis>
 coordinates corresponding to each atom in the structure and then a distance matrix of all pair wise distances can be generated. In practice, especially in NMR experiments, only distances between two protons less than 5Å are typically available. In our testing we used a cut-off distance of 5 Å and deleted all distances that were larger (if there were any). This resulted in sparse distance data that only contains distances less than 5Å. However, due to the poor performance of general GB algorithm on sparse distance data, we also generated a second matrix using a different cut-off distance of 8Å. For each test case of a protein, we applied the GB, the UGB and the RUGB algorithms. We analyzed results by comparing numerical error and running time for the three algorithms.
										</Para>


										<Para>The Table 

											<InternalRef RefID="T2">2</InternalRef>
 lists numerical results of a set of proteins tested on the RUGB algorithm and a regular UGB algorithm. The first column contains PDB IDs of tested proteins; the second column contains the number of atoms in each protein; the third column shows the running time using RUGB (in seconds); the fourth column shows the running time of using UGB (in seconds); the fifth column shows the RUGB RMSD error between the determined structure and the real structure; the six column shows the UGB RMSD error between the determined structure the real structure.
										</Para>


										<Table Float="No" ID="T2">

											<Caption Language="En">

												<CaptionNumber>Table 2</CaptionNumber>


												<CaptionContent>

													<SimplePara>The numerical results of RUGB and UGB</SimplePara>


												</CaptionContent>


											</Caption>


											<tgroup cols="6">

												<colspec colname="c0" colnum="0" />


												<colspec colname="c1" colnum="1" />


												<colspec colname="c2" colnum="2" />


												<colspec colname="c3" colnum="3" />


												<colspec colname="c4" colnum="4" />


												<colspec colname="c5" colnum="5" />


												<thead>

													<row>

														<entry colname="c0">

															<SimplePara> 

																<Emphasis
Type="Bold">PDB Name</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara> 

																<Emphasis
Type="Bold"># Atoms</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara> 

																<Emphasis
Type="Bold">RUGB time (s)</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara> 

																<Emphasis
Type="Bold">UGB time (s)</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara> 

																<Emphasis
Type="Bold">RUGB error (Å)</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara> 

																<Emphasis
Type="Bold">UGB error (Å)</Emphasis>

															</SimplePara>


														</entry>


													</row>


												</thead>


												<tbody>

													<row>

														<entry colname="c0">

															<SimplePara />


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'2DX2.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>174</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>3.5803</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>4.2447</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>2.31E-11</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>1.71E-08</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1ID7.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>189</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>3.0529</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>4.4187</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>8.62E-14</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>2.87E-12</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1B5N.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>332</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>8.1185</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>10.1274</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.93E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>8.67E-08</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1FW5.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>332</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>6.9327</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>9.6053</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.65E-12</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>6.29E-08</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1SOL.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>353</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>8.318</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>13.5202</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>7.33E-13</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>5.72E-11</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1JAV.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>360</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>7.9572</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>11.4536</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>2.78E-12</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>1.50E-08</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1meq.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>405</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>8.7641</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>14.076</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>2.43E-12</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>1.20E-10</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1AMB.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>438</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>13.966</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>16.9998</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>7.11E-12</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>4.35E-07</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1R7C.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>532</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>13.3252</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>26.2002</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>8.62E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>5.50E-08</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1HLL.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>540</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>13.0888</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>28.5319</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>2.83E-12</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>5.41E-07</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1VII.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>596</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>13.0338</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>24.7907</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>3.56E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>2.28E-07</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1HIP.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>617</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>15.9565</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>35.5588</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>4.80E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>5.45E-07</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1ULR.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>677</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>19.9154</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>127.6762</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>3.84E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>5.43E-11</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1BOM.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>700</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>15.6276</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>37.5214</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.36E-09</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>3.16E-09</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1AIK.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>729</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>17.302</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>39.4843</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>9.19E-09</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>7.89E-09</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1CEU.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>854</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>21.3126</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>49.3975</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>3.15E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>2.43E-09</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1KVX.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>954</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>27.6469</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>83.2725</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>7.21E-04</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>6.61E-04</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1VMP.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>1166</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>32.7741</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>95.3844</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.01E-06</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>5.57E-06</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1HSM.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>1251</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>37.8582</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>108.2448</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>5.88E-07</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>3.22E-07</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>'1HAA.pdb'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>1310</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>35.6037</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>129.6353</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>4.49E-10</SimplePara>


														</entry>


														<entry colname="c5">

															<SimplePara>8.25E-07</SimplePara>


														</entry>


													</row>


												</tbody>


											</tgroup>


											<tfooter>

												<SimplePara>E-5 means 10 

													<Superscript>-5</Superscript>
 and E+5 means 10 

													<Superscript>5</Superscript>
 ; others follow similarly
												</SimplePara>


											</tfooter>


										</Table>


										<Para>In Table 

											<InternalRef RefID="T2">2</InternalRef>
 the RUGB algorithm shows a decreased runtime in all of tested proteins when compared to the regular UGB algorithm. In particular, for some large proteins such as 1HSM and 1HAA having 1251 and 1310 atoms respectively, the RUGB algorithm can determine each protein structure about 3 times faster. In addtion, the RUGB algorithm results in less RMSD error for most proteins when compared with the UGB algorithm. Figure 

											<InternalRef RefID="F4">4</InternalRef>
 shows that our determined protein structure of 1HAA (on the left) is practically identical to the original structure of 1HAA deposited in PDB (on the right). Both structures are displayed in Rasmol [15].
										</Para>


										<Figure Category="Standard" Float="No"
ID="F4">

											<Caption Language="En">

												<CaptionContent>

													<SimplePara>protein 3D structure determined by RUGB and the original protein 3Dstructure of 1HAA</SimplePara>


												</CaptionContent>


											</Caption>


											<MediaObject>

												<ImageObject Color="Color"
FileRef="1472-6807-10-S1-S7-4" Format="GIF" Rendition="Preview"
Type="Linedraw" />


												<TextObject>

													<Para> 

														<Emphasis
Type="Bold">Protein 3D structure determined by RUGB and the original protein 3Dstructure of 1HAA.</Emphasis>
 The left picture is for protein 3D structure of 1HAA determined by RUGB and the right picture is for protein the original protein 3Dstructure of 1HAA
													</Para>


												</TextObject>


											</MediaObject>


										</Figure>


										<Para>This is surprising since the up-date regimes are very similar. The main reason could be the following: The RUGB algorithm uses only three base atoms to numerically determine an unpositioned atom with two solutions and one additional atom to fix the real solution. This up-dating procedure involves less numerical calculation when compared with the 4 atom up-dating routine of the UGB algorithm. So it could be that the RUGB up-dating produces a smaller numerical error.</Para>


										<Para>The theoretical analysis (Theorems 3.1 and 3.2) discuss the upper bound of run-time of RUGB. Clearly the numerical data shows that the algorithm runs much faster than the theoretical worst-case analysis using the proteins in our data set. The run-time data is plotted in Figure 

											<InternalRef RefID="F5">5</InternalRef>
 . Since all proteins selected in our example can be determined with any initial four atoms, our results should show a much better run-time than Theorem 3.2. However, the numerical results also show a runtime that is better than Theorem 3.1. Recall that the theoretical result in Theorem 3.1 only shows an upper bound on the run-time of the RUGB algorithm. In addition, in the way we constructed our data sets the algorithm may not require 

											<Emphasis Type="Italic">O(n)</Emphasis>
 steps in the while-loop to find an unpositioned atom whose coordinates can be determined. Therefore, the results show a lower than quadratic runtime behavior in our tests. Our data compares nicely with the linear fit,(see dashed line in Figure 

											<InternalRef RefID="F5">5</InternalRef>
 ). However, the non-linear fit(see solid line in Figure 

											<InternalRef RefID="F5">5</InternalRef>
 ) produces a slightly higher correlation coefficient. (A log-log computation using least square shows that 

											<Emphasis Type="Italic">r 

												<Superscript>2</Superscript>

											</Emphasis>
 is maximal for the power 

											<Emphasis Type="Italic">n 

												<Superscript>1.2</Superscript>

											</Emphasis>
 .)
										</Para>


										<Figure Category="Standard" Float="No"
ID="F5">

											<Caption Language="En">

												<CaptionContent>

													<SimplePara>Plot of run-time of the UGB algorithm with the two best-fit functions</SimplePara>


												</CaptionContent>


											</Caption>


											<MediaObject>

												<ImageObject Color="Color"
FileRef="1472-6807-10-S1-S7-5" Format="GIF" Rendition="Preview"
Type="Linedraw" />


												<TextObject>

													<Para>Plot of run-time of the UGB algorithm with the two best-fit functions</Para>


												</TextObject>


											</MediaObject>


										</Figure>


										<Para>In Table 

											<InternalRef RefID="T2">2</InternalRef>
 , the structural determination of 1KVX shows unusually larger numerical errors, compared with several other selected proteins that have a similar number of amino acids, such as 1CEU and 1VMP. One reason might be that a triangle selected in the RUGB algorithm leads to a very flat tetrahedron. In this case the positions of four atoms are almost co-planar, and the determination of position of the unknown atom produces a solution of coordinates with a larger error then the error produced by a tetrahedron that is not consisting of four almost coplanar points.
										</Para>


										<Para>The Table 

											<InternalRef RefID="T3">3</InternalRef>
 compares the results of the RUGB algorithm and the regular GB algorithm for the same proteins as Table 

											<InternalRef RefID="T2">2</InternalRef>
 . The first three columns are identical to the corresponding columns in Table 

											<InternalRef RefID="T2">2</InternalRef>
 . The fourth column shows the RMSD error between the structure determined by GB and the real structure with the distance matrix using the 8Å cut-off distance; the fifth column shows the same as the fourth column but using a distance matrix with a 5Å cut-off distance.
										</Para>


										<Table Float="No" ID="T3">

											<Caption Language="En">

												<CaptionNumber>Table 3</CaptionNumber>


												<CaptionContent>

													<SimplePara>Numerical results of using RUGB and GB methods in protein structure determination</SimplePara>


												</CaptionContent>


											</Caption>


											<tgroup cols="5">

												<colspec colname="c0" colnum="0" />


												<colspec colname="c1" colnum="1" />


												<colspec colname="c2" colnum="2" />


												<colspec colname="c3" colnum="3" />


												<colspec colname="c4" colnum="4" />


												<thead>

													<row>

														<entry colname="c0">

															<SimplePara> 

																<Emphasis
Type="Bold">PDB Name</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara> 

																<Emphasis
Type="Bold">Atoms</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara> 

																<Emphasis
Type="Bold">RUGB error*</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara> 

																<Emphasis
Type="Bold">GB error1**</Emphasis>

															</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara> 

																<Emphasis
Type="Bold">GB error2*</Emphasis>

															</SimplePara>


														</entry>


													</row>


												</thead>


												<tbody>

													<row>

														<entry colname="c0">

															<SimplePara />


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>2DX2</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>174</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>2.31E-11</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>7.81E-12</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>4.80E-05</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1ID7</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>189</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>8.62E-14</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>1.94E-13</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>8.48E-08</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1B5N</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>332</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>1.93E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>1.87E-07</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>4.31E+07</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1FW5</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>332</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>1.65E-12</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>2.31E-08</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.55E+00</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1SOL</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>353</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>7.33E-13</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>1.58E-05</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.70E+04</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1JAV</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>360</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>2.78E-12</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>3.33E-03</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>4.97E+01</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1MEQ</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>405</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>2.43E-12</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>4.54E-08</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>2.21E+04</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1AMB</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>438</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>7.11E-12</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>3.01E-09</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.11E+00</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1R7C</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>532</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>8.62E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>1.54E-2</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>6.07E+12</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1HLL</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>540</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>2.83E-12</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>2.04</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.83E+09</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1VII</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>596</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>3.56E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>0.373</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.52E+05</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1HIP</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>617</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>4.80E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>1.25E+5</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>N.A.</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1ULR</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>677</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>3.84E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>3.20E+3</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>7.33E+09</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1BOM</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>700</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>1.36E-09</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>2.7E-2</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.68E+12</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1AIK</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>729</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>9.19E-09</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>26.9</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>N.A.</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1CEU</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>854</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>3.15E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>5E-5</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>9.35E+09</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1KVX'</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>954</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>7.21E-04</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>977.49</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>7.45E+30</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1VMP</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>1166</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>1.01E-06</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>2.78071E+13</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>N.A.</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1HSM</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>1251</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>5.88E-07</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>1857.809626</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>1.37E+15</SimplePara>


														</entry>


													</row>


													<row>

														<entry colname="c0">

															<SimplePara>1HAA</SimplePara>


														</entry>


														<entry colname="c1">

															<SimplePara>1310</SimplePara>


														</entry>


														<entry colname="c2">

															<SimplePara>4.49E-10</SimplePara>


														</entry>


														<entry colname="c3">

															<SimplePara>83.15</SimplePara>


														</entry>


														<entry colname="c4">

															<SimplePara>6.62E+09</SimplePara>


														</entry>


													</row>


												</tbody>


											</tgroup>


											<tfooter>

												<SimplePara>All errors are in Å.N.A. means such protein can not be determined due to a large numerical errorE-5 means 10 

													<Superscript>-5</Superscript>
 and E+5 means 10 

													<Superscript>5</Superscript>
 ; others follow similarly* for each tested protein, a given set of distances are prepared with a cut-off distance 5Å** for each tested protein, a given set of distances are prepared with a cut-off distance 8Å
												</SimplePara>


											</tfooter>


										</Table>


										<Para>In Table 

											<InternalRef RefID="T3">3</InternalRef>
 , it is easy to see that the updating procedure plays a very important role in controlling numerical errors, see also similar results in 

											<CitationRef
CitationID="B9">9</CitationRef>
 . Using a 8Å cut-off distance, the GB algorithm can determine the structure all tested proteins in some sense, however the rounding errors are so large that these structures are no longer useful.
										</Para>


										<Para>Using a 5Å cut-off distance, the GB algorithm fails in producing a complete protein structure in some instances due to a round-off error that gets out of control. For the 8Å cut-off distance the given set of pair wise distances is much denser. This work verifies that the importance of updating that is used in both the RUGB and the UGB algorithms. Both algorithms indeed can determine a protein structure with a high accuracy.</Para>


									</Section1>


									<Section1 ID="Sec_08427">

										<Heading>Conclusions</Heading>


										<Para>A very accurate protein structure is essential to understand the function and dynamics of the protein in biological systems and activities. Applications of distance geometry in protein structures determination arise from the fact that pair wise distances of atoms in a protein can often be obtained from experiments or our knowledge of chemistry. Hence a protein structure can be determined if there exists a solution to the distance geometry problem. However, the problem is proved to be NP-complete. GB algorithms do not solve all distance geometry problems. In the cases where they do give a solution, GB algorithms can determine protein structure efficiently and accurately. In the GB algorithm, the positions of atoms are determined iteratively and rely on other already determined positions of atoms, which cause the accumulation of numerical errors. The strategy of updating allows us to control the size of numerical errors. However, in the UBG algorithm updating requires implementing an expensive step that contributes up to 

											<Emphasis Type="Italic">O(n 

												<Superscript>4</Superscript>
 )
											</Emphasis>
 in the running time and the condition that the four base atoms to be updated must have all their distances known is quite strong. In this paper, the RUGB algorithm relaxes the condition for updating and incorporates the data structure for accessing neighbours of an atom. This results in an improvement of both the overall runtime and the numeric error over the UGB algorithm.
										</Para>


										<Para>The RUGB algorithm has shown important properties of controlling numerical errors and effectiveness. However, this paper provides only theoretical studies of the method. The practical problems generally have distance ranges in a data set, such as NMR structure determination and protein structure prediction. In the future, we will address the application of RUGB methods in these cases. Also the theoretical results provide the upper bound of run-time when a sparse set of distances is given. More advanced methods should also be Applications of knowledge in graph theory or other advanced data structures may improve the algorithm further and will be a topic of future research.</Para>


									</Section1>


									<Section1 ID="Sec_10189">

										<Heading>Competing interests</Heading>


										<Para>The authors declare that they have no competing interests.</Para>


									</Section1>


									<Section1 ID="Sec_28121">

										<Heading>Authors' contributions</Heading>


										<Para>RTD carried out the programming and numerical tests. CE participated in the design of the study and helped to draft the manuscript. DW participated in the design of the study and helped to draft the manuscript. All authors read and approved the final manuscript.</Para>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/1472-6807/10/S1/S7"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TE</Initials>


													<FamilyName>Creighton</FamilyName>


												</BibAuthorName>


												<Year>1993</Year>


												<ArticleTitle
Language="En">Proteins: Structures and Molecular Properties</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Wuthrich</FamilyName>


												</BibAuthorName>


												<Year>1986</Year>


												<ArticleTitle
Language="En">NMR of Proteins and Nucleic Acids</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>GM</Initials>


													<FamilyName>Crippen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TF</Initials>


													<FamilyName>Havel</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">Distance Geometry and Molecular Conformation</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JB</Initials>


													<FamilyName>Saxe</FamilyName>


												</BibAuthorName>


												<Year>1979</Year>


												<ArticleTitle
Language="En">Embeddability of Weighted Graphs in k-space is Strongly NP-hard.</ArticleTitle>


												<JournalTitle>Proceedings of the 17th Allerton Conf. on Communication, Control and Computing: Oct. 1979; University of Illinois</JournalTitle>


												<VolumeID />


												<FirstPage>480</FirstPage>


												<LastPage>489</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Glunt</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TL</Initials>


													<FamilyName>Hayden</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Raydsan</FamilyName>


												</BibAuthorName>


												<Year>1993</Year>


												<ArticleTitle
Language="En">Molecular Conformations from Distance Matrices.</ArticleTitle>


												<JournalTitle>J Comput Chem</JournalTitle>


												<VolumeID>14</VolumeID>


												<FirstPage>114</FirstPage>


												<LastPage>120</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1002/jcc.540140115</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>BA</Initials>


													<FamilyName>Hendrickson</FamilyName>


												</BibAuthorName>


												<Year>1991</Year>


												<ArticleTitle
Language="En">The Molecular Problem: Determining Conformation from Pairwise Distances.</ArticleTitle>


												<JournalTitle>Ph.D, thesis</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>WS</Initials>


													<FamilyName>Torgerson</FamilyName>


												</BibAuthorName>


												<Year>1953</Year>


												<ArticleTitle
Language="En">Theory and Applications of Distance Geometry</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>More</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">Global Continuation for Distance Geometry Problems</ArticleTitle>


												<JournalTitle>SIAM Journal of Optim</JournalTitle>


												<VolumeID>3</VolumeID>


												<FirstPage>814</FirstPage>


												<LastPage>836</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1137/S1052623495283024</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>More</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<Year>1999</Year>


												<ArticleTitle
Language="En">Distance Geometry Optimization for Protein Structures.</ArticleTitle>


												<JournalTitle>J of Global Optim</JournalTitle>


												<VolumeID>15</VolumeID>


												<FirstPage>219</FirstPage>


												<LastPage>234</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1023/A:1008380219900</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>Q</Initials>


													<FamilyName>Dong</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">A linear-time algorithm for solving the molecular distance geometry problem with exact inter-atomic distances.</ArticleTitle>


												<JournalTitle>J Global Optim</JournalTitle>


												<VolumeID>22</VolumeID>


												<FirstPage>365</FirstPage>


												<LastPage>375</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1023/A:1013857218127</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<Year>2007</Year>


												<ArticleTitle
Language="En">An Updated Geometric Build-up Algorithm for solving the Molecular Distance Geometry Problem with Sparse Distance Data.</ArticleTitle>


												<JournalTitle>J Global Optim</JournalTitle>


												<VolumeID>37</VolumeID>


												<FirstPage>661</FirstPage>


												<LastPage>673</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1007/s10898-006-9080-6</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>Q</Initials>


													<FamilyName>Dong</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">A geometric build-up algorithm for solving the molecular distance geometry problem with sparse distance data</ArticleTitle>


												<JournalTitle>J Global Optim</JournalTitle>


												<VolumeID>26</VolumeID>


												<FirstPage>321</FirstPage>


												<LastPage>333</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1023/A:1023221624213</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Wu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Yuan</FamilyName>


												</BibAuthorName>


												<Year>2008</Year>


												<ArticleTitle
Language="En">The Solution of the Distance Geometry Problem in Protein Modeling via Geometric Build-Up.</ArticleTitle>


												<JournalTitle>Biophy Rev and Lett</JournalTitle>


												<VolumeID>3</VolumeID>


												<FirstPage>43</FirstPage>


												<LastPage>75</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1142/S1793048008000617</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>HM</Initials>


													<FamilyName>Berman</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Westbrook</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Z</Initials>


													<FamilyName>Feng</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Gilliland</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TN</Initials>


													<FamilyName>Bhat</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Weissig</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>LN</Initials>


													<FamilyName>Shindyalov</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>PE</Initials>


													<FamilyName>Bourne</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">The protein data bank</ArticleTitle>


												<JournalTitle>Nucleic Acids Res</JournalTitle>


												<VolumeID>28</VolumeID>


												<FirstPage>235</FirstPage>


												<LastPage>242</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/28.1.235</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:DateLoaded>2010-05-18T16:57:16.541294+02:00</meta:DateLoaded>

					<meta:Authors>

						<meta:Author>Davis, Robert</meta:Author>

						<meta:Author>Ernst, Claus</meta:Author>

						<meta:Author>Wu, Di</meta:Author>

					</meta:Authors>

					<meta:Institutions />

					<meta:Date>2010-05-17</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1472-6807-10-S1-S7</meta:DOI>

					<meta:Title>Protein structure determination via an efficient geometric build-up algorithm</meta:Title>

					<meta:ISXN>1472-6807</meta:ISXN>

					<meta:Journal>BMC Structural Biology</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>S7</meta:ArticleFirstPage>

					<meta:Publication>BMC Structural Biology</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Protein Science</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Crystallography</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Mass Spectrometry</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Spectroscopy/Spectrometry</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Biochemistry, general</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-0" xml:lang="en"
language="en">

					<Caption>

						<p>The outline of the General Geometric Build-Up Algorithm for Solving MDGP [Dong and Wu 2002b]</p>


					</Caption>

					<FullText>

						<p>
This general geometric build-up algorithm is outlined in the Figure
1 and also in reference 12 .
						</p>

					</FullText>

					<APPId>F1</APPId>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1472-6807-10-S1-S7-1.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-1" xml:lang="en"
language="en">

					<Caption>

						<p>The outline of the updated geometric build-up algorithm for solving the molecular distance geometry problem with sparse exact distances [Wu and Wu]</p>


					</Caption>

					<FullText>

						<p>
This algorithm is outlined in the Figure 2 and also in reference 11
..
						</p>

					</FullText>

					<APPId>F2</APPId>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1472-6807-10-S1-S7-2.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-2" xml:lang="en"
language="en">

					<Caption>

						<p>The outline of the revised updated geometric build-up algorithm (RUGB) for solving the molecular distance geometry problem with sparse exact distances</p>


					</Caption>

					<FullText>

						<p>
The revised geometric build-up algorithm UGB is outlined in Figure
3 .
						</p>

						<p>
However, the RUGB algorithm of Figure 3 relies on the successful
selection of the initial four metric base atoms.
						</p>

					</FullText>

					<APPId>F3</APPId>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1472-6807-10-S1-S7-3.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-3" xml:lang="en"
language="en">

					<Caption>

						<p>protein 3D structure determined by RUGB and the original protein 3Dstructure of 1HAA</p>


					</Caption>

					<FullText>

						<p>
Figure 4 shows that our determined protein structure of 1HAA (on
the left) is practically identical to the original structure of
1HAA deposited in PDB (on the right).
						</p>

					</FullText>

					<APPId>F4</APPId>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1472-6807-10-S1-S7-4.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-4" xml:lang="en"
language="en">

					<Caption>

						<p>Plot of run-time of the UGB algorithm with the two best-fit functions</p>


					</Caption>

					<FullText>

						<p>
The run-time data is plotted in Figure 5 .
						</p>

						<p>
Our data compares nicely with the linear fit,(see dashed line in
Figure 5 ).
						</p>

						<p>
However, the non-linear fit(see solid line in Figure 5 ) produces a
slightly higher correlation coefficient.
						</p>

					</FullText>

					<APPId>F5</APPId>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1472-6807-10-S1-S7-5.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-5" xml:lang="en"
language="en">

					<Caption>

						<p>The size of d 

							<sub>max</sub>
  compared to the total number of atoms n 
						</p>


					</Caption>

					<FullText>

						<p>
The relative size difference between the number of atoms n and d

							<sub>max</sub>
 is illustrated in Table 1 , consisting of the ten
largest of the tested proteins.

						</p>

						<p>
Note that in Table 1 the size of d 

							<sub>max</sub>
 is somewhat
dependent on how the data set is generated.

						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T1">

							<Caption Language="En">

								<CaptionNumber>Table 1</CaptionNumber>


								<CaptionContent>

									<SimplePara>The size of 

										<Emphasis Type="Italic">d 

											<Subscript>max</Subscript>

										</Emphasis>
 compared to the total number of atoms 

										<Emphasis Type="Italic">n</Emphasis>

									</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="4">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">PDB Name</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold"># Atoms</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis Type="Italic"> 

													<Emphasis Type="Bold">d 

														<Subscript>max</Subscript>

													</Emphasis>

												</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis Type="Italic"> 

													<Emphasis Type="Bold">d 

														<Subscript>max</Subscript>
 /n
													</Emphasis>

												</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1VII.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>596</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>77</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.129195</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HIP.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>617</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>37</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.059968</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1ULR.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>677</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>36</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.053176</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1BOM.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>700</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>69</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.098571</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1AIK.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>729</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>49</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.067215</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1CEU.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>854</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>65</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.076112</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1KVX.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>954</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>38</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.039832</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1VMP.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1166</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>74</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.063465</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HSM.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1251</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>73</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.058353</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HAA.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1310</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>69</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.052672</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


						</Table>

					</Table>

					<APPId>T1</APPId>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-6" xml:lang="en"
language="en">

					<Caption>

						<p>The numerical results of RUGB and UGB</p>


					</Caption>

					<FullText>

						<p>

							<p>The Table 2 lists numerical results of a set of proteins tested
on the RUGB algorithm and a regular UGB algorithm.</p>


						</p>

						<p>

							<p>In Table 2 the RUGB algorithm shows a decreased runtime in all
of tested proteins when compared to the regular UGB algorithm.</p>


						</p>

						<p>

							<p>In Table 2 , the structural determination of 1KVX shows
unusually larger numerical errors, compared with several other
selected proteins that have a similar number of amino acids, such
as 1CEU and 1VMP.</p>


						</p>

						<p>

							<p>The Table 3 compares the results of the RUGB algorithm and the
regular GB algorithm for the same proteins as Table 2 .</p>


						</p>

						<p>
The first three columns are identical to the corresponding columns
in Table 2 .
						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T2">

							<Caption Language="En">

								<CaptionNumber>Table 2</CaptionNumber>


								<CaptionContent>

									<SimplePara>The numerical results of RUGB and UGB</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="6">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<colspec colname="c5" colnum="5" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">PDB Name</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold"># Atoms</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis
Type="Bold">RUGB time (s)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis
Type="Bold">UGB time (s)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis
Type="Bold">RUGB error (Å)</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara> 

												<Emphasis
Type="Bold">UGB error (Å)</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'2DX2.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>174</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3.5803</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>4.2447</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2.31E-11</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>1.71E-08</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1ID7.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>189</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3.0529</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>4.4187</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>8.62E-14</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>2.87E-12</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1B5N.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>332</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>8.1185</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>10.1274</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.93E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>8.67E-08</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1FW5.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>332</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>6.9327</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>9.6053</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.65E-12</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>6.29E-08</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1SOL.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>353</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>8.318</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>13.5202</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>7.33E-13</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5.72E-11</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1JAV.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>360</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>7.9572</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>11.4536</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2.78E-12</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>1.50E-08</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1meq.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>405</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>8.7641</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>14.076</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2.43E-12</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>1.20E-10</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1AMB.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>438</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>13.966</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>16.9998</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>7.11E-12</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>4.35E-07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1R7C.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>532</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>13.3252</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>26.2002</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>8.62E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5.50E-08</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HLL.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>540</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>13.0888</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>28.5319</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2.83E-12</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5.41E-07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1VII.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>596</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>13.0338</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>24.7907</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>3.56E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>2.28E-07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HIP.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>617</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>15.9565</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>35.5588</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4.80E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5.45E-07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1ULR.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>677</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>19.9154</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>127.6762</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>3.84E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5.43E-11</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1BOM.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>700</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>15.6276</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>37.5214</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.36E-09</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>3.16E-09</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1AIK.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>729</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>17.302</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>39.4843</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>9.19E-09</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>7.89E-09</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1CEU.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>854</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>21.3126</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>49.3975</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>3.15E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>2.43E-09</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1KVX.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>954</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>27.6469</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>83.2725</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>7.21E-04</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>6.61E-04</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1VMP.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1166</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>32.7741</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>95.3844</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.01E-06</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>5.57E-06</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HSM.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1251</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>37.8582</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>108.2448</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>5.88E-07</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>3.22E-07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>'1HAA.pdb'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1310</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>35.6037</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>129.6353</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4.49E-10</SimplePara>


										</entry>


										<entry colname="c5">

											<SimplePara>8.25E-07</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>E-5 means 10 

									<Superscript>-5</Superscript>
 and E+5 means 10 

									<Superscript>5</Superscript>
 ; others follow similarly
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<APPId>T2</APPId>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1472-6807-10-S1-S7-7" xml:lang="en"
language="en">

					<Caption>

						<p>Numerical results of using RUGB and GB methods in protein structure determination</p>


					</Caption>

					<FullText>

						<p>

							<p>The Table 3 compares the results of the RUGB algorithm and the
regular GB algorithm for the same proteins as Table 2 .</p>


						</p>

						<p>

							<p>In Table 3 , it is easy to see that the updating procedure plays
a very important role in controlling numerical errors, see also
similar results in 9 .</p>


						</p>

					</FullText>

					<Table>

						<Table Float="No" ID="T3">

							<Caption Language="En">

								<CaptionNumber>Table 3</CaptionNumber>


								<CaptionContent>

									<SimplePara>Numerical results of using RUGB and GB methods in protein structure determination</SimplePara>


								</CaptionContent>


							</Caption>


							<tgroup cols="5">

								<colspec colname="c0" colnum="0" />


								<colspec colname="c1" colnum="1" />


								<colspec colname="c2" colnum="2" />


								<colspec colname="c3" colnum="3" />


								<colspec colname="c4" colnum="4" />


								<thead>

									<row>

										<entry colname="c0">

											<SimplePara> 

												<Emphasis
Type="Bold">PDB Name</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara> 

												<Emphasis Type="Bold">Atoms</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara> 

												<Emphasis
Type="Bold">RUGB error*</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara> 

												<Emphasis
Type="Bold">GB error1**</Emphasis>

											</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara> 

												<Emphasis
Type="Bold">GB error2*</Emphasis>

											</SimplePara>


										</entry>


									</row>


								</thead>


								<tbody>

									<row>

										<entry colname="c0">

											<SimplePara />


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>2DX2</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>174</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>2.31E-11</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>7.81E-12</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4.80E-05</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1ID7</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>189</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>8.62E-14</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>1.94E-13</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>8.48E-08</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1B5N</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>332</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>1.93E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>1.87E-07</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4.31E+07</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1FW5</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>332</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>1.65E-12</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2.31E-08</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.55E+00</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1SOL</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>353</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>7.33E-13</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>1.58E-05</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.70E+04</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1JAV</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>360</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>2.78E-12</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>3.33E-03</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>4.97E+01</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1MEQ</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>405</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>2.43E-12</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>4.54E-08</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>2.21E+04</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1AMB</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>438</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>7.11E-12</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>3.01E-09</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.11E+00</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1R7C</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>532</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>8.62E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>1.54E-2</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>6.07E+12</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1HLL</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>540</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>2.83E-12</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2.04</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.83E+09</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1VII</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>596</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3.56E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>0.373</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.52E+05</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1HIP</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>617</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>4.80E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>1.25E+5</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>N.A.</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1ULR</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>677</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3.84E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>3.20E+3</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>7.33E+09</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1BOM</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>700</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>1.36E-09</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2.7E-2</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.68E+12</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1AIK</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>729</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>9.19E-09</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>26.9</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>N.A.</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1CEU</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>854</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>3.15E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>5E-5</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>9.35E+09</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1KVX'</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>954</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>7.21E-04</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>977.49</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>7.45E+30</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1VMP</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1166</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>1.01E-06</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>2.78071E+13</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>N.A.</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1HSM</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1251</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>5.88E-07</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>1857.809626</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>1.37E+15</SimplePara>


										</entry>


									</row>


									<row>

										<entry colname="c0">

											<SimplePara>1HAA</SimplePara>


										</entry>


										<entry colname="c1">

											<SimplePara>1310</SimplePara>


										</entry>


										<entry colname="c2">

											<SimplePara>4.49E-10</SimplePara>


										</entry>


										<entry colname="c3">

											<SimplePara>83.15</SimplePara>


										</entry>


										<entry colname="c4">

											<SimplePara>6.62E+09</SimplePara>


										</entry>


									</row>


								</tbody>


							</tgroup>


							<tfooter>

								<SimplePara>All errors are in Å.N.A. means such protein can not be determined due to a large numerical errorE-5 means 10 

									<Superscript>-5</Superscript>
 and E+5 means 10 

									<Superscript>5</Superscript>
 ; others follow similarly* for each tested protein, a given set of distances are prepared with a cut-off distance 5Å** for each tested protein, a given set of distances are prepared with a cut-off distance 8Å
								</SimplePara>


							</tfooter>


						</Table>

					</Table>

					<APPId>T3</APPId>

					<Authors>

						<Author>Davis, Robert</Author>

						<Author>Ernst, Claus</Author>

						<Author>Wu, Di</Author>

					</Authors>

					<Institutions>

						<Institution>Department of Mathematics and Computer Science, Bioinformatics and Information Sciences Center, Western Kentucky University, Bowling Green, KY 42103, USA</Institution>

					</Institutions>

					<ArticleTitle>Protein structure determination via an efficient geometric build-up algorithm</ArticleTitle>

					<DOI>10.1186/1472-6807-10-S1-S7</DOI>

					<PubDate>2010-05-17</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Structural Biology</SourceTitle>

					<JournalId>1472-6807</JournalId>

					<VolumeId>10</VolumeId>

					<IssueId>Suppl 1</IssueId>

					<ISXN ISSN="1472-6807" ISBN="" EISBN="">1472-6807</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Protein Science</Subject>

						<Subject Type="Secondary"
Priority="2">Crystallography</Subject>

						<Subject Type="Secondary"
Priority="3">Mass Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="4">Spectroscopy/Spectrometry</Subject>

						<Subject Type="Secondary"
Priority="5">Biochemistry, general</Subject>

					</Subjects>

					<Keywords>

						<Keyword>build-up</Keyword>

						<Keyword>structure</Keyword>

						<Keyword>via</Keyword>

						<Keyword>determination</Keyword>

						<Keyword>geometric</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

						<Keyword>Protein</Keyword>

					</Keywords>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Wu et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<ImageType>Table</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1472-6807-10-S1-S7.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2010-10-18T14:31:25.278915+02:00</DateLoaded>

				</Image>

			</Images>

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>


					<PublisherName>Springer-Verlag</PublisherName>



					<PublisherLocation>Berlin/Heidelberg</PublisherLocation>


				</PublisherInfo>

				<Journal OutputMedium="All">


					<JournalInfo
JournalProductType="NonStandardArchiveJournal" NumberingStyle="ContentOnly">


						<JournalID>12544</JournalID>



						<JournalPrintISSN>1867-0717</JournalPrintISSN>



						<JournalElectronicISSN>1866-8887</JournalElectronicISSN>



						<JournalSPIN>32484465</JournalSPIN>



						<JournalTitle>European Transport Research Review</JournalTitle>



						<JournalSubTitle>An Open Access Journal</JournalSubTitle>



						<JournalAbbreviatedTitle>Eur. Transp. Res. Rev.</JournalAbbreviatedTitle>



						<JournalSubjectGroup>


							<JournalSubject
Type="Primary">Engineering</JournalSubject>



							<JournalSubject
Type="Secondary">Regional/Spatial Science</JournalSubject>



							<JournalSubject
Type="Secondary">Automotive Engineering</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>



					<Volume OutputMedium="All">


						<VolumeInfo TocLevels="0" VolumeType="Regular">


							<VolumeIDStart>1</VolumeIDStart>



							<VolumeIDEnd>1</VolumeIDEnd>



							<VolumeIssueCount>4</VolumeIssueCount>


						</VolumeInfo>



						<Issue IssueType="Regular" OutputMedium="All">


							<IssueInfo IssueType="Regular" TocLevels="0">


								<IssueIDStart>3</IssueIDStart>



								<IssueIDEnd>3</IssueIDEnd>



								<IssueArticleCount>4</IssueArticleCount>



								<IssueHistory>


									<OnlineDate>


										<Year>2009</Year>



										<Month>11</Month>



										<Day>5</Day>


									</OnlineDate>



									<PrintDate>


										<Year>2009</Year>



										<Month>11</Month>



										<Day>4</Day>


									</PrintDate>



									<CoverDate>


										<Year>2009</Year>



										<Month>10</Month>


									</CoverDate>



									<PricelistYear>2009</PricelistYear>


								</IssueHistory>



								<IssueCopyright>


									<CopyrightHolderName>European Conference of Transport Research Institutes (ECTRI)</CopyrightHolderName>



									<CopyrightYear>2009</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>



							<Article ID="s12544-009-0013-6" OutputMedium="All">


								<ArticleInfo ArticleCitation="ArticleFirstPage"
ArticleType="OriginalPaper" ContainsESM="No" Language="En"
NumberingStyle="ContentOnly" TocLevels="0">


									<ArticleID>13</ArticleID>



									<ArticleDOI>10.1007/s12544-009-0013-6</ArticleDOI>



									<ArticleSequenceNumber>2</ArticleSequenceNumber>



									<ArticleTitle Language="En"
OutputMedium="All">An off-line map-matching algorithm for incomplete map databases</ArticleTitle>



									<ArticleCategory>Original Paper</ArticleCategory>



									<ArticleFirstPage>107</ArticleFirstPage>



									<ArticleLastPage>124</ArticleLastPage>



									<ArticleHistory>


										<RegistrationDate>


											<Year>2009</Year>



											<Month>8</Month>



											<Day>13</Day>


										</RegistrationDate>



										<Received>


											<Year>2008</Year>



											<Month>9</Month>



											<Day>19</Day>


										</Received>



										<Accepted>


											<Year>2009</Year>



											<Month>7</Month>



											<Day>17</Day>


										</Accepted>



										<OnlineDate>


											<Year>2009</Year>



											<Month>9</Month>



											<Day>11</Day>


										</OnlineDate>


									</ArticleHistory>



									<ArticleCopyright>


										<CopyrightHolderName>European Conference of Transport Research Institutes (ECTRI)</CopyrightHolderName>



										<CopyrightYear>2009</CopyrightYear>


									</ArticleCopyright>



									<ArticleGrants Type="OpenChoice">


										<MetadataGrant Grant="OpenAccess" />



										<AbstractGrant Grant="OpenAccess" />



										<BodyPDFGrant Grant="OpenAccess" />



										<BodyHTMLGrant Grant="OpenAccess" />



										<BibliographyGrant Grant="OpenAccess" />



										<ESMGrant Grant="OpenAccess" />


									</ArticleGrants>


								</ArticleInfo>



								<ArticleHeader>


									<AuthorGroup>


										<Author AffiliationIDS="Aff1"
CorrespondingAffiliationID="Aff1">


											<AuthorName DisplayOrder="Western">


												<GivenName>Francisco</GivenName>



												<GivenName>Câmara</GivenName>



												<FamilyName>Pereira</FamilyName>


											</AuthorName>



											<Contact>


												<Email>camara@dei.uc.pt</Email>


											</Contact>


										</Author>



										<Author AffiliationIDS="Aff1">


											<AuthorName DisplayOrder="Western">


												<GivenName>Hugo</GivenName>



												<FamilyName>Costa</FamilyName>


											</AuthorName>



											<Contact>


												<Email>hecosta@student.dei.uc.pt</Email>


											</Contact>


										</Author>



										<Author AffiliationIDS="Aff1">


											<AuthorName DisplayOrder="Western">


												<GivenName>Nuno</GivenName>



												<GivenName>Martinho</GivenName>



												<FamilyName>Pereira</FamilyName>


											</AuthorName>



											<Contact>


												<Email>nfmart@student.dei.uc.pt</Email>


											</Contact>


										</Author>



										<Affiliation ID="Aff1">


											<OrgDivision>Centro de Informática e Sistemas da Universidade de Coimbra (CISUC), Departmento de Engenharia Informática</OrgDivision>



											<OrgName>Universidade de Coimbra</OrgName>



											<OrgAddress>


												<Street>Pólo II, Pinhal de Marrocos</Street>



												<Postcode>3030</Postcode>



												<City>Coimbra</City>



												<Country>Portugal</Country>


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>



									<Abstract ID="Abs1" Language="En"
OutputMedium="All">


										<Heading>Abstract</Heading>



										<Para>The task of map-matching consists of finding a correspondence between a geographical point or sequence of points (e.g. obtained from GPS) and a given map. Due to many reasons, namely the noisy input data and incomplete or inaccurate maps, such a task is not trivial and can affect the validity of applications that depend on it. This includes any Transport Research projects that rely on post-hoc analysis of traces (e.g. via Floating Car Data). In this article, we describe an off-line map-matching algorithm that allows us to handle incomplete map databases. We test and compare this with other approaches and ultimately provide guidelines for use within other applications. This project is provided as open source.</Para>


									</Abstract>



									<KeywordGroup Language="En" OutputMedium="All">


										<Heading>Keywords</Heading>



										<Keyword>Map matching</Keyword>



										<Keyword>Map generation</Keyword>



										<Keyword>Artificial intelligence</Keyword>


									</KeywordGroup>


								</ArticleHeader>



								<Body>


									<Section1 ID="Sec1" Type="Introduction">


										<Heading>Introduction</Heading>



										<Para>Map Matching algorithms are needed in any geographical system to associate information to specific geo-referenced locations. Thus, while we may get exact maps that represent any portion of the planet, dynamic information obtained from common Global Navigation Satellite Systems (GNSS) devices (e.g. GPS) almost always carry errors that may negatively affect their usefulness. For example, for car navigation, for example, extreme care must be taken to continually locate the real position of the driver as opposed to what the GPS receiver estimates. Another example is the Floating Car Data probes (i.e. vehicles that periodically report their GPS position), from which it is possible to obtain information on traffic situations [

											<CitationRef
CitationID="CR1">1</CitationRef>
]. These can be used to generate real-time information as well as to provide traffic analysis and forecasting (e.g. [

											<CitationRef
CitationID="CR2">2</CitationRef>
, 

											<CitationRef
CitationID="CR3">3</CitationRef>
]), or simply to analyse mobility behaviour within an area. A stronger example could be the dynamic toll charging; charging each vehicle based on its profile, used roads and/or daily mileage. In any of these situations, accurate Map Matching algorithms become fundamental for the success of the applications. Furthermore, the analysis involved can be taken in an offline, post-processing manner. While on-line algorithms have evolved to their limits recently, essentially due to commercial car navigation applications, off-line approaches are still under explored. At a first sight, the former should be both a more challenging and a more generic task (solving the “real-time” problem often makes the post-processing solution simple), but under a more careful examination this shows that there are two different approaches to two entirely different problems. Real time applications demand solutions that provide instant response and can only rely on “past” points. This implies a compromise of performance over accuracy. On the other hand, off-line applications can take advantage of “future” points and allow for slower performances in favour of accuracy. As a result, on-line solutions applied on an off-line basis show extremely poor results, thus specific research is needed for solving the latter problem.
										</Para>



										<Para>The task of off-line map matching is to determine a correspondence between sequences of geo-referenced points previously obtained (e.g. from GPS) and a given map. The difficulty of the challenge is inversely proportional to the accuracy of the localization technology. Thus, it could be said that with Differential GPS or with Real Time Kinematics (RTK), which allow centimetre level accuracy, the task becomes considerably simpler. However, these technologies still demand expensive receivers as well as a dedicated ground infrastructure, which enhances the importance of the common off-the-shelf GPS solutions that are presently widespread and available. With noticeable less accuracy, other low cost localization approaches are becoming common, such as cell-phone based localization (e.g. [

											<CitationRef
CitationID="CR4">4</CitationRef>
, 

											<CitationRef
CitationID="CR5">5</CitationRef>
]). For these, accurate Map Matching becomes a quite complex and determinant task.
										</Para>



										<Para>Another aspect is that, either for on-line or off-line applications, the available maps are often incomplete due to the dynamics of the road networks almost everywhere in the world. Direction changes, areas under construction, new roads, off-road tracks and road closures are just some examples of phenomena that happen on a daily basis. In roads that are absent on maps, the Map Matching algorithms typically take some time to become aware of it. They stay “glued” to existing road links until they become too distant, and then typically enter into an “initialization mode” that starts promoting a new match when sufficiently close to a recognized map link. However, the “new road” segment becomes blurred in this process. For applications that demand some accuracy this may affect results. There is at least one application on the market that covers some of these issues, TomTom Map Share. However, we should point out that the approaches are very different and this application focuses on 

											<Emphasis
Type="Italic">correction</Emphasis>
 of the map provided by TomTom (direction changes, areas under construction, etc.), as opposed to the 

											<Emphasis
Type="Italic">aggregation</Emphasis>
 of new roads or geometry 

											<Emphasis Type="Italic">updates</Emphasis>
. This is done with the intervention of human hands (as happens in OpenStreetMap.org [

											<CitationRef
CitationID="CR14">14</CitationRef>
]), and not fully automatically as in our project, YouTrace.
										</Para>



										<Para>In this article, we propose M-GEMMA, an off-line Map Matching algorithm for incomplete maps. It is based on two other algorithms: an improved version of Marchal’s algorithm [

											<CitationRef
CitationID="CR6">6</CitationRef>
] that allows incomplete maps; and the GEnetic Map Matching Algorithm (GEMMA), an algorithm based on the evolutionary computation paradigm of Genetic Algorithms (GAs) that intends to overcome the main problems raised by Marchal’s approach. M-GEMMA was designed to combine the strengths of these two approaches and is to become a versatile Map Matching tool.
										</Para>



										<Para>We implemented and tested a total of four algorithms (Marchal’s original and improved versions; GEMMA and M-GEMMA) and made a thorough comparison, which is reported in this article.</Para>



										<Para>M-GEMMA’s source code is available with a “creative commons license” and its use is free. We hope to provide information in this article that can help on its application and comprehension. M-GEMMA, Improved Marchal and GEMMA were developed within the context of the YouTrace platform (which will also be made available as open source), a project that allows for the collaborative incremental construction of trajectory maps

											<Footnote ID="Fn1">


												<Para>We refer to trajectory maps since the geometry obtained corresponds to the driving trajectory as opposed to the road infrastructure geometry. Particularly on curves, the visual result can become very distinct.</Para>


											</Footnote>
. The following section will provide an overview of the YouTrace project in order to provide some context to M-GEMMA.
										</Para>



										<Para>The state-of-the-art of Map Matching is presented in Section 

											<InternalRef RefID="Sec3">3</InternalRef>
, while Marchal’s algorithms (original and improved version) are described in Section 

											<InternalRef RefID="Sec8">4</InternalRef>
. We then describe GEMMA in Section 

											<InternalRef RefID="Sec11">5</InternalRef>
, with M-GEMMA finally presented in Section 

											<InternalRef RefID="Sec19">6</InternalRef>
.
										</Para>



										<Para>The experiments and a comparative analysis are shown in Section 

											<InternalRef RefID="Sec20">7</InternalRef>
 concluded with a consensus of our thoughts of strengths and weaknesses about the algorithm.
										</Para>


									</Section1>



									<Section1 ID="Sec2">


										<Heading>Giving some context: the YouTrace project</Heading>



										<Para>The YouTrace project intends to be a social platform that allows users to collaborate with the construction of a map-of-the-world [

											<CitationRef
CitationID="CR7">7</CitationRef>
] (Fig. 

											<InternalRef RefID="Fig1">1</InternalRef>
). A key element is the Map Generation Engine that is responsible for aggregating the users’ traces into a single map. A YouTrace user can upload their traces while contributing to the construction of a joint map of the world. The users can then receive an updated map that will allow, for example, a more efficient car navigational application. An innovative characteristic of collaborative mapping is the strength of its dynamics, as opposed to the current static maps, as we are well aware: Roads are constantly being updated and aggregated as new traces are introduced. The collected traces can then provide information for more efficient route planning as the traces are a useful and realistic source about road/trajectory usage, average speeds and user preferences on road alternatives. Besides providing a dynamic map of the world, YouTrace can also be a useful source of information about users mobility and city dynamics. This information can be extremely valuable to urban planners, as they can base their planning decisions on more realistic information (as opposed to surveys or probabilistic reasoning). YouTrace users can access the system through a web portal that will be responsible for feeding the Map Generation Engine with traces, which in turn, will be added to the map.
										</Para>



										<Para>The first step of trace processing is filtering. The filtered trace is then addressed to the Map Matching, where GPS points are matched to the map, in order to find the existing segments on the map. The matched points of the trace are used to update the existing segments on the map, which thus improves road precision. The non-matched points of the trace are aggregated on to the map, creating new roads (or trajectories). Two databases are then generated from this process; the map database and the statistics database. These databases serve to provide data for external services such as route planning and traffic analysis. For more information on the YouTrace project, please refer to [

											<CitationRef
CitationID="CR7">7</CitationRef>
]. As can be understood, the Map Matching is a key element for YouTrace, which is entirely responsible for finding the parts of the trace that already exist on the map. This allows for the distinction between the parts that should be aggregated and those that should be updated, therefore the quality of the final map is dependable of the quality of the match.
										</Para>


									</Section1>



									<Section1 ID="Sec3">


										<Heading>Current trends on map-matching</Heading>



										<Para>Map-Matching algorithms are used to fix location data into a spatial road network. They are used in the most varied applications. The most common are noticeably the GPS car navigation devices, which are constantly indicating the road segment where the user is located based on information retrieved from GPS satellites. The purpose of a Map-Matching algorithm can be divided in two parts. Firstly, the algorithm determines which road segment, from a given network, corresponds to each given position. Afterwards, it will determine the exact location of the same position inside the segment previously selected [

											<CitationRef
CitationID="CR8">8</CitationRef>
, 

											<CitationRef
CitationID="CR9">9</CitationRef>
].
										</Para>



										<Para>There are algorithms designed specifically for given applications and others that are generic. In some situations, the path is known in advance so the set of roads to perform the matching is restricted. For instance, the match of bus location data can be improved by restricting the road network to the known path taken [

											<CitationRef
CitationID="CR9">9</CitationRef>
]. Generic algorithms can also be one of two types: online or offline [

											<CitationRef
CitationID="CR8">8</CitationRef>
]. Real-time applications, such as GPS navigation devices use online algorithms, meaning that the matching is performed as the data is being received and thus it is based only on past matches. On the other hand, post-processing applications use offline algorithms. Online algorithms are less effective. Offline algorithms can take the advantage of not only matching each point according to past data but also based on the following of a “future” point, which helps the algorithm to select the correct road when near to junctions. The literature review made by [

											<CitationRef
CitationID="CR8">8</CitationRef>
] states that the majority of the existent algorithms are for real-time applications since the demand is higher than in post-processing ones. In fact, only one offline algorithm is presented [

											<CitationRef
CitationID="CR6">6</CitationRef>
]. These algorithms use GPS coordinates as input source to perform the matching, but most consider using an integration of GPS data with Dead-Reckoning (DR) in order to improve the matching accuracy [

											<CitationRef
CitationID="CR10">10</CitationRef>
]. DR systems use some sensors like odometers and gyroscopes in order to calculate subsequent positions in relation to the initial one. In these systems, the probability of incorrectly estimated positions increases drastically as more readings are made since a new position is calculated based on previous readings from inaccurate sensors [

											<CitationRef
CitationID="CR10">10</CitationRef>
]. In [

											<CitationRef
CitationID="CR8">8</CitationRef>
], the author classifies Map-Matching algorithms into four groups, depending on the techniques applied by each to perform the match. They are: geometric, topological, probabilistic and other advanced algorithms.

											<Figure Category="Standard" Float="Yes"
ID="Fig1">


												<Caption Language="En">


													<CaptionNumber>Fig. 1</CaptionNumber>



													<CaptionContent>


														<SimplePara>YouTrace Map Generation Engine architecture</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO1">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig1_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Section2 ID="Sec4">


											<Heading>Geometric algorithms</Heading>



											<Para>Geometric algorithms tend only to base the match on the geometry of road segments, with preference to the closest segment to the point. These tend to ignore the way in which the network is connected, leading to various topological errors. There are three types of geometric algorithms, generally named: point-to-point, point-to-curve and curve-to curve. The first will match a point to the closest point belonging to the road network. The second one will prefer the closest map link. The last is based on the point-to-point match where it selects a set of candidates and then the final curve is chosen as the closest composed by the current matched points. According to [

												<CitationRef
CitationID="CR10">10</CitationRef>
] the point-to-curve tends to be the best choice and the curve-to-curve the worst. Since curve-to-curve depends on point-to-point, this usually produces bad results due to outliers. Moreover, the algorithmic complexity involved becomes prohibitive for large segments. Figure 

												<InternalRef
RefID="Fig2">2</InternalRef>
 shows an example of a point-to-curve match with a topological error.

												<Figure Category="Standard"
Float="Yes" ID="Fig2">


													<Caption Language="En">


														<CaptionNumber>Fig. 2</CaptionNumber>



														<CaptionContent>


															<SimplePara>Point-to-curve match. Points P3 and P4 are topological wrongly matched. Example from [

																<CitationRef
CitationID="CR10">10</CitationRef>
]
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO2">


														<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig2_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


													</MediaObject>


												</Figure>


											</Para>


										</Section2>



										<Section2 ID="Sec5">


											<Heading>Topological algorithms</Heading>



											<Para>Since maps are usually represented as graphs, topological algorithms tend to preserve continuity in the matching, avoiding frequent errors. However, they do generally ignore additional readings from certain GPS readable data such as speed or heading and might be sensitive to outliers as well. One example of a topological algorithm is the Marchal’s algorithm [

												<CitationRef
CitationID="CR6">6</CitationRef>
], which will be explained below in detail. These algorithms can generally be divided into two stages. The first is the initial matching process, where the algorithm will select the most suitable link from the closest to the initial points. At the second stage, the algorithm will continue matching the points while keeping the network topology in consideration. In [

												<CitationRef
CitationID="CR8">8</CitationRef>
], the author also adds that these kinds of algorithms have some problems at certain junctions where the direction of links is not similar. This can only be solved via a sub-routine that selects the appropriate subsequent road segment. Since this routine runs in a post-processing mode, these algorithms tend to be useless in real-time applications.
											</Para>


										</Section2>



										<Section2 ID="Sec6">


											<Heading>Probabilistic algorithms</Heading>



											<Para>Probabilistic algorithms use a region, an “error region” which is usually an ellipse or a rectangle to match the given point. From that region, the matched link is selected according to the direction, speed, connectivity and proximity from the point to the link. Some algorithms create error regions for each trace point [

												<CitationRef
CitationID="CR7">7</CitationRef>
], others [

												<CitationRef
CitationID="CR8">8</CitationRef>
] however only create these near to junctions, which improves the performance of the algorithm and also avoids mismatches in case of having other road segments nearby. Figure 

												<InternalRef
RefID="Fig3">3</InternalRef>
 shows an example of an error region.

												<Figure Category="Standard"
Float="Yes" ID="Fig3">


													<Caption Language="En">


														<CaptionNumber>Fig. 3</CaptionNumber>



														<CaptionContent>


															<SimplePara>Error ellipse having inside part of the link AB. Example taken from [

																<CitationRef
CitationID="CR8">8</CitationRef>
]
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO3">


														<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig3_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


													</MediaObject>


												</Figure>


											</Para>


										</Section2>



										<Section2 ID="Sec7">


											<Heading>Advanced algorithms</Heading>



											<Para>The advanced algorithms generally use the most varied techniques and approaches, or combine them with the simplest algorithms described above or even a simple combination of Map-Matching algorithms. The major goal is always to improve the accuracy of the matching. Aside from GPS coordinates, these algorithms are often aided with extra information such as speed, heading, connectivity of the roadmap, quality of the input data or even using correction errors from third party systems (e.g. Differential GPS). The approaches most used here are: fuzzy logic models, Dempster–Shafer’s mathematical theory of evidence, Multiple Hypothesis Technique (MHT) or Bayesian inferences. Kalman Filters and Extended Kalman Filters are widely used as well, especially to integrate the data from GPS and from DR systems or, in other cases, to smooth the GPS data before proceeding to the matching.</Para>



											<Para>In every algorithm, the accuracy of the matching highly depends on map resolution and completeness: the higher the resolution, the more accurate matching. Some comparisons about map resolutions have been made in [

												<CitationRef
CitationID="CR6">6</CitationRef>
, 

												<CitationRef
CitationID="CR11">11</CitationRef>
]. By default, the majority of the algorithms assume that the map network is complete and that it is always possible to have matching completed. However, this is often an incorrect assumption and algorithms might show unexpected behaviour where there are no roads nearby to match.
											</Para>


										</Section2>


									</Section1>



									<Section1 ID="Sec8">


										<Heading>The Marchal algorithm—an improved version</Heading>



										<Section2 ID="Sec9">


											<Heading>Overview of the Marchal algorithm for offline map matching</Heading>



											<Para>The algorithm presented by [

												<CitationRef
CitationID="CR6">6</CitationRef>
] is an offline topological algorithm inspired in MHT used on previous algorithms [

												<CitationRef
CitationID="CR10">10</CitationRef>
]. Authors say their algorithm is more focused on computational speed rather than on accuracy as opposed to the remaining ones, yet the algorithm only uses GPS coordinates to perform the matching in a road network represented by a directed graph.
											</Para>



											<Para>The algorithm works as follows: firstly, map links nearby the first trace point are picked and each one will constitute a scored path candidate (a possible match sequence). The score of each candidate is then based on the sum of the least Euclidean distance between each trace point and its matched link (also named as matching distance). Hence, best candidates have lowest scores.</Para>



											<Para>After the initial matching process, for each point of the trace, it is assumed that the current point matches the last link of each candidate. Then, an update of the score occurs and the candidate is put into a new set. Afterwards, if the trace has reached the end of the link, new path candidates are created. To see if an intersection has been reached, a comparison is made between the travelled length through the trace points and the travelled length through the links of the path candidate. If the first length is longer than a given percentage of the second, it is assumed that the next junction has possibly been reached

												<Footnote ID="Fn2">


													<Para>To understand this reasoning more clearly, the reader should be aware that in the map representation used in [

														<CitationRef
CitationID="CR6">6</CitationRef>
], each link connects two intersections (at each end). Therefore, if the trace has far exceeded the length of the link, then it is likely that one of the intersections was reached.
													</Para>


												</Footnote>
. For this percentage, the authors fixed the value in 50%, which in their opinion tends to give fair results. New candidates are created, they are similar to the current one and a link per new road segment starting on that junction will be added to each one of the new candidates. Their score is updated and they are inserted into the new set. When no more candidates to match the current point are available, the algorithm will pick only the best N candidates of the new set, it passes to the following trace point and does everything all over again. The authors tested some values for N and, based on these experiments, they say that with values above 30, improvements on matching accuracy are insignificant. The best candidate obtained gives the final match. This algorithm has some additional mechanisms that permits breaking the match and restarting it from scratch when the distance between two consecutive points or the difference between timestamps of two consecutive points is above given thresholds. The authors use, as an example, 300 m for the distance and 30 s for time difference.
											</Para>


										</Section2>



										<Section2 ID="Sec10">


											<Heading>Our implementation and modifications</Heading>



											<Para>The implementation of the algorithm has been modified from the original version due to several reasons. The first and more obvious reason is that the addition of a mechanism to detect whenever a new set of points correspond to a non-existent road in the map. For this reason, every matched distance (between GPS point and matched map position) should fall below a given threshold (fixed at 20 m, after several experiments). This condition must be verified in the initial matching process and during subsequent matches, for the best candidate path. This confirms that every set of unmatched points will be then added to the map as a new road. If the subsequent matching is broken, then the best candidate (without considering the last point) is saved and the algorithm restarts at the initial matching with the point that caused the break. After processing the entire trace, we have two distinct sets of points; the first with the matched segments and the other with the unmatched segments. These two sets correspond to the output of our implemented Map-Matching algorithm module (Fig. 

												<InternalRef
RefID="Fig4">4</InternalRef>
).

												<Figure Category="Standard"
Float="Yes" ID="Fig4">


													<Caption Language="En">


														<CaptionNumber>Fig. 4</CaptionNumber>



														<CaptionContent>


															<SimplePara>Matching a new trace. 

																<Emphasis
Type="Italic">Yellow lines</Emphasis>
 represent the existent map and 

																<Emphasis
Type="Italic">arrows</Emphasis>
 represent its direction. The 

																<Emphasis
Type="Italic">magenta line</Emphasis>
 is the new trace. 

																<Emphasis
Type="Italic">Blue lines</Emphasis>
 represent the matched trace points and the 

																<Emphasis
Type="Italic">triangular signs</Emphasis>
 the unmatched ones
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO4">


														<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig4_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


													</MediaObject>


												</Figure>


											</Para>



											<Para>Other major modifications were recently made owing to a few map representation differences, which affect the entire matching process. For the authors’ purpose, the map is represented as a graph where each node is a junction and the links are “polylines” representing the curvature of each link [

												<CitationRef
CitationID="CR6">6</CitationRef>
]. Our map is also represented as a directed graph, yet it consists of two layers. The upper layer, contains one node per junction (called super-node) and each link (called super-link) connects two super-nodes. This layer is the closest one to the representation used in [

												<CitationRef
CitationID="CR6">6</CitationRef>
] and corresponds to the notion of the “road network” where each link connects two junctions (as opposed to being connected to another link). The lower layer presents super-nodes which are shown as nodes but super-links are replaced by a set of nodes connected by directed links that represent the pattern of the super-link. These connections help to keep the road geometry as close to the originally obtained traces as possible while maintaining a flexible representation in terms of geometry corrections. Since the matching process utilizes the lower layer, the algorithm had to be adapted in order to improve the final results. Another modification relates to the detection of whenever it is necessary to jump to the subsequent links. The original function did not produce fair results because of two distinct situations: the length of the path through the trace points and through the path candidate can be slightly different; GPS readings are subject to errors and differences between both (GPS readings and road segment pattern) which are very common in curves (e.g. Figs. 

												<InternalRef
RefID="Fig5-6">5</InternalRef>
 and 

												<InternalRef
RefID="Fig5-6">6</InternalRef>
). Another reason has to do with the map representation. Since we are working with the lowest layer on the map, the function can be triggered on every link and not only in junctions. This may lead us to have a considerable set of candidates, however similar which makes the matching break unnecessarily near some junctions. This break occurs because the pattern of the trace and the correct path candidate are different, which causes the algorithm to remove such candidates from the set since they had weak scoring when compared to the majority of the candidates. Then, two or three points ahead, matching would need to stop because none of the candidates corresponded to the correct path and a restart had to be performed. Since distances between consecutive trace points and the link’s length are not homogeneous, we introduced a new concept named 

												<Emphasis
Type="Italic">tolerance link</Emphasis>
 (see Fig. 

												<InternalRef
RefID="Fig7">7</InternalRef>
). The idea is to provide the possibility of accurately matching a point to a link without the need of the previous link being matched with any point. This allows a reduced number of non-matched links to be included in the matched output without affecting the accuracy of the overall match (since these links can only exist between two matched ones, there is a high probability that the user has passed over this area). This also helps to avoid the need of the matching process to stop and restart unnecessarily. At this moment, the number of 

												<Emphasis
Type="Italic">tolerance links</Emphasis>
 is fixed to 2.

												<Figure Category="Standard"
Float="Yes" ID="Fig5-6">


													<Caption Language="En">


														<CaptionNumber>Figs. 5 – 6</CaptionNumber>



														<CaptionContent>


															<SimplePara>Difference between map and trace patterns in curves. The 

																<Emphasis
Type="Italic">yellow line</Emphasis>
 represents the map and 

																<Emphasis
Type="Italic">magenta</Emphasis>
 the trace
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO5">


														<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig5-6_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


													</MediaObject>


												</Figure>



												<Figure Category="Standard"
Float="Yes" ID="Fig7">


													<Caption Language="En">


														<CaptionNumber>Fig. 7</CaptionNumber>



														<CaptionContent>


															<SimplePara>Concept of 

																<Emphasis
Type="Italic">tolerance link</Emphasis>
. There is one link without being matched by any trace point between two consecutive matched links
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO6">


														<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig7_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


													</MediaObject>


												</Figure>


											</Para>



											<Para>Due to all these new situations, we decided not to include the condition of testing if a jump to the following link is performed or not. Instead of this, at each trace point we decided to create a set of candidates per current candidate. Each new path candidate has a distinct link to perform the matching. The links are as follows: the link that matched the previous point of the past candidate and, all reachable links to a maximum depth level of the number of 

												<Emphasis
Type="Italic">tolerance links</Emphasis>
 plus one. After scoring all the new candidates, they are filtered before passing to the following point. Two restrictions are applied in order to avoid similar path candidates that only would increase the number of candidates exponentially without having any improvement and to invalidate matches that, although being topologically correct, are far away from the trace points and so we assume that it is a new road segment instead. For the first restriction, only the best candidate passes per most recently matched super-link. This way, the number of candidates is drastically reduced and we guarantee to have the best possible candidates available. On the second restriction, the distance between the last trace point and its matched link must be lower than a given threshold (fixed to 45 m). All candidates where the last match is above this threshold are simply removed, so these candidates will not be considered better than the “real” accurate ones on the following points in unexpected and rare situations.
											</Para>


										</Section2>


									</Section1>



									<Section1 ID="Sec11">


										<Heading>GEMMA—GEnetic Map Matching Algorithm</Heading>



										<Section2 ID="Sec12">


											<Heading>Overview</Heading>



											<Para>We have created a new genetic algorithm since we did not find any reference in the Map-Matching literature to algorithms that use evolutionary approaches and which would meet our expectations. We knew that in terms of computational performance it would be less efficient than other types of algorithm, especially Marchal’s. The main concern was on improving the quality of the matches. The goal was ultimately to design an algorithm that would not have the same problems commonly seen in other algorithms, as described in the state-of-the-art section (e.g. matching errors due to topological situations or outliers) and also one that could perform smooth transitions between a matched segment to an unmatched and vice-versa, and in transitions between two matched segments that are not yet interconnected. In our genetic algorithm

												<Footnote ID="Fn3">


													<Para>It is far beyond the scope of this paper to describe how Genetic Algorithms function. Further information on this subject may easily be sourced; one such example is shown in [

														<CitationRef
CitationID="CR13">13</CitationRef>
].
													</Para>


												</Footnote>
, each individual consists of a matching sequence (from beginning to the end of the trace). Each gene corresponds to a trace point. The possible alleles for each gene are the links that are close to the respective trace point. A special value is also inserted to give the opportunity not to perform any match for the given point. After an initial population that is randomly created, the algorithm will run for a given number of generations and the best individual of the last population is considered to be the correct match. In each generation, individuals have the possibility to be recombined and mutated. Afterwards, they are evaluated using a fitness function that considers many factors (described below). Since the search space (and thus the program complexity) increases exponentially with trace length, we decided to break the trace into small segments inspired on [

												<CitationRef
CitationID="CR12">12</CitationRef>
]. In doing so, better individuals are obtained in less time. The break points are then selected based on a score function that prefers less crowded areas, noticeably away from junctions.
											</Para>


										</Section2>



										<Section2 ID="Sec13">


											<Heading>Segmentation</Heading>



											<Para>As previously mentioned, the segmentation was inspired on [

												<CitationRef
CitationID="CR12">12</CitationRef>
] in order to speed up the algorithm and to have better results. Before starting the matching process, the algorithm segments the trace in the following manner: firstly, the algorithm scores every trace point. Lowest scores represent less ambiguous areas to the matching process; then, it looks for sets of four consecutive points that are under a given threshold (fixed in 0.9); finally, using the previous sets as segment borders, the algorithm will try to form the widest segments available, yet these are restricted to a maximum number of points per segment (currently fixed at 50). The score is based on the sum of four distinct variables, which are normalized according to their units. The variables are: the difference of heading between the point and the closest map link, the distance between the point and the same link, the number of map links that are nearby the trace point (the defined distance is 20 m) and the heading variation in the neighbourhood of the trace point—the closer the trace curves are the larger the heading variation is.
											</Para>


										</Section2>



										<Section2 ID="Sec14">


											<Heading>Link candidates</Heading>



											<Para>For each trace point, a set of link candidates is available as alleles. A special allele is also inserted in order to give the possibility of an unmatched point to be performed. At this time, these link candidates can be collected according to two distinct methods. The first comes directly from the map. Firstly, links in the area of each point are picked up. Then, the closest one per super-link is selected. The maximum distance allowed is set at 20 m. Links with opposite heading to the trace point are discarded in order to avoid matches with roads running in the opposite direction. The second method, Marchal’s algorithm is first run for each candidate search. The candidates of each point are all links to which that point matched during the entire running of that algorithm. Afterwards, candidates are filtered using the same rules applied in the first method (maximum distance of 20 m, opposite heading and one link per super-link). This second approach improved results especially close to junctions during the first versions of the fitness function, but at the moment, the difference between them is minimal.</Para>


										</Section2>



										<Section2 ID="Sec15">


											<Heading>Individual representation</Heading>



											<Para>Individuals of the population are match candidates for each given trace. Each gene of the individual corresponds to a trace point and everyone has their own set of alleles that are unchangeable between them. Candidates are ranked according to a fitness function.</Para>


										</Section2>



										<Section2 ID="Sec16">


											<Heading>Fitness function</Heading>



											<Para>Each individual is scored with a set of criteria that intend to evaluate the geometric part of the match, the continuity and the transitions between an unmatched and matched part and vice-versa. The transitions between road segments that are not interconnected are also evaluated. The fitness function is constituted by the sum of eight parameters that are normalized according to their dimensions. Since this is a minimization problem, best individuals who have the lowest scores are always greater or equal to zero. One of these parameters is the average distance between each trace point and its matched link. Unmatched points are also considered with a default value of 20 m. The maximum distance previously obtained is another parameter, and the other is the maximum of the minimum distances of each matched segments. The latter tends to penalize matched zones where every matched point is too distant from the road segment. A parameter is also used to penalize candidates that privilege non-matching rather than matching on acceptable conditions. This is measured with the length of the trace that is unmatched. The concept of 

												<Emphasis
Type="Italic">tolerance link</Emphasis>
 also exists here (see Fig. 

												<InternalRef
RefID="Fig7">7</InternalRef>
). In fact, it was created firstly for this algorithm and was later adapted to our Marchal’s implementation. Another parameter consists of the sum of distances between two links when it is not possible to reach the second from the first one. The number of 

												<Emphasis
Type="Italic">tolerance links</Emphasis>
 used in the genetic algorithm is currently 5, so if it is not possible to go from one link to the other at a maximum deep of 6, the Euclidean distance between the end point of the first link and the start point of the second link is added to the specific parameter. In order to keep matching continuity as much as possible, another parameter is used to store the sum of the square of the matched lengths of each segment. Since we want to minimize the score, the inverse of the obtained value is used. Finally, the last two parameters are used to smooth the transitions between matched and unmatched areas and vice-versa. One of these parameters stores the average distance between an unmatched trace point and the following matched link or the distance between the last matched link and the following unmatched trace point. The other one stores the maximum of these distances. Each parameter is weighted, thus allowing for different orders of importance. For example, we prefer continuity to geometric proximity, so the three parameters that measure the unmatched lengths, the distances between unreachable links and the square of the continued matched lengths have the highest weights (Fig. 

												<InternalRef
RefID="Fig8">8</InternalRef>
).

												<Figure Category="Standard"
Float="Yes" ID="Fig8">


													<Caption Language="En">


														<CaptionNumber>Fig. 8</CaptionNumber>



														<CaptionContent>


															<SimplePara>


																<Emphasis
Type="Italic">Green pushpins</Emphasis>
 represent the borders of each segment, which are located in less ambiguous and crowded areas. Trace goes from the top left corner to the bottom right corner
															</SimplePara>


														</CaptionContent>


													</Caption>



													<MediaObject ID="MO7">


														<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig8_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


													</MediaObject>


												</Figure>


											</Para>


										</Section2>



										<Section2 ID="Sec17">


											<Heading>Running the algorithm</Heading>



											<Para>For each segment of the trace, the algorithm generates a random population. Each gene has a roulette wheel with respective alleles. Every allele has the same probability except for the special one that represents an unmatched situation, which has a fixed probability of 15%. The population in each generation has the opportunity to be recombined and mutated. Pairs of two individuals are then selected using the tournament selection method and have a probability to be recombined, fixed at 75%. This recombination method uses one point crossover, with a randomly selected point. Afterwards, each gene of the individual in the population has a slight probability to be mutated (0.5%). The new link is picked up randomly from the correspondent roulette wheel built at the beginning. From one generation to the next, we decided to include the best previous individuals without being recombined or mutated. 3% of the population passes directly, which corresponds to six individuals (since the population size is two hundred). One stop condition only exists for the algorithm; it stops when the best individual has not being changed for the last given generations. Currently, this number of generations is three hundred.</Para>


										</Section2>



										<Section2 ID="Sec18">


											<Heading>Observations</Heading>



											<Para>The output of the algorithm is equal to the one presented above: a set with the matched paths and another one with the unmatched trace points. These are built from the best individuals of each trace segment. As it is in its nature, the genetic algorithm itself has been suffering some evolution through time, especially in regard to the fitness function. Consequently all the thresholds discussed here were defined based on observations in our set of traces, after a relatively large number of experiments (three months of daily tests, in which we refined both the algorithm and the parameters), meaning that new situations can always come up and new improvements to the fitness function or some thresholds adjustments might be needed. The same happens with the remaining parameters of the algorithm, including crossover and mutation rates, and size of population. The values have not been as frequently changed as in the fitness function, and it is less likely that they need new modifications. Adjustments were made after running several tests showing that these could improve the quality of the matching process. We currently have a set of traces with a total of 526,728 points that corresponds to an approximate length of 11,486 km throughout Portugal (essentially the central area).</Para>


										</Section2>


									</Section1>



									<Section1 ID="Sec19">


										<Heading>M-GEMMA—joining the best from two worlds</Heading>



										<Para>As we could see in the last section, the strengths and weaknesses of both algorithms are complementary. For this reason, we thought that an integration of both algorithms could lead us to better results than to carry any of these out separately. Since GEMMA is extremely time consuming, we needed to restrict its usage to ambiguous areas where Marchal’s has performed some difficulties. Firstly, we run Marchal’s algorithm in both directions (processing the trace forwards and then backwards). This gives us two possible path matches, which can be different or exactly the same, depending on trace and map complexity. If the results are exactly the same then there is no ambiguity in the match and thus, no need to run GEMMA. However, even when both runs give the same output, the final points of the trace (or/and the initial) can be wrongly matched as seen in Fig. 

											<InternalRef RefID="Fig9">9</InternalRef>
, so GEMMA is run for the first and last few points (about 5) of the trace just to confirm that the output given by Marchal’s is correct.

											<Figure Category="Standard" Float="Yes"
ID="Fig9">


												<Caption Language="En">


													<CaptionNumber>Fig. 9</CaptionNumber>



													<CaptionContent>


														<SimplePara>Final points wrongly matched by Marchal based algorithm</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO8">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig9_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>For areas where both Marchal’s forwards and backwards runs produce different matches (and sets of candidate links), the whole set of candidate links for every participating point are added to a list. After testing all points, segments are created based on consecutive points that are on the list. For each segment, two unambiguous trace points are added in the borders (to force the start and end of the segment to “fit” into the remaining matches). This way, GEMMA can guarantee the continuity of the match and avoid topological errors. Candidates running on GEMMA are thus taken from the output of both Marchal’s runs. Since continuity is guaranteed on GEMMA, the output for the ambiguous areas fits automatically in the remaining map. With this integration, some parameters on GEMMA had to be adapted, namely the population size, the number of generations of the best individual that leads the algorithm to stop and some weights in the fitness function. Since the new segments are commonly very small, the population size remained fixed to fifty individuals and the number of stabilized generations necessary for stopping is set at seventy-five. This helped the algorithm find the best individual in the first few generations. Processing time became longer than simply running Marchal’s alone since we have to run Marchal’s twice per trace and GEMMA on some segments. Despite that, results show a gain in quality which justifies a loss of performance and as the map becomes more complete, fewer ambiguous segments appear to run on the genetic algorithm, thus further reducing the time.</Para>


									</Section1>



									<Section1 ID="Sec20">


										<Heading>Experiments and comparative analysis</Heading>



										<Para>Having implemented the four algorithms described, it is necessary to find which one adapts better to the objectives and performs better results. Each algorithm has benefits and drawbacks, either related to computational speed or to matching accuracy.</Para>



										<Para>The base map to work with was extracted from OpenStreetMap.org [

											<CitationRef
CitationID="CR14">14</CitationRef>
]. This choice was due to several reasons: it is open source and freely available; it is partially complete; in covered areas, it is comparable to TeleAtlas or NavTeq commercial solutions in terms of accuracy and completeness. For the sake of the experiments, we are confident that this choice is as valid as any other map database available (commercial or not). At most, it could be said that OpenStreetMap is globally less complete and more imprecise than those other professional databases, which becomes more of a challenge for our purposes.
										</Para>



										<Para>We made two main experiments, one in the large area of Coimbra (Portugal) to assess general performance issues; the other in a smaller area, near to our department, which contains many junctions, buildings, areas under construction and new roads (see Fig. 

											<InternalRef RefID="Fig10">10</InternalRef>
). With the latter we intended to find accuracy issues.

											<Figure Category="Standard" Float="Yes"
ID="Fig10">


												<Caption Language="En">


													<CaptionNumber>Fig. 10</CaptionNumber>



													<CaptionContent>


														<SimplePara>The base map. In 

															<Emphasis
Type="Italic">yellow</Emphasis>
 we show the OpenStreetMap map database links. New roads and junctions are observable (e.g. a new speedway from left to right of the bottom of the image, which is not yet in the main commercial maps)
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO9">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig10_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>For the first experiment, we initially built a base map with YouTrace with 225 km of traces in the area of Coimbra, originating 730 intersections and 15,901 links (1,264 super-links). We applied both algorithms to match 11 traces with a total of 16 km. With an IntelCore™ 2 Duo processor running at 2.2 GHz with 2 GB of RAM, Improved Marchal’s algorithm took 0.171 s to determine the entire match. M-GEMMA took 0.874 s to do the same task, of which 0.468 were necessary for the GEMMA part to process 152 ambiguous points in a total of 1.363 km. On average, M-GEMMA needed nearly five times more processing effort than Marchal’s approach in areas with high density of intersections.</Para>



										<Para>More experiments would be necessary (in other cities, rural areas, areas with plenty of multi-path effect, etc.) to achieve more conclusive results. However, these results are coherent with the experience we had during the development of the algorithms and with other experiments. We are also aware that a thorough algorithmic complexity analysis is needed in order to present a more explicit view of the efficiency involved. On a first analysis, Marchal’s algorithm time grows linearly with the size of the trace, with a quadratic component for local search of Euclidean distance. GEMMA behaves in an 

											<Emphasis
Type="Italic">O(n * p * m * g)</Emphasis>
, with 

											<Emphasis Type="Italic">n</Emphasis>
 being the trace size, 

											<Emphasis Type="Italic">p</Emphasis>
 the population size, 

											<Emphasis Type="Italic">m</Emphasis>
 the average number of alleles and 

											<Emphasis Type="Italic">g</Emphasis>
 the number of generations. M-GEMMA corresponds to a combination of these two measures. This, however, is a naive analysis, since no attention is given to aspects such as distribution of segmentation, sensitive areas or other parameters on any of the algorithms.
										</Para>



										<Para>Regarding the second experiment, we focused on a smaller area, extracting the exact base map from OpenStreetMap.org (the actual roads drawn, not the traces) as we can see in Fig. 

											<InternalRef RefID="Fig10">10</InternalRef>
. Within this scenario, we tested a set of ten small traces in order to raise and focus on the main problems found in each algorithm: Marchal “original version”; Marchal “improved version”; GEMMA; and M-GEMMA.

											<Figure Category="Standard" Float="Yes"
ID="Fig11">


												<Caption Language="En">


													<CaptionNumber>Fig. 11</CaptionNumber>



													<CaptionContent>


														<SimplePara>Marchal’s original algorithm: given the absence of a descending road in the center, the algorithm insists on keeping the match to the upper road. We recall that, in 

															<Emphasis
Type="Italic">Yellow</Emphasis>
, we have the “base map”; in 

															<Emphasis
Type="Italic">magenta</Emphasis>
, with arrows for direction, we have the incoming trace; in 

															<Emphasis
Type="Italic">blue</Emphasis>
, we have the matches connecting the trace to the base map
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO10">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig11_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig12">


												<Caption Language="En">


													<CaptionNumber>Fig. 12</CaptionNumber>



													<CaptionContent>


														<SimplePara>Marchal’s original version: another situation with incomplete maps</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO11">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig12_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>Although Marchal’s algorithm behaves reasonably well with traces with regular samples (in space and time) and with a complete map, it becomes inaccurate when either of these premises fail (Figs. 

											<InternalRef RefID="Fig11">11</InternalRef>
 and 

											<InternalRef RefID="Fig12">12</InternalRef>
). Aiming to solve some of those weaknesses we added the improvements described in Section 

											<InternalRef
RefID="Sec10">4.2</InternalRef>
. As can be seen in Fig. 

											<InternalRef RefID="Fig13">13</InternalRef>
 we did achieve some better results, particularly in unmatched areas. However, it still had mistakes in transitions where the algorithm tends to insist in making matches (where it is already in a “new road”). See Figs. 

											<InternalRef RefID="Fig14">14</InternalRef>
, 

											<InternalRef RefID="Fig15">15</InternalRef>
 and 

											<InternalRef RefID="Fig16">16</InternalRef>
.

											<Figure Category="Standard" Float="Yes"
ID="Fig13">


												<Caption Language="En">


													<CaptionNumber>Fig. 13</CaptionNumber>



													<CaptionContent>


														<SimplePara>Marchal’s improved version: same scenario as Fig. 

															<InternalRef
RefID="Fig11">11</InternalRef>
. Accepting “un-matched segments” brings drastic improvements
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO12">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig13_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig14">


												<Caption Language="En">


													<CaptionNumber>Fig. 14</CaptionNumber>



													<CaptionContent>


														<SimplePara>Marchal’s improved version: wrong match within un-matched sequence</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO13">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig14_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig15">


												<Caption Language="En">


													<CaptionNumber>Fig. 15</CaptionNumber>



													<CaptionContent>


														<SimplePara>Marchal’s improved version: transition between un-matched to matched area</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO14">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig15_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig16">


												<Caption Language="En">


													<CaptionNumber>Fig. 16</CaptionNumber>



													<CaptionContent>


														<SimplePara>Marchal’s improved version: transition between un-matched to matched area</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO15">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig16_HTML.gif" Format="GIF"
Rendition="HTML" Type="LinedrawHalftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>For both versions of Marchal’s algorithm, U-turns are also an issue as it is topologically impossible to move from a road segment to another one running in the opposite direction. The first points that correspond to the new direction still match wrongly the previous links (see Fig. 

											<InternalRef RefID="Fig17">17</InternalRef>
). This can simply be reduced to a problem of a transition between a matched zone and an unmatched one.

											<Figure Category="Standard" Float="Yes"
ID="Fig17">


												<Caption Language="En">


													<CaptionNumber>Fig. 17</CaptionNumber>



													<CaptionContent>


														<SimplePara>U-turn matching by Marchal based algorithm</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO16">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig17_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>The last problem found in Marchal’s approaches has to do with small matched segments. Generally, paths that have only matched up to three trace points correspond to incorrect matches. The most common situation occurs when a trace crosses a perpendicular road segment. If that trace is a new road, the algorithm tends to match the closest points to the existent road links with them. Bridges are zones where this happens frequently. For this reason we decided to ignore all these small matched paths (that have at most three trace points). However, this does not always hold true. Figure 

											<InternalRef RefID="Fig18">18</InternalRef>
 shows an example where an accurate match could have been made.

											<Figure Category="Standard" Float="Yes"
ID="Fig18">


												<Caption Language="En">


													<CaptionNumber>Fig. 18</CaptionNumber>



													<CaptionContent>


														<SimplePara>Small matches being ignored by Marchal based algorithm</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO17">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig18_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>In general, the changes we made to Marchal’s algorithm allowed for an accurate detection of unmatched areas. This may be sufficient for those applications that do not need the precise breaking spots and that are tolerant to the transition errors described above. However, for many applications, such as in the case of YouTrace, higher accuracy is necessary. The multi-objective properties of the Genetic Algorithm allow the fitness function to be tuned for smooth transitions (those that affect less negatively the several distance measures involved are preferred). To better illustrate GEMMA’s improvements in comparison with Marchal’s, we present Figs. 

											<InternalRef RefID="Fig19">19</InternalRef>
, 

											<InternalRef RefID="Fig20">20</InternalRef>
 and 

											<InternalRef RefID="Fig21">21</InternalRef>
.

											<Figure Category="Standard" Float="Yes"
ID="Fig19">


												<Caption Language="En">


													<CaptionNumber>Fig. 19</CaptionNumber>



													<CaptionContent>


														<SimplePara>GEMMA: the same scenario as in Fig. 

															<InternalRef
RefID="Fig17">17</InternalRef>
. The U-turn with the genetic algorithm
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO18">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig19_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig20">


												<Caption Language="En">


													<CaptionNumber>Fig. 20</CaptionNumber>



													<CaptionContent>


														<SimplePara>


															<Emphasis
Type="Italic">On the left</Emphasis>
, Marchal’s improved version; 

															<Emphasis
Type="Italic">on the right</Emphasis>
, GEMMA’s solution. The transition is clearly smoother and more realistic for the latter case
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO19">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig20_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig21">


												<Caption Language="En">


													<CaptionNumber>Fig. 21</CaptionNumber>



													<CaptionContent>


														<SimplePara>


															<Emphasis
Type="Italic">On the left</Emphasis>
, Marchal’s improved version; 

															<Emphasis
Type="Italic">on the right</Emphasis>
, GEMMA’s solution. The transition is clearly smoother and more realistic for the latter case
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO20">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig21_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>In terms of accuracy, the major drawback of GEMMA happens when two (parallel) matches consistently compete along a large width (Fig. 

											<InternalRef RefID="Fig22">22</InternalRef>
). In these cases, the algorithm tends to bounce repeatedly from one to the other. It could be said that this situation is rare: it needs two roads that keep parallel to each other along a number of links (typically at least 3) in the same direction. However, given the average error of the GPS (of around 10 m) this may become common. For example, when main and secondary roads parallel each other through entire avenues. It is clear that GEMMA fails in regard to their topology, conversely this is the main strength of Marchal’s algorithm.

											<Figure Category="Standard" Float="Yes"
ID="Fig22">


												<Caption Language="En">


													<CaptionNumber>Fig. 22</CaptionNumber>



													<CaptionContent>


														<SimplePara>GEMMA—parallel road ambiguity</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO21">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig22_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>Quite naturally, M-GEMMA takes advantage of the complements of the two approaches mentioned above. It not only assures topological continuity but also finds smoother transitions between matched and unmatched areas. Figure 

											<InternalRef RefID="Fig23">23</InternalRef>
 shows an example of a trace that includes a variety of transitions, matched areas, unmatched areas and irregular sized samples. The journey starts on the right and then goes around two blocks, finally moving out of the area through the speedway at the bottom.

											<Figure Category="Standard" Float="Yes"
ID="Fig23">


												<Caption Language="En">


													<CaptionNumber>Fig. 23</CaptionNumber>



													<CaptionContent>


														<SimplePara>M-GEMMA: an example of the hybrid approach</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO22">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig23_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>In terms of map-matching accuracy, M-GEMMA still has its limitations. For example, the problem of parallel roads is only partially solved. The Marchal’s part of the algorithm does prevent the bouncing between two roads, however the proximity between the roads leads M-GEMMA to choose between two options: to make the entire match (incorrect option); or not make any match (correct option). The fine-tuning of this system is thus complicated: if we make it too restrictive (slight distance of tolerance) it becomes resistant to “parallel roads”, but then it will often wrongly report unmatched segments that could easily be properly processed. Making it too loose gives opposite result. The GPS NMEA protocol allows for (Dilution Of) Precision estimates (HDOP, VDOP) or SNR (Signal-to-Noise Ratio), but curiously these values are not consistent among different receivers with respect to the quality of the trace. For the same DOP or SNR values, we have observed very different qualities of traces along the 4 GPS receivers tested. For the case of YouTrace, we rely on the statistics to distinguish between an error and a parallel road (with many traces, there should be two centrelines gradually emerging out of the “statistical evidence”). The problem with parallel roads increases drastically when speaking of several road platforms on top of each other, as so happens at the entrance and exit of highways (although, normally, geometry helps distinguish this correct solutions).</Para>



										<Para>Regarding time performance and complexity of the algorithms, we knew from the beginning that, in terms of speed GEMMA would have a poor performance due to its nature. Some modifications were made such as the creation of the upper layer of the map in order to speed up some searches in the map, which benefitted Marshal’s algorithm as well. Despite these modifications and other minor ones, GEMMA alone remains slow. Moreover, with adding some more modifications to the Marchal’s algorithm, this solution was speeded up. The difference of time when running both algorithms with the same set of data is noticeable.</Para>


									</Section1>



									<Section1 ID="Sec21">


										<Heading>Applying M-GEMMA to YouTrace</Heading>



										<Para>Regarding the inclusion of these algorithms in YouTrace, the large base map from above (225 km, 730 intersections) needed 186 s to be generated, while with Improved Marchal 125 s were necessary. The results were different, however, Marchal’s algorithm failed in some areas. On a different test, with a set with 87,005 points which corresponded approximately to 1,392 km of trace length, Marchal’s algorithm (either version) took 20 s in the matching process, for GEMMA it took approximately one hour and 79 s for M-GEMMA.</Para>



										<Para>To allow the reader to have a clearer insight on the quality of the results, we then show and describe some snapshots of the map in the same area used for the tests. A map was generated from scratch (starting with an empty map) using a small set of 24 traces (2,926 points). It took a total of 15 s to generate the entire map, with 33% of GEMMA matched points (and obviously 67% of Marchal’s improved version). In Fig. 

											<InternalRef RefID="Fig24">24</InternalRef>
, we can see the overall picture of the final map. We can observe that YouTrace could gather many of the involved roads and crossings. There are, however, some issues. It remains to be observed whether the addition of a large amount of new traces solves this partially or totally. In Fig. 

											<InternalRef RefID="Fig25">25</InternalRef>
, we can see two inferred crossings. Topologically and in terms of correspondence to the original trajectories, both are correct, although geometrically the one on the left seems smoother. This is obviously due to the quality of the traces. Figure 

											<InternalRef RefID="Fig26">26</InternalRef>
 shows a zoom on the right side of the map. The system inferred all the road segments (some of which did not even exist in the base map of the tests of Section 

											<InternalRef RefID="Sec20">7</InternalRef>
).

											<Figure Category="Standard" Float="Yes"
ID="Fig24">


												<Caption Language="En">


													<CaptionNumber>Fig. 24</CaptionNumber>



													<CaptionContent>


														<SimplePara>Map generated from scratch out of 24 traces (2,926 points)</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO23">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig24_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig25">


												<Caption Language="En">


													<CaptionNumber>Fig. 25</CaptionNumber>



													<CaptionContent>


														<SimplePara>Two generated crossings (zoom of Fig. 

															<InternalRef
RefID="Fig24">24</InternalRef>
)
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO24">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig25_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>



											<Figure Category="Standard" Float="Yes"
ID="Fig26">


												<Caption Language="En">


													<CaptionNumber>Fig. 26</CaptionNumber>



													<CaptionContent>


														<SimplePara>Inferred map (zoom of Fig. 

															<InternalRef
RefID="Fig24">24</InternalRef>
)
														</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO25">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig26_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>


									</Section1>



									<Section1 ID="Sec22" Type="Conclusion">


										<Heading>Conclusions</Heading>



										<Para>In this article, we presented an off-line Map Matching algorithm that showed reliability and robustness in regard to the potential incompleteness of the base map at hand. This algorithm is the result of an iterative process in which the authors implemented and tested previous work and added their own new implementations. The result is the integration of two algorithms: Marchal’s algorithm [

											<CitationRef
CitationID="CR6">6</CitationRef>
] and GEMMA. Marchal’s algorithm is used primarily for using topological continuity to infer matches. When ambiguities arise, the portion of the ambiguous segment is isolated and GEMMA is used.
										</Para>



										<Para>M-GEMMA is visibly slower than Marchal’s original algorithm, but its performance is more than acceptable when running for a single user. The scalability to multiple simultaneous users (as is expected in YouTrace) remains to be tested and may demand improvements. Despite this issue, it is preferable to use the integration of both algorithms because of the improvements on having smoother transitions—which have a direct impact on map’s quality.</Para>



										<Para>Although the results represent clear improvements to the state of the art, offline Map-Matching algorithms continue to have problems that none of our solutions could solve. One has to do with the threshold for the maximum matched distances (as previously mentioned, set at 20 m). We fixed this value because, after various observations, it could fit most common situations. However, there are some exceptions where this is not true. For situations where we have parallel roads that do not interchange, and when one of the roads already exists on the map and the other one is absent, all the presented algorithms will assume that this is always the same road. Even when reducing the maximum matched distance threshold, this could not solve all the cases. Figure 

											<InternalRef RefID="Fig27">27</InternalRef>
 describes a typical scenario.

											<Figure Category="Standard" Float="Yes"
ID="Fig27">


												<Caption Language="En">


													<CaptionNumber>Fig. 27</CaptionNumber>



													<CaptionContent>


														<SimplePara>Matching wrongly a parallel road. Part of the trace matches a parallel road segment instead of leaving it unmatched. The matching distance is about 18 m</SimplePara>


													</CaptionContent>


												</Caption>



												<MediaObject ID="MO26">


													<ImageObject Color="Color"
FileRef="MediaObjects/12544_2009_13_Fig27_HTML.jpg" Format="JPEG"
Rendition="HTML" Type="Halftone" />


												</MediaObject>


											</Figure>


										</Para>



										<Para>In terms of the integration into YouTrace, M-GEMMA is presenting satisfying results during the preliminary experiments. Testing this whole system thoroughly demands considerably larger chunks of traces and it is clearly beyond the scope of this paper. Future publications will focus on this task.</Para>



										<Para>The code of M-GEMMA is written in C++ and is available as open source at 

											<ExternalRef>


												<RefSource>http://eden.dei.uc.pt/~camara/files/mgemma.zip</RefSource>



												<RefTarget
Address="http://eden.dei.uc.pt/∼camara/files/mgemma.zip" TargetType="URL" />


											</ExternalRef>
. The reader is invited to download and use this at will.
										</Para>


									</Section1>


								</Body>



								<BodyRef
FileRef="BodyRef/PDF/12544_2009_Article_13.pdf" TargetType="OnlinePDF" />



								<ArticleBackmatter>


									<Bibliography ID="Bib1">


										<Heading>References</Heading>



										<Citation ID="CR1">


											<CitationNumber>1.</CitationNumber>



											<BibUnstructured>NCHRP Project 70–01 (2005) Private-sector provision of congestion data. Probe-based traffic monitoring. State-of-the-practice report . University of Virginia Center for Transportation Studies. Virginia Transportation Research Council, November 21</BibUnstructured>


										</Citation>



										<Citation ID="CR2">


											<CitationNumber>2.</CitationNumber>



											<BibUnstructured>Ben-Akiva M, Bierlaire M, Koutsopoulos HN, Mishalani R (1998) DynaMIT: a simulation-based system for traffic prediction. Proceedings of the DACCORD Short-term forecasting workshop (DACCORD) February, 1998</BibUnstructured>


										</Citation>



										<Citation ID="CR3">


											<CitationNumber>3.</CitationNumber>



											<BibUnstructured>Logi F, Ullrich M, Keller H (2001) Traffic estimation in Munich: practical problems and pragmatic solutions. 2001 IEEE Intelligent Transportation Systems Conference Proceedings—Oakland (CA), USA—August 25-29</BibUnstructured>


										</Citation>



										<Citation ID="CR4">


											<CitationNumber>4.</CitationNumber>



											<BibUnstructured>Travel Time Estimation Using Cell Phones (TTECP) for Highways and Roadways. Department of Electrical and Computer Engineering Florida International University</BibUnstructured>


										</Citation>



										<Citation ID="CR5">


											<CitationNumber>5.</CitationNumber>



											<BibUnstructured>Performance and Limitations of Cellular-Based Traffic Monitoring Systems. CellINT Traffic Solutions (2007) Ohio Transport Engineering Conference</BibUnstructured>


										</Citation>



										<Citation ID="CR6">


											<CitationNumber>6.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>F</Initials>



													<FamilyName>Marchal</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>J</Initials>



													<FamilyName>Hackney</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>KW</Initials>



													<FamilyName>Axhausen</FamilyName>


												</BibAuthorName>



												<Year>2005</Year>



												<ArticleTitle
Language="En">Efficient map-matching of large global positioning system data sets: tests on speed monitoring experiment in ZŸrich</ArticleTitle>



												<JournalTitle>Transp Res Rec</JournalTitle>



												<VolumeID>1935</VolumeID>



												<FirstPage>93</FirstPage>



												<LastPage>100</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.3141/1935-11</Handle>


												</Occurrence>


											</BibArticle>



											<BibUnstructured>Marchal F, Hackney J, Axhausen KW (2005) Efficient map-matching of large global positioning system data sets: tests on speed monitoring experiment in ZŸrich. Transp Res Rec 1935:93–100</BibUnstructured>


										</Citation>



										<Citation ID="CR7">


											<CitationNumber>7.</CitationNumber>



											<BibUnstructured>Edelkamp S, Pereira FC, Sulewski D, Costa H (2008) Collaborative map generation—survey and architecture proposal. In: Hoeven F, Shaick J, Speck SC, Smith MJ (eds) Urbanism on track—application of tracking technologies in urbanism. IOS Press (Research in Urbanism Series, Vol. 1), Amsterdam</BibUnstructured>


										</Citation>



										<Citation ID="CR8">


											<CitationNumber>8.</CitationNumber>



											<BibArticle>


												<BibAuthorName>


													<Initials>MA</Initials>



													<FamilyName>Quddus</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>WY</Initials>



													<FamilyName>Ochieng</FamilyName>


												</BibAuthorName>



												<BibAuthorName>


													<Initials>RB</Initials>



													<FamilyName>Noland</FamilyName>


												</BibAuthorName>



												<Year>2007</Year>



												<ArticleTitle
Language="En">Current map-matching algorithms for transport applications: state-of-the art and future research directions</ArticleTitle>



												<JournalTitle>Transp Res, Part C Emerg Technol</JournalTitle>



												<VolumeID>15</VolumeID>



												<IssueID>5</IssueID>



												<FirstPage>312</FirstPage>



												<LastPage>328</LastPage>



												<Occurrence Type="DOI">


													<Handle>10.1016/j.trc.2007.05.002</Handle>


												</Occurrence>



												<BibComments>ISSN 0968-090X</BibComments>


											</BibArticle>



											<BibUnstructured>Quddus MA, Ochieng WY, Noland RB (2007) Current map-matching algorithms for transport applications: state-of-the art and future research directions. Transp Res, Part C Emerg Technol 15(5):312–328, ISSN 0968-090X</BibUnstructured>


										</Citation>



										<Citation ID="CR9">


											<CitationNumber>9.</CitationNumber>



											<BibUnstructured>Greenfeld JS (2002) Matching GPS observations to locations on a digital map. In proceedings of the 81st annual meeting of the transportation research board, January, Washington D.C.</BibUnstructured>


										</Citation>



										<Citation ID="CR10">


											<CitationNumber>10.</CitationNumber>



											<BibUnstructured>Quddus MA (2006) High integrity map-matching algorithms for advanced transport telematics applications, PhD Thesis. Centre for Transport Studies, Imperial College London, UK</BibUnstructured>


										</Citation>



										<Citation ID="CR11">


											<CitationNumber>11.</CitationNumber>



											<BibUnstructured>Quddus MA, Noland RB, Ochieng WY (2006) The effects of navigation sensors and digital map quality on the performance of map-matching algorithms. Presented at the Transportation Research Board (TRB) Annual Meeting of the Transportation Research Board, Washington D.C., January 2006</BibUnstructured>


										</Citation>



										<Citation ID="CR12">


											<CitationNumber>12.</CitationNumber>



											<BibUnstructured>Chawathe S (2007) Segment-based map matching. Proceedings of the 2007 IEEE Intelligent Vehicles Symposium, Istanbul, Turkey</BibUnstructured>


										</Citation>



										<Citation ID="CR13">


											<CitationNumber>13.</CitationNumber>



											<BibBook>


												<BibAuthorName>


													<Initials>D</Initials>



													<FamilyName>Goldberg</FamilyName>


												</BibAuthorName>



												<Year>1989</Year>



												<BookTitle>Genetic algorithms in search, optimization and machine learning</BookTitle>



												<PublisherName>Kluwer Academic</PublisherName>



												<PublisherLocation>Boston</PublisherLocation>



												<Occurrence Type="ZLBID">


													<Handle>0721.68056</Handle>


												</Occurrence>


											</BibBook>



											<BibUnstructured>Goldberg D (1989) Genetic algorithms in search, optimization and machine learning. Kluwer Academic, Boston</BibUnstructured>


										</Citation>



										<Citation ID="CR14">


											<CitationNumber>14.</CitationNumber>



											<BibUnstructured>Open Street Map platform. URL: 

												<ExternalRef>


													<RefSource>http://www.openstreetmap.org</RefSource>



													<RefTarget
Address="http://www.openstreetmap.org" TargetType="URL" />


												</ExternalRef>


											</BibUnstructured>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:DateLoaded>2010-04-20T17:45:33.487965+02:00</meta:DateLoaded>

					<meta:Authors>

						<meta:Author>Pereira, Francisco Câmara</meta:Author>

						<meta:Author>Costa, Hugo</meta:Author>

						<meta:Author>Pereira, Nuno Martinho</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution geo="-8.4108308,40.2111837,0">

							<meta:OrgName>Universidade de Coimbra</meta:OrgName>

							<meta:GeoOrg>-8.4108308,40.2111837,0#Universidade de Coimbra</meta:GeoOrg>

							<meta:Country>Portugal</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2009-11-04</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1007/s12544-009-0013-6</meta:DOI>

					<meta:Title>An off-line map-matching algorithm for incomplete map databases</meta:Title>

					<meta:ISXN>1866-8887</meta:ISXN>

					<meta:PubName>Springer</meta:PubName>

					<meta:Journal>European Transport Research Review</meta:Journal>

					<meta:Publication>European Transport Research Review</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Engineering</meta:Subject>

						<meta:Subject
Type="Secondary">Regional/Spatial Science</meta:Subject>

						<meta:Subject
Type="Secondary">Automotive Engineering</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images />

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1471-2105</JournalID>


						<JournalPrintISSN>1471-2105</JournalPrintISSN>


						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>


						<JournalTitle>BMC Bioinformatics</JournalTitle>


						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Bioinformatics</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Microarrays</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences </JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Combinatorial Libraries</JournalSubject>


							<JournalSubject Priority="6"
Type="Secondary">Algorithms</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>7</VolumeIDStart>


							<VolumeIDEnd>7</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>Suppl 4</IssueIDStart>


								<IssueIDEnd>Suppl 4</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2006</Year>


										<Month>12</Month>


										<Day>12</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Chen et al; licensee BioMed Central Ltd</CopyrightHolderName>


									<CopyrightYear>2006</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1471-2105-7-S4-S4</ArticleID>


									<ArticleDOI>10.1186/1471-2105-7-S4-S4</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">A fast parallel algorithm for finding the longest common sequence of multiple biosequences</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Research</ArticleCategory>


									<ArticleFirstPage>S4</ArticleFirstPage>


									<ArticleLastPage>S4</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year />


											<Month />


											<Day />


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year />


											<Month />


											<Day />


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Chen et al; licensee BioMed Central Ltd</CopyrightHolderName>


										<CopyrightYear>2006</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>7</VolumeIDStart>


										<VolumeIDEnd>7</VolumeIDEnd>


										<IssueIDStart>Suppl 4</IssueIDStart>


										<IssueIDEnd>Suppl 4</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Yixin</GivenName>


												<FamilyName>Chen</FamilyName>


											</AuthorName>


											<Contact>

												<Email>chen@cse.wustl.edu</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Andrew</GivenName>


												<FamilyName>Wan</FamilyName>


											</AuthorName>


											<Contact>

												<Email>awan@wustl.edu</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I2">

											<AuthorName DisplayOrder="Western">

												<GivenName>Wei</GivenName>


												<FamilyName>Liu</FamilyName>


											</AuthorName>


											<Contact>

												<Email>yzliuwei@126.com</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO 63130, USA</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


										<Affiliation ID="I2">

											<OrgName>Department of Computer Science, Yangzhou University, Yangzhou 225009, China</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>Searching for the longest common sequence (LCS) of multiple biosequences is one of the most fundamental tasks in bioinformatics. In this paper, we present a parallel algorithm named FAST_LCS to speedup the computation for finding LCS.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>A fast parallel algorithm for LCS is presented. The algorithm first constructs a novel successor table to obtain all the identical pairs and their levels. It then obtains the LCS by tracing back from the identical character pairs at the last level. Effective pruning techniques are developed to significantly reduce the computational complexity. Experimental results on gene sequences in the 

												<Emphasis Type="Italic">tigr</Emphasis>
 database show that our algorithm is optimal and much more efficient than other leading LCS algorithms.
											</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusion</Heading>


											<Para>We have developed one of the fastest parallel LCS algorithms on an MPP parallel computing model. For two sequences 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 with lengths 

												<Emphasis Type="Italic">n</Emphasis>
 and 

												<Emphasis Type="Italic">m</Emphasis>
 , respectively, the memory required is max{4*( 

												<Emphasis Type="Italic">n</Emphasis>
 +1)+4*( 

												<Emphasis Type="Italic">m</Emphasis>
 +1), 

												<Emphasis Type="Italic">L</Emphasis>
 }, where 

												<Emphasis Type="Italic">L</Emphasis>
 is the number of identical character pairs. The time complexity is O( 

												<Emphasis Type="Italic">L</Emphasis>
 ) for sequential execution, and 

												<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )|) for parallel execution, where |LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )| is the length of the LCS of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 . For 

												<Emphasis Type="Italic">n</Emphasis>
 sequences 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 , the time complexity is O( 

												<Emphasis Type="Italic">L</Emphasis>
 ) for sequential execution, and 

												<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )|) for parallel execution. Experimental results support our analysis by showing significant improvement of the proposed method over other leading LCS algorithms.
											</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Keyword>parallel</Keyword>


										<Keyword>common</Keyword>


										<Keyword>biosequences</Keyword>


										<Keyword>longest</Keyword>


										<Keyword>algorithm</Keyword>


										<Keyword>sequence</Keyword>


										<Keyword>finding</Keyword>


										<Keyword>fast</Keyword>


										<Keyword>multiple</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_79815">

										<Heading>Background</Heading>


										<Para>Biological sequence 

											<CitationRef
CitationID="B1">1</CitationRef>
 can be represented as a sequence of symbols. For instance, a protein 

											<CitationRef
CitationID="B2">2</CitationRef>
 is a sequence of 20 different letters (amino acids), and DNA sequences (genes) can be represented as sequences of four letters A, C, G and T corresponding to the four sub-molecules forming DNA. When a new biosequence is found, we want to know what other sequences it is most similar to. Sequence comparison 

											<CitationRef
CitationID="B3">3</CitationRef>


											<CitationRef
CitationID="B4">4</CitationRef>


											<CitationRef
CitationID="B5">5</CitationRef>
 has been used successfully to establish the link between cancer-causing genes and a gene evolved in normal growth and development. One way of detecting the similarity of two or more sequences is to find their LCS.
										</Para>


										<Para>The LCS problem is to find a substring that is common to two or more given strings and is the longest one of such strings. Since the LCS problem is essentially a special case of the global sequence alignment, all the algorithms for the sequence alignment can be used to solve the LCS problem. Presented in 1981, the Smith-Waterman algorithm 

											<CitationRef
CitationID="B6">6</CitationRef>
 is a well known LCS algorithm which was evolved from the Needleman-Wunsch 

											<CitationRef
CitationID="B7">7</CitationRef>
 algorithm, and can guarantee the correctness. Aho and et al. 

											<CitationRef
CitationID="B8">8</CitationRef>
 gave a lower bound of 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">mn</Emphasis>
 ) on time for the LCS problem using a decision tree model. It is shown in 

											<CitationRef
CitationID="B9">9</CitationRef>
 that the problem can be solved in 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">mn</Emphasis>
 ) time using 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">mn</Emphasis>
 ) space by dynamic programming. Mayers and Miller 

											<CitationRef
CitationID="B10">10</CitationRef>
 used the technique proposed by Hirschberg 

											<CitationRef
CitationID="B11">11</CitationRef>
 to reduce the space complexity to 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">m</Emphasis>
 + 

											<Emphasis Type="Italic">n</Emphasis>
 ) on the premise of the same time complexity.
										</Para>


										<Para>To further reduce the computation time, some parallel algorithms 

											<CitationRef
CitationID="B12">12</CitationRef>


											<CitationRef
CitationID="B13">13</CitationRef>


											<CitationRef
CitationID="B14">14</CitationRef>
 have been proposed for different computational models. For the CREW-PRAM model, Aggarwal 

											<CitationRef
CitationID="B15">15</CitationRef>
 and Apostolico et al 

											<CitationRef
CitationID="B16">16</CitationRef>
 independently proposed an 

											<Emphasis Type="Italic">O</Emphasis>
 (log 

											<Emphasis Type="Italic">m</Emphasis>
 log 

											<Emphasis Type="Italic">n</Emphasis>
 ) time algorithm using 

											<Emphasis Type="Italic">mn</Emphasis>
 /log 

											<Emphasis Type="Italic">m</Emphasis>
 processors. Lu et al 

											<CitationRef
CitationID="B17">17</CitationRef>
 designed two parallel LCS algorithms, one uses 

											<Emphasis Type="Italic">mn</Emphasis>
 /log 

											<Emphasis Type="Italic">m</Emphasis>
 processors with a time complexity of 

											<Emphasis Type="Italic">O</Emphasis>
 (log 

											<Superscript>2</Superscript>


											<Emphasis Type="Italic">n</Emphasis>
 +log 

											<Emphasis Type="Italic">m</Emphasis>
 ), and the other uses 

											<Emphasis Type="Italic">mn/</Emphasis>
 (log 

											<Superscript>2</Superscript>


											<Emphasis Type="Italic">m</Emphasis>
 loglog 

											<Emphasis Type="Italic">m</Emphasis>
 ) processors with a running time of 

											<Emphasis Type="Italic">O</Emphasis>
 (log 

											<Superscript>2</Superscript>


											<Emphasis Type="Italic">m</Emphasis>
 loglog 

											<Emphasis Type="Italic">m</Emphasis>
 ). For the CRCW-PRAM model, Apostolico et al 

											<CitationRef
CitationID="B16">16</CitationRef>
 gave an 

											<Emphasis Type="Italic">O</Emphasis>
 (log 

											<Emphasis Type="Italic">n</Emphasis>
 (loglog 

											<Emphasis Type="Italic">m</Emphasis>
 ) 

											<Superscript>2</Superscript>
 ) time algorithm using 

											<Emphasis Type="Italic">mn</Emphasis>
 /loglogm processors. Babu and Saxena 

											<CitationRef
CitationID="B18">18</CitationRef>
 improved these algorithms for the CRCW-PRAM model. They designed an 

											<Emphasis Type="Italic">O</Emphasis>
 (log 

											<Emphasis Type="Italic">m</Emphasis>
 ) algorithm with 

											<Emphasis Type="Italic">mn</Emphasis>
 processors and an O(log 

											<Superscript>2</Superscript>


											<Emphasis Type="Italic">n</Emphasis>
 ) time parallel algorithm. Many parallel LCS algorithms have also been proposed using systolic arrays. Robert et al 

											<CitationRef
CitationID="B19">19</CitationRef>
 proposed a parallel algorithm with 

											<Emphasis Type="Italic">n</Emphasis>
 + 5 

											<Emphasis Type="Italic">m</Emphasis>
 steps using 

											<Emphasis Type="Italic">m</Emphasis>
 ( 

											<Emphasis Type="Italic">m</Emphasis>
 +1) processing elements. Chang et al 

											<CitationRef
CitationID="B20">20</CitationRef>
 put forward an algorithm with 4 

											<Emphasis Type="Italic">n</Emphasis>
 +2 

											<Emphasis Type="Italic">m</Emphasis>
 steps using 

											<Emphasis Type="Italic">mn</Emphasis>
 processing elements. Luce et al 

											<CitationRef
CitationID="B21">21</CitationRef>
 designed a systolic array with 

											<Emphasis Type="Italic">m</Emphasis>
 ( 

											<Emphasis Type="Italic">m</Emphasis>
 +1)/2 processing elements and 

											<Emphasis Type="Italic">n</Emphasis>
 +3 

											<Emphasis Type="Italic">m</Emphasis>
 + 

											<Emphasis Type="Italic">q</Emphasis>
 steps where 

											<Emphasis Type="Italic">q</Emphasis>
 is the length of the LCS. Freschi and Bogliolo 

											<CitationRef
CitationID="B22">22</CitationRef>
 addressed the problem of computing the LCS between run-length-encoded (RLE) strings. Their algorithm requires 

											<Emphasis Type="Italic">O</Emphasis>
 ( 

											<Emphasis Type="Italic">m</Emphasis>
 + 

											<Emphasis Type="Italic">n</Emphasis>
 ) steps on a systolic array of 

											<Emphasis Type="Italic">M</Emphasis>
 + 

											<Emphasis Type="Italic">N</Emphasis>
 processing elements, where 

											<Emphasis Type="Italic">M</Emphasis>
 and 

											<Emphasis Type="Italic">N</Emphasis>
 are the lengths of the original strings and 

											<Emphasis Type="Italic">m</Emphasis>
 and 

											<Emphasis Type="Italic">n</Emphasis>
 are the number of runs in their RLE representation.
										</Para>


										<Para>For the LCS problem of multiple sequences, the time complexity tends to grow very fast when the number of the sequences increases. For instance, using the Smith-Waterman algorithm to solve the LCS for multiple sequences, the time complexity is, where 

											<Emphasis Type="Italic">n</Emphasis>
 is the number of sequences, and 

											<Emphasis Type="Italic">n</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">i</Emphasis>

											</Subscript>
 is the length of the 

											<Emphasis Type="Italic">i</Emphasis>
 th sequence. It is not practicable when 

											<Emphasis Type="Italic">n</Emphasis>
 is large. Some improvements have been made on the algorithm. The MSA program 

											<CitationRef
CitationID="B23">23</CitationRef>
 can process up to ten closely related sequences. It is an implementation of the Carrillo and Lipman algorithm 

											<CitationRef
CitationID="B24">24</CitationRef>
 that identifies in advance the portions of the hyperspace not contributing to the solution and excludes them from the computation. Stoye described a new divide and conquer algorithm DCA 

											<CitationRef
CitationID="B25">25</CitationRef>
 that extends the capability of MSA. Recently, OMA 

											<CitationRef
CitationID="B26">26</CitationRef>
 , an iterative implementation of DCA is proposed to speed up the DCA strategy and reduce memory requirements. Based on Feng and Doolittle's algorithm 

											<CitationRef
CitationID="B28">28</CitationRef>
 , Clustal-W 

											<CitationRef
CitationID="B27">27</CitationRef>
 is one of the most widely used multiple sequence alignment software that can also be used for LCS computation.
										</Para>


									</Section1>


									<Section1 ID="Sec_56796">

										<Heading>Results</Heading>


										<Para>In this paper, we present a fast algorithm named FAST_LCS for efficient computation of LCS. The algorithm first seeks the successors of the initial identical character pairs according to a successor table to obtain all the identical pairs and their levels. Then by tracing back from the identical character pair in the last level, it obtains the result of LCS.</Para>


										<Para>The key technique of our algorithm is the use of several effective 

											<Emphasis
Type="Bold">pruning operations</Emphasis>
 . In the process of generating the successors, pruning techniques can remove the identical pairs which can not generate the LCS so as to reduce the search space and accelerate the search speed. The algorithm can be extended to find the LCS of multiple biosequences.
										</Para>


										<Para>Experimental results on the gene sequences of the 

											<Emphasis Type="Italic">tigr</Emphasis>
 database, using an MPP parallel computer Shenteng 1800, show that our algorithm can obtain the exact optimal results and is much faster than some other leading LCS algorithms.
										</Para>


									</Section1>


									<Section1 ID="Sec_67248">

										<Heading>Conclusion</Heading>


										<Para>In this paper, we have developed FAST_LCS, one of the fastest parallel LCS algorithms on an MPP parallel computing model. For two sequences 

											<Emphasis Type="Italic">X</Emphasis>
 and 

											<Emphasis Type="Italic">Y</Emphasis>
 with lengths 

											<Emphasis Type="Italic">n</Emphasis>
 and 

											<Emphasis Type="Italic">m</Emphasis>
 , respectively, the memory complexity of FAST_LCS is max{4*( 

											<Emphasis Type="Italic">n</Emphasis>
 +1)+4*( 

											<Emphasis Type="Italic">m</Emphasis>
 +1), 

											<Emphasis Type="Italic">L</Emphasis>
 }, where 

											<Emphasis Type="Italic">L</Emphasis>
 is the number of identical character pairs. The time complexity is O 

											<Emphasis Type="Italic">(L</Emphasis>
 ) for sequential execution of the algorithm, and 

											<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

											<Emphasis Type="Italic">X</Emphasis>
 , 

											<Emphasis Type="Italic">Y</Emphasis>
 )|) for parallel execution, where |LCS( 

											<Emphasis Type="Italic">X</Emphasis>
 , 

											<Emphasis Type="Italic">Y</Emphasis>
 )| is the length of the LCS of 

											<Emphasis Type="Italic">X</Emphasis>
 and 

											<Emphasis Type="Italic">Y</Emphasis>
 . The algorithm can be extended to solve the LCS for multiple biosequences. For 

											<Emphasis Type="Italic">n</Emphasis>
 sequences 

											<Emphasis Type="Italic">X</Emphasis>


											<Subscript>1</Subscript>
 , 

											<Emphasis Type="Italic">X</Emphasis>


											<Subscript>2</Subscript>
 , ..., 

											<Emphasis Type="Italic">X</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">n</Emphasis>

											</Subscript>
 , the time complexity is O 

											<Emphasis Type="Italic">(L</Emphasis>
 ) for sequential execution, and 

											<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

											<Emphasis Type="Italic">X</Emphasis>


											<Subscript>1</Subscript>
 , 

											<Emphasis Type="Italic">X</Emphasis>


											<Subscript>2</Subscript>
 , ..., 

											<Emphasis Type="Italic">X</Emphasis>


											<Subscript> 

												<Emphasis Type="Italic">n</Emphasis>

											</Subscript>
 )|), which is independent of the number of sequences, for parallel execution. Experimental results support our analysis by showing significant improvement of the proposed method over some other leading LCS algorithms.
										</Para>


									</Section1>


									<Section1 ID="Sec_04977">

										<Heading>Methods</Heading>


										<Section2 ID="Sec_21030">

											<Heading>The identical character pair and its successor table</Heading>


											<Para>Let 

												<Emphasis Type="Italic">X</Emphasis>
 = ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), 

												<Emphasis Type="Italic">Y</Emphasis>
 = ( 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">m</Emphasis>

												</Subscript>
 ) be two biosequences, where 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 ∈ { 

												<Emphasis Type="Italic">A</Emphasis>
 , 

												<Emphasis Type="Italic">C</Emphasis>
 , 

												<Emphasis Type="Italic">G</Emphasis>
 , 

												<Emphasis Type="Italic">T</Emphasis>
 }. We can define an array CH of the four characters so that CH(1) = " 

												<Emphasis Type="Italic">A</Emphasis>
 ", CH(2)= "C", CH(3)= "G" and CH(4)= "T". To find the LCS of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 , we first build the successor tables of the identical characters for the two strings. The successor tables of the identical characters of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 are denoted as 

												<Emphasis Type="Italic">TX</Emphasis>
 and 

												<Emphasis Type="Italic">TY</Emphasis>
 , which are two dimensional arrays of size 4(n+1) × 4*(m+1). For sequence 

												<Emphasis Type="Italic">X</Emphasis>
 = ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) in table 

												<Emphasis Type="Italic">TX</Emphasis>
 is defined as follows.
											</Para>


											<Para>Here, 

												<Emphasis Type="Italic">SX</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) = { 

												<Emphasis Type="Italic">k</Emphasis>
 | 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 = CH( 

												<Emphasis Type="Italic">i</Emphasis>
 ), 

												<Emphasis Type="Italic">k</Emphasis>
 &gt; 

												<Emphasis Type="Italic">j</Emphasis>
 }, where 

												<Emphasis Type="Italic">i</Emphasis>
 = 1,2,3,4 and 

												<Emphasis Type="Italic">j</Emphasis>
 = 0,1,... 

												<Emphasis Type="Italic">n</Emphasis>
 . It can be seen from the definition that if 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) is not "-", it indicates the position of the next character identical to CH( 

												<Emphasis Type="Italic">i</Emphasis>
 ) after the 

												<Emphasis Type="Italic">j</Emphasis>
 th position in sequence 

												<Emphasis Type="Italic">X</Emphasis>
 , otherwise it means there is no such character after the 

												<Emphasis Type="Italic">j</Emphasis>
 th position.
											</Para>


											<Para>Let 

												<Emphasis Type="Italic">X</Emphasis>
 = "T G C A T A" and 

												<Emphasis Type="Italic">Y</Emphasis>
 = "A T C T G A T". Their successor tables 

												<Emphasis Type="Italic">TX</Emphasis>
 and 

												<Emphasis Type="Italic">TY</Emphasis>
 are shown in Table 

												<InternalRef RefID="T1">1</InternalRef>
 .
											</Para>


											<Table Float="No" ID="T1">

												<Caption Language="En">

													<CaptionNumber>Table 1</CaptionNumber>


													<CaptionContent>

														<SimplePara>Their successor tables 

															<Emphasis
Type="Italic">TX</Emphasis>
 and 

															<Emphasis
Type="Italic">TY</Emphasis>
 in Example 1.
														</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="3">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">TX</Emphasis>
 :
																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">i</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>CH( 

																	<Emphasis
Type="Italic">i</Emphasis>
 )
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0 1 2 3 4 5 6</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">A</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>4 4 4 4 6 6 -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">C</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>3 3 3 - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">G</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2 2 - - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">T</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1 5 5 5 5 - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">TY:</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">i</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>CH( 

																	<Emphasis
Type="Italic">i</Emphasis>
 )
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0 1 2 3 4 5 6 7</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">A</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1 6 6 6 6 6 - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">C</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>3 3 3 - - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">G</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>5 5 5 5 5 - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">T</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2 2 4 4 7 7 7 -</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


											<Para>For the sequences 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 , if 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 = 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 = CH( 

												<Emphasis Type="Italic">k</Emphasis>
 ), we call them an identical pair of CH( 

												<Emphasis Type="Italic">k</Emphasis>
 ) and denote it as ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ). The set of all the identical character pairs of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 is denoted as 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 ).
											</Para>


											<Para>Let ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) and ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ) be two identical character pairs of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 . If 

												<Emphasis Type="Italic">i</Emphasis>
 &lt; 

												<Emphasis Type="Italic">k</Emphasis>
 and 

												<Emphasis Type="Italic">j</Emphasis>
 &lt; 

												<Emphasis Type="Italic">l</Emphasis>
 , we call ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) a predecessor of ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ), or ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ) a successor of ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ), and denote the relationship as ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) &lt; ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ).
											</Para>


											<Para>Let 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) = {( 

												<Emphasis Type="Italic">r</Emphasis>
 , 

												<Emphasis Type="Italic">s</Emphasis>
 ) | ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) &lt; ( 

												<Emphasis Type="Italic">r</Emphasis>
 , 

												<Emphasis Type="Italic">s</Emphasis>
 ), ( 

												<Emphasis Type="Italic">r</Emphasis>
 , 

												<Emphasis Type="Italic">s</Emphasis>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X,Y</Emphasis>
 )} be the set of all the successors of identical pair ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ), if ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 )∈ 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) and there is no ( 

												<Emphasis Type="Italic">k</Emphasis>
 ', 

												<Emphasis Type="Italic">l</Emphasis>
 ')∈ 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) satisfying the condition: ( 

												<Emphasis Type="Italic">k</Emphasis>
 ', 

												<Emphasis Type="Italic">l</Emphasis>
 ') &lt; ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ) and 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k'</Emphasis>

												</Subscript>
 = 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 , we call ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ) the direct successor of ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ), and denote the relationship as ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )≺( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ).
											</Para>


											<Para>If an identical pair ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) ∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X, Y</Emphasis>
 ) and there is no ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X, Y</Emphasis>
 ) satisfying ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 ) &lt; ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ), we call ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) an initial identical pair.
											</Para>


											<Para>For an identical pair ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X, Y</Emphasis>
 ), its level is defined as follows:
											</Para>


											<Para>From the definitions above, the following theorems can be easily deduced:</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem1.</Emphasis>
 Denote the length of the LCS of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 as |LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )|, then |LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )| = max{level ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )|( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )}.
											</Para>


											<Para> 

												<Emphasis Type="Bold">Proof:</Emphasis>
 Suppose the identical character pairs corresponding to the longest common subsequence of 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 are ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 1
												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>
 1
												</Subscript>
 ), ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 2
												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>
 2
												</Subscript>
 ), ..., ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ir</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">jr</Emphasis>

												</Subscript>
 ), here 

												<Emphasis Type="Italic">r</Emphasis>
 = |LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )|. Since ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 ) is an initial identical character pair, we have: ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 )≺( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>
 +1
												</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>
 +1
												</Subscript>
 ), for 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,..., 

												<Emphasis Type="Italic">r</Emphasis>
 -1, and the level of ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ik</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">jk</Emphasis>

												</Subscript>
 ) is 

												<Emphasis Type="Italic">k</Emphasis>
 . Then we can conclude that the maximal level of all the identical character pairs is 

												<Emphasis Type="Italic">r</Emphasis>
 , i.e. 

												<Emphasis Type="Italic">r</Emphasis>
 = max{level( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )|( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )}. The reason is as follows: if 

												<Emphasis Type="Italic">r</Emphasis>
 is not the maximal level of the identical character pairs of 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 , there must be an integer 

												<Emphasis Type="Italic">r</Emphasis>
 ' &gt; 

												<Emphasis Type="Italic">r</Emphasis>
 and identical character pairs: ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 1'
												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>
 1
												</Subscript>
 ,)≺( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 2'
												</Subscript>
 , 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>
 2
												</Subscript>
 ,)≺ .....≺( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ir</Emphasis>

												</Subscript>
 ', 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">jr</Emphasis>

												</Subscript>
 '). It corresponds to another common subsequence of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 with length 

												<Emphasis Type="Italic">r</Emphasis>
 ' &gt; 

												<Emphasis Type="Italic">r</Emphasis>
 . This is in contradiction with the condition 

												<Emphasis Type="Italic">r</Emphasis>
 = |LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )|.
											</Para>


										</Section2>


										<Section2 ID="Sec_15744">

											<Heading>The operation of producing successors</Heading>


											<Para>In our algorithm, all direct successors of all the initial identical character pairs are first produced in parallel using the successor tables. Then the direct successors of all those successors produced in the first step are generated in parallel. Repeat these operations of generating the direct successors until no more successors could be produced. Therefore, producing all the direct successors for the identical character pairs is a basic operation in our algorithm.</Para>


											<Para>For an identical character pair ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 ), the operation of producing all its direct successors is as follows:
											</Para>


											<Para>( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) →{( 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k,i</Emphasis>
 ), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k,j</Emphasis>
 ))| 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k,i</Emphasis>
 )≠'-' and 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k,j</Emphasis>
 )≠'-'}     (3)
											</Para>


											<Para>From (3) we can see that this operation is to couple the elements of the 

												<Emphasis Type="Italic">i</Emphasis>
 th column of 

												<Emphasis Type="Italic">TX</Emphasis>
 and the 

												<Emphasis Type="Italic">j</Emphasis>
 th column of 

												<Emphasis Type="Italic">TY</Emphasis>
 to get the pairs.
											</Para>


											<Para>For instance, the successors of the identical character pair (2,5) in Example 1 can be obtained by coupling the elements of the 2 

												<Superscript>nd</Superscript>
 column of 

												<Emphasis Type="Italic">TX</Emphasis>
 and the 5 

												<Superscript>th</Superscript>
 column of 

												<Emphasis Type="Italic">TY</Emphasis>
 . They are (4, 6), (3, -), (-, -) and (5, 7). Here (3, -) and (-,-) do not represent identical character pairs, they only indicate the end of the process of searching for the successors in the branches they located. After discarding (3,-) and (-,-), the successors of (2, 5) are just (4, 6) and (5, 7).
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem 2.</Emphasis>
 For an identical character pair ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ), the method in (3) above can produce all its direct successors.
											</Para>


											<Para> 

												<Emphasis Type="Bold">Proof:</Emphasis>
 By (3), we can produce all direct successors ( 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 ), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )), where 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, of ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ). According to (1), 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 ) is the location of the nearest character identical to CH( 

												<Emphasis Type="Italic">k</Emphasis>
 ) after 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 in string 

												<Emphasis Type="Italic">X</Emphasis>
 , and 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) is the location of the nearest such character of CH( 

												<Emphasis Type="Italic">k</Emphasis>
 ) after 

												<Emphasis Type="Italic">y</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 in string 

												<Emphasis Type="Italic">Y</Emphasis>
 . This means that identical pairs ( 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 ), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )), where 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, contain all the direct successors of ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ). Consequently, by the same operation on the newly generated identical pairs ( 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 ), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 )), where 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, we can get all of their direct successors. It can be seen that by repeating this operation of producing successors, we can obtain all the successors of ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ).
											</Para>


											<Para>It is obvious that ( 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 0), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 0)), where 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, are all the initial identical pairs of 

												<Emphasis Type="Italic">X</Emphasis>
 and 

												<Emphasis Type="Italic">Y</Emphasis>
 . By Theorem 2, we know that starting from these initial identical pairs, all the other identical pairs of 

												<Emphasis Type="Italic">X, Y</Emphasis>
 , and their levels can be produced.
											</Para>


										</Section2>


										<Section2 ID="Sec_05543">

											<Heading>The operations of pruning</Heading>


											<Para>In the process of generating the successors, pruning techniques can be implemented to remove the identical pairs which can not generate the LCS so as to reduce the search space and improve the efficiency.</Para>


											<Section3 ID="Sec_85481">

												<Heading>Pruning Operation 1</Heading>


												<Para>If on the same level, there are two identical character pairs ( 

													<Emphasis Type="Italic">i</Emphasis>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ) and 

													<Emphasis
Type="Italic">(k, l)</Emphasis>
 satisfying ( 

													<Emphasis Type="Italic">k</Emphasis>
 , 

													<Emphasis Type="Italic">l</Emphasis>
 ) 

													<Emphasis
Type="Italic">&gt;</Emphasis>
 ( 

													<Emphasis Type="Italic">i</Emphasis>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ), then ( 

													<Emphasis Type="Italic">k</Emphasis>
 , 

													<Emphasis Type="Italic">l</Emphasis>
 ) can be pruned without affecting the correctness of the algorithm in obtaining the LCS of 

													<Emphasis Type="Italic">X</Emphasis>
 and 

													<Emphasis Type="Italic">Y</Emphasis>
 .
												</Para>


												<Section4 ID="Sec_27998">

													<Heading>Rationale</Heading>


													<Para>The reason we can prune ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) is as follows. Suppose the two identical character pairs ( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) and ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) are produced by the identical pairs ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>1</Subscript>
 ) and ( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript>1</Subscript>
 ) on the previous level. Let the LCS produced via ( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript>1</Subscript>
 ) and ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) be 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript>1</Subscript>


														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript>2</Subscript>
 ... 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>

														</Subscript>


														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">r</Emphasis>

														</Subscript>
 , here 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>

														</Subscript>
 corresponds to ( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript>1</Subscript>
 ) and 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 corresponds to ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ). Similarly, let the subsequence produced via ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>1</Subscript>
 ) and ( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) be 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript>1</Subscript>


														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript>2</Subscript>
 ... 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>

														</Subscript>


														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">s</Emphasis>

														</Subscript>
 ... 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">q</Emphasis>

														</Subscript>
 , here, 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>

														</Subscript>
 corresponds to ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>1</Subscript>
 ) and 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 corresponds to ( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ). Since ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 )&gt;( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ), by Theorem 2 ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) must be produced after ( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ). Then there must exist 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">s</Emphasis>

														</Subscript>
 ( 

														<Emphasis
Type="Italic">m</Emphasis>
 +1 &lt; 

														<Emphasis
Type="Italic">s</Emphasis>
 &lt; 

														<Emphasis
Type="Italic">q</Emphasis>
 ) corresponding to ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ). Since 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">r</Emphasis>

														</Subscript>
 and 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">s</Emphasis>

														</Subscript>


														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">s</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">q</Emphasis>

														</Subscript>
 are both the local longest common subsequences obtained by the operations of producing successors on ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ), we have " 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">r</Emphasis>

														</Subscript>
 " = " 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">s</Emphasis>

														</Subscript>


														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">s</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">b</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">q</Emphasis>

														</Subscript>
 " which means 

														<Emphasis
Type="Italic">q-s = r-(m+1)</Emphasis>
 , and 

														<Emphasis
Type="Italic">q = r</Emphasis>
 +s-(m+1). Since 

														<Emphasis
Type="Italic">s&gt;m+1</Emphasis>
 , we have 

														<Emphasis
Type="Italic">q &gt; r</Emphasis>
 . Therefore the subsequence " 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>

														</Subscript>


														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">a</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">r</Emphasis>

														</Subscript>
 ", which is produced on the 

														<Emphasis
Type="Italic">m</Emphasis>
 th level via ( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript>1</Subscript>
 ), can not be included in the LCS of 

														<Emphasis
Type="Italic">X</Emphasis>
 and 

														<Emphasis
Type="Italic">Y</Emphasis>
 , and ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) can be pruned without affecting the algorithm to get the LCS of 

														<Emphasis
Type="Italic">X</Emphasis>
 and 

														<Emphasis
Type="Italic">Y</Emphasis>
 .
													</Para>


													<Para>This pruning operation can be implemented to remove all those redundant identical pairs. After each level of identical pairs are generated, the algorithm checks all the newly generated identical pairs on the same level to find all such identical pairs ( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) and ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) satisfying ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ) &gt; ( 

														<Emphasis
Type="Italic">i</Emphasis>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) and then prune ( 

														<Emphasis
Type="Italic">k</Emphasis>
 , 

														<Emphasis
Type="Italic">l</Emphasis>
 ).
													</Para>


													<Para>For instance, (4, 6) and (5, 7) in Example 1 are the successors of the identical pair (2, 5). Since they are on the same level and (4, 6)≺(5, 7), we can prune (5, 7).</Para>


													<Para>For another identical character pair (1,2) in Example 1, its successors are (4,6), (3,3), (2,5) and (5,2) which are obtained by coupling the 1 

														<Superscript> 

															<Emphasis
Type="Italic">st</Emphasis>

														</Superscript>
 column of 

														<Emphasis
Type="Italic">TX</Emphasis>
 and the 2 

														<Superscript> 

															<Emphasis
Type="Italic">nd</Emphasis>

														</Superscript>
 column of 

														<Emphasis
Type="Italic">TY</Emphasis>
 .
													</Para>


													<Para>Since (3, 3) &lt; (4, 6), (4, 6) can be omitted by pruning operation 1. On the next level, the successors of (3, 3), (2, 5) and (5, 2) are (4, 6), (5, 4), (5, 7) and (6, 6). Since identical pair (6, 6) is a successor of (5, 4), (5, 7) is a successor of (4, 6) and they are on the same level, (6, 6) and (5, 7) can be pruned.</Para>


												</Section4>


											</Section3>


											<Section3 ID="Sec_87961">

												<Heading>Pruning Operation 2</Heading>


												<Para>If on the same level, there are two identical character pairs ( 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>1</Subscript>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ) and ( 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>2</Subscript>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ) satisfying 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>1</Subscript>
 &lt; 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>2</Subscript>
 , then ( 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>2</Subscript>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ) can be pruned without affecting the correctness of the algorithm in obtaining the LCS of 

													<Emphasis Type="Italic">X</Emphasis>
 and 

													<Emphasis Type="Italic">Y</Emphasis>
 .
												</Para>


												<Section4 ID="Sec_06872">

													<Heading>Rationale</Heading>


													<Para>The reason we can prune ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>2</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) is as follows. Let the successors of ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>1</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) be ( 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript>2</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>2</Subscript>
 )≺( 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript>3</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>3</Subscript>
 )≺...≺( 

														<Emphasis
Type="Italic">l</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">r</Emphasis>

														</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">r</Emphasis>

														</Subscript>
 ), then the length of common subsequence of " 

														<Emphasis
Type="Italic">x</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">i</Emphasis>
 1
														</Subscript>


														<Emphasis
Type="Italic">x</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">i</Emphasis>
 1+1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">x</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">n</Emphasis>

														</Subscript>
 " and " 

														<Emphasis
Type="Italic">y</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">j</Emphasis>

														</Subscript>


														<Emphasis
Type="Italic">y</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">j</Emphasis>
 +1
														</Subscript>
 ... 

														<Emphasis
Type="Italic">y</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">m</Emphasis>

														</Subscript>
 " is just 

														<Emphasis
Type="Italic">r</Emphasis>
 . Let the successors of ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>2</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) are ( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript>2</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>2</Subscript>
 ')≺( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript>3</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript>3</Subscript>
 ')≺...≺( 

														<Emphasis
Type="Italic">k</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">q</Emphasis>

														</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>


														<Subscript> 

															<Emphasis
Type="Italic">q</Emphasis>

														</Subscript>
 '). Because i1 &lt; i2, if an LCS contains a subsequence following (i2, j), this exact subsequence(note that all the x index is larger than i2) can be added at the end of (i1, j). Since (i1, j) and (i2, j) are on the same level, there must exist an LCS containing (i1, j). In other words, for any LCS containing identical character pair (i2, j), there is at least a corresponding LCS containing identical character pair (i1, j). Thus ( 

														<Emphasis
Type="Italic">i</Emphasis>


														<Subscript>2</Subscript>
 , 

														<Emphasis
Type="Italic">j</Emphasis>
 ) can be pruned without affecting the algorithm to get the LCS of 

														<Emphasis
Type="Italic">X</Emphasis>
 and 

														<Emphasis
Type="Italic">Y</Emphasis>
 .
													</Para>


													<Para>By extending pruning operation 2, we can get the following pruning operation.</Para>


												</Section4>


											</Section3>


											<Section3 ID="Sec_94167">

												<Heading>Pruning Operation 3</Heading>


												<Para>If there are identical character pairs (i 

													<Subscript>1</Subscript>
 , j), (i 

													<Subscript>2</Subscript>
 , j), ...,(i 

													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 , j) and 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>1</Subscript>
 &lt; 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>2</Subscript>
 &lt;...&lt; 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 , then we can prune( 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript>2</Subscript>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ),...,( 

													<Emphasis Type="Italic">i</Emphasis>


													<Subscript> 

														<Emphasis
Type="Italic">r</Emphasis>

													</Subscript>
 , 

													<Emphasis Type="Italic">j</Emphasis>
 ).
												</Para>


											</Section3>


										</Section2>


										<Section2 ID="Sec_64267">

											<Heading>Framework of FAST_LCS and complexity analysis</Heading>


											<Para>Based on the operations of generating the successors of the identical character pairs using successor tables and the pruning operations, we present a fast parallel LCS algorithm named FAST_LCS. The algorithm consists of two phases: searching for all the identical character pairs and tracing back to get the LCS. The first phase begins with the initial identical character pairs; then continuously searches for successors using the successor tables. In this phase, the pruning technology is implemented to discard those search branches that obviously can not obtain the optimum solution so as to reduce the search space and speed up the process of searching.</Para>


											<Para>The framework of the FAST_LCS algorithm is as follows, where the phase of searching for all the identical character pairs consists of steps 1, 2, 3 and the phase of tracing back is in step 4.</Para>


											<Para>Step 1. Build tables 

												<Emphasis Type="Italic">TX</Emphasis>
 and 

												<Emphasis Type="Italic">TY</Emphasis>
 ;
											</Para>


											<Para>Step 2. Find all the initial identical character pairs:( 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 0), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 0)), where 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, and add the records of the initial identical pairs ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">TX</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 0), 

												<Emphasis Type="Italic">TY</Emphasis>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 0), 1, 

												<Emphasis Type="Italic">Φ</Emphasis>
 , 

												<Emphasis
Type="Italic">active</Emphasis>
 ), 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4 to the table 

												<Emphasis Type="Italic">pairs</Emphasis>
 .
											</Para>


											<Para>Step 3. Repeat the following until there is no record in 

												<Emphasis
Type="Italic">active</Emphasis>
 state in table 

												<Emphasis Type="Italic">pairs</Emphasis>
 .
											</Para>


											<Para>Step 3.1 For all 

												<Emphasis
Type="Italic">active</Emphasis>
 identical pairs ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 , 

												<Emphasis Type="Italic">level</Emphasis>
 , 

												<Emphasis Type="Italic">pred</Emphasis>
 , 

												<Emphasis
Type="Italic">active</Emphasis>
 ) in 

												<Emphasis Type="Italic">pairs</Emphasis>
 parallel-do
											</Para>


											<Para>Step 3.1.1 Produce all the direct successors of ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 , 

												<Emphasis Type="Italic">level</Emphasis>
 , 

												<Emphasis Type="Italic">pred</Emphasis>
 , 

												<Emphasis
Type="Italic">active</Emphasis>
 ).
											</Para>


											<Para>Step 3.1.2 For each identical character pair ( 

												<Emphasis Type="Italic">g</Emphasis>
 , 

												<Emphasis Type="Italic">h</Emphasis>
 ) in the direct successors set of ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 , 

												<Emphasis Type="Italic">level</Emphasis>
 , 

												<Emphasis Type="Italic">pred</Emphasis>
 , 

												<Emphasis
Type="Italic">active</Emphasis>
 ), a new record ( 

												<Emphasis Type="Italic">k'</Emphasis>
 , 

												<Emphasis Type="Italic">g</Emphasis>
 , 

												<Emphasis Type="Italic">h</Emphasis>
 , 

												<Emphasis Type="Italic">level</Emphasis>
 +1, 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis
Type="Italic">active</Emphasis>
 ) is generated and inserted into the table 

												<Emphasis Type="Italic">pairs</Emphasis>
 .
											</Para>


											<Para>Step 3.1.3 Change the state of ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 , 

												<Emphasis Type="Italic">level</Emphasis>
 , 

												<Emphasis Type="Italic">pred</Emphasis>
 , 

												<Emphasis
Type="Italic">active</Emphasis>
 ) into 

												<Emphasis
Type="Italic">inactive</Emphasis>
 .
											</Para>


											<Para>Step 3.2 Use the pruning operations on all the successors produced on this level to remove all the redundant identical pairs from table 

												<Emphasis Type="Italic">pairs</Emphasis>
 .
											</Para>


											<Para>Step 4. Compute 

												<Emphasis Type="Italic">r</Emphasis>
 = the maximum level in the table 

												<Emphasis Type="Italic">pairs</Emphasis>
 .
											</Para>


											<Para>For all the identical pairs ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 , 

												<Emphasis Type="Italic">r</Emphasis>
 , 

												<Emphasis Type="Italic">l</Emphasis>
 , 

												<Emphasis
Type="Italic">inactive</Emphasis>
 ) in 

												<Emphasis Type="Italic">pairs</Emphasis>
 parallel-do
											</Para>


											<Para>Step 4.1. 

												<Emphasis Type="Italic">pred</Emphasis>
 = 

												<Emphasis Type="Italic">l</Emphasis>
 ; 

												<Emphasis Type="Italic">LCS</Emphasis>
 ( 

												<Emphasis Type="Italic">r</Emphasis>
 ) = 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 .
											</Para>


											<Para>Step 4.2 While 

												<Emphasis Type="Italic">pred</Emphasis>
 ≠ 

												<Emphasis Type="Italic">Φ</Emphasis>
 do
											</Para>


											<Para>Step 4.2.1 get the 

												<Emphasis Type="Italic">pred</Emphasis>
 -th record ( 

												<Emphasis Type="Italic">pred</Emphasis>
 , 

												<Emphasis Type="Italic">g</Emphasis>
 , 

												<Emphasis Type="Italic">h</Emphasis>
 , 

												<Emphasis Type="Italic">r</Emphasis>
 ', 

												<Emphasis Type="Italic">l</Emphasis>
 ', 

												<Emphasis
Type="Italic">inactive</Emphasis>
 ) from table 

												<Emphasis Type="Italic">pairs</Emphasis>
 .
											</Para>


											<Para>Step 4.2.2 

												<Emphasis Type="Italic">pred</Emphasis>
 = 

												<Emphasis Type="Italic">l</Emphasis>
 '; 

												<Emphasis Type="Italic">LCS</Emphasis>
 ( 

												<Emphasis Type="Italic">r</Emphasis>
 ') = 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">g</Emphasis>

												</Subscript>
 .
											</Para>


											<Para>In the algorithm, a table called 

												<Emphasis Type="Italic">pairs</Emphasis>
 is used to store the identical character pairs obtained in the algorithm. In the table 

												<Emphasis Type="Italic">pairs</Emphasis>
 , each record takes the form of ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 , 

												<Emphasis Type="Italic">level</Emphasis>
 , 

												<Emphasis Type="Italic">pred</Emphasis>
 , 

												<Emphasis Type="Italic">state</Emphasis>
 ) where the data denote the index of the record, the identical character pair ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ), its level, the index of its direct predecessor, and its current state, respectively. Each record in 

												<Emphasis Type="Italic">pairs</Emphasis>
 has two states. For the identical pairs whose successors have not been searched, it is in the 

												<Emphasis
Type="Italic">active</Emphasis>
 state, otherwise it is in the i 

												<Emphasis
Type="Italic">nactive</Emphasis>
 state. In every step of search process, the algorithm searches for the successors of all the identical pairs in 

												<Emphasis
Type="Italic">active</Emphasis>
 state in parallel, and repeat this search process until there is no more identical pairs in 

												<Emphasis
Type="Italic">active</Emphasis>
 state in the table. The phase of tracing back starts from the identical pairs with the maximum level in the table, and traces back according to the 

												<Emphasis Type="Italic">pred</Emphasis>
 of each identical pair. This tracing back process ends when it reaches an initial identical pair, and the trail indicates the LCS. If there are more than one identical pairs with the maximum level in the table, the tracing back procedure for those identical pairs can be carried out in parallel and several LCS can be obtained concurrently.
											</Para>


											<Para>The LCS of 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 is stored in the array 

												<Emphasis Type="Italic">LCS</Emphasis>
 . In our algorithm, every identical pair must have the operation of producing successors at least once. Because of the pruning technology, this operation will only be run exactly once on each identical character pair. Therefore, assuming that the number of the identical character pairs of 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 is 

												<Emphasis Type="Italic">L</Emphasis>
 , the time complexity for a sequential execution of the algorithm FAST_LCS ( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 ) is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ). Since the table 

												<Emphasis Type="Italic">pairs</Emphasis>
 has to store all the identical character pairs, it requires 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) memory space. Considering that the space cost of 

												<Emphasis Type="Italic">TX</Emphasis>
 , 

												<Emphasis Type="Italic">TY</Emphasis>
 are 4*( 

												<Emphasis Type="Italic">n</Emphasis>
 +1) and 4*( 

												<Emphasis Type="Italic">m</Emphasis>
 +1), the storage complexity of our algorithm is max{4*( 

												<Emphasis Type="Italic">n</Emphasis>
 +1)+4*( 

												<Emphasis Type="Italic">m</Emphasis>
 +1), 

												<Emphasis Type="Italic">L</Emphasis>
 }. In parallel implementation of the algorithm, since the process for each identical pair can be assigned to one processor, all the operations on the identical pairs can be carried out in parallel. Thus, the processing of each level requires 

												<Emphasis Type="Italic">O</Emphasis>
 (1) time, and the number of time steps required for a parallel execution of FAST_LCS is equal to the maximum level of the identical pairs. By Theorem 1, we know that the length of the LCS of 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 , |LCS ( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )|, is equal to the maximum level of the identical pairs. Therefore, the time complexity of parallel FAST_LCS is 

												<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

												<Emphasis Type="Italic">X</Emphasis>
 , 

												<Emphasis Type="Italic">Y</Emphasis>
 )|).
											</Para>


										</Section2>


										<Section2 ID="Sec_18838">

											<Heading>Finding the LCS of multiple sequences using FAST_LCS</Heading>


											<Para>Our algorithm FAST_LCS can be easily extended to the LCS problem of multiple sequences. Suppose there are 

												<Emphasis Type="Italic">n</Emphasis>
 sequences 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 , where 

												<Emphasis Type="Italic">X</Emphasis>
 = ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 1
												</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 2
												</Subscript>
 , ..., 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>
 , 

													<Emphasis Type="Italic">ni</Emphasis>

												</Subscript>
 ), 

												<Emphasis Type="Italic">n</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 is the length of 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ij</Emphasis>

												</Subscript>
 ∈ { 

												<Emphasis Type="Italic">A</Emphasis>
 , 

												<Emphasis Type="Italic">C</Emphasis>
 , 

												<Emphasis Type="Italic">G</Emphasis>
 , 

												<Emphasis Type="Italic">T</Emphasis>
 } where j = 1, 2, ..., 

												<Emphasis Type="Italic">n</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 . To find their LCS, similar to the case of two sequences, the algorithm for multiple sequences first builds the successor tables for all the sequences. Denote the successor tables of 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 as 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 , respectively, where 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>

												</Subscript>
 is a two-dimensional array of size 4*( 

												<Emphasis Type="Italic">n</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>

												</Subscript>
 + 1 [-del-n 

												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 +1]) for the sequence 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>s</Subscript>
 = ( 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>
 1
												</Subscript>
 , 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>
 2
												</Subscript>
 , ..., 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>
 , 

													<Emphasis Type="Italic">ns</Emphasis>

												</Subscript>
 ), 

												<Emphasis Type="Italic">s</Emphasis>
 = 1,2,... 

												<Emphasis Type="Italic">n</Emphasis>
 , and successor table 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>

												</Subscript>
 of identical characters is defined as:
											</Para>


											<Para>where 

												<Emphasis Type="Italic">SX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">s</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">i</Emphasis>
 , 

												<Emphasis Type="Italic">j</Emphasis>
 ) = { 

												<Emphasis Type="Italic">k</Emphasis>
 | 

												<Emphasis Type="Italic">x</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">sk</Emphasis>

												</Subscript>
 = CH( 

												<Emphasis Type="Italic">i</Emphasis>
 ), 

												<Emphasis Type="Italic">k</Emphasis>
 &gt; 

												<Emphasis Type="Italic">j</Emphasis>
 } where i = 1, 2, 3, 4.
											</Para>


											<Para>Similar to identical character pairs in the case of two sequences, we define the concept of 

												<Emphasis Type="Bold"> 

													<Emphasis
Type="Italic">identical character tuple</Emphasis>

												</Emphasis>
 for LCS of multiple sequences. For the sequences 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 , if,we record them in an identical character tuple of the sequences 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 and denote it as ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ). The set of all the identical character tuples of 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 is denoted as 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ).
											</Para>


											<Para>Let ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) and ( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) be two identical character tuples of 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 . If 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 &lt; 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">k</Emphasis>

												</Subscript>
 , for 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,... 

												<Emphasis Type="Italic">n</Emphasis>
 , we call ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) a predecessor of ( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), or ( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) a successor of ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), and denote them as ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) &lt; ( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ).
											</Para>


											<Para>Let 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) = {( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) | ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) &lt; ( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), ( 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">j</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )} be the set of all the successors of identical tuple ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), if ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )∑ 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) and there is no ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 ', 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 ', ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ')∈ 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) satisfying the condition: ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 ', 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 ', ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ') &lt; ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) and, we call ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) the direct successor of ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), and denoted it as ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )≺( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ).
											</Para>


											<Para>If an identical tuple ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) and there is no ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) satisfying ( 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">k</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )≺( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), we call ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) an initial identical tuple.
											</Para>


											<Para>For an identical tuple ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), its level is defined as follows:
											</Para>


											<Para>Similar to the case of two sequences LCS, the following theorems can be easily deduced.</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem 3.</Emphasis>
 Denote the length of the LCS of 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 as |LCS( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )|, then
											</Para>


											<Para>|LCS( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )| = max { 

												<Emphasis Type="Italic">level</Emphasis>
 ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 ,... 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )|( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 ,... 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )}.
											</Para>


											<Para>Proof of Theorem 3 is similar to that of Theorem 1.</Para>


											<Para>In our parallel algorithm for LCS of multiple sequences, all direct successors of all the initial identical character tuples are produced in parallel using the successor tables. Then, the direct successors of all those successors produced in the first step are generated in parallel. Repeat these operations of generating the direct successors until no more successors could be produced. For an identical character tuple ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ) ∈ 

												<Emphasis Type="Italic">S</Emphasis>
 ( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), this operation is as follows:
											</Para>


											<Para>( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 ,..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )→{( 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>1</Subscript>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 ), 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>2</Subscript>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 ),..., 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ))| 

												<Emphasis Type="Italic">k</Emphasis>
 = 1,2,3,4, 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">k</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 ) ≠ '-', 

												<Emphasis Type="Italic">j</Emphasis>
 = 1,2,..., 

												<Emphasis Type="Italic">n</Emphasis>
 }     (6)
											</Para>


											<Para>From (6) we can see that this operation is to assemble the elements of the 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 -th column of 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 , for 

												<Emphasis Type="Italic">j</Emphasis>
 = 1,2,..., 

												<Emphasis Type="Italic">n</Emphasis>
 to get the new tuples.
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Example 2.</Emphasis>
 Let 

												<Emphasis Type="Italic">n</Emphasis>
 = 3, and 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 = "T G C A T A", 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 = "A T C T G A T", and 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>3</Subscript>
 = "C T G A T T C". Their successor tables 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>2</Subscript>
 and 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>3</Subscript>
 are shown in Table 

												<InternalRef RefID="T2">2</InternalRef>
 .
											</Para>


											<Table Float="No" ID="T2">

												<Caption Language="En">

													<CaptionNumber>Table 2</CaptionNumber>


													<CaptionContent>

														<SimplePara>Their successor tables 

															<Emphasis
Type="Italic">TX</Emphasis>


															<Subscript>1</Subscript>
 , 

															<Emphasis
Type="Italic">TX</Emphasis>


															<Subscript>2</Subscript>
 and 

															<Emphasis
Type="Italic">TX</Emphasis>


															<Subscript>3</Subscript>
 in Example 2
														</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="3">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">TX</Emphasis>


																	<Subscript>1</Subscript>
 :
																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">i</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>CH( 

																	<Emphasis
Type="Italic">i</Emphasis>
 )
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0 1 2 3 4 5 6</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">A</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>4 4 4 4 6 6 -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">C</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>3 3 3 - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">G</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2 2 - - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">T</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1 5 5 5 5 - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">TX</Emphasis>


																	<Subscript>2</Subscript>
 :
																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">i</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>CH( 

																	<Emphasis
Type="Italic">i</Emphasis>
 )
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0 1 2 3 4 5 6 7</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">A</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1 6 6 6 6 6 - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">C</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>3 3 3 - - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">G</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>5 5 5 5 5 - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">T</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2 2 4 4 7 7 7 -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">TX</Emphasis>


																	<Subscript>3</Subscript>
 :
																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara> 

																	<Emphasis
Type="Italic">i</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>CH( 

																	<Emphasis
Type="Italic">i</Emphasis>
 )
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0 1 2 3 4 5 6 7</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>1</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">A</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>4 4 4 4 - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>2</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">C</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1 7 7 7 7 7 7 -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>3</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">G</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>3 3 3 - - - - -</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>4</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara> 

																	<Emphasis
Type="Bold">T</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2 2 5 5 5 6 - -</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


											<Para>By assembling the 0 

												<Superscript>th</Superscript>
 columns of the successor tables, we can get the initial identical triples (4,1,4), (3,3,1), (2,5,3) and (1,2,2).
											</Para>


											<Para>The direct successors of the identical character triple (1,2,2) can be obtained by grouping the elements of the 1 

												<Superscript>st</Superscript>
 column of 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>1</Subscript>
 and the 2 

												<Superscript>nd</Superscript>
 columns of 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>2</Subscript>
 and 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript>3</Subscript>
 , which are (4,6,4),(3,3,7) (2,5,3) and (5,4,5).
											</Para>


											<Para> 

												<Emphasis
Type="Bold">Theorem 4.</Emphasis>
 For an identical character tuple ( 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">i</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 ), the method in (6) can produce all its successors.
											</Para>


											<Para>Proof of Theorem 4 is similar to that of Theorem 2.</Para>


											<Para>From Theorem 4, we know that all the successors of the identical tuples on each level can be generated by the operation of producing successors. Starting from the initial identical tuples, all the identical tuples can be produced. In such process of generating the successors, pruning techniques can be implemented to remove the identical tuples which can not generate the longest common subsequence so as to reduce the search space. All the pruning operations for two-sequence LCS can be easily extended to the case of multiple sequences.</Para>


											<Para>For instance, in Example 2, among the successors of triple (1,2,2), we have (2,5,3)≺(4,6,4), since they are on the same level, we can prune (4,6,4). For another instance in Example 2, among the initial identical triples (4,1,4), (3,3,1), (2,5,3) and (1,2,2), since they are in the same level and (1,2,2)≺(2,5,3) we can prune (2,5,3).</Para>


											<Para>Assume that the number of the identical character tuples of the sequences 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 is 

												<Emphasis Type="Italic">L</Emphasis>
 . In our algorithm, since every identical tuple must have the operation of producing successors exactly once, the time complexity for sequentially executing of the algorithm on the sequences 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ). The algorithm uses a table 

												<Emphasis
Type="Italic">tuples</Emphasis>
 to store all the identical character tuples, it requires 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) memory space. Considering that the memory space cost of 

												<Emphasis Type="Italic">TX</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 , for 

												<Emphasis Type="Italic">j</Emphasis>
 = 1,2,..., 

												<Emphasis Type="Italic">n</Emphasis>
 , is 4, the storage complexity of our algorithm is max{4, 

												<Emphasis Type="Italic">L</Emphasis>
 }. In a parallel implementation, since the computation for each identical tuple can be assigned on one processor, all the process on the identical tuples can be carried out in parallel. Therefore, the processing of each level requires 

												<Emphasis Type="Italic">O</Emphasis>
 (1) time, and the time required for the parallel computation is equal to the maximum level of the identical tuples which is equal to the length of the longest common subsequence of 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 . Therefore the time complexity of the parallel execution of FAST_LCS on multiple sequences is 

												<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )|).
											</Para>


											<Para>It should be pointed out that for most of the algorithms for multiple-sequence LCS, their time complexity strongly depends on the number of the sequences. For instance, if we use the Smith-Waterman algorithm to find the LCS of multiple sequences, the time complexity is, where 

												<Emphasis Type="Italic">n</Emphasis>
 is the number of sequences, which is not practicable when 

												<Emphasis Type="Italic">n</Emphasis>
 is large. The time complexity of our algorithm is 

												<Emphasis Type="Italic">O</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) for sequential computation and 

												<Emphasis Type="Italic">O</Emphasis>
 (|LCS( 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>1</Subscript>
 , 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript>2</Subscript>
 , ..., 

												<Emphasis Type="Italic">X</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">n</Emphasis>

												</Subscript>
 )|) for parallel implementation, where the time complexity of FAST_LCS is 

												<Emphasis Type="Bold"> 

													<Emphasis
Type="Italic">independent of the number of sequences n</Emphasis>

												</Emphasis>
 . This means that our algorithm is much more efficient for finding the LCS of a large number of sequences.
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_91239">

										<Heading>Results</Heading>


										<Section2 ID="Sec_71922">

											<Heading>The results of sequential computation on two sequences</Heading>


											<Para>We test our algorithm FAST_LCS on the rice gene sequences of the 

												<Emphasis Type="Italic">tigr</Emphasis>


												<CitationRef
CitationID="B29">29</CitationRef>
 database and compare the performance of FAST_LCS with that of Smith-Waterman algorithm 

												<CitationRef
CitationID="B30">30</CitationRef>
 and FASTA algorithm 

												<CitationRef
CitationID="B31">31</CitationRef>


												<CitationRef
CitationID="B32">32</CitationRef>
 which are currently the most widely used LCS algorithms. Since both our algorithm and Smith-Waterman's can obtain exactly correct solution, we compare the computation speed of our algorithm FAST_LCS with that of Smith-Waterman algorithm. Also, we compare the precision of our algorithm with that of FASTA using the same computation time.
											</Para>


											<Para>Table 

												<InternalRef RefID="T3">3</InternalRef>
 compares the computation speed of FAST_LCS with that of Smith-Waterman algorithm on groups of gene sequence pairs with different lengths. Since a test on one pair of sequences takes very short time, it is hard to compare the speed of the algorithms using a single pair of sequence. Therefore we test the algorithms on groups of sequence pairs with similar lengths. We test five groups of sequence pairs each of which consisting of 100 pairs of sequences. The total time for each group by the two algorithms are listed in Table 

												<InternalRef RefID="T3">3</InternalRef>
 .
											</Para>


											<Table Float="No" ID="T3">

												<Caption Language="En">

													<CaptionNumber>Table 3</CaptionNumber>


													<CaptionContent>

														<SimplePara>Comparison of computation speed of FAST_LCS with that of Smith-Waterman algorithm</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="7">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara>Time of FAST_LCS (s)</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>Time of S-W algorithm(s)</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Name of Sequences</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>Length 

																	<Emphasis
Type="Italic">l</Emphasis>

																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>Number of pairs</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>Total time</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>Average time</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>Total time</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>Average time</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466196 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0≤ 

																	<Emphasis
Type="Italic">l</Emphasis>
 ≤50
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.49</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.0049</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>1.09</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.0109</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466195 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466168 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466167 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466166 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30250556 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30230255 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30230254 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229613 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229612 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229449 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229448 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229047 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>50≤ 

																	<Emphasis
Type="Italic">l</Emphasis>
 ≤ 100
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>5.88</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.0588</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>11.55</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.1155</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229046 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229001 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229000 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228999 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228998 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228849 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228848 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229447 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>10≤ 

																	<Emphasis
Type="Italic">l</Emphasis>
 ≤ 150
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>29.41</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.2941</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>65.95</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.6595</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229446 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229249 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229248 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228846 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>15≤ 

																	<Emphasis
Type="Italic">l</Emphasis>
 ≤ 200
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>94.11</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.9411</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>172.213</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>1.7213</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228845 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228648 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228647 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229247 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>20≤ 

																	<Emphasis
Type="Italic">l</Emphasis>
 ≤ 250
																</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>100</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>230.51</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>2.3051</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>425.16</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>4.2516</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229246 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229049 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229048 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


											<Para>Fig. 

												<InternalRef RefID="F1">1</InternalRef>
 shows the comparison of the computation time of our algorithm with that of Smith-Waterman algorithm. From the table and the figure, we can see that our algorithm is obviously faster than Smith-Waterman algorithm for sequence sets of all different lengths. The difference of the computation time between the two algorithms grows exponentially when the length of sequences becomes greater than 150. This means that our algorithm is much faster and more efficient than Smith-Waterman's for finding the LCS of long sequences.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F1">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparison of the computation time of FAST_LCS with that of Smith – Waterman algorithm</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Comparison of the computation time of FAST_LCS with that of Smith – Waterman algorithm.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>We also compare the precision of our algorithm with that of FASTA on the premise of the same computing time. Here precision is defined as:</Para>


											<Para>Fig. 

												<InternalRef RefID="F2">2</InternalRef>
 shows the comparison of the precision of the results by our algorithm with that by FASTA using the same computation time. From Fig. 

												<InternalRef RefID="F2">2</InternalRef>
 , we can see that our algorithm can obtain the correct result no matter how long the sequence is, while the precision of FASTA declines when the length of the sequences is increased. Therefore the precision of our algorithm is much higher than that of FASTA.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F2">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparison of the precision of FAST_LCS with that of FASTA using the same computation time</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Comparison of the precision of FAST_LCS with that of FASTA using the same computation time.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


										</Section2>


										<Section2 ID="Sec_01035">

											<Heading>The results of sequential computing on multiple sequences</Heading>


											<Para>We test our algorithm FAST_LCS on the multiple sequences and compare with the Clustal-W 

												<CitationRef
CitationID="B27">27</CitationRef>
 algorithm which is a popular algorithm for multiple-sequence LCS. Fig. 

												<InternalRef RefID="F3">3</InternalRef>
 and Fig. 

												<InternalRef RefID="F4">4</InternalRef>
 show the comparison of the computation time of our algorithm FAST_LCS with that of Clustal-W algorithm. Table 

												<InternalRef RefID="T4">4</InternalRef>
 lists the computation time of the two algorithms on 5 sets of different numbers of sequences with length of 50. Comparison of the computation time of the two algorithms on sequences sets of different numbers of sequences is shown in Fig. 

												<InternalRef RefID="F3">3</InternalRef>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F3">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparison of the computation time of FAST_LCS with that of Clustal-W on sequence sets of different numbers of sequences</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-3" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Comparison of the computation time of FAST_LCS with that of Clustal-W on sequence sets of different numbers of sequences.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Figure Category="Standard" Float="No"
ID="F4">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparison of the computation time of FAST_LCS with that of Clustal-W on sequence sets of different lengths</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-4" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Comparison of the computation time of FAST_LCS with that of Clustal-W on sequence sets of different lengths.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Table Float="No" ID="T4">

												<Caption Language="En">

													<CaptionNumber>Table 4</CaptionNumber>


													<CaptionContent>

														<SimplePara>Comparison of computation time of FAST_LCS and that of Clustal-W on sequences sets of different numbers of sequences</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="4">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara>Sequence name</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>Sequence Number</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>Time of FAST_LCS (s)</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>Time of Clustal-W (s)</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466194</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>3</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.609</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.804</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466196</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466192</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>5</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2.656</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2.732</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466196</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466189</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>8</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>6.14</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>7.91</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466196</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466186</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>11</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>5.71</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>8.20</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466196</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466183</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>14</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>6.34</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>8.49</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>...</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466196</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


											<Para>From Fig. 

												<InternalRef RefID="F3">3</InternalRef>
 and Table 

												<InternalRef RefID="T4">4</InternalRef>
 , we can see that FAST_LCS is faster than Clustal-W for sets with different numbers of sequences. When the number of sequences is larger than five, the speed up is significant.
											</Para>


											<Para>Table 

												<InternalRef RefID="T5">5</InternalRef>
 lists the computation time of the two algorithms on five sequences sets with different lengths. Comparison of the computation time of the two algorithms on sequence sets of different lengths is shown in Fig. 

												<InternalRef RefID="F4">4</InternalRef>
 . From Table 

												<InternalRef RefID="T5">5</InternalRef>
 and Fig. 

												<InternalRef RefID="F4">4</InternalRef>
 , we can see that FAST_LCS is faster than Clustal-W for sequence sets with different lengths.
											</Para>


											<Table Float="No" ID="T5">

												<Caption Language="En">

													<CaptionNumber>Table 5</CaptionNumber>


													<CaptionContent>

														<SimplePara>Comparison of computation time of FAST_LCS with that of Clustal-W on sequences sets of different lengths</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="6">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara>The length of input sequences</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Algorithm</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>20</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>30</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>50</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>60</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>80</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Time of FAST_LCS (S)</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.109</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.391</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2.656</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>3.516</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>6.166</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Time of Clustal-W (S)</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>0.312</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1.053</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2.732</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>3.612</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>5.992</SimplePara>


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


											<Para>We also compare the precision of our FAST_LCS algorithm with that of Clustal-W algorithm. Fig. 

												<InternalRef RefID="F5">5</InternalRef>
 shows the comparison of precision of FAST_LCS with that of Clustal-W on the sets with different numbers of sequences, and Fig. 

												<InternalRef RefID="F6">6</InternalRef>
 shows the comparison of precision of the two algorithms on the sets of sequences with different lengths. From these two figures, we can see that no matter how the length and the number of sequences are increased, our algorithm can obtain the exactly correct results. The precision of Clustal-W declines when the number or the length of the sequences is increased. Therefore the precision of our algorithm is much higher than Clustal-W.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F5">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparison of the precision of FAST_LCS with that of Clustal-W on sequence sets with different numbers of sequences</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-5" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Comparison of the precision of FAST_LCS with that of Clustal-W on sequence sets with different numbers of sequences.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Figure Category="Standard" Float="No"
ID="F6">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparison of the precision of FAST_LCS with that of Clustal-W on sequence sets of different lengths</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-6" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Comparison of the precision of FAST_LCS with that of Clustal-W on sequence sets of different lengths.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


										</Section2>


										<Section2 ID="Sec_46125">

											<Heading>The results of parallel computing</Heading>


											<Para>We also test our algorithm on the rice gene sequence from the 

												<Emphasis Type="Italic">tigr</Emphasis>
 database 

												<CitationRef
CitationID="B29">29</CitationRef>
 on the Shenteng 1800 supercomputer using MPI (C bounding). In the parallel implementation of FAST_LCS, the identical character pairs in the 

												<Emphasis
Type="Italic">active</Emphasis>
 state are assigned and processed in different processors. The experimental results by using different numbers of processors are shown in Fig. 

												<InternalRef RefID="F7">7</InternalRef>
 . Three pairs of gene sequences are tested. The names, lengths, and computation times are listed in Table 

												<InternalRef RefID="T6">6</InternalRef>
 . From Fig. 

												<InternalRef RefID="F7">7</InternalRef>
 and Table 

												<InternalRef RefID="T6">6</InternalRef>
 we can see that the computation will become faster as the number of processors increases. Due to the communication overhead between processors, the speedup of our algorithm is slower than linear, which conforms to the Amdahl's Law.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F7">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Parallel computational time of FAST_LCS using different processor numbers</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-7-S4-S4-7" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para>Parallel computational time of FAST_LCS using different processor numbers.</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Table Float="No" ID="T6">

												<Caption Language="En">

													<CaptionNumber>Table 6</CaptionNumber>


													<CaptionContent>

														<SimplePara>Computational time of parallel FAST_LCS using different numbers of processors</SimplePara>


													</CaptionContent>


												</Caption>


												<tgroup cols="7">

													<colspec colname="c0" colnum="0" />


													<colspec colname="c1" colnum="1" />


													<colspec colname="c2" colnum="2" />


													<colspec colname="c3" colnum="3" />


													<colspec colname="c4" colnum="4" />


													<colspec colname="c5" colnum="5" />


													<colspec colname="c6" colnum="6" />


													<thead>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara>Computational time using different numbers of processors (s)</SimplePara>


															</entry>


														</row>


													</thead>


													<tbody>

														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>Sequence name</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>Length</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>1</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>2</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>4</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>8</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>16</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|21466166 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>250</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>2.3051</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.7955</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.51085</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.3300</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.2313</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30250556</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228999 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>200</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.9411</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.4247</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.2669</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.15421</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.09379</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30228998</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara />


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229447 ~</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara>150</SimplePara>


															</entry>


															<entry colname="c2">

																<SimplePara>0.3941</SimplePara>


															</entry>


															<entry colname="c3">

																<SimplePara>0.2015</SimplePara>


															</entry>


															<entry colname="c4">

																<SimplePara>0.10271</SimplePara>


															</entry>


															<entry colname="c5">

																<SimplePara>0.06975</SimplePara>


															</entry>


															<entry colname="c6">

																<SimplePara>0.04702</SimplePara>


															</entry>


														</row>


														<row>

															<entry colname="c0">

																<SimplePara>gi|30229446</SimplePara>


															</entry>


															<entry colname="c1">

																<SimplePara />


															</entry>


															<entry colname="c2">

																<SimplePara />


															</entry>


															<entry colname="c3">

																<SimplePara />


															</entry>


															<entry colname="c4">

																<SimplePara />


															</entry>


															<entry colname="c5">

																<SimplePara />


															</entry>


															<entry colname="c6">

																<SimplePara />


															</entry>


														</row>


													</tbody>


												</tgroup>


											</Table>


										</Section2>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/content/pdf/1471-2105-7-S4-info.pdf"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Hao</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SY</Initials>


													<FamilyName>Zhang</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">The manual of bioinformatics</ArticleTitle>


												<JournalTitle>Shanghai Science and Technology Publishing Company</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>YD</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>ZR</Initials>


													<FamilyName>Sun</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">Bioinformatics – The practice guide for the analysis of gene and protein</ArticleTitle>


												<JournalTitle>Tsinghua University Publishing Company</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>EW</Initials>


													<FamilyName>Edmiston</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>NG</Initials>


													<FamilyName>Core</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JH</Initials>


													<FamilyName>Saltz</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">Parallel processing of biological sequence comparison algorithms</ArticleTitle>


												<JournalTitle>International Journal of Parallel Programming</JournalTitle>


												<VolumeID>17</VolumeID>


												<FirstPage>259</FirstPage>


												<LastPage>275</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Lander</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">Protein sequence comparison on a data parallel computer</ArticleTitle>


												<JournalTitle>Proceedings of the 1988 International Conference on Parallel Processing</JournalTitle>


												<VolumeID />


												<FirstPage>257</FirstPage>


												<LastPage>263</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>AR</Initials>


													<FamilyName>Galper</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DL</Initials>


													<FamilyName>Brutlag</FamilyName>


												</BibAuthorName>


												<Year>1990</Year>


												<ArticleTitle
Language="En">Parallel similarity search and alignment with the dynamic programming method</ArticleTitle>


												<JournalTitle>Technical Report</JournalTitle>


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>TF</Initials>


													<FamilyName>Smith</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MS</Initials>


													<FamilyName>Waterman</FamilyName>


												</BibAuthorName>


												<Year>1990</Year>


												<ArticleTitle
Language="En">Identification of common molecular subsequence</ArticleTitle>


												<JournalTitle>Journal of Molecular Biology</JournalTitle>


												<VolumeID>215</VolumeID>


												<FirstPage>403</FirstPage>


												<LastPage>410</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>SB</Initials>


													<FamilyName>Needleman</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>CD</Initials>


													<FamilyName>Wunsch</FamilyName>


												</BibAuthorName>


												<Year>1970</Year>


												<ArticleTitle
Language="En">A general method applicable to the search for similarities in the amino acid sequence of two proteins</ArticleTitle>


												<JournalTitle>J Mol Biol</JournalTitle>


												<VolumeID>48</VolumeID>


												<FirstPage>443</FirstPage>


												<LastPage>453</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Aho</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Hirschberg</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Ullman</FamilyName>


												</BibAuthorName>


												<Year>1976</Year>


												<ArticleTitle
Language="En">Bounds on the complexity of the longest common subsequence problem</ArticleTitle>


												<JournalTitle>J Assoc Comput Mach</JournalTitle>


												<VolumeID>23</VolumeID>


												<FirstPage>1</FirstPage>


												<LastPage>12</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>O</Initials>


													<FamilyName>Gotoh</FamilyName>


												</BibAuthorName>


												<Year>1982</Year>


												<ArticleTitle
Language="En">An improved algorithm for matching biological sequences</ArticleTitle>


												<JournalTitle>J Molec Biol</JournalTitle>


												<VolumeID>162</VolumeID>


												<FirstPage>705</FirstPage>


												<LastPage>708</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>EW</Initials>


													<FamilyName>Mayers</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Miller</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Optimal alignment in linear space</ArticleTitle>


												<JournalTitle>Comput Appl Biosci</JournalTitle>


												<VolumeID>4</VolumeID>


												<FirstPage>11</FirstPage>


												<LastPage>17</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>DS</Initials>


													<FamilyName>Hirschberg</FamilyName>


												</BibAuthorName>


												<Year>1975</Year>


												<ArticleTitle
Language="En">A Linear space algorithm for computing maximal Common Subsequences</ArticleTitle>


												<JournalTitle>Commun ACM</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>341</FirstPage>


												<LastPage>343</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Pan</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Linear array with a reconfigurable pipelined bus system – concepts and applications</ArticleTitle>


												<JournalTitle>Journal of Information Science</JournalTitle>


												<VolumeID>106</VolumeID>


												<FirstPage>237</FirstPage>


												<LastPage>258</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JF</Initials>


													<FamilyName>Myoupo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>David Seme</FamilyName>


												</BibAuthorName>


												<Year>1999</Year>


												<ArticleTitle
Language="En">Time-efficient parallel algorithms for the longest common subsequence and related problems</ArticleTitle>


												<JournalTitle>Journal of Parallel and Distributed Computing</JournalTitle>


												<VolumeID>57</VolumeID>


												<FirstPage>212</FirstPage>


												<LastPage>223</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Bergroth</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Hakonen</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Raita</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">A survey of longest common subsequence algorithms</ArticleTitle>


												<JournalTitle>Seventh International Symposium on String Processing Information Retrieval</JournalTitle>


												<VolumeID />


												<FirstPage>39</FirstPage>


												<LastPage>48</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR15">

											<CitationNumber>15.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Aggarwal</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Park</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">Notes on searching in multidimensional monotone Arrays</ArticleTitle>


												<JournalTitle>Proc 29th Ann IEEE Symp Foundations of Comput Sci</JournalTitle>


												<VolumeID />


												<FirstPage>497</FirstPage>


												<LastPage>512</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR16">

											<CitationNumber>16.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Apostolico</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Atallah</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>L</Initials>


													<FamilyName>Larmore</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Mcfaddin</FamilyName>


												</BibAuthorName>


												<Year>1990</Year>


												<ArticleTitle
Language="En">Efficient parallel algorithms for string editing and related problems</ArticleTitle>


												<JournalTitle>SIAM J Computing</JournalTitle>


												<VolumeID>19</VolumeID>


												<FirstPage>968</FirstPage>


												<LastPage>988</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR17">

											<CitationNumber>17.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Lu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Lin</FamilyName>


												</BibAuthorName>


												<Year>1994</Year>


												<ArticleTitle
Language="En">Parallel algorithms for the longest common subsequence Problem</ArticleTitle>


												<JournalTitle>IEEE Transaction on Parallel and Distributed System</JournalTitle>


												<VolumeID>5</VolumeID>


												<FirstPage>835</FirstPage>


												<LastPage>848</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR18">

											<CitationNumber>18.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>KN</Initials>


													<FamilyName>Babu</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Systems</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Saxena</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">Parallel algorithms for the longest common subsequence problem</ArticleTitle>


												<JournalTitle>4th International Conference on High Performance Computing</JournalTitle>


												<VolumeID />


												<FirstPage>18</FirstPage>


												<LastPage>21</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR19">

											<CitationNumber>19.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Robert</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Tchuente</FamilyName>


												</BibAuthorName>


												<Year>1985</Year>


												<ArticleTitle
Language="En">A Systolic Array for the Longest Common Subsequence Problem</ArticleTitle>


												<JournalTitle>Inform Process Lett</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>191</FirstPage>


												<LastPage>198</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR20">

											<CitationNumber>20.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JH</Initials>


													<FamilyName>Chang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>OH</Initials>


													<FamilyName>Ibarra</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>MA</Initials>


													<FamilyName>Pallis</FamilyName>


												</BibAuthorName>


												<Year>1987</Year>


												<ArticleTitle
Language="En">Parallel parsing on a one-way array of finite-state machines</ArticleTitle>


												<JournalTitle>IEEE Trans Computers</JournalTitle>


												<VolumeID>C-36</VolumeID>


												<FirstPage>64</FirstPage>


												<LastPage>75</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR21">

											<CitationNumber>21.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Luce</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JF</Initials>


													<FamilyName>Myoupo</FamilyName>


												</BibAuthorName>


												<Year>1998</Year>


												<ArticleTitle
Language="En">Systolic-based parallel architecture for the longest common subsequences problem</ArticleTitle>


												<JournalTitle>Integration</JournalTitle>


												<VolumeID>25</VolumeID>


												<FirstPage>53</FirstPage>


												<LastPage>70</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR22">

											<CitationNumber>22.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Freschi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Bogliolo</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">Longest common subsequence between run-length-encoded strings:a new algorithm with improved parallelism</ArticleTitle>


												<JournalTitle>Information Processing Letters</JournalTitle>


												<VolumeID>90</VolumeID>


												<FirstPage>167</FirstPage>


												<LastPage>173</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR23">

											<CitationNumber>23.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>DJ</Initials>


													<FamilyName>Lipman</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SF</Initials>


													<FamilyName>Altschul</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JD</Initials>


													<FamilyName>Kececioglu</FamilyName>


												</BibAuthorName>


												<Year>1989</Year>


												<ArticleTitle
Language="En">A tool for multiple sequence alignment</ArticleTitle>


												<JournalTitle>Proc Natl Acad Sci USA</JournalTitle>


												<VolumeID>86</VolumeID>


												<FirstPage>4412</FirstPage>


												<LastPage>4415</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR24">

											<CitationNumber>24.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Carrillo</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DJ</Initials>


													<FamilyName>Lipman</FamilyName>


												</BibAuthorName>


												<Year>1988</Year>


												<ArticleTitle
Language="En">The multiple sequence alignment problem in biology</ArticleTitle>


												<JournalTitle>SIAM J Appl Math</JournalTitle>


												<VolumeID>48</VolumeID>


												<FirstPage>1073</FirstPage>


												<LastPage>1082</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR25">

											<CitationNumber>25.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Stoye</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>V</Initials>


													<FamilyName>Moulton</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>AW</Initials>


													<FamilyName>Dress</FamilyName>


												</BibAuthorName>


												<Year>1997</Year>


												<ArticleTitle
Language="En">DCA: an efficient implementation of the divide-andconquer approach to simultaneous multiple sequence alignment</ArticleTitle>


												<JournalTitle>Comput Appl Biosci</JournalTitle>


												<VolumeID>13</VolumeID>


												<FirstPage>625</FirstPage>


												<LastPage>6</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR26">

											<CitationNumber>26.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Reinert</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>J</Initials>


													<FamilyName>Stoye</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Will</FamilyName>


												</BibAuthorName>


												<Year>2000</Year>


												<ArticleTitle
Language="En">An iterative method for faster sum-of-pair multiple sequence alignment</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>16</VolumeID>


												<FirstPage>808</FirstPage>


												<LastPage>814</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR27">

											<CitationNumber>27.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JD</Initials>


													<FamilyName>Thompson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DG</Initials>


													<FamilyName>Higgins</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>TJ</Initials>


													<FamilyName>Gibson</FamilyName>


												</BibAuthorName>


												<Year>1994</Year>


												<ArticleTitle
Language="En">CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position specific gap penalties and weight matrix choice</ArticleTitle>


												<JournalTitle>Nucleic Acids Research</JournalTitle>


												<VolumeID>22</VolumeID>


												<FirstPage>4673</FirstPage>


												<LastPage>4680</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR28">

											<CitationNumber>28.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>DF</Initials>


													<FamilyName>Feng</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RF</Initials>


													<FamilyName>Doolittle</FamilyName>


												</BibAuthorName>


												<Year>1987</Year>


												<ArticleTitle
Language="En">Progressive sequence alignment as a prerequisite to correct phylogenetic trees</ArticleTitle>


												<JournalTitle>J Mol Evol</JournalTitle>


												<VolumeID>25</VolumeID>


												<FirstPage>351</FirstPage>


												<LastPage>360</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR29">

											<CitationNumber>29.</CitationNumber>


											<BibArticle>

												<Year />


												<NoArticleTitle />


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR30">

											<CitationNumber>30.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>SF</Initials>


													<FamilyName>Altschul</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Gish</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Miller</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>EW</Initials>


													<FamilyName>Myers</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>DJ</Initials>


													<FamilyName>Lipman</FamilyName>


												</BibAuthorName>


												<Year>1990</Year>


												<ArticleTitle
Language="En">Basic local alignment search tool</ArticleTitle>


												<JournalTitle>J Mol Biol</JournalTitle>


												<VolumeID>215</VolumeID>


												<FirstPage>403</FirstPage>


												<LastPage>410</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR31">

											<CitationNumber>31.</CitationNumber>


											<BibArticle>

												<Year />


												<NoArticleTitle />


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR32">

											<CitationNumber>32.</CitationNumber>


											<BibArticle>

												<Year />


												<NoArticleTitle />


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:Authors>

						<meta:Author>Chen, Yixin</meta:Author>

						<meta:Author>Wan, Andrew</meta:Author>

						<meta:Author>Liu, Wei</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution geo="-90.3230807,38.6682669,0">

							<meta:OrgName>Washington University in St. Louis</meta:OrgName>

							<meta:GeoOrg>-90.3230807,38.6682669,0#Washington University in St. Louis</meta:GeoOrg>

							<meta:Country>United States</meta:Country>

						</meta:Institution>

						<meta:Institution geo="114.263447,22.336812,0">

							<meta:OrgName>Department of Computer Science</meta:OrgName>

							<meta:GeoOrg>114.263447,22.336812,0#Department of Computer Science</meta:GeoOrg>

							<meta:Country>Hong Kong</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2006-12-12</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1471-2105-7-S4-S4</meta:DOI>

					<meta:Title>A fast parallel algorithm for finding the longest common sequence of multiple biosequences</meta:Title>

					<meta:ISXN>1471-2105</meta:ISXN>

					<meta:Journal>BMC Bioinformatics</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>S4</meta:ArticleFirstPage>

					<meta:Publication>BMC Bioinformatics</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Bioinformatics</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Microarrays</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Combinatorial Libraries</meta:Subject>

						<meta:Subject Priority="6"
Type="Secondary">Algorithms</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images />

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>


					<PublisherName>Springer Netherlands</PublisherName>



					<PublisherLocation>Dordrecht</PublisherLocation>


				</PublisherInfo>

				<Journal OutputMedium="All">


					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">


						<JournalID>10676</JournalID>



						<JournalPrintISSN>1388-1957</JournalPrintISSN>



						<JournalElectronicISSN>1572-8439</JournalElectronicISSN>



						<JournalTitle>Ethics and Information Technology</JournalTitle>



						<JournalAbbreviatedTitle>Ethics Inf Technol</JournalAbbreviatedTitle>



						<JournalSubjectGroup>


							<JournalSubject
Type="Primary">Computer Science</JournalSubject>



							<JournalSubject
Type="Secondary">Library Science</JournalSubject>



							<JournalSubject
Type="Secondary">User Interfaces and Human Computer Interaction</JournalSubject>



							<JournalSubject
Type="Secondary">Ethics</JournalSubject>



							<JournalSubject
Type="Secondary">Technology Management</JournalSubject>



							<JournalSubject
Type="Secondary">Management of Computing and Information Systems</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>



					<JournalOnlineFirst>


						<Article ID="s10676-010-9233-7">


							<ArticleInfo ArticleCitation="ArticleFirstPage"
ArticleType="OriginalPaper" ContainsESM="No" Language="En"
NumberingStyle="Unnumbered" TocLevels="0">


								<ArticleID>9233</ArticleID>



								<ArticleDOI>10.1007/s10676-010-9233-7</ArticleDOI>



								<ArticleSequenceNumber>0</ArticleSequenceNumber>



								<ArticleTitle Language="En"
OutputMedium="All">Is there an ethics of algorithms?</ArticleTitle>



								<ArticleFirstPage>1</ArticleFirstPage>



								<ArticleLastPage>10</ArticleLastPage>



								<ArticleHistory>


									<RegistrationDate>


										<Year>2010</Year>



										<Month>6</Month>



										<Day>15</Day>


									</RegistrationDate>



									<OnlineDate>


										<Year>2010</Year>



										<Month>7</Month>



										<Day>3</Day>


									</OnlineDate>


								</ArticleHistory>



								<ArticleCopyright>


									<CopyrightHolderName>The Author(s)</CopyrightHolderName>



									<CopyrightYear>2010</CopyrightYear>


								</ArticleCopyright>



								<ArticleGrants Type="OpenChoice">


									<MetadataGrant Grant="OpenAccess" />



									<AbstractGrant Grant="OpenAccess" />



									<BodyPDFGrant Grant="OpenAccess" />



									<BodyHTMLGrant Grant="OpenAccess" />



									<BibliographyGrant Grant="OpenAccess" />



									<ESMGrant Grant="OpenAccess" />


								</ArticleGrants>



								<ArticleContext>


									<JournalID>10676</JournalID>



									<VolumeIDStart>
										<?InsertByIssueBuilding
VolumeIDStart?>
									</VolumeIDStart>



									<VolumeIDEnd>
										<?InsertByIssueBuilding
VolumeIDEnd?>
									</VolumeIDEnd>



									<IssueIDStart>
										<?InsertByIssueBuilding
IssueIDStart?>
									</IssueIDStart>



									<IssueIDEnd>
										<?InsertByIssueBuilding IssueIDEnd?>
									</IssueIDEnd>


								</ArticleContext>


							</ArticleInfo>



							<ArticleHeader>


								<AuthorGroup>


									<Author AffiliationIDS="Aff1">


										<AuthorName DisplayOrder="Western">


											<GivenName>Felicitas</GivenName>



											<FamilyName>Kraemer</FamilyName>


										</AuthorName>



										<Contact>


											<Email>f.kraemer@tue.nl</Email>


										</Contact>


									</Author>



									<Author AffiliationIDS="Aff1">


										<AuthorName DisplayOrder="Western">


											<GivenName>Kees</GivenName>



											<Particle>van</Particle>



											<FamilyName>Overveld</FamilyName>


										</AuthorName>



										<Contact>


											<Email>cvoverve@tue.nl</Email>


										</Contact>


									</Author>



									<Author AffiliationIDS="Aff1"
CorrespondingAffiliationID="Aff1">


										<AuthorName DisplayOrder="Western">


											<GivenName>Martin</GivenName>



											<FamilyName>Peterson</FamilyName>


										</AuthorName>



										<Contact>


											<Phone>+31-40-2475941</Phone>



											<Email>m.peterson@tue.nl</Email>



											<URL>www.martinpeterson.org</URL>


										</Contact>


									</Author>



									<Affiliation ID="Aff1">


										<OrgDivision>Section for Philosophy and Ethics</OrgDivision>



										<OrgName>Eindhoven University of Technology</OrgName>



										<OrgAddress>


											<Postbox>P.O. Box 513</Postbox>



											<Postcode>5600 MB</Postcode>



											<City>Eindhoven</City>



											<Country>The Netherlands</Country>


										</OrgAddress>


									</Affiliation>


								</AuthorGroup>



								<Abstract ID="Abs1" Language="En"
OutputMedium="All">


									<Heading>Abstract</Heading>



									<Para>We argue that some algorithms are value-laden, and that two or more persons who accept different value-judgments may have a rational reason to design such algorithms differently. We exemplify our claim by discussing a set of algorithms used in medical image analysis: In these algorithms it is often necessary to set certain thresholds for whether e.g. a cell should count as diseased or not, and the chosen threshold will partly depend on the software designer’s preference between avoiding false positives and false negatives. This preference ultimately depends on a number of value-judgments. In the last section of the paper we discuss some general principles for dealing with ethical issues in algorithm-design.</Para>


								</Abstract>



								<KeywordGroup Language="En" OutputMedium="All">


									<Heading>Keywords</Heading>



									<Keyword>Algorithm</Keyword>



									<Keyword>Image analysis</Keyword>



									<Keyword>Medical technology</Keyword>



									<Keyword>False positive</Keyword>



									<Keyword>False negative</Keyword>


								</KeywordGroup>


							</ArticleHeader>



							<Body>


								<Section1 ID="Sec1" Type="Introduction">


									<Heading>Introduction</Heading>



									<Para>The focus of this article is on ethical aspects of algorithms. An algorithm is, roughly speaking, a finite sequence of well-defined instructions that describe in sufficiently great detail how to solve a problem. Both computers and humans use algorithms for solving a wide range of problems. However, in this paper we shall be exclusively concerned with algorithms implemented in computers.</Para>



									<Para>At first glance it might be tempting to conclude that algorithms are value-free entities that do not, at least not in their most abstract form, have any ethical dimensions. However, in this article we argue that this commonsense view about algorithms is false. Many, but not all, algorithms implicitly or explicitly comprise essential value-judgments. By an ‘essential value-judgment’ we mean the following: If two algorithms are designed to perform the same task, such as classifying a cell as diseased or non-diseased, these algorithms are essentially value-laden if one cannot rationally choose between them without explicitly or implicitly taking ethical concerns into account. Another way of saying this is that the algorithm cannot be designed without implicitly or explicitly taking a stand on ethical issues, some of which may be highly controversial.</Para>



									<Para>If true, our claim about essentially value-laden algorithms has to be taken seriously by software engineers who design algorithms. If some algorithms are essentially value-laden, i.e. if people who design algorithms cannot avoid making ethical judgments about what is good and bad, then it is reasonable to maintain that software designers are morally responsible for the algorithms they design.

										<Footnote ID="Fn1">


											<Para>Software designers can, of course, be morally responsible also for algorithms that are not essentially value-laden. A fatal accident caused by a faulty algorithm can be (partially) blamed on the software designer, irrespective of whether the algorithm is essentially value-laden or not.</Para>


										</Footnote>
 Although the term ‘ethics of algorithms’ might have far-reaching connotations, it nevertheless captures what is at stake here. If our claim about essentially value-laden algorithms is correct, there will indeed be an ethics of algorithms.
									</Para>



									<Para>The issue that we address has to some extent already been touched upon in the literature. Turilli seeks to develop a solution to “the problem of specifying computational systems that behave in accordance with a given set of ethical principles”.

										<Footnote ID="Fn2">


											<Para>Turilli (

												<CitationRef
CitationID="CR7">2007</CitationRef>
: 49).
											</Para>


										</Footnote>
 Allen et al. argue that, “we need to integrate artificial moral agents into … new technologies to manage their complexity”.

										<Footnote ID="Fn3">


											<Para>Allen et al. (

												<CitationRef
CitationID="CR1">2006</CitationRef>
: 13).
											</Para>


										</Footnote>
 Finally, Arkin argues that there is a need for, “immediate investment in … machine/robot ethics”.

										<Footnote ID="Fn4">


											<Para>Arkin (

												<CitationRef
CitationID="CR2">2009</CitationRef>
: 2).
											</Para>


										</Footnote>
 Although we do not necessarily disagree with these authors, we wish to point out that their papers are concerned with questions that are slightly different from the issue discussed here. Turilli, Allen et al., and Arkin discuss how we can construct algorithms and systems that behave in 

										<Emphasis
Type="Italic">accordance with</Emphasis>
 ethical principles. As we argue in the next section, such algorithms need not be essentially value-laden themselves. Indeed, an algorithm or system that behaves in 

										<Emphasis
Type="Italic">accordance with</Emphasis>
 some ethical principles can itself be ethically neutral. What we wish to discuss in this article is the ethical features of the algorithm itself.
									</Para>



									<Para>The structure of this article is as follows. In “

										<InternalRef
RefID="Sec2">Value-laden algorithms and the design process</InternalRef>
” we characterise some of the central concepts used in the paper and argue that algorithm design is in many respects similar to designing other technological artefacts. In “

										<InternalRef
RefID="Sec3">A real example</InternalRef>
” we discuss a real example of an essentially value-laden algorithm used in medical image analysis, and in “

										<InternalRef
RefID="Sec4">The precautionary principle and ethical theories</InternalRef>
” we discuss some ways of dealing with the ethical problems raised by this algorithm. Finally, in “

										<InternalRef
RefID="Sec5">Should ethical values be user-defined?</InternalRef>
”, we argue that software designers should as far as possible leave ethical choices to users, and when this is not possible the ethical assumptions underlying the algorithm should at least be made transparent. This may lead to new or refined procedures in the process of software engineering.
									</Para>


								</Section1>



								<Section1 ID="Sec2">


									<Heading>Value-laden algorithms and the design process</Heading>



									<Para>It is notoriously difficult to give a precise characterisation of what an algorithm is. Numerous definitions have been proposed in the literature.

										<Footnote ID="Fn5">


											<Para>See e.g. Sipser (

												<CitationRef
CitationID="CR6">1997</CitationRef>
).
											</Para>


										</Footnote>
 However, nothing in our reasoning hinges on how exactly algorithms are formally defined. The definition of an algorithm suggested in “

										<InternalRef
RefID="Sec1">Introduction</InternalRef>
” will suffice for our present purposes.
									</Para>



									<Para>Arguably, the claim that many algorithms implicitly or explicitly comprise essential value-judgments cannot be accurately assessed without first defining some of its central concepts. We take a value-judgment to be any proposition expressing a view on how things ought to be or not to be, or what is good or bad, or desirable or undesirable. It takes little reflection to see that not all algorithms express value-judgments. Consider, for instance, algorithms used for calculating the lexicographical order of a finite set of strings, such as the ones used to put words in alphabetical order. Given the input to this algorithm, there is only one possible correct output. Therefore, this task is fully specified in terms of various logical relationships, which require no approximation or interpretation.</Para>



									<Para>That said, some algorithms clearly produce genuine value-judgments. Consider, for example, algorithms used in decision support programs, i.e. systems that help decision makers to make better decisions by ranking a set of alternative actions with respect to some predefined criteria. A typical outcome of an algorithm used in such a program is a verdict like “Alternative X is the best option” or “Alternative X is better than alternative Y with respect to criterion Z”. It would be pointless to deny that these sentences express genuine value-judgments. This is acknowledged by Turilli, who discusses how to specify “systems that behave in accordance with a given set of ethical principles”.

										<Footnote ID="Fn6">


											<Para>Turilli (

												<CitationRef
CitationID="CR7">2007</CitationRef>
: 49).
											</Para>


										</Footnote>
 However, the fact that an algorithm or system yields a value-judgment as its output does not prove that the algorithms used for producing the value-judgments are essentially value-laden, as pointed out in “

										<InternalRef
RefID="Sec1">Introduction</InternalRef>
”. The string of sentences (or data) that is produced by running through an algorithm might very well be value-laden even if the algorithm itself is not value-laden.
									</Para>



									<Para>Let us take a closer look at this important point. The general features of the kind of value-judgments generated by decision support programs can be clarified by invoking Kant’s famous distinction between hypothetical and categorical imperatives. A hypothetical imperative is a statement about what you ought to do, given that you wish to respect some exogenously defined goal. “Don’t study philosophy if you wish to become rich” is a hypothetical imperative. A categorical imperative is a value-judgment that tells us what we ought to do irrespective of our desires or goals. The distinction between hypothetical and categorical imperatives does not exhaust the logical space of all possible value-judgments, but it illuminates the nature of the kind of value-judgments that are most likely to be encountered in computer programs used for aiding decisions. Such value-judgments are best characterised as hypothetical value-judgments, because they tell you what you ought to do given that you wish to achieve a specific aim. However, as pointed out above, the presence of a hypothetical value-judgment does not entail that the algorithm used for generating the recommendations comprises value-judgments. The underlying algorithm can, when taken in isolation, be completely value-free.</Para>



									<Para>So what, then, could it possibly mean to say that an algorithm comprises a value-judgment and therefore has an essential ethical dimension? Consider the following suggestion.

										<UnorderedList Mark="None">


											<ItemContent>


												<Para>ALGORITHM COMPRISING AN ESSENTIAL VALUE-JUDGMENT</Para>


											</ItemContent>



											<ItemContent>


												<Para>An algorithm comprises an essential value-judgment if and only if, everything else being equal, software designers who accept different value-judgments would have a rational reason to design the algorithm differently (or choose different algorithms for solving the same problem).</Para>


											</ItemContent>


										</UnorderedList>


									</Para>



									<Para>Note that this criterion does not make the definition circular. It is indeed true that the term ‘value-judgment’ occurs both in the left-hand part and in the right-hand part of the criterion, but the term that is to be explained is ‘algorithm comprising an essential value-judgment’, not ‘value-judgment’. (Recall that we have already explained above how we understand the term ‘value-judgment’.) It is also worth noting that our necessary and sufficient criterion explains why an algorithm producing hypothetical imperatives, such as the decision aid program discussed above, does not comprise essential value-judgments. If two or more software designers are asked to write a decision aid program it does not matter what ethical principles (or other value-judgments) the software designers themselves subscribe to. Since the aim of the program is to produce hypothetical imperatives, the algorithm itself can be entirely value-free.

										<Footnote ID="Fn7">


											<Para>It is worth noticing that some decision support systems may apply a reasoning algorithm that, although deterministic, is too complicated to be followed by human users (e.g., since it would take too long to be practical). In that case, the user cannot do anything else but follow up on the algorithm’s suggestion 

												<Emphasis Type="Italic">or</Emphasis>
 ignore the suggestion altogether. This renders the decision support algorithm value-laden.
											</Para>


										</Footnote>
 It is certainly true that value-judgments are produced by a successful implementation of the program, but it is not true that two or more software designers accepting different value-judgments would ever have a rational reason to design the underlying algorithm differently.
									</Para>



									<Para>We now come to a fundamental assumption in this paper, viz. that a software designer designing an algorithm is facing a decision process that is in many respects similar to that faced by people designing other technological artifacts. If true, this entails that value-judgments that are built into an algorithm will in many respects be similar to value judgments that feature in other design processes. For this reason it is helpful to elaborate a bit on the design processes from which algorithms and other technological artifacts are derived.</Para>



									<Para>Let us first explain what we mean by ‘design’ by proposing a necessary condition we believe all such activities have to fulfill. (For present purposes, we can do without a sufficient condition.)

										<UnorderedList Mark="None">


											<ItemContent>


												<Para>DESIGN ACTIVITY: Something is a design activity only if it is an activity where a designer (or a team of designers) make decisions in order to reach a pre-defined goal.</Para>


											</ItemContent>


										</UnorderedList>


									</Para>



									<Para>We use the term ‘variable’ both for characterising what it means to ‘make decisions’ and for ‘reaching a goal’. We assume that a variable stores exactly one piece of information. For instance, when designing a bicycle, the designer can decide that the front wheel shall have a diameter of 60 cm; this ‘diameter of the front wheel’ is a variable, and ‘60 cm’ is the value stored in this variable. However, we must carefully distinguish the (four) different ways in which values can be stored in a variable.</Para>



									<Para>First, the designer can 

										<Emphasis Type="Italic">decide</Emphasis>
 that a variable shall have a particular value. For the rest of this paper, this settles our definitions of ‘making a decision’ as ‘assigning a value to a variable’, assuming that the decision-maker has the authority to assign the chosen value to the variable in question. For instance, no designer can decide that ‘this bicycle will have a maximum velocity of 250 km/hour’, since the maximum velocity of a bicycle is the result of a range of other decisions and empirical circumstances. Variables that may occur in a decision are called 

										<Emphasis
Type="Italic">decision variables</Emphasis>
 or 

										<Emphasis Type="Italic">category</Emphasis>
-

										<Emphasis Type="Italic">I</Emphasis>
 variables.
									</Para>



									<Para>Second, a variable can take a value as part of the assessment of the degree to which a design-objective was met. For instance, suppose the bicycle we are designing should be as light as possible; then the value of the variable ‘weight of the bicycle’ helps to assess if we were successful. Such variables will be called 

										<Emphasis
Type="Italic">objective variables</Emphasis>
 or 

										<Emphasis Type="Italic">category</Emphasis>
-

										<Emphasis Type="Italic">II</Emphasis>
 variables. Notice that any category-II variable receives a value as the final consequence of assigning values to category-I variables, plus a number of (physical, economical, and social) mechanisms that causally propagate the category-I values through the designed artifact and its usage.
									</Para>



									<Para>From the account sketched above, we see that category-II variables are (in a mathematical sense) a function of category-I variables: the design function. However, they are also a function of another class of variables. In order to see this, note that the weight of the bicycle depends on e.g. the density of the chosen material. If we choose carbon fiber (a possible value for the category-I variable ‘material of the bicycle’), we have to accept that the density of carbon fiber occurs as an argument in our design function. Such variables cannot be controlled by the designer. We call them 

										<Emphasis
Type="Italic">context variables</Emphasis>
 or 

										<Emphasis Type="Italic">category</Emphasis>
-

										<Emphasis Type="Italic">III</Emphasis>
 variables. From the perspective of the designer, they are constants

										<Emphasis Type="Italic">.</Emphasis>


									</Para>



									<Para>In all but trivial cases, the design function is immensely complicated. It is next to impossible to write down the values of category-II variables even if we know the values of category-I and category-III variables. We can regard it, however, as a composition of a large number of much simpler functions. The weight, for instance, is the sum of the weights of the two wheels plus the weight of the frame. The weight of one wheel is not a category-II variable (because the design is not necessarily better if this weight is less); it is also not a category-I variable (if we decide the diameter, the weight is no longer independent); finally it is not in category-III because it is not a constant. It is what we call an 

										<Emphasis
Type="Italic">auxiliary variable</Emphasis>
 or a 

										<Emphasis Type="Italic">category</Emphasis>
-

										<Emphasis Type="Italic">IV</Emphasis>
 variable.
									</Para>



									<Para>This completes our general outline of the design process. What it amounts to is the establishment of a network of functional relationships between variables in categories I, II, III, and IV and the assignment of values to the category-I variables; the ‘quality’ of the design is assessed by inspecting the resulting values for the category-II variables. Consider Fig. 

										<InternalRef RefID="Fig1">1</InternalRef>
.

										<Figure Category="Standard" Float="Yes"
ID="Fig1">


											<Caption Language="En">


												<CaptionNumber>Fig. 1</CaptionNumber>



												<CaptionContent>


													<SimplePara>Designing as a process of connecting 4 types of variables. 

														<Emphasis
Type="Italic">Arrows</Emphasis>
 indicate functional dependencies
													</SimplePara>


												</CaptionContent>


											</Caption>



											<MediaObject ID="MO1">


												<ImageObject Color="BlackWhite"
Format="GIF" Rendition="HTML" Type="LinedrawHalftone"
FileRef="MediaObjects/10676_2010_9233_Fig1_HTML.gif" />


											</MediaObject>


										</Figure>


									</Para>



									<Para>The four categories characterised in Fig. 

										<InternalRef RefID="Fig1">1</InternalRef>
 suggest an important link between ethics and the freedom of the designer: The choice of category-II variables may reflect, among other things, the designer’s ethical view. For example, the designer might stipulate that the bicycle be constructed from recycled materials (reflecting the value of sustainability), and that it must be manufacturable in third-world countries (reflecting the value of geopolitical justice). In this respect we should observe that category-II variables come in two versions: 

										<Emphasis
Type="Italic">requirements</Emphasis>
, that is: predicates that must be true (say, ‘the bicycle shall be lighter than 15 kg’), and 

										<Emphasis Type="Italic">desires</Emphasis>
 that relate to variables subject to optimisation (say, ‘the bicycle shall be as light as possible’). Among other things, the framework of four categories allows us to clarify trade-offs in the design, it helps us to pinpoint compromises, and it provides insight into possible design alternatives. We will use it here to decouple, on the one hand, the acceptance of an ethical position (such as the categorical imperative) from the implementation of the consequences of this position in a design (a hypothetical imperative): The first consists of deciding which ethical values should be represented; the second of representing these ethical values in terms of category-II variables.
									</Para>


								</Section1>



								<Section1 ID="Sec3">


									<Heading>A real example</Heading>



									<Para>In this section we discuss some examples of algorithms that are essentially value-laden. All our examples, one of which is real, are concerned with a particular kind of algorithm used in medical image technologies. Very briefly put, medical image technologies aim at representing human biological structures in computers in an accurate way, such as human organs and cells in blood samples, and thereby improve the diagnostic or therapeutic prospects of diseases affecting the biological structures in question. One of the many ethical issues raised by such algorithms is the risk of using algorithms that produce false positive and false negative results. By definition, a false positive result occurs whenever the algorithm triggers the system to count something (a cell, a symptom of a disease) in a digital image that is not actually there. A false negative occurs if an algorithm in a similar vein fails to identify a structure in the picture that is actually there.</Para>



									<Para>For all practical means, it is virtually impossible to totally eliminate the risk of getting false positives and false negatives. However, if one is willing to accept a large number of false positive results one will typically get a smaller number of false negatives. In order to understand why, imagine you are asked by an eccentric millionaire to build a device that automatically counts the number of tigers in the jungle who passes through a certain spot. Naturally, if you build a device that simply counts everything yellow that passes through the jungle you will get a rather large number of false positive results, since e.g. bees, bananas, and yellow flowers will trigger the device. However, if on the other hand, you impose more conservative criteria on what is to count as a positive result and design the device such that only very large yellow items are detected, then you can expect the number of false negatives to rise (since e.g. new-born tigers babies will not be detected by the device).</Para>



									<Para>Clearly, software designers designing toy algorithms for detecting tigers, as well as real-life algorithms used in medical image technologies, have to make a trade-off between minimising the number of false positives or the number of false negative results. This trade-off will inevitably be based on a value-judgment. There is simply no objective fact of the matter about whether it is more desirable to avoid a false positive or a false negative. Different users may have different preferences, and several conflicting preferences appear to be equally rational. It therefore seems hard to deny that an algorithm used for, say, counting the percentage of cells infected by a virus in relation to the number of non-infected cells will invariably contain a value-judgment about how many false positive results are tolerable in relation to the number of false negative ones. That said, both false positive and false negative results may give rise to severe negative effects for individual patients. Doctors base diagnostic as well as therapeutic decisions on what they come to believe about e.g. the number of infected cells in relation to the number of non-infected cells.</Para>



									<Para>We can also illustrate our point about essentially value-laden algorithms by way of a real example. To start with, recall that in “

										<InternalRef
RefID="Sec2">Value-laden algorithms and the design process</InternalRef>
” we discussed some of the choices faced by designers by discussing the design of a material object (a bicycle). However, as pointed out in that section, the design process is roughly the same when designing immaterial artifacts such as algorithms. We can therefore use the general design framework introduced in “

										<InternalRef
RefID="Sec2">Value-laden algorithms and the design process</InternalRef>
” for illustrating how values for category-I variables propagate in the direction of category-II variables.
									</Para>



									<Para>In the image to the right we see an MR-scan depicting a cross section of a human heart. For the purpose of diagnosing a variety of possible pathologies, it is necessary to accurately estimate the blood volume of the heart during various stages of a heart-beat cycle. The difference between blood and heart muscle tissue occurs as a difference in grey values in MR images. Estimating the blood volume starts with establishing which part of the image is colored lighter grey, and counting the number of pixels in this light grey area. This is called 

										<Emphasis
Type="Italic">segmentation</Emphasis>
 between light and dark, as depicted schematically in the diagram below. 

										<InlineMediaObject>


											<ImageObject Color="Color"
Format="JPEG" Rendition="HTML" Type="Halftone"
FileRef="MediaObjects/10676_2010_9233_Figa_HTML.jpg" />


										</InlineMediaObject>


									</Para>



									<Para>One way of carrying out a segmentation is to use a numerical threshold, say T. This means that pixels with a lightness value above T are labeled ‘light’ and pixels with a lightness value below T are labeled ‘dark’. The border between light and dark areas is, however, not sharp (Fig. 

										<InternalRef RefID="Fig2">2</InternalRef>
).

										<Footnote ID="Fn8">


											<Para>The algorithm outlined here is a deliberate simplification of actual, modern segmentation algorithms. In particular, the algorithms demonstrated in the cited website are much more sophisticated. For our argument, however, this is not relevant. Even advanced algorithms typically involve parameters, the values of which are to be chosen in order to decide, eventually, between ‘normal’ and ‘pathological’ cases.</Para>


										</Footnote>



										<Figure Category="Standard" Float="Yes"
ID="Fig2">


											<Caption Language="En">


												<CaptionNumber>Fig. 2</CaptionNumber>



												<CaptionContent>


													<SimplePara>Segmentation in a noisy image. For a larger threshold value, T1, the estimated area V1 of the ‘blood’ segment will be lower than the estimated area V2 which is found with a lower threshold T2. It is not a priori clear, however, which of the two threshold values is the ‘correct’ one. The software engineer chooses a threshold without real argument—thereby biasing the outcome of the algorithm when it is used on patient data. This may statistically influence the change of false positive diagnostic errors in favour of false negatives, or vice versa</SimplePara>


												</CaptionContent>


											</Caption>



											<MediaObject ID="MO2">


												<ImageObject Color="BlackWhite"
Format="GIF" Rendition="HTML" Type="Linedraw"
FileRef="MediaObjects/10676_2010_9233_Fig2_HTML.gif" />


											</MediaObject>


										</Figure>


									</Para>



									<Para>It is quite common to introduce thresholds in segmentation-like algorithms. Such thresholds serve as nice examples of category-I variables in the design process of the algorithm. There is no 

										<Emphasis Type="Italic">a priori</Emphasis>
 correct value for such thresholds. There is no rigorous first principles from which they can be derived, since the noise in the image is an inevitable artifact of the MR measuring process, caused by numerous non-modeled sources. Software engineers typically choose a value that ‘seems reasonable’ for thresholds. In the example above we see how the choice for this threshold influences the estimated blood volume. This estimate will in turn affect further values, and eventually the diagnosis. So in borderline cases the diagnosis will (indirectly) depend on the value of the threshold T.
									</Para>



									<Para>Of course, if the anatomical configuration is far from a borderline case (either ‘very’ healthy or ‘very’ pathological), there is obviously no problem. For the majority of pathological conditions, however, there is a continuum between ‘healthy’ and ‘pathological’. Somewhere in this continuum there is a grey zone where the diagnostic outcome will critically depend on thresholds somewhere along the computational pipeline.

										<Footnote ID="Fn9">


											<Para>In principle, an algorithm can detect when it bases a decision on a ‘near-borderline’-case. Sophisticated algorithms sometimes give an estimate of the reliability of their conclusions (for instance: long-term weather predictions, e.g. in the form of error-bandwidths). In this way, an algorithm can become less value-laden, since it moves part of the responsibility in the direction of the (human) user.</Para>


										</Footnote>


									</Para>



									<Para>Now, given that we take the computational output of a medical diagnosis support system to be a designed artifact, we can ask ourselves what the category-II variables are in this example. Apart from trivial ones, such as patient safety during clinical measurement, and operational comfort for the medical specialist, we may want the quality of the (automated) diagnosis to be ‘good’. But the notion of goodness must of course be rendered more precise, and this is exactly where our observations concerning false positives and false negatives come in. In borderline cases the bias towards false positives or false negatives may to a large extent be determined by the values of thresholds or other category-I variables.</Para>



									<Para>At this point it could be objected that the best way forward is to design the algorithm such that it totally eliminates all false positive and false negative results. However, a closer analysis of the risks for getting false positives and false negatives in medical images shows that this is not likely to be a feasible option. First, all algorithms that are applied to a large set of measurements will be sensitive to stochastic effects in the data set, and this will automatically yield false positive and negative effects. Second, it should be noted that the images are in most cases numerical (re-)constructions of large amounts of physical measurements. This point has important consequences: Nowadays the quality of such computer generated images is getting close to being photo realistic. That is, the data is presented (rendered) as if it is an actual photo of some 3-D internal organ or tissue structure, perhaps even isolated from its environment. Such rendering or isolation, however, requires complicated algorithms that include segmentation (as mentioned above), as well as much other subtle image processing. So rather than being objective photographs, the 3D medical images that specialists look at (and base their diagnosis on) are the result of an elaborate algorithmic process, depending on arbitrary thresholds in a difficult-to-predict manner. Now, medical specialists, like any other humans, are accustomed to interpret plausible 3-D images as accurate projections of corresponding 3-D objects. It is very difficult 

										<Emphasis Type="Italic">not</Emphasis>
 to interpret a realistically looking 3-D image as a trustworthy projection of a 3-D object. This introduces the risk that one will forget that in order to generate these 3-D images, a number of decisions about thresholds had to be taken. Had some decisions been taken differently, the image could have looked (very) different—and, as we argue, there is no a priori way to give the correct value for such thresholds.
									</Para>



									<Para>Understanding the relations between what is seen in the image and what really exists in a patient’s body (or in the microscopic slice of tissue being examined) therefore requires a thorough understanding of the various algorithmic steps that are applied to the physical data in their transformation towards a collection of colored pixels. Such transformations include a choice of filters (involving variables with values that may significantly affect the eventual ‘looks’ of the image – and therefore the conspicuousness of certain types of anomalies) and perhaps geometrical algorithms (such as contouring, segmentation, tracking, etcetera). These filters and algorithms have to be chosen with some goals in mind – but these goals cannot typically, for practical reasons, be formulated in terms of minimising the risk of acquiring false negatives.</Para>



									<Para>The third and last reason why it is not feasible to design algorithms that avoid all errors can be formulated as follows. Many (semi)automated diagnostic tools have been developed in recent years, and these diagnostic tools typically incorporate implied hypotheses of expected pathologies. For instance: a probe to identify a potential aneurism, stenosis or tumor can only do so because it 

										<Emphasis Type="Italic">expects</Emphasis>
 certain characteristic features of, say the shape or volume of such anomalies. An experienced medical doctor realises when she sees something ‘unexpected’; a software program usually cannot. Again, these diagnostic tools may not have been developed with the goal of minimising the percentage of false negatives in mind. Finally, there is one further problem. Software for complicated tasks such as medical diagnosis is often immensely complicated. Much if it consists of components that may have been developed earlier for ‘general purposes’; these components are re-used in order to make the software production process economically feasible. Segmentation algorithms are an example of such components. Components are preferably treated as black boxes: based on their formal, functional specifications, their behavior can be assumed to be ‘correct’. The ethical position, however, that was adopted during the construction of such a component when either choosing a more conservative or more liberal threshold, is typically not part of their formal specification.

										<Footnote ID="Fn10">


											<Para>By ‘ethical position’ we mean the decision at which level the threshold should lie. We will deal with this in detail in the next section.</Para>


										</Footnote>
 That means that the same segmentation algorithm, applied in two different systems, will behave equally with respect to its formal, functional requirements, but at the same time it may behave oppositely with respect to its tendency to produce false positive or false negative judgments.
									</Para>



									<Para>The total complexity of highly interrelated physical techniques, algorithms, heuristics and visualisation methods is immense, and there is little hope to optimise the entire, integrated chain in terms of minimising the chance of false negatives. What may be feasible, however, it to take this chain apart in a systematic way, and to analyse the various stages as if they were (to some extent) independent. This route may be facilitated to use a framework such as the 8-layers model developed by one of the authors to argue about the information contents in images. This model separates this information into coherent chunks (e.g., the shape-related information is separated from the texture-related information, which is separated from the 3-D surface related information, etcetera.)</Para>


								</Section1>



								<Section1 ID="Sec4">


									<Heading>The precautionary principle and ethical theories</Heading>



									<Para>In this section we discuss two different ways of addressing ethical issues raised by the risk of obtaining false positive and false negative results when using medical image algorithms. In the next section we shall identify and discuss some ethical issues that apply to algorithm design in general.</Para>



									<Para>Briefly put, we propose that a possible way of managing false positive and false negative results obtained from medical image algorithms is to adopt an epistemic interpretation of the precautionary principle.

										<Footnote ID="Fn11">


											<Para>This section is partly based on some ideas first presented in Peterson (

												<CitationRef
CitationID="CR4">2007</CitationRef>
).
											</Para>


										</Footnote>
 The second option, which we will also explore in depth, is to approach this choice as an ethical judgment to be determined by an ethical theory.
									</Para>



									<Para>Let us first consider the precautionary-oriented, epistemic strategy. The precautionary principle was originally invoked by policy makers for addressing environmental issues, such as global warming, toxic waste disposal, and marine pollution. In recent years it has also been suggested that the precautionary principle may also be applied to medical issues. David B. Resnik argues that “properly understood, the [Precautionary Principle] can provide physicians and patients with a useful approach to medical decision making.” The precautionary principle can, however, be interpreted in many different ways. According to the epistemic (belief-guiding) interpretation outlined in Peterson (

										<CitationRef
CitationID="CR4">2007</CitationRef>
), the precautionary principle should be characterised in terms of what it urges us to believe. To put it more precisely, the epistemic version of the precautionary principle holds that it is better to get a false positive rather than a false negative result, contrary to what is commonly accepted in science, since this will prevent doctors and patients from falsely believing that someone who is in fact ill is healthy.
									</Para>



									<Para>Scientists generally agree that it is more important to avoid false positives than false negatives. This is because scientific knowledge tends to be cumulative: We add new beliefs about what the world is like to a set of already existing beliefs; and the justification for the new beliefs typically depends in more or less intricate ways on the old beliefs. Therefore, if we start to accept too many false positive beliefs we may end up in a situation in which future research is directed in the wrong direction by our false positives. A more conservative approach, in which false negatives are preferred over false positives, is more likely to be successful in the long run, since it makes it more likely that we will not base new beliefs on old but false ones.</Para>



									<Para>However, when discussing medical image algorithms it is far from clear that we should adopt the same set of (epistemic) values that guide, or at least ought to guide, scientific research.

										<Footnote ID="Fn12">


											<Para>To be precise, we should distinguish between on the one hand, medical imaging algorithms used exclusively in interaction with a medical specialist, and on the other hand, algorithms used to screen large volumes of data to seek for pathologies. In the latter type of cases the data volume is too large for (human) medical staff to handle in given clinical contexts (consider e.g. yearly screenings of ten thousands of women for early signs of breast cancer). In such cases, the risk for statistically significant biases in the results (that are ethically value-laden) is much larger than in the first case.</Para>


										</Footnote>
 The reason for this is that the consequences of falsely believing something to be safe when it is not might be disastrous. If the algorithm is designed such that doctors come to believe that patients who are actually diseased are not, then the doctors may indirectly cause indirect harm to patients by failing to treat them. We therefore conclude that when addressing ethical aspects of medical image algorithms it is far from clear that medical decision makers should prefer algorithms that make them believe what is most likely to be true. On the contrary, a strong case can be made that medical image algorithms should be designed such that they are more likely to produce false positive rather than false negative results. That said, it is of course essential to make sure that an increased number of false positives does not lead to too many unnecessary and potentially dangerous operations. The computer image is just a tool. The final responsibility for deciding whether a surgical intervention is appropriate has to be taken by the doctor.
									</Para>



									<Para>It is important to bear in mind that our conclusion about the importance of avoiding false negatives in medical image algorithms is a value-judgment, not a factual one. No observations or other purely empirical methods can be appealed to for backing up this conclusion. This is part of the explanation of why medical image algorithms are essentially value-laden

										<Emphasis Type="Italic">.</Emphasis>
 However, as with nearly all value-judgments, good reasons can also be given against this value-judgment. For example, it could be claimed that the software designer designing the algorithm ought to perform a detailed cost-benefit analysis of the pros and cons of accepting different trade-off rates between false positives and false negatives. This is an approach that, broadly speaking, tallies well with traditional consequentialist intuitions. It may turn out, for instance that the total utility of accepting some false negatives but avoiding a very large number of false positives may actually be optimal, because of various empirical circumstances. It is beyond the scope of this paper to take a definitive stand on whether the consequentialist view outlined above is at least as plausible (or even more plausible) than the precautionary approach. Here we just wish to highlight that the choice of a trade-off rate between false positives and false negatives is a genuine value-judgment, and that this makes medical image algorithms essentially value-laden.
									</Para>



									<Para>Let us now consider an alternative, slightly more theoretical approach to the choice between false negatives and false positives. The ethical assumption underlying this approach is in short the following: A doctor’s decision about whether avoiding false negatives is better than avoiding false positives should depend in part on the answer to the question of how dangerous the suspected disease is for the patient. The more dangerous it is, the more important it will be to detect it, and the more acceptable it is to take the risk of mistakenly identifying it in a patient (i.e. to produce a false positive). The same holds, secondly, for contagious diseases that could jeopardise others. Third, a rising number of false negatives is less acceptable if the faulty diagnosis is likely to bring about drastic side-effects such as e.g. a dramatically diminished quality of life after diagnosis, or even the triggering of suicidal tendencies in those who are mistakenly informed of a positive result in the testing of a disease. For instance, the mass screening for breast-cancer in women has recently raised severe criticism due to the fact that it bears a high risk of false-positives. Being wrongly diagnosed with breast cancer is usually extremely disturbing for women and their families.

										<Footnote ID="Fn13">


											<Para>See e.g. Gigerenzer et al. (

												<CitationRef
CitationID="CR3">2009</CitationRef>
) about these problems of screening and their statistical implications.
											</Para>


										</Footnote>


									</Para>



									<Para>The choice for a certain threshold in an algorithm is a decision that is a judgment about which there is, or at least could be, a controversy between advocates of the major theories of normative ethics. This supports our claim that algorithms manifest or reflect certain ethical judgments. This point can be illustrated as a choice between deontological and consequentialist or utilitarian theories of normative ethics.</Para>



									<Para>Deontologically minded users of the software are most likely to focus on the physical and mental integrity of the individual patient. Therefore, in a Kantian vein, medical doctors will most probably opt for the implementation of some more liberal algorithm when it comes to computerised image analysis of severe diseases, aiming at more false positives and less false negatives. In such a Kantian vein, it is pre-eminent to protect the individual patient, i.e. to avoid doing harm to him or her. Such harm would be most likely to be done if a severe disease remains undetected and therefore untreated. For this reason, the deontologist is willing to put up with problematic consequences of more false positives that are brought about by her choice.</Para>



									<Para>In contrast, a scientist who tries to gather statistical data about a population of patients may wish to use the same software, but she will typically prefer more strictly or conservatively designed algorithms. This will bring about results that are more in line with consequentialist or utilitarian decision-making: What is important in this perspective is that the body of scientific data as a whole remains valid and intact, because this will typically bring about better consequences in the long run, although some individual patients may have to suffer along the road. This is in accordance with the consequentialist or utilitarian view that overall well-being in society should be our focus of attention, rather than the rights or needs of single individuals. Therefore, on this view, aiming at scientific insights that benefit the greatest number of people is the highest priority. This means that within the consequentialist approach, scientific results should be free from false positives as far as possible. It is therefore acceptable that a relatively large amount of false negatives will arise.</Para>



									<Para>It is beyond the scope of this paper to take a stand on the controversy over deontological and consequentialist normative theories. However, it is important to note that the choice of a threshold value for a certain algorithm goes hand in hand with some normative background assumptions specified by the software designer. Problems arise if it is unforeseeable whether the background assumptions of the designer will be in accordance with those of future users (i.e. a doctor or a scientist). If the ethical assumptions made by the software designer remain implicit and differ from those preferred by the user, we face a rather severe problem: Without knowing it, the user who bases her decisions on the output of the software will base those normative choices on ethical assumptions that are in conflict with the implicit ethical assumptions made by the software designer.</Para>


								</Section1>



								<Section1 ID="Sec5">


									<Heading>Should ethical values be user-defined?</Heading>



									<Para>Our general view is that software designers should, as far as possible, leave ethical decisions to users (say, with an external switch to choose between ‘preference for false positives’ or ‘preference for false negatives’). Moreover, when this is not possible the ethical assumptions in the algorithm should at least be transparent and easy to identify by users. In the case of medical image algorithms the users amount to medical doctors and scientists. For the reasons outlined in “

										<InternalRef
RefID="Sec4">The precautionary principle and ethical theories</InternalRef>
”, physicians who use a software will typically work on the assumption that it is better to get a false positive than a false negative. But when scientists use exactly the same software they are likely to have the opposite ethical view. Such users will typically avoid false positives in the first instance. These divergent ethical judgments could, of course, easily be implemented into the software by including some code that allows the user to specify her preference between false positive and negatives.
									</Para>



									<Para>We are aware that such a leave-it-to-the-user approach may sometimes be impossible to implement, as indicated above. There are cases in which it may just not be feasible to directly ask users to express the required kind of ethical judgments in numerical or quasi-numerical terms. Having to take a stand on fundamental ethical issues before using a software may be too difficult, and it may also be too impractical to ask users to make the relevant ethical choices. In such cases the underlying ethical assumptions should at least be made transparent. However, in what follows we shall set such cases aside and focus on cases in which it may actually be feasible to eliminate ethical judgments as far as possible from the algorithms underlying the software.</Para>



									<Para>At the heart of the matter is the fact that it is not just a practical decision that must be made when designing an algorithm, but a genuinely ethical one. To opt for either a consequentialist or utilitarian or a deontological attitude towards e.g. the choice between false positives and false negatives amounts to a value judgment that brings about ethically relevant implications for the patient’s health, for those who are in touch with her, and for the progress of science as a whole. As explained above, we therefore advocate that the software designer should design the algorithm such that it remains flexible and applicable to the requirements of different ethical settings. To be more precise, we propose that algorithms as far as possible remain open to the user’s ethical preferences. The design of the algorithm must allow the user to choose the circumstances in which she situates herself. We therefore think that it is not enough that the designer makes his or her assumptions transparent by letting the user know what the ethical assumptions in the design stance were. Rather, it is necessary that the designer leaves it to the user to specify what ethical parameters to choose.</Para>



									<Para>Having said this, we are of course aware that we now go beyond what is usually required in the literature on the ethics of image analysis. One of the few comments on the ethics of image analysis stems from Gert-Jan Lokhorst who was interviewed by van Strien (

										<CitationRef
CitationID="CR8">2008</CitationRef>
). He suggests that software has to be trained on image materials that resemble the ones it will have to analyse in the clinic, because the software will recognise and identify only those matters on which it was explicitly trained before. Moreover, Lokhorst suggests that doctors have to undergo proper training before they use the software. This will ensure that users become aware of which sorts of image analysis samples the system is familiar with, and this in turn ensures that users will be able to properly assess the competences of the software. Lokhorst’s final suggestion is that users and other relevant parties should be provided with some basic knowledge about the situations in which the algorithms were developed by the designer, because (i) this will enable them to decide in which clinical situations the program should be applied and in which not, and (ii) it must also be made clear in advance who is responsible for which step in the process of image analysis.
									</Para>



									<Para>Lokhorst thus implicitly agrees with us that it is essential to make the software designer’s ethical choices transparent to the user. However, our way of formulating this ethical requirement is more far-reaching than Lokhorst’s. As explained above, we maintain that the software designer must leave the responsibility for defining the default state of the software to the user herself. This is required for avoiding possible discrepancies between the ethical background assumptions of the software designer and users.</Para>



									<Para>The example of the stenosis in “

										<InternalRef
RefID="Sec3">A real example</InternalRef>
” shows that there is no a priori “correct” threshold value. However, the very first choice made by the software designer will influence all further steps of diagnosis. In borderline-cases, those choices may even affect the treatment suggested to the patient by the doctor. This is disturbing, especially in face of the fact that the software designer may lack the proper medical background of a fully trained physician.
									</Para>



									<Para>According to our argument for implementing user-defined ethical values, the doctor treating the individual patient should be enabled to make the relevant choice herself, as far as possible. If the patient is facing a severe condition, the threshold value should perhaps be set more liberally, to make sure that at any sign of deterioration, treatment is initiated. However, if the health condition of the patient is estimated as overall good, the threshold value should be set more conservatively to avoid “false alarms” that could lead to surgical interventions causing unnecessary harm to the patient.</Para>


								</Section1>



								<Section1 ID="Sec6" Type="Conclusion">


									<Heading>Conclusion</Heading>



									<Para>We conclude that a strong case can be made for the claim that some algorithms are essentially value-laden. Some algorithms, such as those used for classifying cells as diseased or non-diseased, forces the designer of the algorithm to take a stand on controversial ethical issues, e.g. whether it is more desirable to prefer false positive errors over false negative ones. This is a controversial ethical issue, on which there is a lot of disagreement among ethicists. We propose that designers of algorithms should, as far as possible, leave ethical issues to users, and when this is not possible, the ethical assumptions in the algorithm should at least be transparent and easy to identify by users.
									</Para>


								</Section1>


							</Body>



							<BodyRef TargetType="OnlinePDF"
FileRef="BodyRef/PDF/10676_2010_Article_9233.pdf" />



							<ArticleBackmatter>


								<Acknowledgments>


									<Heading>Acknowledgment</Heading>



									<SimplePara>The authors wish to thank Robin van der Sligte for extremely helpful discussions, and Sven Diekmann and Rosemary Lowry for helpful comments on earlier drafts.</SimplePara>



									<FormalPara RenderingStyle="Style1">


										<Heading>Open Access</Heading>



										<Para>This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</Para>


									</FormalPara>


								</Acknowledgments>



								<Bibliography ID="Bib1">


									<Heading>References</Heading>



									<Citation ID="CR1">


										<BibArticle>


											<BibAuthorName>


												<Initials>C</Initials>



												<FamilyName>Allen</FamilyName>


											</BibAuthorName>



											<BibAuthorName>


												<Initials>W</Initials>



												<FamilyName>Wallach</FamilyName>


											</BibAuthorName>



											<BibAuthorName>


												<Initials>I</Initials>



												<FamilyName>Smit</FamilyName>


											</BibAuthorName>



											<Year>2006</Year>



											<ArticleTitle
Language="En">Why machine ethics?</ArticleTitle>



											<JournalTitle>IEEE Intelligent Systems</JournalTitle>



											<VolumeID>21</VolumeID>



											<FirstPage>12</FirstPage>



											<LastPage>17</LastPage>



											<Occurrence Type="DOI">


												<Handle>10.1109/MIS.2006.83</Handle>


											</Occurrence>


										</BibArticle>



										<BibUnstructured>Allen, C., Wallach, W., &amp; Smit, I. (2006). Why machine ethics? 

											<Emphasis
Type="Italic">IEEE Intelligent Systems,</Emphasis>



											<Emphasis Type="Italic">21</Emphasis>
, 12–17.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR2">


										<BibUnstructured>Arkin, R. C. (2009). 

											<Emphasis
Type="Italic">Accountable autonomous agents: The next level.</Emphasis>
 Position paper for the DARPA Complete Intelligence Workshop, Feb. 2009.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR3">


										<BibUnstructured>Gigerenzer, G., Mata, J., &amp; Frank, R. (2009). Public knowledge of benefits of breast and prostate cancer screening in Europe. 

											<Emphasis
Type="Italic">Journal of the National Cancer Institute</Emphasis>
. doi:

											<ExternalRef>


												<RefSource>10.1093/jnci/djp1237</RefSource>



												<RefTarget
Address="10.1093/jnci/djp1237" TargetType="DOI" />


											</ExternalRef>
.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR4">


										<BibArticle>


											<BibAuthorName>


												<Initials>M</Initials>



												<FamilyName>Peterson</FamilyName>


											</BibAuthorName>



											<Year>2007</Year>



											<ArticleTitle
Language="En">Should the precautionary principle guide our actions or our beliefs?</ArticleTitle>



											<JournalTitle>Journal of Medical Ethics</JournalTitle>



											<VolumeID>33</VolumeID>



											<IssueID>1</IssueID>



											<FirstPage>5</FirstPage>



											<LastPage>10</LastPage>



											<Occurrence Type="DOI">


												<Handle>10.1136/jme.2005.015495</Handle>


											</Occurrence>


										</BibArticle>



										<BibUnstructured>Peterson, M. (2007). Should the precautionary principle guide our actions or our beliefs? 

											<Emphasis
Type="Italic">Journal of Medical Ethics,</Emphasis>



											<Emphasis Type="Italic">33</Emphasis>
(1), 5–10.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR5">


										<BibArticle>


											<BibAuthorName>


												<Initials>DB</Initials>



												<FamilyName>Resnik</FamilyName>


											</BibAuthorName>



											<Year>2004</Year>



											<ArticleTitle
Language="En">The precautionary principle and medical decision making</ArticleTitle>



											<JournalTitle>Journal of Medicine and Philosophy</JournalTitle>



											<VolumeID>29</VolumeID>



											<IssueID>3</IssueID>



											<FirstPage>281</FirstPage>



											<LastPage>299</LastPage>



											<Occurrence Type="DOI">


												<Handle>10.1080/03605310490500509</Handle>


											</Occurrence>


										</BibArticle>



										<BibUnstructured>Resnik, D. B. (2004). The precautionary principle and medical decision making. 

											<Emphasis
Type="Italic">Journal of Medicine and Philosophy,</Emphasis>



											<Emphasis Type="Italic">29</Emphasis>
(3), 281–299.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR6">


										<BibUnstructured>Sipser, M. (1997). 

											<Emphasis
Type="Italic">Introduction to the theory of computation</Emphasis>
, PWS Publishing Company.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR7">


										<BibArticle>


											<BibAuthorName>


												<Initials>M</Initials>



												<FamilyName>Turilli</FamilyName>


											</BibAuthorName>



											<Year>2007</Year>



											<ArticleTitle
Language="En">Ethical protocols design</ArticleTitle>



											<JournalTitle>Ethics and Information Technology</JournalTitle>



											<VolumeID>9</VolumeID>



											<FirstPage>49</FirstPage>



											<LastPage>62</LastPage>



											<Occurrence Type="DOI">


												<Handle>10.1007/s10676-006-9128-9</Handle>


											</Occurrence>


										</BibArticle>



										<BibUnstructured>Turilli, M. (2007). Ethical protocols design. 

											<Emphasis
Type="Italic">Ethics and Information Technology,</Emphasis>



											<Emphasis Type="Italic">9</Emphasis>
, 49–62.
										</BibUnstructured>


									</Citation>



									<Citation ID="CR8">


										<BibUnstructured>van Strien, W. (2008). Opvangen zwakheden maakt beeldtechnieken waardevoller (interview met G.J.C. Lokhorst). 

											<Emphasis
Type="Italic">Ethiek, Onderzoek en Bestuur</Emphasis>
, pp. 20–25. March 2008.
										</BibUnstructured>


									</Citation>


								</Bibliography>


							</ArticleBackmatter>


						</Article>


					</JournalOnlineFirst>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:DateLoaded>2010-08-30T17:05:53.686361+02:00</meta:DateLoaded>

					<meta:Authors>

						<meta:Author>Kraemer, Felicitas</meta:Author>

						<meta:Author>Overveld, Kees</meta:Author>

						<meta:Author>Peterson, Martin</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution>

							<meta:OrgName>Eindhoven University of Technology</meta:OrgName>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2010-07-03</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1007/s10676-010-9233-7</meta:DOI>

					<meta:Title>Is there an ethics of algorithms?</meta:Title>

					<meta:ISXN>1572-8439</meta:ISXN>

					<meta:PubName>Springer</meta:PubName>

					<meta:Journal>Ethics and Information Technology</meta:Journal>

					<meta:Publication>Ethics and Information Technology</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject
Type="Primary">Computer Science</meta:Subject>

						<meta:Subject
Type="Secondary">Library Science</meta:Subject>

						<meta:Subject
Type="Secondary">User Interfaces and Human Computer Interaction</meta:Subject>

						<meta:Subject Type="Secondary">Ethics</meta:Subject>

						<meta:Subject
Type="Secondary">Technology Management</meta:Subject>

						<meta:Subject
Type="Secondary">Management of Computing and Information Systems</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images />

		</result>

		<result>

			<Publisher xml:lang="en">

				<PublisherInfo>

					<PublisherName>BMC</PublisherName>


					<PublisherLocation />


				</PublisherInfo>

				<Journal>

					<JournalInfo JournalProductType="ArchiveJournal"
NumberingStyle="Unnumbered">

						<JournalID>1471-2105</JournalID>


						<JournalPrintISSN>1471-2105</JournalPrintISSN>


						<JournalElectronicISSN>1471-2105</JournalElectronicISSN>


						<JournalTitle>BMC Bioinformatics</JournalTitle>


						<JournalAbbreviatedTitle>BMC Bioinformatics</JournalAbbreviatedTitle>


						<JournalSubjectGroup>

							<JournalSubject
Type="Primary">Life Sciences</JournalSubject>


							<JournalSubject Priority="1"
Type="Secondary">Bioinformatics</JournalSubject>


							<JournalSubject Priority="2"
Type="Secondary">Microarrays</JournalSubject>


							<JournalSubject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</JournalSubject>


							<JournalSubject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences </JournalSubject>


							<JournalSubject Priority="5"
Type="Secondary">Combinatorial Libraries</JournalSubject>


							<JournalSubject Priority="6"
Type="Secondary">Algorithms</JournalSubject>


						</JournalSubjectGroup>


					</JournalInfo>


					<Volume>

						<VolumeInfo TocLevels="0" VolumeType="Regular">

							<VolumeIDStart>8</VolumeIDStart>


							<VolumeIDEnd>8</VolumeIDEnd>


							<VolumeIssueCount />


						</VolumeInfo>


						<Issue IssueType="Regular">

							<IssueInfo TocLevels="0">

								<IssueIDStart>1</IssueIDStart>


								<IssueIDEnd>1</IssueIDEnd>


								<IssueArticleCount />


								<IssueHistory>

									<PrintDate>

										<Year>2007</Year>


										<Month>3</Month>


										<Day>06</Day>


									</PrintDate>


								</IssueHistory>


								<IssueCopyright>

									<CopyrightHolderName>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolderName>


									<CopyrightYear>2007</CopyrightYear>


								</IssueCopyright>


							</IssueInfo>


							<Article ID="Art1">

								<ArticleInfo ArticleType="Abstract"
ContainsESM="No" Language="En" NumberingStyle="Unnumbered" TocLevels="0">

									<ArticleID>1471-2105-8-76</ArticleID>


									<ArticleDOI>10.1186/1471-2105-8-76</ArticleDOI>


									<ArticleSequenceNumber />


									<ArticleTitle
Language="En">An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>


									<ArticleSubTitle Language="En" />


									<ArticleCategory>Research article</ArticleCategory>


									<ArticleFirstPage>76</ArticleFirstPage>


									<ArticleLastPage>76</ArticleLastPage>


									<ArticleHistory>

										<Received>

											<Year>2006</Year>


											<Month>10</Month>


											<Day>23</Day>


										</Received>


										<Revised>

											<Year />


											<Month />


											<Day />


										</Revised>


										<Accepted>

											<Year>2007</Year>


											<Month>3</Month>


											<Day>06</Day>


										</Accepted>


									</ArticleHistory>


									<ArticleEditorialResponsibility />


									<ArticleCopyright>

										<CopyrightHolderName>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolderName>


										<CopyrightYear>2007</CopyrightYear>


									</ArticleCopyright>


									<ArticleGrants Type="OpenChoice">

										<MetadataGrant Grant="OpenAccess" />


										<AbstractGrant Grant="OpenAccess" />


										<BodyPDFGrant Grant="Restricted" />


										<BodyHTMLGrant Grant="Restricted" />


										<BibliographyGrant Grant="Restricted" />


										<ESMGrant Grant="Restricted" />


									</ArticleGrants>


									<ArticleContext>

										<JournalID />


										<VolumeIDStart>8</VolumeIDStart>


										<VolumeIDEnd>8</VolumeIDEnd>


										<IssueIDStart>1</IssueIDStart>


										<IssueIDEnd>1</IssueIDEnd>


									</ArticleContext>


								</ArticleInfo>


								<ArticleHeader>

									<AuthorGroup>

										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Kaname</GivenName>


												<FamilyName>Kojima</FamilyName>


											</AuthorName>


											<Contact>

												<Email>kaname@ims.u-tokyo.ac.jp</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Masao</GivenName>


												<FamilyName>Nagasaki</FamilyName>


											</AuthorName>


											<Contact>

												<Email>masao@ims.u-tokyo.ac.jp</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Euna</GivenName>


												<FamilyName>Jeong</FamilyName>


											</AuthorName>


											<Contact>

												<Email>eajeong@ims.u-tokyo.ac.jp</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Mitsuru</GivenName>


												<FamilyName>Kato</FamilyName>


											</AuthorName>


											<Contact>

												<Email>mitsuru@ims.u-tokyo.ac.jp</Email>


											</Contact>


										</Author>


										<Author AffiliationIDS="I1">

											<AuthorName DisplayOrder="Western">

												<GivenName>Satoru</GivenName>


												<FamilyName>Miyano</FamilyName>


											</AuthorName>


											<Contact>

												<Email>miyano@ims.u-tokyo.ac.jp</Email>


											</Contact>


										</Author>


										<Affiliation ID="I1">

											<OrgName>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</OrgName>


											<OrgAddress>

												<Postcode />


												<City />


												<State />


											</OrgAddress>


										</Affiliation>


									</AuthorGroup>


									<Abstract ID="Abs1" Language="En">

										<Heading>Abstract</Heading>


										<AbstractSection>

											<Heading>Background</Heading>


											<Para>Clearly visualized biopathways provide a great help in understanding biological systems. However, manual drawing of large-scale biopathways is time consuming. We proposed a grid layout algorithm that can handle gene-regulatory networks and signal transduction pathways by considering edge-edge crossing, node-edge crossing, distance measure between nodes, and subcellular localization information from Gene Ontology. Consequently, the layout algorithm succeeded in drastically reducing these crossings in the apoptosis model. However, for larger-scale networks, we encountered three problems: (i) the initial layout is often very far from any local optimum because nodes are initially placed at random, (ii) from a biological viewpoint, human layouts still exceed automatic layouts in understanding because except subcellular localization, it does not fully utilize biological information of pathways, and (iii) it employs a local search strategy in which the neighborhood is obtained by moving one node at each step, and automatic layouts suggest that simultaneous movements of multiple nodes are necessary for better layouts, while such extension may face worsening the time complexity.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Results</Heading>


											<Para>We propose a new grid layout algorithm. To address problem (i), we devised a new force-directed algorithm whose output is suitable as the initial layout. For (ii), we considered that an appropriate alignment of nodes having the same biological attribute is one of the most important factors of the comprehension, and we defined a new score function that gives an advantage to such configurations. For solving problem (iii), we developed a search strategy that considers swapping nodes as well as moving a node, while keeping the order of the time complexity. Though a naïve implementation increases by one order, the time complexity, we solved this difficulty by devising a method that caches differences between scores of a layout and its possible updates.</Para>


										</AbstractSection>


										<AbstractSection>

											<Heading>Conclusion</Heading>


											<Para>Layouts of the new grid layout algorithm are compared with that of the previous algorithm and human layout in an endothelial cell model, three times as large as the apoptosis model. The total cost of the result from the new grid layout algorithm is similar to that of the human layout. In addition, its convergence time is drastically reduced (40% reduction).</Para>


										</AbstractSection>


									</Abstract>


									<KeywordGroup Language="En">

										<Keyword>networks</Keyword>


										<Keyword>biological</Keyword>


										<Keyword>layout</Keyword>


										<Keyword>grid</Keyword>


										<Keyword>various</Keyword>


										<Keyword>attributes</Keyword>


										<Keyword>utilizing</Keyword>


										<Keyword>efficient</Keyword>


										<Keyword>algorithm</Keyword>


									</KeywordGroup>


								</ArticleHeader>


								<Body>

									<Section1 ID="Sec_13546">

										<Heading>Background</Heading>


										<Para>Modeling and simulations of large scale biological pathways are some of the most important tasks in Bioinformatics. Many applications, e.g., Cell Illustrator 

											<CitationRef
CitationID="B1">1</CitationRef>


											<CitationRef
CitationID="B2">2</CitationRef>
 , Cytoscape 

											<CitationRef
CitationID="B3">3</CitationRef>
 , Pajek 

											<CitationRef
CitationID="B4">4</CitationRef>
 , PATIKA 

											<CitationRef
CitationID="B5">5</CitationRef>


											<CitationRef
CitationID="B6">6</CitationRef>
 , and CADLIVE 

											<CitationRef
CitationID="B7">7</CitationRef>


											<CitationRef
CitationID="B8">8</CitationRef>
 have been developed in this area. Related to these topics, the visualization of biopathways is considered to play a key role in understanding biological systems. However, manual drawing of large-scale biopathways is a time consuming work, hence suitable biopathway layout algorithms and their applications are strongly demanded.
										</Para>


										<Para>Biopathways are categorized into three types, i.e., metabolic pathways, signal transduction pathways, and gene-regulatory networks. For metabolic pathways, several algorithms have been already proposed 

											<CitationRef
CitationID="B9">9</CitationRef>


											<CitationRef
CitationID="B10">10</CitationRef>


											<CitationRef
CitationID="B11">11</CitationRef>


											<CitationRef
CitationID="B12">12</CitationRef>


											<CitationRef
CitationID="B13">13</CitationRef>
 , and some of them succeeded in capturing the flow of the reactions well. In contrast, few layout algorithms that provide a convenient biological understanding have been proposed for signal transduction pathways 

											<CitationRef
CitationID="B14">14</CitationRef>


											<CitationRef
CitationID="B15">15</CitationRef>
 and gene-regulatory networks 

											<CitationRef
CitationID="B16">16</CitationRef>


											<CitationRef
CitationID="B17">17</CitationRef>
 . Thus, our new layout algorithm is focused on signal transduction pathways and gene-regulatory networks. For signal transduction pathways and gene-regulatory networks, extant layout algorithms can be categorized into two types; force-directed and grid layout algorithms.
										</Para>


										<Para>Force-directed algorithms are used in 

											<CitationRef
CitationID="B16">16</CitationRef>


											<CitationRef
CitationID="B17">17</CitationRef>
 by taking into account the directional constraint following different types of molecular and simple regional constraints from subcellular localizations. These algorithms have been successfully integrated into PATIKA. However, as pointed out in 

											<CitationRef
CitationID="B14">14</CitationRef>
 , force-directed algorithms may not be suitable for compact layouts of complex biopathways. Furthermore, intricately shaped regions such as torus-shaped region cannot be handled well as regional constraints in these force-directed algorithms. Hence, they are not suitable for models containing torus-shaped plasma membrane and nuclear membrane although such types of models are common as biopathways.
										</Para>


										<Para>A grid layout algorithm (referred to as LK-grid layout algorithm) was initially proposed by Li and Kurata. The grid layout algorithm restricts the positions of all nodes to grid points. Li and Kurata defined a cost function for two nodes that depends on some distance between these nodes and the topology of their connections in the graph. They applied LK-grid layout algorithm to a yeast cell-cycle pathway and concluded that this algorithm can geometrically classify the pathway into functional categories without using biological information. Moreover, they noticed that the algorithm generates compact layouts while avoiding overlaps between nodes. 

											<CitationRef
CitationID="B15">15</CitationRef>
 proposed CB-grid layout algorithm, in which so as to reduce edge-edge crossings and node-edge crossings, a penalty for these cases is added to the cost function. The algorithm can also deal with any complex regional constraints following subcellular localizations, and besides search space is reduced due to these constrains. As a result, in the apoptosis model, the layout algorithm succeeded in a drastic reduction of edge-edge crossings and node-edge crossings, while placing nodes in biologically proper regions.
										</Para>


										<Para>However, in the case of larger-scale networks, this algorithm encountered three problems. First, a layout with randomly placed nodes is used as the initial layout. This random layout contains a large number of edge-edge crossings and node-edge crossings; subsequently, many iterations will be required to obtain a locally optimal layout. Secondly, although one of the features of CB-grid layout algorithm is to use the subcellular localization information, it still does not fully utilize biological characteristics. For example, it does not consider such biological attributes as types of entities (protein, mRNA, and microRNA) or types of processes (phosphorylation, binding, and translation), although in human layouts these biological attributes are apt to contribute to the comprehension of interesting biopathways easier. Thirdly, according to a greedy strategy, CB-grid layout algorithm updates a layout by moving one node at each step until the layout reaches an optimum. However, resulting layouts are just local optima, hence their quality fundamentally depends on the initial layout. Although in 

											<CitationRef
CitationID="B15">15</CitationRef>
 a multi-step CB-grid layout algorithm was also proposed to solve this drawback, it requires higher time complexity and hence is not suitable for practical applications.
										</Para>


										<Para>To overcome these three problems, we propose a new grid layout algorithm. For the first problem, we propose a new force-directed algorithm whose output is suitable as the initial layout of grid layout algorithms. For the second problem, we introduce the concept that assigns a score i.e., a negative cost, to a layout depending on how nodes with the same attribute are aligned. This concept is realized with a combo score function, which is combined with the cost function defined in CB-grid layout algorithm. For the third problem, the search strategy in CB-grid layout algorithm is improved by adding the swap operation while keeping the time complexity. By the swap operation, the new grid layout can also consider layouts generated by exchanging the positions of two nodes in the current layout at each step.</Para>


										<Para>The Methods section is organized as follows: (i) first, we introduce the previous grid layout, i.e., CB-grid layout algorithm; (ii) for the first improvement in the initial layout of CB-grid layout algorithm, the new force-directed algorithm termed Eades initial layout algorithm is described; (iii) for the second improvement, CCB-grid layout algorithm, which is CB-grid layout algorithm with the combo score function is described; (iv) for the third improvement, SCCB-grid layout algorithm, which enhances CCB-grid layout algorithm by adding the swap operation is presented. In the Results and Discussion section, the performances of these new algorithms are compared and verified by applying them to the signal transduction pathway of an endothelial cell, which is larger than the pathways in 

											<CitationRef
CitationID="B14">14</CitationRef>
 and 

											<CitationRef
CitationID="B15">15</CitationRef>
 .
										</Para>


									</Section1>


									<Section1 ID="Sec_35340">

										<Heading>Methods</Heading>


										<Section2 ID="Sec_03440">

											<Heading>CB-grid layout algorithm: Introduction of the grid layout algorithm</Heading>


											<Para>Given a graph 

												<Emphasis Type="Italic">G</Emphasis>
 = ( 

												<Emphasis Type="Italic">V</Emphasis>
 , 

												<Emphasis Type="Italic">E</Emphasis>
 ) with nodes 

												<Emphasis Type="Italic">V</Emphasis>
 and edges 

												<Emphasis Type="Italic">E</Emphasis>
 , a 

												<Emphasis
Type="Italic">layout L</Emphasis>
 = ( 

												<Emphasis Type="Italic">V</Emphasis>
 , 

												<Emphasis Type="Italic">E</Emphasis>
 , 

												<Emphasis Type="Italic">U</Emphasis>
 , 

												<Emphasis Type="Italic">P</Emphasis>
 ) of 

												<Emphasis Type="Italic">G</Emphasis>
 consists of the underlying graph 

												<Emphasis Type="Italic">G</Emphasis>
 , grid points 

												<Emphasis Type="Italic">U</Emphasis>
 and a function 

												<Emphasis Type="Italic">P</Emphasis>
 : 

												<Emphasis Type="Italic">V</Emphasis>
 → 

												<Emphasis Type="Italic">U</Emphasis>
 such that 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 ) ≠ 

												<Emphasis Type="Italic">P</Emphasis>
 ( 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 ) for any two distinct nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 ∈ 

												<Emphasis Type="Italic">V</Emphasis>
 . This definition does not allow overlaps between nodes in the layout. For a layout 

												<Emphasis Type="Italic">L</Emphasis>
 , this paper uses the following notations.
											</Para>


											<Para>• 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">L</Emphasis>

												</Subscript>
 : a set of vacant points of 

												<Emphasis Type="Italic">L</Emphasis>
 .
											</Para>


											<Para>• 

												<Emphasis Type="Italic">E</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>

												</Subscript>
 : the set of all edges connected to node 

												<Emphasis Type="Italic">v</Emphasis>
 .
											</Para>


											<Para>• | 

												<Emphasis Type="Italic">V</Emphasis>
 |: the number of nodes in 

												<Emphasis Type="Italic">V</Emphasis>
 .
											</Para>


											<Para>• | 

												<Emphasis Type="Italic">W</Emphasis>
 |: the number of vacant points in 

												<Emphasis Type="Italic">L</Emphasis>
 , instead of | 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">L</Emphasis>

												</Subscript>
 | if there is no confusion possible.
											</Para>


											<Para>We define the following operations.</Para>


											<Para>• 

												<Emphasis Type="Italic">T</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>
 → 

													<Emphasis Type="Italic">p</Emphasis>

												</Subscript>


												<Emphasis Type="Italic">L</Emphasis>
 : the layout generated by moving a node 

												<Emphasis Type="Italic">v</Emphasis>
 to a vacant point 

												<Emphasis Type="Italic">p</Emphasis>
 ∈ 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">L</Emphasis>

												</Subscript>
 .
											</Para>


											<Para>•Svα↔vβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWudaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHbqabaWccqGHugYQcqWG2bGDdaWgaaadbaGae8NSdigabeaaaSqabaaaaa@368F@ 

												<Emphasis Type="Italic">L</Emphasis>
 : the layout generated by swapping nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 .
											</Para>


											<Para>• 

												<Emphasis Type="Italic">D</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>

												</Subscript>


												<Emphasis Type="Italic">L</Emphasis>
 : the layout generated by removing a node 

												<Emphasis Type="Italic">v</Emphasis>
 and all edges connected to 

												<Emphasis Type="Italic">v</Emphasis>
 .
											</Para>


											<Para>In addition, we define the following functions.</Para>


											<Para>•Crossei,ejMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqWGYbGCcqWGVbWBcqWGZbWCcqWGZbWCdaWgaaWcbaGaemyzau2aaSbaaWqaaiabdMgaPbqabaWccqGGSaalcqWGLbqzdaWgaaadbaGaemOAaOgabeaaaSqabaaaaa@3A47@( 

												<Emphasis Type="Italic">L</Emphasis>
 ): a binary function that returns 1 if an edge 

												<Emphasis Type="Italic">e</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 crosses with an edge 

												<Emphasis Type="Italic">e</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 and 0 otherwise.
											</Para>


											<Para>•Crossvi,ejMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqWGYbGCcqWGVbWBcqWGZbWCcqWGZbWCdaWgaaWcbaGaemODay3aaSbaaWqaaiabdMgaPbqabaWccqGGSaalcqWGLbqzdaWgaaadbaGaemOAaOgabeaaaSqabaaaaa@3A69@( 

												<Emphasis Type="Italic">L</Emphasis>
 ): a binary function that returns 1 if an edge 

												<Emphasis Type="Italic">e</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 crosses with a node 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 and 0 otherwise.
											</Para>


											<Para>•Distancevi,vjMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaaieGacqWFebarcqWFPbqAcqWFZbWCcqWF0baDcqWFHbqycqWFUbGBcqWFJbWycqWFLbqzdaWgaaWcbaGaemODay3aaSbaaWqaaiabdMgaPbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOAaOgabeaaaSqabaaaaa@3E53@( 

												<Emphasis Type="Italic">L</Emphasis>
 ): a function that returnswvi,vj⋅md(vi,vj)MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG3bWDdaWgaaWcbaGaemODay3aaSbaaWqaaiabdMgaPbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOAaOgabeaaaSqabaGccqGHflY1cqWGTbqBcqWGKbazcqGGOaakcqWG2bGDdaWgaaWcbaGaemyAaKgabeaakiabcYcaSiabdAha2naaBaaaleaacqWGQbGAaeqaaOGaeiykaKcaaa@42E9@, wherewvi,vjMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG3bWDdaWgaaWcbaGaemODay3aaSbaaWqaaiabdMgaPbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOAaOgabeaaaSqabaaaaa@3541@is the weight to the couple of nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 , and 

												<Emphasis Type="Italic">md</Emphasis>
 ( 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 ) is the Manhattan distance between 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">i</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">j</Emphasis>

												</Subscript>
 .
											</Para>


											<Para>In our previous approach 

												<CitationRef
CitationID="B15">15</CitationRef>
 (mainly referred to as CB-grid layout algorithm), the 

												<Emphasis
Type="Italic">layout cost</Emphasis>


												<Emphasis Type="Italic">C</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) of 

												<Emphasis Type="Italic">L</Emphasis>
 was defined as follows:
											</Para>


											<Para>where 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ee</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ne</Emphasis>

												</Subscript>
 , and 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">d</Emphasis>

												</Subscript>
 are called respectively 

												<Emphasis
Type="Italic">edge-edge crossing weight, node-edge crossing weight</Emphasis>
 , and 

												<Emphasis
Type="Italic">distance cost weight</Emphasis>
 .
											</Para>


											<Para>The CB-grid layout algorithm repeats the operation of moving a unique node to a vacant point one-by-one until it reaches a locally optimal layout. At each step, the algorithm calculates costs of all layouts that can be generated by moving one of all nodes to one of all vacant points. The layout with the lowest cost is selected as a starting layout for the next step. After reaching convergence, the algorithm outputs a locally optimal layout. If the cost calculation of all possible adjacent layouts is implemented in a naïve way, high time complexity is required. To overcome this problem, the previous method 

												<CitationRef
CitationID="B15">15</CitationRef>
 introduced Δ matrix that stores each possible cost difference at the previous step and succeeded in reducing the time complexity at each step from 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">W</Emphasis>
 | (| 

												<Emphasis Type="Italic">V</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">E</Emphasis>
 | 

												<Superscript>2</Superscript>
 ) to 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">E</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">W</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)), where 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 is the node moved at the previous step.
											</Para>


											<Para>When CB-grid layout algorithm was applied to several biopathways, we encountered three problems. Thus, we propose new grid layout algorithms that solve these problems. Problems and solutions are summarized as follows:</Para>


											<Para>1. Improving the choice of the initial layout: since a locally optimal layout depends noticeably on the initial layout, we first apply Eades initial layout algorithm to a random layout, and use its output as the initial layout. In the previous approach, a random layout was directly used as the initial layout.</Para>


											<Para>2. Improving the cost function: we introduce the concept of a combo score that gives a good score, i.e., a negative cost when nodes with the same biological attribute are aligned (CCB-grid layout algorithm). In CB-grid layout algorithm, the biological attributes, except subcellular localization, were ignored.</Para>


											<Para>3. Improving the search strategy: we propose a better search strategy, which allows us to obtain improved results, keeping the time complexity. For obtaining a better layout, the search space is extended by adding the swap operation. At each step, all layouts obtained by swapping two nodes are also considered (SCCB-grid layout algorithm).</Para>


											<Para>In the remainder of this section, we describe these three new algorithms mentioned above.</Para>


										</Section2>


										<Section2 ID="Sec_55754">

											<Heading>Eades initial layout algorithm: generating a new initial layout for grid layout algorithms</Heading>


											<Para>In the previous paper 

												<CitationRef
CitationID="B15">15</CitationRef>
 , a random layout was used as an initial layout for CB-grid layout algorithm. When the initial layout is far from the global optimum, the local optimum obtained tends to be unacceptable. Therefore, we decided to develop Eades algorithm 

												<CitationRef
CitationID="B18">18</CitationRef>
 and use its output as the initial layout. Eades algorithm is one of the force-directed algorithms, consisting of the following two steps.
											</Para>


											<Para>1. Two types of forces are defined for each pair of nodes. If two nodes are adjacent, there exists an attractive force 

												<Emphasis Type="Italic">a</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>
 1
												</Subscript>
 log( 

												<Emphasis Type="Italic">d</Emphasis>
 / 

												<Emphasis Type="Italic">a</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>
 2
												</Subscript>
 ) between them, where 

												<Emphasis Type="Italic">a</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>
 1
												</Subscript>
 and 

												<Emphasis Type="Italic">a</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>
 2
												</Subscript>
 are constants, and 

												<Emphasis Type="Italic">d</Emphasis>
 is the distance between the two nodes. On the other hand, if two nodes are not adjacent, there exists a repulsive force 

												<Emphasis Type="Italic">r</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>

												</Subscript>
 /dMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaadaGcaaqaaiabdsgaKbWcbeaaaaa@2E18@between them, where 

												<Emphasis Type="Italic">r</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>

												</Subscript>
 is a constant. At each step, the positions of all the nodes are updated according to the sum of the repulsive and attractive forces between them.
											</Para>


											<Para>2. The above step is iterated a predetermined number of times, and the final result is obtained.</Para>


											<Para>We have customized two points in Eades algorithm. First, nodes in Eades algorithm can be placed anywhere. All the nodes in the initial layout for CB-grid layout algorithm, however, should be placed on the grid points that satisfy the subcellular localization. Thus, the output of Eades algorithm cannot be used directly as an input for CB-grid layout algorithm.</Para>


											<Para>To handle this problem, we propose to move each node to the closest vacant point that satisfies the subcellular localization after moving nodes at each step.</Para>


											<Para>Second improvement is the following one. Since Eades algorithm doesn't consider edge-edge crossings and node-edge crossings in its implementation, the resulting layout could contain a lot of such crossings. For example, suppose a biological pathway with a subcellular localization, membrane, which slimly surrounds other subcellular localizations as shown in Figure 

												<InternalRef
RefID="F1">1(a)</InternalRef>
 , the graph in (a) could be a layout resulting from Eades algorithm. In this case, the layout might contain a large number of edge-edge crossings and node-edge crossings because edges cross over other subcellular localizations. In order to avoid this problem, we propose to gather nodes around a particular grid point for each subcellular localization as shown in Figure 

												<InternalRef
RefID="F1">1(b)</InternalRef>
 . Eades algorithm with the above improvements is called 

												<Emphasis
Type="Italic">Eades initial layout algorithm</Emphasis>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F1">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Two layouts with the same canvas and three subcellular localizations</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-1" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">Two layouts with the same canvas and three subcellular localizations</Emphasis>
 . The grid canvases (a) and (b) have the same biological subcellular localizations extracellular space, plasma membrane, and cytoplasm. Both canvases contain the same graph with four nodes that are located in plasma membrane, which surrounds cytoplasm. In (a), nodes are spread apart in plasma membrane, and edges among these nodes cross over cytoplasm. In (b), the nodes are gathered in the left-top corner, and no edge crosses over cytoplasm. Due to its crossing patterns in (a) these edges have a higher probability to cross other nodes in cytoplasm. This is the drawback of using the layout in (a) as the initial layout for Eades initial layout algorithm.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


										</Section2>


										<Section2 ID="Sec_74217">

											<Heading>CCB-grid layout algorithm: utilizing various biological attributes</Heading>


											<Para>When humans draw biopathway models, nodes with the same attribute are usually arranged according to a rule. In CB-grid layout algorithm, this type of information is completely ignored. To implement this type of property, we introduce the concept of combo scores called 

												<Emphasis Type="Bold">combo1</Emphasis>
 and 

												<Emphasis Type="Bold">combo2</Emphasis>
 (see Figure 

												<InternalRef RefID="F2">2</InternalRef>
 ). Note that a combo score is applied only to nodes having an attribute since some nodes do not have any attributes. We denote the set of nodes having an attribute by 

												<Emphasis Type="Italic">V'</Emphasis>
 ⊆ 

												<Emphasis Type="Italic">V</Emphasis>
 . In this algorithm, (i) upperGrid( 

												<Emphasis Type="Italic">p</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 )/lowerGrid( 

												<Emphasis Type="Italic">p</Emphasis>
 , 

												<Emphasis Type="Italic">i</Emphasis>
 ) returns the upper/lower 

												<Emphasis Type="Italic">i</Emphasis>
 th grid point over/under a grid point 

												<Emphasis Type="Italic">p</Emphasis>
 ∈ 

												<Emphasis Type="Italic">P</Emphasis>
 , and (ii) Attr( 

												<Emphasis Type="Italic">v</Emphasis>
 ) is the attribute of a node 

												<Emphasis Type="Italic">v</Emphasis>
 ∈ 

												<Emphasis Type="Italic">V'</Emphasis>
 , and 

												<Emphasis Type="Italic">CW</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 = (1 + 

												<Emphasis Type="Italic">C</Emphasis>
 /|V′aMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGwbGvgaqbamaaBaaaleaacqWGHbqyaeqaaaaa@2F64@|), where 

												<Emphasis Type="Italic">C</Emphasis>
 is a constant and normally set to | 

												<Emphasis Type="Italic">V</Emphasis>
 |, andV′aMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGwbGvgaqbamaaBaaaleaacqWGHbqyaeqaaaaa@2F64@is the set of nodes having an attribute 

												<Emphasis Type="Italic">a</Emphasis>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F2">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Pseudo codes of combo score functions: combo1 and combo2</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-2" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">Pseudo codes of combo score functions: combo1 and combo2</Emphasis>
 . (a) 

															<Emphasis
Type="Bold">combo1</Emphasis>
 : a score function that considers nodes with one vertical grid distance from the target node. (b) 

															<Emphasis
Type="Bold">combo2</Emphasis>
 : a score function that considers nodes with up to two vertical grid distances from the target node, (c) isCombo: a boolean function that takes a node and a grid point as its arguments and returns "true" if the attribute of the node and that of the node on the grid point are the same.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>The combo score is designed such that the more nodes with the same attribute are aligned vertically, the higher the score is. The combo score is defined between two nodes, and a combo score of a layout 

												<Emphasis Type="Italic">L</Emphasis>
 is defined to be the sum of all the combo scores occurring in 

												<Emphasis Type="Italic">L</Emphasis>
 . We say that two nodes have a 

												<Emphasis
Type="Italic">combo relation</Emphasis>
 when a combo score occurs between them. Note that the horizontal alignment score is not implemented because if the above combo score supported both the vertical and horizontal directions, the numbers of edge-edge crossings and node-edge crossings would be considerably increased. Therefore, we should choose only one direction for combo scores. In this paper, we defined combo scores in the vertical direction. We have considered two types of combo scores, i.e., 

												<Emphasis Type="Bold">combo1</Emphasis>
 and 

												<Emphasis Type="Bold">combo2</Emphasis>
 for layouts in Figure 

												<InternalRef
RefID="F3">3(a)</InternalRef>
 and 

												<InternalRef
RefID="F3">3(b)</InternalRef>
 , respectively. Let nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 to 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 in Figure 

												<InternalRef RefID="F3">3</InternalRef>
 have the same attribute. The 

												<Emphasis Type="Bold">combo1</Emphasis>
 considers only the nodes with one vertical grid distance from the target node. In contrast, 

												<Emphasis Type="Bold">combo2</Emphasis>
 considers the nodes with up to two vertical grid distances from the target node. For the layout in Figure 

												<InternalRef
RefID="F3">3(a)</InternalRef>
 , the number of combo relations with 

												<Emphasis Type="Bold">combo1</Emphasis>
 and 

												<Emphasis Type="Bold">combo2</Emphasis>
 are 8 and 12, respectively. If node 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 is moved as shown in Figure 

												<InternalRef
RefID="F3">3(b)</InternalRef>
 , the number of combo relations with 

												<Emphasis Type="Bold">combo1</Emphasis>
 is the same as before, whereas that with 

												<Emphasis Type="Bold">combo2</Emphasis>
 is 14. Thus, only by using 

												<Emphasis Type="Bold">combo2</Emphasis>
 , we can improve the combo score when node 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">f</Emphasis>

												</Subscript>
 is moved as shown in Figure 

												<InternalRef
RefID="F3">3(a)</InternalRef>
 and 

												<InternalRef
RefID="F3">3(b)</InternalRef>
 . As shown in the dotted rectangle in Figure 

												<InternalRef
RefID="F3">3(a)</InternalRef>
 , a pair of vertically aligned nodes often occurs during the process of updating a layout. In this case, Figure 

												<InternalRef
RefID="F3">3(b)</InternalRef>
 should be a better layout than Figure 

												<InternalRef
RefID="F3">3(a)</InternalRef>
 . For this reason, we decide to employ 

												<Emphasis Type="Bold">combo2</Emphasis>
 . Henceforth, for a node 

												<Emphasis Type="Italic">v</Emphasis>
 ∈ 

												<Emphasis Type="Italic">V</Emphasis>
 in a layout 

												<Emphasis Type="Italic">L</Emphasis>
 , 

												<Emphasis Type="Italic">Combo</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) denotes the same combo score as 

												<Emphasis Type="Bold">combo2</Emphasis>
 ( 

												<Emphasis Type="Italic">v</Emphasis>
 , 

												<Emphasis Type="Italic">L</Emphasis>
 ). The total score∑v∈VCombov(L)MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaadaaeqbqaaiabdoeadjabd+gaVjabd2gaTjabdkgaIjabd+gaVnaaBaaaleaacqWG2bGDaeqaaOGaeiikaGIaemitaWKaeiykaKcaleaacqWG2bGDcqGHiiIZcqWGwbGvaeqaniabggHiLdaaaa@3E08@for 

												<Emphasis Type="Italic">L</Emphasis>
 is denoted by 

												<Emphasis Type="Italic">Combo</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ).
											</Para>


											<Figure Category="Standard" Float="No"
ID="F3">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>An example that compares the features of combo1 and combo2 score functions</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-3" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">An example that compares the features of combo1 and combo2 score functions</Emphasis>
 . (a) An intermediate layout of CCB-grid layout algorithm. In this layout, all six nodes have the same attribute. (b) The next candidate layout that is generated from (a) by moving node 

															<Emphasis
Type="Italic">v</Emphasis>


															<Subscript> 

																<Emphasis
Type="Italic">f</Emphasis>

															</Subscript>
 below node 

															<Emphasis
Type="Italic">v</Emphasis>


															<Subscript> 

																<Emphasis
Type="Italic">d</Emphasis>

															</Subscript>
 . Combo scores of (a) and (b) are the same with 

															<Emphasis
Type="Bold">combo1</Emphasis>
 score function. Instead, the combo score of (b) will be better than (a) with 

															<Emphasis
Type="Bold">combo2</Emphasis>
 score function.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>If 

												<Emphasis Type="Italic">CW</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 returns the same value for any attribute 

												<Emphasis Type="Italic">a</Emphasis>
 , many of the nodes with the same attribute will be vertically aligned easily since they have a greater chance to neighbor one another. So as to reduce the biases among the attributes, we define 

												<Emphasis Type="Italic">CW</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 to be inversely related to the total number of the nodes whose attribute is 

												<Emphasis Type="Italic">a</Emphasis>
 .
											</Para>


											<Para>By modifying the layout score of CB-grid layout algorithm, we can define the layout cost 

												<Emphasis Type="Italic">C</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) of a layout 

												<Emphasis Type="Italic">L</Emphasis>
 with the new concept of the combo score as follows:
											</Para>


											<Para>where 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">cs</Emphasis>

												</Subscript>
 is called 

												<Emphasis
Type="Italic">combo score weight</Emphasis>
 . CB-grid layout algorithm improved by the above modification is named 

												<Emphasis
Type="Italic">Combo score, Cross cost and Biological information grid layout algorithm</Emphasis>
 (CCB-grid layout algorithm). The reason for multiplying the sum of the combo scores by 1/2 is that combo scores are counted twice since a combo score between nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 is included in bothCombovαMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqWGVbWBcqWGTbqBcqWGIbGycqWGVbWBdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHbqabaaaleqaaaaa@36B8@( 

												<Emphasis Type="Italic">L</Emphasis>
 ) andCombovβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqWGVbWBcqWGTbqBcqWGIbGycqWGVbWBdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@36BA@( 

												<Emphasis Type="Italic">L</Emphasis>
 ). The algorithm is the same as 

												<Emphasis
Type="Italic">C-optimization</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) step in 

												<CitationRef
CitationID="B15">15</CitationRef>
 except for the use of the above layout cost 

												<Emphasis Type="Italic">C</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ), i.e., the algorithm for calculating Δ matrix is also the same.
											</Para>


											<Para>For calculating the combo score for each node, only four nodes need to be checked at most, i.e., its time complexity is constant, while for calculating the edge-edge crossing cost, the node-edge crossing cost, and the distance cost for each node, these time complexities depend on | 

												<Emphasis Type="Italic">E</Emphasis>
 |, | 

												<Emphasis Type="Italic">V</Emphasis>
 |, and | 

												<Emphasis Type="Italic">W</Emphasis>
 |, respectively. Thus, without using Δ matrix, the time complexity related to combo scores is 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 || 

												<Emphasis Type="Italic">W</Emphasis>
 |) at each step.
											</Para>


											<Para>At each step, we need to calculate the difference between the combo score of the previous layout 

												<Emphasis Type="Italic">L</Emphasis>
 and that of the current layout that is generated by moving a node 

												<Emphasis Type="Italic">v</Emphasis>
 to a vacant point 

												<Emphasis Type="Italic">p</Emphasis>
 , i.e., 

												<Emphasis Type="Italic">Combo</Emphasis>
 ( 

												<Emphasis Type="Italic">T</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>
 → 

													<Emphasis Type="Italic">p</Emphasis>

												</Subscript>


												<Emphasis Type="Italic">L</Emphasis>
 ) – 

												<Emphasis Type="Italic">Combo</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ). We can efficiently calculate the difference of the combo scoreΔvpcsMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHuoardaqhaaWcbaGaemODayNaemiCaahabaGaem4yamMaem4Camhaaaaa@33DB@( 

												<Emphasis Type="Italic">L</Emphasis>
 ) as follows:
											</Para>


											<Para>where</Para>


											<Para>We introduced 

												<Emphasis Type="Italic">Adj</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) due to the following reason. First, suppose that three nodes with the same attribute are aligned vertically. We call them 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 , and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">γ</Emphasis>

												</Subscript>
 beginning from the bottom. There are three combo relations among the three nodes: one is between 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 , another between 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">γ</Emphasis>

												</Subscript>
 , and the third between 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">γ</Emphasis>

												</Subscript>
 . Although 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 is involved in these three combo relations, the combo relation between 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">γ</Emphasis>

												</Subscript>
 is not considered inCombovβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGdbWqcqWGVbWBcqWGTbqBcqWGIbGycqWGVbWBdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@36BA@( 

												<Emphasis Type="Italic">L</Emphasis>
 ). Therefore, 

												<Emphasis Type="Italic">Adj</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">v</Emphasis>

												</Subscript>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) is needed to correct this type of undercount.
											</Para>


										</Section2>


										<Section2 ID="Sec_10250">

											<Heading>SCCB-grid layout algorithm: extension of the search space due to the swap operation</Heading>


											<Para>Another drawback of CB-grid layout algorithm is that only one node can be moved to a vacant point at each step. For example, the layout shown in Figure 

												<InternalRef
RefID="F4">4(a)</InternalRef>
 is optimal for CB-grid layout algorithm despite the fact the layout in Figure 

												<InternalRef
RefID="F4">4(b)</InternalRef>
 should be selected as the better layout. This limitation is due to the strategy of CB-grid layout algorithm. Thus, we have devised a new algorithm by allowing the swap operations between two nodes while keeping the time complexity. With this improvement, the layout in Figure 

												<InternalRef
RefID="F4">4(a)</InternalRef>
 will be arranged as shown in Figure 

												<InternalRef
RefID="F4">4(b)</InternalRef>
 . The new algorithm is named CCB-grid layout with the swap operation (SCCB-grid layout algorithm). The layout cost function is the same as in CCB-grid layout algorithm. However, a naïve implementation would increase the time complexity to calculate the layout cost for swapped layouts.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F4">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>An optimal layout of CB-grid and improved layout with the swap operation</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-4" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">An optimal layout of CB-grid and improved layout with the swap operation</Emphasis>
 . (a) An optimal layout for CB-grid layout algorithm. (b) From (a) a better layout will be generated with the swap operation.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>In the previous approach 

												<CitationRef
CitationID="B15">15</CitationRef>
 , Δ matrix stores cost differences that are induced only by moving nodes to vacant points. As a result, if a grid point of interest was occupied at the previous step, we cannot exploit Δ matrix to calculate cost differences corresponding to that grid point. Since grid points of interest on the swap operation are obviously occupied at the previous step, Δ matrix cannot be used. However, if Δ matrix also stores cost differences related to occupied points, Δ matrix can be exploited for this problematic case, too. We then propose an extended Δ matrix, which considers occupied points as well as vacant points. Since the definition of the cost differences for vacant points cannot be applied directly to occupied points, we decide to calculate the cost differences for the occupied points by calculating it without taking into account the node occupying that grid point and all edges connected to it. In the remainder of this section, we will show how to calculate the extended Δ matrix and then compare the time complexity of the extended Δ matrix and the original Δ matrix.
											</Para>


											<Para>Henceforth, let us refer to the extended Δ matrix as Δ matrix. Given a layout 

												<Emphasis Type="Italic">L</Emphasis>
 , at the first step, we update Δ ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) matrix as follows:
											</Para>


											<Para>FvαMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGgbGrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHbqabaaaleqaaaaa@3140@is the following function:</Para>


											<Para>If the previous layout is updated by moving node 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 to vacant point 

												<Emphasis Type="Italic">q</Emphasis>
 , Δ (Tvα→pMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGubavdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHbqabaWccqGHsgIRcqWGWbaCaeqaaaaa@34B2@ 

												<Emphasis Type="Italic">L</Emphasis>
 ) can be updated efficiently by using Δ ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) as follows:
											</Para>


											<Para>where 

												<Emphasis Type="Italic">DIFF</Emphasis>


												<Subscript>0</Subscript>
 to 

												<Emphasis Type="Italic">DIFF</Emphasis>


												<Subscript>4</Subscript>
 are defined in the following way:
											</Para>


											<Para>where 

												<Emphasis Type="Italic">Q</Emphasis>
 shall be defined below.
											</Para>


											<Para>If the previous layout is updated by swapping two nodesvβ1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabigdaXaqabaaaleqaaaaa@311D@andvβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabikdaYaqabaaaleqaaaaa@311F@, Δ (Svβ1↔vβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWudaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aInaaBaaabaGaeGymaedabeaaaeqaaSGaeyiLHSQaemODay3aaSbaaWqaaiab=j7aInaaBaaabaGaeGOmaidabeaaaeqaaaWcbeaaaaa@38B5@ 

												<Emphasis Type="Italic">L</Emphasis>
 ) is then updated efficiently by using Δ ( 

												<Emphasis Type="Italic">L</Emphasis>
 ) as follows:
											</Para>


											<Para>where 

												<Emphasis Type="Italic">DIFF</Emphasis>


												<Subscript>5</Subscript>
 to 

												<Emphasis Type="Italic">DIFF</Emphasis>


												<Subscript>9</Subscript>
 are defined in the following way:
											</Para>


											<Para>The case of 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 =vβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabikdaYaqabaaaleqaaaaa@311F@is not considered in Equation (13) because equations of this case can be obtained by simply replacingvβ1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabigdaXaqabaaaleqaaaaa@311D@withvβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabikdaYaqabaaaleqaaaaa@311F@in case 1 and 3.
											</Para>


											<Para>Qva,vbMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaWgaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqabaaaaa@34D5@(·) andQ^va,vb,vcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaBaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleqaaaaa@38C1@(·) in 

												<Emphasis Type="Italic">DIFF</Emphasis>


												<Subscript>0</Subscript>
 to 

												<Emphasis Type="Italic">DIFF</Emphasis>


												<Subscript>9</Subscript>
 are partial cost functions depending on the two nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">b</Emphasis>

												</Subscript>
 and the three nodes 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">b</Emphasis>

												</Subscript>
 , and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>

												</Subscript>
 , respectively, they are the sums of the corresponding partial edge-edge crossing costs, node-edge crossing costs and distance costs as follows:
											</Para>


											<Para>whereQva,vbeeMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaqhaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqaaiabdwgaLjabdwgaLbaaaaa@377C@(·) andQ^va,vb,vceeMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaDaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleaacqWGLbqzcqWGLbqzaaaaaa@3B68@(·) are related to edge-edge crossings, whileQva,vbneMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaqhaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqaaiabd6gaUjabdwgaLbaaaaa@378E@(·) andQ^va,vb,vcneMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaDaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleaacqWGUbGBcqWGLbqzaaaaaa@3B7A@(·) are related to node-edge crossings, andQva,vbdcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaqhaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqaaiabdsgaKjabdogaJbaaaaa@3776@(·) andQ^va,vb,vcdcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaDaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleaacqWGKbazcqWGJbWyaaaaaa@3B62@(·) are related to the distance cost. The details are described as below.</Para>


											<Para>(a)Qva,vbeeMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaqhaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqaaiabdwgaLjabdwgaLbaaaaa@377C@(·) is a partial edge-edge crossing cost function ofEvaMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaaaleqaaaaa@30E3@andEvbMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdkgaIbqabaaaleqaaaaa@30E5@, and is defined as follows:</Para>


											<Para>Similarly,Q^va,vb,vceeMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaDaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleaacqWGLbqzcqWGLbqzaaaaaa@3B68@(·) is a partial edge-edge crossing cost function ofEvaMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaaaleqaaaaa@30E3@,EvbMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdkgaIbqabaaaleqaaaaa@30E5@, andEvcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdogaJbqabaaaleqaaaaa@30E7@, and is defined as follows:</Para>


											<Para>(b)Qva,vbneMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaqhaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqaaiabd6gaUjabdwgaLbaaaaa@378E@is a partial node-edge crossing cost function of 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">b</Emphasis>

												</Subscript>
 ,EvaMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaaaleqaaaaa@30E3@, andEvbMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdkgaIbqabaaaleqaaaaa@30E5@, and is defined as follows:
											</Para>


											<Para>Similarly,Q^va,vb,vcneMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaDaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleaacqWGUbGBcqWGLbqzaaaaaa@3B7A@(·) is a partial node-edge crossing cost function of 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">b</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>

												</Subscript>
 ,EvaMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaaaleqaaaaa@30E3@,EvbMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdkgaIbqabaaaleqaaaaa@30E5@, andEvcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaiabdogaJbqabaaaleqaaaaa@30E7@, and is defined as follows:
											</Para>


											<Para>(c)Qva,vbdcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGrbqudaqhaaWcbaGaemODay3aaSbaaWqaaiabdggaHbqabaWccqGGSaalcqWG2bGDdaWgaaadbaGaemOyaigabeaaaSqaaiabdsgaKjabdogaJbaaaaa@3776@is a partial distance cost function of 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">b</Emphasis>

												</Subscript>
 , and is defined as follows:
											</Para>


											<Para>Similarly,Q^va,vb,vcdcMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacuWGrbqugaqcamaaDaaaleaacqWG2bGDdaWgaaadbaGaemyyaegabeaaliabcYcaSiabdAha2naaBaaameaacqWGIbGyaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiabdogaJbqabaaaleaacqWGKbazcqWGJbWyaaaaaa@3B62@(·) is a partial distance cost function of 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">b</Emphasis>

												</Subscript>
 , and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">c</Emphasis>

												</Subscript>
 , and is defined as follows:
											</Para>


											<Para>Thus far, we found out a method to efficiently calculate Δ matrix. The purpose of extending Δ matrix is to calculate the cost difference of the swap operation. When nodesvα1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8xSde2aaSbaaWqaaiabigdaXaqabaaaleqaaaaa@311B@andvα2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8xSde2aaSbaaWqaaiabikdaYaqabaaaleqaaaaa@311D@are swapped, we can calculateSwapvα1,vα2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWucqWG3bWDcqWGHbqycqWGWbaCdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHnaaBaaabaGaeGymaedabeaaaeqaaSGaeiilaWIaemODay3aaSbaaWqaaiab=f7aHnaaBaaabaGaeGOmaidabeaaaeqaaaWcbeaaaaa@3BD0@using these Δ costs as follows:</Para>


											<Para>where</Para>


											<Para>In SCCB-grid layout algorithm, the combo score also needs to be considered. Given a layout such that a node 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">α</Emphasis>

												</Subscript>
 is moved to a vacant point 

												<Emphasis Type="Italic">p</Emphasis>
 ,ΔvαpcsMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqqHuoardaqhaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHbqabaWccqWGWbaCaeaacqWGJbWycqWGZbWCaaaaaa@35B9@can be calculated as shown in Equation (3). In contrast, if two nodesvα1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8xSde2aaSbaaWqaaiabigdaXaqabaaaleqaaaaa@311B@andvα2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8xSde2aaSbaaWqaaiabikdaYaqabaaaleqaaaaa@311D@are swapped, the difference of combo scores, 

												<Emphasis Type="Italic">Combo</Emphasis>
 (Svα1↔vα2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGtbWudaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=f7aHnaaBaaabaGaeGymaedabeaaaeqaaSGaeyiLHSQaemODay3aaSbaaWqaaiab=f7aHnaaBaaabaGaeGOmaidabeaaaeqaaaWcbeaaaaa@38B1@ 

												<Emphasis Type="Italic">L</Emphasis>
 ) – 

												<Emphasis Type="Italic">Combo</Emphasis>
 ( 

												<Emphasis Type="Italic">L</Emphasis>
 ), is effectively calculated as follows:
											</Para>


											<Para>where</Para>


											<Para>A pseudo code of SCCB-grid layout algorithm is described in Figure 

												<InternalRef RefID="F5">5</InternalRef>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F5">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>SCCB-grid layout algorithm</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-5" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">SCCB-grid layout algorithm</Emphasis>
 . A pseudo code of SCCB-grid layout algorithm.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>If node 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 is moved at the previous step, the time complexity of calculating Δ matrix is 

												<Emphasis Type="Italic">O</Emphasis>
 ((| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)|EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@|| 

												<Emphasis Type="Italic">U</Emphasis>
 |). If twovβ1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabigdaXaqabaaaleqaaaaa@311D@andvβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWG2bGDdaWgaaWcbaacciGae8NSdi2aaSbaaWqaaiabikdaYaqabaaaleqaaaaa@311F@are swapped at the previous step, the time complexity of calculating Δ matrix was 

												<Emphasis Type="Italic">O</Emphasis>
 ((| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |) (|Evβ1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aInaaBaaabaGaeGymaedabeaaaeqaaaWcbeaaaaa@3251@| + |Evβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aInaaBaaabaGaeGOmaidabeaaaeqaaaWcbeaaaaa@3253@|) | 

												<Emphasis Type="Italic">U</Emphasis>
 |) = 

												<Emphasis Type="Italic">O</Emphasis>
 ((| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |) |Evβ′MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciqb=j7aIzaafaaabeaaaSqabaaaaa@314C@|| 

												<Emphasis Type="Italic">U</Emphasis>
 |), where |Evβ′MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciqb=j7aIzaafaaabeaaaSqabaaaaa@314C@| = (|Evβ1MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aInaaBaaabaGaeGymaedabeaaaeqaaaWcbeaaaaa@3251@| + |Evβ2MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aInaaBaaabaGaeGOmaidabeaaaeqaaaWcbeaaaaa@3253@|)/2. In addition, the time complexity of all the swap operations considered at each step is 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">E</Emphasis>
 | 

												<Superscript>2</Superscript>
 ). Therefore, the time complexity of SCCB-grid layout algorithm is 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">E</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">U</Emphasis>
 ||Evβ′MathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciqb=j7aIzaafaaabeaaaSqabaaaaa@314C@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)) at each step.
											</Para>


											<Para>Since the time complexity of CB-grid layout algorithm is 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">E</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">W</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)) at each step 

												<CitationRef
CitationID="B15">15</CitationRef>
 , the time complexity of SCCB-grid layout algorithm is 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)) larger than that of CB-grid layout algorithm (note that 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β</Emphasis>

												</Subscript>
 and 

												<Emphasis Type="Italic">v</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">β'</Emphasis>

												</Subscript>
 are not distinguished here). Here, we consider two cases, | 

												<Emphasis Type="Italic">V</Emphasis>
 | ≤ | 

												<Emphasis Type="Italic">W</Emphasis>
 | (case 1) and | 

												<Emphasis Type="Italic">V</Emphasis>
 | &gt; | 

												<Emphasis Type="Italic">W</Emphasis>
 | (case 2) and show these two algorithms have the same time complexity with high probability. For case 1, the above difference is negligible since 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)) ≤ 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">W</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@|(| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)). In contrast, the 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)) difference cannot be neglected in case 2. However, if we assume that all nodes can be moved to form the next layout with equal probability, | 

												<Emphasis Type="Italic">V</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| = 2 | 

												<Emphasis Type="Italic">E</Emphasis>
 |, and 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 ||EvβMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaacqWGfbqrdaWgaaWcbaGaemODay3aaSbaaWqaaGGaciab=j7aIbqabaaaleqaaaaa@3140@| (| 

												<Emphasis Type="Italic">V</Emphasis>
 | + | 

												<Emphasis Type="Italic">E</Emphasis>
 |)) = 

												<Emphasis Type="Italic">O</Emphasis>
 (| 

												<Emphasis Type="Italic">V</Emphasis>
 | 

												<Superscript>2</Superscript>
 + | 

												<Emphasis Type="Italic">E</Emphasis>
 | 

												<Superscript>2</Superscript>
 ) subsequently. Therefore, the time complexity of SCCB-grid layout algorithm will be the same as that of CB-grid layout algorithm even in the case 2. For the above reasons, the time complexities of SCCB-grid and CB-grid layout algorithms are the same in practice.
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_34437">

										<Heading>Results and Discussion</Heading>


										<Section2 ID="Sec_80089">

											<Heading>Data and Parameters</Heading>


											<Para>To evaluate our algorithms on a large-scale signal transduction pathway with a gene regulatory network, we create the pathway model of an endothelial cell with Cell Illustrator 

												<CitationRef
CitationID="B1">1</CitationRef>


												<CitationRef
CitationID="B2">2</CitationRef>
 by extracting information from 

												<CitationRef
CitationID="B19">19</CitationRef>
 . The model consists of 309 nodes and 371 edges (three times as large as the apoptosis model in 

												<CitationRef
CitationID="B15">15</CitationRef>
 , which consists of 117 nodes and 126 edges), and the maximum degree of a node is ten (eight in the apoptosis model). Grid widths and heights are fixed to 100 pixels; the total numbers of vertical and horizontal grid points are 36 and 40, respectively. We used the following information pertaining to seven GO subcellular localizations: extracellular space (GO:0005615), cytoplasm (GO:0005737), nucleus (GO:0005634), mitochondrion (GO:0005739), plasma membrane (GO:0005886), nuclear membrane (GO:0005635), and mitochondria membrane (GO:0005740). We also used the following information pertaining to sixteen processes and entities used as attributes of nodes: migration, phosphorylation, protein with a modification, ligand, assembly, transcription, translation, mRNA, ligand and receptor, receptor, unknown, protein, exchange, trimer, ubiquitination, and degradation.
											</Para>


											<Para>Usually, these types of biological models have many nodes termed as degradation. The degradation process always has only one edge. To exploit this property, we apply these layout algorithms after removing degradation nodes (97 nodes). After applying layout algorithms, we attach each eliminated degradation node just below the entity to which it was initially connected. Thus, in practice, the numbers of nodes and edges in the model given to layout algorithms are 212 and 274, respectively. Note that when the performances of algorithms are compared with the numbers of edge-edge crossings and node-edge crossings in the latter part of this section, crossings that are caused by degradations and edges connected to them are not taken into account.</Para>


											<Para>We apply the following rule to edge-edge crossing weight 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ee</Emphasis>

												</Subscript>
 , node-edge crossing weight 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ne</Emphasis>

												</Subscript>
 , combo score weight 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">cs</Emphasis>

												</Subscript>
 , and distance cost weight 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">dc</Emphasis>

												</Subscript>
 of a layout cost, in Equation (2), to ensure that the importance of the distance cost is less than those of the others:
											</Para>


											<Para>In our study, 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">dc</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ee</Emphasis>

												</Subscript>
 , 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">ne</Emphasis>

												</Subscript>
 , and 

												<Emphasis Type="Italic">W</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">cs</Emphasis>

												</Subscript>
 were set to 1, 70, 150, and 110, respectively. Also, the constant C in 

												<Emphasis Type="Italic">CW</Emphasis>


												<Subscript> 

													<Emphasis Type="Italic">a</Emphasis>

												</Subscript>
 was set to 12.
											</Para>


											<Para>Using the combo score, many nodes can be aligned vertically. However, in many cases, the nodes cannot be moved once they have combo relations. Plasma membrane, nuclear membrane, and mitochondrial membrane are thin and torus shaped, thus, vertical alignments of the nodes on these subcellular localizations will not be of interest for users (e.g., the width of plasma membrane in our model is only two grids). Therefore, in this paper, we decided to ignore combo scores in plasma membrane, nuclear membrane, and mitochondrial membrane.</Para>


										</Section2>


										<Section2 ID="Sec_45082">

											<Heading>Comparison of layouts</Heading>


											<Para>Figure 

												<InternalRef RefID="F6">6</InternalRef>
 shows the number of edge-edge crossings, the number of node-edge crossings, combo scores, and total costs of the layouts with CB-grid, CCB-grid, and SCCB-grid layout algorithms, and the human layout. We generate ten initial layouts by applying Eades initial layout algorithm to ten random layouts. These initial layouts are commonly used for each layout algorithm (CB Eades, CCB Eades, and SCCB Eades in Figure 

												<InternalRef RefID="F6">6</InternalRef>
 ). In addition, we use the ten random layouts directly as initial layouts of CB-grid layout algorithms (CB random in Figure 

												<InternalRef RefID="F6">6</InternalRef>
 , which corresponds to the previous layout algorithm) to confirm the significance of preparing proper initial layouts. Figure 

												<InternalRef RefID="F8">8</InternalRef>
 and 

												<InternalRef RefID="F9">9</InternalRef>
 respectively show the best layouts of CB-grid and SCCB-grid layout algorithms, which have the lowest total cost among ten resulting layouts of each algorithm. The human layout is shown in Figure 

												<InternalRef
RefID="F10">10</InternalRef>
 .
											</Para>


											<Figure Category="Standard" Float="No"
ID="F6">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparisons of edge-edge crossings, node-edge crossings, combo score, and total cost among the results of four grid layout algorithms and the human layout</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-6" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">Comparisons of edge-edge crossings, node-edge crossings, combo score, and total cost among the results of four grid layout algorithms and the human layout</Emphasis>
 . Costs and scores of the generated layouts with the CB random, CB Eades, CCB Eades, SCCB Eades, and human layout from the same initial layout. These algorithms are applied to ten initial layouts. (a) the number of edge-edge crossings. (b) the number of node-edge crossings. (c) combo score. (d) total cost.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Figure Category="Standard" Float="No"
ID="F8">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>A resulting layout of CB-grid layout algorithm</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-8" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">A resulting layout of CB-grid layout algorithm</Emphasis>
 . A resulting layout of CB-grid layout algorithm in an endothelial signal transduction pathway. The pathway model is the same as that in Figure 10.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Figure Category="Standard" Float="No"
ID="F9">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>A resulting layout of SCCB-grid layout algorithm</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-9" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">A resulting layout of SCCB-grid layout algorithm</Emphasis>
 . A resulting layout of SCCB-grid layout algorithm in an endothelial signal transduction pathway. The pathway model is the same as that in Figure 10.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Figure Category="Standard" Float="No"
ID="F10">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>The human layout</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-10" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">The human layout</Emphasis>
 . The human layout of an endothelial signal transduction pathway. This pathway model is arranged with CB-grid and SCCB-grid layout algorithms in Figure 8 and Figure 9, respectively.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>In 

												<CitationRef
CitationID="B15">15</CitationRef>
 , the initial layout for CB-grid layout algorithm was a random layout, which had a large number of edge-edge crossings and node-edge crossings. Many iterations will, therefore, be needed until convergence. This fact prompted us to use the output of Eades initial layout algorithm as an initial layout. Figure 

												<InternalRef RefID="F7">7</InternalRef>
 shows the number of iterations until convergence. As shown in this figure, CB-grid Eades successfully reduces the number of iterations when compared to CB-grid random (40% reduction on average). Moreover, the total score of CB-grid Eades is greatly improved over that of CB-grid random (see Figure 

												<InternalRef
RefID="F6">6(d)</InternalRef>
 ). A discussion in 

												<CitationRef
CitationID="B15">15</CitationRef>
 was suggesting that reducing edge-edge crossings and node-edge crossings will lead to a better approximation of the human layout. In contrast as shown in Figure 

												<InternalRef
RefID="F6">6(a)</InternalRef>
 and 

												<InternalRef
RefID="F6">6(b)</InternalRef>
 , the human layout also has several edge-edge and node-edge crossings, and has a higher combo score than that of CB-grid layout algorithm. Based on these facts, we proposed an additional scoring criterion – combo score – in CCB-grid layout algorithm. As seen through the value of combo scores (see Figure 

												<InternalRef
RefID="F6">6(c)</InternalRef>
 ), CCB-grid layout algorithm drastically improves this score, and this score becomes closer to that of the human layout. However, the numbers of edge-edge crossings and node-edge crossings in CCB-grid layout algorithm increase, comparing to CB-grid Eades (see Figure 

												<InternalRef
RefID="F6">6(a)</InternalRef>
 and 

												<InternalRef
RefID="F6">6(b)</InternalRef>
 ). In this paper, the swap operation is proposed to increase the number of candidate layouts at each step. As shown in Figure 

												<InternalRef
RefID="F6">6(a)</InternalRef>
 and 

												<InternalRef
RefID="F6">6(b)</InternalRef>
 , SCCB-grid layout algorithm succeeds in reducing edge-edge crossings and node-edge crossings, i.e., the above drawback of CCB-grid layout algorithm is partially diminished. In addition, as shown in Figure 

												<InternalRef
RefID="F6">6(c)</InternalRef>
 , the combo score of SCCB-grid layout algorithm is also improved slightly.
											</Para>


											<Figure Category="Standard" Float="No"
ID="F7">

												<Caption Language="En">

													<CaptionContent>

														<SimplePara>Comparisons of the total numbers of iterations for optimal layouts among four grid layout algorithms</SimplePara>


													</CaptionContent>


												</Caption>


												<MediaObject>

													<ImageObject Color="Color"
FileRef="1471-2105-8-76-7" Format="GIF" Rendition="Preview"
Type="Linedraw" />


													<TextObject>

														<Para> 

															<Emphasis
Type="Bold">Comparisons of the total numbers of iterations for optimal layouts among four grid layout algorithms</Emphasis>
 . Total number of iterations for optimal layouts with CB random, CB Eades, and SCCB Eades from the same initial layout. Ten initial layouts are applied with these algorithms.
														</Para>


													</TextObject>


												</MediaObject>


											</Figure>


											<Para>We also apply grid-layout algorithms to Fas-induced apoptosis pathway model 

												<CitationRef
CitationID="B20">20</CitationRef>
 and ASE cell fate simulation model 

												<CitationRef
CitationID="B21">21</CitationRef>
 to obtain a more generalized comparison. Resulting layouts and the number of crossings in each layout are summarized in Additional file1. These models including the endothelial cell model are also available as Additional file2, and the application of SCCB-grid layout algorithm for these models can be downloaded from 

												<CitationRef
CitationID="B22">22</CitationRef>
 .
											</Para>


										</Section2>


									</Section1>


									<Section1 ID="Sec_88947">

										<Heading>Conclusion</Heading>


										<Para>For better biopathway layouts, three improvements to CB-grid layout algorithm were proposed: (i) the improvement of initial layouts (ii) the improvement of cost function (iii) the improvement of search strategy itself without increasing the time complexity. For (i), Eades initial layout algorithm was proposed and the improvement was confirmed with a signal transduction pathway of an endothelial cell. For (ii), CCB-grid layout algorithm, which includes combo score function, was proposed and the improvement was verified with the same signal transduction pathway. For (iii), SCCB-grid layout algorithm was proposed. Due to (i) and (iii), our layout algorithm can be started from the better layout, and more robust to the condition of the initial layout than extant methods. In addition, we succeeded in utilizing the biological attributes that are not considered in extant methods due to combo score.</Para>


										<Para>However, our layout algorithm has limitations and problems, which should be addressed in future work. Firstly, if the parameters of the combo score are not correctly selected, once a node gets a combo relation, the node no longer moves to other grid points anymore. Thus, it is important to devise a method that automatically selects the suitable parameters for the combo score function, edge-edge crossing function, and node-edge crossing function. Secondly, in our algorithm, only undirected graphs are considered to be laid out. On the other hand, for metabolic pathways, 

											<CitationRef
CitationID="B11">11</CitationRef>


											<CitationRef
CitationID="B13">13</CitationRef>
 proposed layout algorithms that decompose a digraph to hierarchical structural parts and directed cycle parts by considering the direction of edges in order to capture the flow of reactions. Therefore, the grid layout algorithm will also need to handle digraphs, utilizing its property that is effective especially in the grid-based layout. Finally, it should be addressed that grid layout algorithms including our new approach requires high time complexity and are not suitable for the real-time drawing. Thus, we would like to devise a further optimized grid layout algorithm to enable the real-time drawing.
										</Para>


									</Section1>


									<Section1 ID="Sec_24727">

										<Heading>Authors' contributions</Heading>


										<Para>The basic idea was conceived by MK and MN. This idea was developed by KK and MN who then conceived a new idea and developed it. EJ created the endothelial model in Figure 

											<InternalRef RefID="F10">10</InternalRef>
 . SM supervised the whole study. The final manuscript was read and approved by all authors.
										</Para>


									</Section1>


								</Body>


								<BodyRef
FileRef="http://www.biomedcentral.com/1471-2105/8/76"
TargetType="Manuscript" />


								<ArticleBackmatter>

									<Bibliography ID="Bib1">

										<Heading>References</Heading>


										<Citation ID="CR1">

											<CitationNumber>1.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Nagasaki</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Doi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Matsuno</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Miyano</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Genomic Object Net: I. A platform for modelling and simulating biopathways</ArticleTitle>


												<JournalTitle>Applied Bioinformatics</JournalTitle>


												<VolumeID>2</VolumeID>


												<FirstPage>181</FirstPage>


												<LastPage>184</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR2">

											<CitationNumber>2.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Doi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Nagasaki</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Fujita</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Matsuno</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Miyano</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Genomic Object Net: II. Modelling biopathways by hybrid functional Petri net with extension</ArticleTitle>


												<JournalTitle>Applied Bioinformatics</JournalTitle>


												<VolumeID>2</VolumeID>


												<FirstPage>185</FirstPage>


												<LastPage>188</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR3">

											<CitationNumber>3.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Shannon</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Markiel</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>O</Initials>


													<FamilyName>Ozier</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>NS</Initials>


													<FamilyName>Baliga</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>JT</Initials>


													<FamilyName>Wang</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>D</Initials>


													<FamilyName>Ramage</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Amin</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Schwikowski</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Ideker</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Cytoscape: a software environment for integrated models of biomolecular interaction networks</ArticleTitle>


												<JournalTitle>Genome Research</JournalTitle>


												<VolumeID>13</VolumeID>


												<FirstPage>2498</FirstPage>


												<LastPage>2504</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1101/gr.1239303</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR4">

											<CitationNumber>4.</CitationNumber>


											<BibArticle>

												<Year />


												<ArticleTitle
Language="En">Networks/Pajek</ArticleTitle>


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR5">

											<CitationNumber>5.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Demir</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>O</Initials>


													<FamilyName>Babur</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Dogrusoz</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Gursoy</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>G</Initials>


													<FamilyName>Nisanci</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>RC</Initials>


													<FamilyName>Atalay</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Ozturk</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">PATIKA: an integrated visual environment for collaborative construction and analysis of cellular pathways</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>996</FirstPage>


												<LastPage>1003</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/18.7.996</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR6">

											<CitationNumber>6.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Dogrusoz</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>EZ</Initials>


													<FamilyName>Erson</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Giral</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Demir</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>O</Initials>


													<FamilyName>Babur</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Cetintas</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Colak</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">PATIKAweb: a Web interface for analyzing biological pathways through advanced querying and visualization</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>22</VolumeID>


												<FirstPage>374</FirstPage>


												<LastPage>375</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bti776</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR7">

											<CitationNumber>7.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Kurata</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Matoba</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>N</Initials>


													<FamilyName>Shimizu</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">CADLIVE for constructing a large-scale biochemical network based on a simulation-directed notation and its application to yeast cell cycle</ArticleTitle>


												<JournalTitle>Nucleic Acids Research</JournalTitle>


												<VolumeID>31</VolumeID>


												<FirstPage>4071</FirstPage>


												<LastPage>4084</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/nar/gkg461</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR8">

											<CitationNumber>8.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Kurata</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Masaki</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Sumida</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>R</Initials>


													<FamilyName>Iwasaki</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">CADLIVE dynamic simulator: direct link of biochemical networks to dynamic models</ArticleTitle>


												<JournalTitle>Genome Research</JournalTitle>


												<VolumeID>15</VolumeID>


												<FirstPage>590</FirstPage>


												<LastPage>600</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1101/gr.3463705</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR9">

											<CitationNumber>9.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Brandes</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Dwyer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>F</Initials>


													<FamilyName>Schreiber</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Visualizing related metabolic pathways in two and a half dimensions</ArticleTitle>


												<JournalTitle>Proceedings of the 11th International Symposium on Graph Drawing</JournalTitle>


												<VolumeID />


												<FirstPage>111</FirstPage>


												<LastPage>122</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR10">

											<CitationNumber>10.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>PD</Initials>


													<FamilyName>Karp</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>SM</Initials>


													<FamilyName>Paley</FamilyName>


												</BibAuthorName>


												<Year>1994</Year>


												<ArticleTitle
Language="En">Automated drawing of metabolic pathways</ArticleTitle>


												<JournalTitle>Proceedings of the 3rd International Conference on Bioinformatics and Genome Research</JournalTitle>


												<VolumeID />


												<FirstPage>225</FirstPage>


												<LastPage>238</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR11">

											<CitationNumber>11.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>MY</Initials>


													<FamilyName>Becker</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>I</Initials>


													<FamilyName>Rojas</FamilyName>


												</BibAuthorName>


												<Year>2001</Year>


												<ArticleTitle
Language="En">A graph layout algorithm for drawing metabolic pathways</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>17</VolumeID>


												<FirstPage>461</FirstPage>


												<LastPage>467</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/17.5.461</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR12">

											<CitationNumber>12.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Sirava</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>T</Initials>


													<FamilyName>Schafer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Eiglsperger</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Kaufmann</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>O</Initials>


													<FamilyName>Kohlgacher</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Bornberg-Bauer</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>HP</Initials>


													<FamilyName>Lenhof</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">BioMiner-modeling, analyzing, and visualizing biochemical pathways and networks</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>18</VolumeID>


												<FirstPage>S219</FirstPage>


												<LastPage>230</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR13">

											<CitationNumber>13.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Wegner</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Kummer</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">A new dynamical layout algorithm for complex biochemical reaction networks</ArticleTitle>


												<JournalTitle>BMC Bioinformatics</JournalTitle>


												<VolumeID>6</VolumeID>


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


										<Citation ID="CR14">

											<CitationNumber>14.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>W</Initials>


													<FamilyName>Li</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Kurata</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">A grid layout algorithm for automatic drawing of biochemical networks</ArticleTitle>


												<JournalTitle>Bioinformatics</JournalTitle>


												<VolumeID>21</VolumeID>


												<FirstPage>2036</FirstPage>


												<LastPage>2042</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1093/bioinformatics/bti290</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR15">

											<CitationNumber>15.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Kato</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Nagasaki</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Doi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Miyano</FamilyName>


												</BibAuthorName>


												<Year>2005</Year>


												<ArticleTitle
Language="En">Automatic drawing of biological networks using cross cost and subcomponent data</ArticleTitle>


												<JournalTitle>Genome Informatics</JournalTitle>


												<VolumeID>16</VolumeID>


												<FirstPage>22</FirstPage>


												<LastPage>31</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR16">

											<CitationNumber>16.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>B</Initials>


													<FamilyName>Genc</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Dogrusoz</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">A constrained, force-directed layout algorithm for biological pathways</ArticleTitle>


												<JournalTitle>Proceedings of the 11th International Symposium on Graph Drawing</JournalTitle>


												<VolumeID />


												<FirstPage>314</FirstPage>


												<LastPage>319</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR17">

											<CitationNumber>17.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>U</Initials>


													<FamilyName>Dogrusoz</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Gral</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Cetintas</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Civril</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>E</Initials>


													<FamilyName>Demir</FamilyName>


												</BibAuthorName>


												<Year>2004</Year>


												<ArticleTitle
Language="En">A compound graph layout algorithm for biological pathways</ArticleTitle>


												<JournalTitle>Proceedings of the 12th International Symposium on Graph Drawing</JournalTitle>


												<VolumeID />


												<FirstPage>442</FirstPage>


												<LastPage>447</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR18">

											<CitationNumber>18.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>P</Initials>


													<FamilyName>Eades</FamilyName>


												</BibAuthorName>


												<Year>1984</Year>


												<ArticleTitle
Language="En">A heuristic for graph drawing</ArticleTitle>


												<JournalTitle>Congressus Nemerantium</JournalTitle>


												<VolumeID>42</VolumeID>


												<FirstPage>149</FirstPage>


												<LastPage>160</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR19">

											<CitationNumber>19.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>JS</Initials>


													<FamilyName>Pober</FamilyName>


												</BibAuthorName>


												<Year>2002</Year>


												<ArticleTitle
Language="En">Endothelial activation: Intracellular signaling pathways</ArticleTitle>


												<JournalTitle>Arthritis Research</JournalTitle>


												<VolumeID>4</VolumeID>


												<FirstPage>S109</FirstPage>


												<LastPage>116</LastPage>


												<Occurrence Type="DOI">

													<Handle>10.1186/ar576</Handle>


												</Occurrence>


											</BibArticle>


										</Citation>


										<Citation ID="CR20">

											<CitationNumber>20.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Matsuno</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>Y</Initials>


													<FamilyName>Tanaka</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>H</Initials>


													<FamilyName>Aoshima</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Doi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Matsui</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Miyano</FamilyName>


												</BibAuthorName>


												<Year>2003</Year>


												<ArticleTitle
Language="En">Biopathways representation and simulation on hybrid functional Petri net</ArticleTitle>


												<JournalTitle>In Silico Biology</JournalTitle>


												<VolumeID>3</VolumeID>


												<FirstPage>389</FirstPage>


												<LastPage>404</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR21">

											<CitationNumber>21.</CitationNumber>


											<BibArticle>

												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Saito</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>M</Initials>


													<FamilyName>Nagasaki</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>A</Initials>


													<FamilyName>Doi</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>K</Initials>


													<FamilyName>Ueno</FamilyName>


												</BibAuthorName>


												<BibAuthorName>

													<Initials>S</Initials>


													<FamilyName>Miyano</FamilyName>


												</BibAuthorName>


												<Year>2006</Year>


												<ArticleTitle
Language="En">Cell fate simulation model of gustatory nuerons with microRNAs double-negative feedback loop by hybrid functional Petri net with extension</ArticleTitle>


												<JournalTitle>Genome Informatics</JournalTitle>


												<VolumeID>17</VolumeID>


												<FirstPage>100</FirstPage>


												<LastPage>111</LastPage>


											</BibArticle>


										</Citation>


										<Citation ID="CR22">

											<CitationNumber>22.</CitationNumber>


											<BibArticle>

												<Year />


												<NoArticleTitle />


												<JournalTitle />


												<VolumeID />


												<FirstPage />


												<LastPage />


											</BibArticle>


										</Citation>


									</Bibliography>


								</ArticleBackmatter>


							</Article>


						</Issue>


					</Volume>


				</Journal>

				<meta:Info  xmlns:meta="http://www.springer.com/app/meta">

					<meta:Authors>

						<meta:Author>Kojima, Kaname</meta:Author>

						<meta:Author>Nagasaki, Masao</meta:Author>

						<meta:Author>Jeong, Euna</meta:Author>

						<meta:Author>Kato, Mitsuru</meta:Author>

						<meta:Author>Miyano, Satoru</meta:Author>

					</meta:Authors>

					<meta:Institutions>

						<meta:Institution geo="">

							<meta:OrgName>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</meta:OrgName>

							<meta:GeoOrg />

							<meta:Country>Japan</meta:Country>

						</meta:Institution>

					</meta:Institutions>

					<meta:Date>2007-03-06</meta:Date>

					<meta:Type>Article</meta:Type>

					<meta:DOI>10.1186/1471-2105-8-76</meta:DOI>

					<meta:Title>An efficient grid layout algorithm for biological networks utilizing various biological attributes</meta:Title>

					<meta:ISXN>1471-2105</meta:ISXN>

					<meta:Journal>BMC Bioinformatics</meta:Journal>

					<meta:PubName>BioMed Central</meta:PubName>

					<meta:ArticleFirstPage>76</meta:ArticleFirstPage>

					<meta:Publication>BMC Bioinformatics</meta:Publication>

					<meta:PublicationType>Journal</meta:PublicationType>

					<meta:SubjectGroup>

						<meta:Subject Type="Primary">Life Sciences</meta:Subject>

						<meta:Subject Priority="1"
Type="Secondary">Bioinformatics</meta:Subject>

						<meta:Subject Priority="2"
Type="Secondary">Microarrays</meta:Subject>

						<meta:Subject Priority="3"
Type="Secondary">Computational Biology/Bioinformatics</meta:Subject>

						<meta:Subject Priority="4"
Type="Secondary">Computer Appl. in Life Sciences</meta:Subject>

						<meta:Subject Priority="5"
Type="Secondary">Combinatorial Libraries</meta:Subject>

						<meta:Subject Priority="6"
Type="Secondary">Algorithms</meta:Subject>

					</meta:SubjectGroup>

				</meta:Info>

			</Publisher>

			<Images>

				<Image Id="5-10.1186_1471-2105-8-76-1" xml:lang="en"
language="en">

					<Caption>

						<p>Pseudo codes of combo score functions: combo1 and combo2</p>


					</Caption>

					<FullText>

						<p>
To implement this type of property, we introduce the concept of
combo scores called 

							<strong>combo1</strong>
 and

							<strong>combo2</strong>
 (see Figure 2 ).

						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-2.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-3" xml:lang="en"
language="en">

					<Caption>

						<p>An optimal layout of CB-grid and improved layout with the swap operation</p>


					</Caption>

					<FullText>

						<p>
For example, the layout shown in Figure 4(a) is optimal for CB-grid
layout algorithm despite the fact the layout in Figure 4(b) should
be selected as the better layout.
						</p>

						<p>
With this improvement, the layout in Figure 4(a) will be arranged
as shown in Figure 4(b) .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-4.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-4" xml:lang="en"
language="en">

					<Caption>

						<p>SCCB-grid layout algorithm</p>


					</Caption>

					<FullText>

						<p>

							<p>A pseudo code of SCCB-grid layout algorithm is described in
Figure 5 .</p>

.

						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-5.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-6" xml:lang="en"
language="en">

					<Caption>

						<p>A resulting layout of CB-grid layout algorithm</p>


					</Caption>

					<FullText>

						<p>
Figure 8 and 9 respectively show the best layouts of CB-grid and
SCCB-grid layout algorithms, which have the lowest total cost among
ten resulting layouts of each algorithm.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-8.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-7" xml:lang="en"
language="en">

					<Caption>

						<p>A resulting layout of SCCB-grid layout algorithm</p>


					</Caption>

					<FullText>

						<p>
Figure 8 and 9 respectively show the best layouts of CB-grid and
SCCB-grid layout algorithms, which have the lowest total cost among
ten resulting layouts of each algorithm.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-9.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-8" xml:lang="en"
language="en">

					<Caption>

						<p>The human layout</p>


					</Caption>

					<FullText>

						<p>
The human layout is shown in Figure 10 ..
						</p>

						<p>
EJ created the endothelial model in Figure 10 .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-10.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-9" xml:lang="en"
language="en">

					<Caption>

						<p>Comparisons of the total numbers of iterations for optimal layouts among four grid layout algorithms</p>


					</Caption>

					<FullText>

						<p>
Figure 7 shows the number of iterations until convergence.
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-7.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-0" xml:lang="en"
language="en">

					<Caption>

						<p>Two layouts with the same canvas and three subcellular localizations</p>


					</Caption>

					<FullText>

						<p>
For example, suppose a biological pathway with a subcellular
localization, membrane, which slimly surrounds other subcellular
localizations as shown in Figure 1(a) , the graph in (a) could be a
layout resulting from Eades algorithm.
						</p>

						<p>
In order to avoid this problem, we propose to gather nodes around a
particular grid point for each subcellular localization as shown in
Figure 1(b) .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-1.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-2" xml:lang="en"
language="en">

					<Caption>

						<p>An example that compares the features of combo1 and combo2 score functions</p>


					</Caption>

					<FullText>

						<p>
We have considered two types of combo scores, i.e.,

							<strong>combo1</strong>
 and 

							<strong>combo2</strong>
 for layouts in
Figure 3(a) and 3(b) , respectively.

						</p>

						<p>
Let nodes v 

							<sub>a</sub>
 to v 

							<sub>f</sub>
 in Figure 3 have the
same attribute.

						</p>

						<p>
For the layout in Figure 3(a) , the number of combo relations with

							<strong>combo1</strong>
 and 

							<strong>combo2</strong>
 are 8 and 12,
respectively.

						</p>

						<p>
If node v 

							<sub>f</sub>
 is moved as shown in Figure 3(b) , the
number of combo relations with 

							<strong>combo1</strong>
 is the same
as before, whereas that with 

							<strong>combo2</strong>
 is 14.

						</p>

						<p>
Thus, only by using 

							<strong>combo2</strong>
 , we can improve the
combo score when node v 

							<sub>f</sub>
 is moved as shown in Figure
3(a) and 3(b) .

						</p>

						<p>
As shown in the dotted rectangle in Figure 3(a) , a pair of
vertically aligned nodes often occurs during the process of
updating a layout.
						</p>

						<p>
In this case, Figure 3(b) should be a better layout than Figure
3(a) .
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-3.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

				<Image Id="5-10.1186_1471-2105-8-76-5" xml:lang="en"
language="en">

					<Caption>

						<p>Comparisons of edge-edge crossings, node-edge crossings, combo score, and total cost among the results of four grid layout algorithms and the human layout</p>


					</Caption>

					<FullText>

						<p>

							<p>Figure 6 shows the number of edge-edge crossings, the number of
node-edge crossings, combo scores, and total costs of the layouts
with CB-grid, CCB-grid, and SCCB-grid layout algorithms, and the
human layout.</p>


						</p>

						<p>
These initial layouts are commonly used for each layout algorithm
(CB Eades, CCB Eades, and SCCB Eades in Figure 6 ).
						</p>

						<p>
In addition, we use the ten random layouts directly as initial
layouts of CB-grid layout algorithms (CB random in Figure 6 , which
corresponds to the previous layout algorithm) to confirm the
significance of preparing proper initial layouts.
						</p>

						<p>
Moreover, the total score of CB-grid Eades is greatly improved over
that of CB-grid random (see Figure 6(d) ).
						</p>

						<p>
In contrast as shown in Figure 6(a) and 6(b) , the human layout
also has several edge-edge and node-edge crossings, and has a
higher combo score than that of CB-grid layout algorithm.
						</p>

						<p>
As seen through the value of combo scores (see Figure 6(c) ),
CCB-grid layout algorithm drastically improves this score, and this
score becomes closer to that of the human layout.
						</p>

						<p>
However, the numbers of edge-edge crossings and node-edge crossings
in CCB-grid layout algorithm increase, comparing to CB-grid Eades
(see Figure 6(a) and 6(b) ).
						</p>

						<p>
As shown in Figure 6(a) and 6(b) , SCCB-grid layout algorithm
succeeds in reducing edge-edge crossings and node-edge crossings,
i.e., the above drawback of CCB-grid layout algorithm is partially
diminished.
						</p>

						<p>
In addition, as shown in Figure 6(c) , the combo score of SCCB-grid
layout algorithm is also improved slightly..
						</p>

					</FullText>

					<File>

						<Color>true</Color>

						<Format>JPG</Format>

						<Path>/Images/BMC/MEDIUM_1471-2105-8-76-6.jpg</Path>

						<Type>Linedraw</Type>

					</File>

					<Authors>

						<Author>Kojima, Kaname</Author>

						<Author>Nagasaki, Masao</Author>

						<Author>Jeong, Euna</Author>

						<Author>Kato, Mitsuru</Author>

						<Author>Miyano, Satoru</Author>

					</Authors>

					<Institutions>

						<Institution>Human Genome Center, Institute of Medical Science, University of Tokyo, 4-6-1 Shirokanedai, Minato-ku, Tokyo 108-8639, Japan</Institution>

					</Institutions>

					<ArticleTitle>An efficient grid layout algorithm for biological networks utilizing various biological attributes</ArticleTitle>

					<DOI>10.1186/1471-2105-8-76</DOI>

					<PubDate>2007-03-06</PubDate>

					<SourceType>Article</SourceType>

					<SourceTitle>BMC Bioinformatics</SourceTitle>

					<JournalId>1471-2105</JournalId>

					<VolumeId>8</VolumeId>

					<IssueId>1</IssueId>

					<ISXN ISSN="1471-2105" ISBN="" EISBN="">1471-2105</ISXN>

					<SubjectCollection>Life Sciences</SubjectCollection>

					<Subjects>

						<Subject Type="Primary">Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="1">Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="2">Microarrays</Subject>

						<Subject Type="Secondary"
Priority="3">Computational Biology/Bioinformatics</Subject>

						<Subject Type="Secondary"
Priority="4">Computer Appl. in Life Sciences</Subject>

						<Subject Type="Secondary"
Priority="5">Combinatorial Libraries</Subject>

						<Subject Type="Secondary"
Priority="6">Algorithms</Subject>

					</Subjects>

					<OpenAccess>true</OpenAccess>

					<CopyrightHolder>Kojima et al; licensee BioMed Central Ltd.</CopyrightHolder>

					<Keywords>

						<Keyword>networks</Keyword>

						<Keyword>biological</Keyword>

						<Keyword>layout</Keyword>

						<Keyword>grid</Keyword>

						<Keyword>various</Keyword>

						<Keyword>attributes</Keyword>

						<Keyword>utilizing</Keyword>

						<Keyword>efficient</Keyword>

						<Keyword>algorithm</Keyword>

					</Keywords>

					<ImageType>Line</ImageType>

					<ArticleURI>/Images/BMC/10.1186@1471-2105-8-76.xml</ArticleURI>

					<Provider>BioMed Central</Provider>

					<DateLoaded>2009-11-26T18:11:40.887Z</DateLoaded>

				</Image>

			</Images>

		</result>

	</records>

	<facets>

		<facet name="subject">

			<facet-value count="9647">Life Sciences</facet-value>

			<facet-value
count="7178">Computational Biology/Bioinformatics</facet-value>

			<facet-value count="7082">Algorithms</facet-value>

			<facet-value count="6623">Microarrays</facet-value>

			<facet-value count="6549">Combinatorial Libraries</facet-value>

			<facet-value count="4235">Bioinformatics</facet-value>

			<facet-value
count="3808">Computer Appl. in Life Sciences</facet-value>

			<facet-value count="3614">Life Sciences, general</facet-value>

			<facet-value
count="3292">Animal Genetics and Genomics</facet-value>

			<facet-value
count="3259">Plant Genetics &amp; Genomics</facet-value>

			<facet-value
count="3213">Medicine &amp; Public Health</facet-value>

			<facet-value
count="3191">Microbial Genetics and Genomics</facet-value>

			<facet-value count="2901">Proteomics</facet-value>

			<facet-value count="1786">Biomedicine</facet-value>

			<facet-value
count="1026">Genetics and Population Dynamics</facet-value>

			<facet-value count="770">Evolutionary Biology</facet-value>

			<facet-value
count="675">Public Health/Gesundheitswesen</facet-value>

			<facet-value count="673">Biochemistry, general</facet-value>

			<facet-value count="638">Entomology</facet-value>

			<facet-value count="612">Imaging / Radiology</facet-value>

		</facet>

		<facet name="keyword">

			<facet-value count="1624">analysis</facet-value>

			<facet-value count="1334">gene</facet-value>

			<facet-value count="1057">expression</facet-value>

			<facet-value count="969">data</facet-value>

			<facet-value count="956">from</facet-value>

			<facet-value count="756">protein</facet-value>

			<facet-value count="734">genes</facet-value>

			<facet-value count="607">study</facet-value>

			<facet-value count="542">human</facet-value>

			<facet-value count="501">microarray</facet-value>

			<facet-value count="470">genome</facet-value>

			<facet-value count="450">sequence</facet-value>

			<facet-value count="434">cancer</facet-value>

			<facet-value count="406">based</facet-value>

			<facet-value count="384">patients</facet-value>

			<facet-value count="362">model</facet-value>

			<facet-value count="356">DNA</facet-value>

			<facet-value count="351">approach</facet-value>

			<facet-value count="339">prediction</facet-value>

			<facet-value count="322">proteins</facet-value>

		</facet>

		<facet name="pub">

			<facet-value count="3703">BMC Bioinformatics</facet-value>

			<facet-value count="2846">BMC Genomics</facet-value>

			<facet-value count="608">BMC Evolutionary Biology</facet-value>

			<facet-value count="378">BMC Systems Biology</facet-value>

			<facet-value count="255">BMC Genetics</facet-value>

			<facet-value count="215">BMC Microbiology</facet-value>

			<facet-value count="209">BMC Neuroscience</facet-value>

			<facet-value count="167">BMC Structural Biology</facet-value>

			<facet-value count="160">BMC Cancer</facet-value>

			<facet-value
count="149">BioMedical Engineering OnLine</facet-value>

			<facet-value count="137">BMC Public Health</facet-value>

			<facet-value count="134">Critical Care</facet-value>

			<facet-value count="132">BMC Plant Biology</facet-value>

			<facet-value count="127">BMC Health Services Research</facet-value>

			<facet-value
count="122">BMC Medical Informatics and Decision Making</facet-value>

			<facet-value count="114">Malaria Journal</facet-value>

			<facet-value count="102">BMC Molecular Biology</facet-value>

			<facet-value
count="101">Journal of Cardiovascular Magnetic Resonance</facet-value>

			<facet-value
count="100">International Journal of Health Geographics</facet-value>

			<facet-value count="100">Virology Journal</facet-value>

		</facet>

		<facet name="year">

			<facet-value count="724">2011</facet-value>

			<facet-value count="2073">2010</facet-value>

			<facet-value count="4074">2009</facet-value>

			<facet-value count="3527">2008</facet-value>

			<facet-value count="2596">2007</facet-value>

			<facet-value count="1841">2006</facet-value>

			<facet-value count="1184">2005</facet-value>

			<facet-value count="585">2004</facet-value>

			<facet-value count="293">2003</facet-value>

			<facet-value count="166">2002</facet-value>

			<facet-value count="66">2001</facet-value>

			<facet-value count="9">2000</facet-value>

			<facet-value count="2">1999</facet-value>

			<facet-value count="2">1997</facet-value>

			<facet-value count="1">1996</facet-value>

			<facet-value count="1">1989</facet-value>

			<facet-value count="1">1986</facet-value>

		</facet>

		<facet name="type">

			<facet-value count="17140">Journal</facet-value>

			<facet-value count="5">Book</facet-value>

		</facet>

		<facet name="country">

			<facet-value count="6082">United States</facet-value>

			<facet-value count="2090">Germany</facet-value>

			<facet-value count="2057">United Kingdom</facet-value>

			<facet-value count="1177">France</facet-value>

			<facet-value count="1128" />

			<facet-value count="1072">Canada</facet-value>

			<facet-value count="987">Netherlands</facet-value>

			<facet-value count="819">Italy</facet-value>

			<facet-value count="618">Australia</facet-value>

			<facet-value count="616">China</facet-value>

			<facet-value count="595">Japan</facet-value>

			<facet-value count="558">Spain</facet-value>

			<facet-value count="539">Switzerland</facet-value>

			<facet-value count="530">Sweden</facet-value>

			<facet-value count="388">Belgium</facet-value>

			<facet-value count="350">The Netherlands</facet-value>

			<facet-value count="339">Norway</facet-value>

			<facet-value count="338">India</facet-value>

			<facet-value count="332">Denmark</facet-value>

			<facet-value count="317">Austria</facet-value>

		</facet>

	</facets>

</response>
